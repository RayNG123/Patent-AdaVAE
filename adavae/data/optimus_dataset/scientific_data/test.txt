The literature on technology management has increasingly focused on the sociocognitive elements of the industry life cycle. One of these elements, category labels (words, in most cases) and its role in shaping market understandings, has recently become of interest to scholars. As industries evolve, stakeholders generate a plethora of category labels. However, we know relatively little about why some category labels are used repeatedly, whereas others are abandoned. Drawing on semantic networks theory, we argue that the familiarity and creativity of category labels drive their adoption. We hypothesize that low levels of familiarity hinder comprehension, but too much familiarity increases the cost of obviousness. Likewise, low levels of creativity do not trigger curiosity, whereas too much creativity spurs dissonance. We use two methods to address these hypotheses. First, we study the early smartphone industry, finding support for an inverted U-shaped relationship between both the familiarity and creativity of category labels and their adoption, even after controlling for alternative explanations, such as technology and design characteristics. Second, we find consistent results through two online experiments that broaden the scope of our study and address potential endogeneity concerns in our field data. Our paper expands the literature on the evolution of technology industries by showing that familiarity and creativity are distinct dimensions that influence the sociocognitive dynamics of an emerging industry. We also contribute to the categorization literature by theorizing about the contestation that occurs among category labels and providing empirical evidence of the factors that affect their adoption.
New information pertinent to organizational decision making, even when publicly available, may not diffuse rapidly in the form of adoption and transformation of organizational practices. In this study, we examine how different markers of expertise, each representative of human capital at both individual and organizational levels, moderates the speed of response to new information. We do so in the context of medical device utilization, viz. stents, for the treatment of stable coronary arterial disease by physicians practicing in hospitals. Results show that physicians possessing specialized expertise developed through deliberate practice adopt new guidelines significantly faster, as compared with physicians endowed with general expertise reflected in elite schooling or tenure. Furthermore, we observe significant spillovers within organizations from expertise gained through deliberate practice, indicating that physicians with expertise markers associated with deliberate practice are able to act as influential agents and help diffuse new practices within the organization. Our study thus extends the literature on both information diffusion and expertise by providing quantitative and qualitative evidence of the mechanisms at play in the adoption of new best practices.The online appendix is available at https://doi.org/10.1287/orsc.2018.1246.
We present a theory of how a rational, profit-maximizing firm would respond to pressure for gender pay equity by strategically distributing raises to reduce the pay gap between its female and male employees at minimum cost. Using formal analysis and pay data from a real employer, we show that (1) employees in low-paying jobs and whose pay-related observables are similar to those of men at the firm are most likely to get raises; (2) counterintuitively, some men may get raises, and giving raises to certain women would increase the pay gap; and (3) a firm can reduce the gender pay gap as measured by a much larger percentage than the overall increase in pay to women at the firm. We also identify the conditions under which a firm could “explain away” a gender pay gap using other pay-related observables, such as job category, as well as the conditions under which this strategy would backfire. Our paper helps explain some empirical puzzles, such as the tendency for some men to get raises after gender equity pay reviews, and yields a rich set of implications for empirical research and practice.The online appendix is available at https://doi.org/10.1287/orsc.2018.1248.
We develop a theory explaining how collectivism causes people to “blur” demographic differences, that is, to see less diversity than actually exists in a group, and reconciling contradictions in how collectivistic norms influence group performance. We draw on the perceived diversity literature, hypothesizing that collectivistic norms cause group members to blur demographic differences, resulting in perceptions that group members are more similar than they actually are. Whether this benefits or harms group performance depends on the group’s objective diversity and the relevance of the perceived diversity attribute for accomplishing the task. For conjunctive tasks, the group’s performance is determined by its weakest member, demanding high levels of cohesion. Our theory suggests that collectivism benefits group conjunctive performance when objective national diversity is high by blurring divisive relational differences but has no effect in groups with low objective national diversity. In contrast, for disjunctive tasks, the group’s performance is determined by its best member. We predict that collectivism harms group disjunctive performance when objective expertness diversity is high by blurring differences in task-relevant expertness but has no effect in low objective expertness diversity groups. We find support for our theory in two studies, an archival study of 5,214 Himalayan climbing expeditions and a laboratory experiment assessing 366 groups. Our results show that collectivism has benefits and detriments for diverse groups and that these contradictory effects can be understood by identifying how the collectivistic blurring of perceived group diversity helps or hurts groups based on the type of tasks on which they are working.
This paper introduces the concept of tie vitality, which indicates the durability and accessibility of team member connections after a team has disbanded as an additional measure of team effectiveness. The authors integrate the team and social network literatures to investigate the effects of team relational capital, team advice density, and dyadic similarities on tie vitality. Two field studies of graduate business student teams show that team relational capital—a psychological team-level state reflecting trust, identification, and mutual obligations among teammates—positively relates to tie vitality. Furthermore, team-level advice network density—a structured behavioral pattern of advice seeking and receiving—amplifies the positive relationship between relational capital and tie vitality. Results also indicate that dyad similarity relates to tie vitality, although it varies depending on which demographic characteristics are considered. Overall, findings indicate that the connections made in teams remain active after teams disband, with the extent of vitality depending on qualities developed during the team experience.
Using a multilevel theoretical framework, we investigate the effects of organizational and perceived learning on employees’ systematic problem solving (SPS) that aims to prevent the recurrence of a problem. At the organizational level, we focus on the deliberate learning mechanisms of knowledge articulation (OKA) and knowledge codification (OKC). At the individual level, we focus on the relative perception of the mechanisms of knowledge articulation (PKA) and knowledge codification (PKC). Drawing on both knowledge management and sensemaking literature, we move from learning only captured through organizational mechanisms, which suppose individuals are passively embedded in the organizational context, to learning captured through perceived mechanisms, which suppose individuals take an active part in the learning processes and interpret them differently. We employ multilevel structural equation modeling to test our theoretical framework using survey data from a sample of 383 shop floor employees in 52 plants. To enhance our results, we perform a set of robustness checks that control different specifications of our model and potential endogeneity issues. Our study indicates that OKC affects SPS, while OKA affects OKC. Moreover, results show that both PKA and PKC have strong positive effects on SPS. Our study draws attention to the multilevel role of organizational learning and expands the understanding of the role of problem solving in routine evolution.The online appendices are available at https://doi.org/10.1287/orsc.2018.1274.
Previous work on institutional complexity has discussed two solutions that organizations internally deploy when externally engaging with multiple institutional logics: blended hybrids, in which logics are combined throughout the organization, and structural hybrids, in which different logics dominate in different compartments within the organization. While blended hybrids have been extensively investigated, few studies have examined how structural hybrids are constructed and maintained. We address this imbalance by studying university–industry research centers as instances of distinct organizational spaces used to engage with a minority logic. We found that these spaces require three kinds of work: (a) leveraging, where dominant logic practices are drawn on to achieve minority logic objectives; (b) hybridizing, where the practices inside the space are modified to allow engagement with the minority logic; and (c) bolstering, where the space is shielded against excessive minority logic influence and anchored back into the organization. Furthermore, contrary to the existing literature, we found that the spaces were hybrid, rather than being dominated by a single logic. Our finding is likely generalizable across many instances of structural hybrids given the integration problems that organizations with pure single logic spaces would face, combined with the usefulness of hybrid spaces. Our study is novel in revealing the work needed to sustain hybrid spaces and questioning the previously held conceptualization of structural hybrids as made up of single-logic compartments.The online appendix is available at https://doi.org/10.1287/orsc.2018.1228.
We develop and test a model that extends the understanding of how people react to news of organizational unethical behavior and how such reactions impact stock performance. We do so by taking into account the interplay between the features of specific unethical acts and the features of the organizational context within which unethical acts occur. We propose a two-stage model in which the first stage predicts that unethical acts that benefit the organization are judged less harshly than are unethical acts that benefit the actor, when the organization is seen as pursuing a moral goal (e.g., producing inexpensive medicine rather than tobacco products). In such cases, the motives behind the unethical act are construed as an individual’s intentions to pursue a moral end. The second stage of our model connects moral judgment to action against the organization as a whole. We propose that moral judgments of an unethical act are more likely to translate into negative economic consequences for the organization when the unethical act is seen as benefiting the organization, because in such cases the organization is construed as an accomplice. Study 1 is an event study of stock market reactions to organizational unethical behavior in which the features of organizational unethical behavior were operationalized by coding media coverage of unethical acts. Study 2 is an experiment that used news stories to manipulate features of unethical behavior and measured participants’ estimates of stock performance, while incentivizing participants for accuracy. Both studies found support for our model.The online appendix is available at https://doi.org/10.1287/orsc.2018.1244.
Past research has shown that founders bring important capabilities and resources from their prior employment into their new firms and that these intergenerational transfers influence the performance of these ventures. However, we know little about whether organizational practices also transfer from parents to spawns, and if so, what types of practices are transferred? Using a combination of survey and registrar data and through a detailed identification strategy, we examine these two previously unaddressed questions. Our results provide strong evidence for organizational heritage in practices. About 70% of the comparisons of start-ups and other established organizations are less similar than the average similarity between a parent organization and its spawn and that the overlap in organizational practices is almost 10% greater between a spawn and its parents than between the spawn and other established firms. Our further investigation shows that not all practices seem to find their way into the new entrepreneurial firms. In particular, practices that are valuable for and fit with the requirements of a start-up organization, and at the same time are more clearly defined and casually less ambiguous, are more likely to be transferred by the founders from their previous employers. These results contribute to our understanding of how entrepreneurs assemble their organizations and practice innovation as well as the diffusion of practices and the origins of firm heterogeneity.
The strategic management of knowledge spillovers to employee ventures may provide firms with knowledge spill-ins or the potential to learn from the ventures that they have spun out. However, the mechanisms underlying knowledge spill-ins and their importance for external learning in uncertain technology environments have not been examined in detail. This article compares corporate venturing (CV) spinouts that are supported by parent firms with independent employee spinouts as well as other strategies for external learning, such as investments and alliances. An analysis of the patenting activities of leading corporations operating in the information and communication technology industry reveals that, ceteris paribus, when firms recombine unfamiliar knowledge components developed by their CV spinouts, their inventions are associated with higher quality than comparable firms’ inventions that recombine knowledge from the firm’s corporate venture capital portfolio ventures, allies, or employee spinouts. This effect is especially pronounced when CV spinout inventors hold parent-specific technological expertise. The results are robust across several econometric specifications that account for selection and intrinsic differences in external learning strategies. Furthermore, the results indicate that CV spinouts benefit their parent firms by reducing uncertainty relative to other strategic formats for external learning in unfamiliar technological areas. The detailed theoretical and practical implications derived from this research reflect perspectives on technology strategy and corporate renewal.
This study considers the nascent period of industry change when the prevalent business model is being threatened by a new model, but there is significant uncertainty with respect to whether and when the new model will dominate. We focus on the challenge of incumbents pursuing both models simultaneously during the nascent period, and the implications on their firms’ valuations. Our theory is premised on the adjustment costs incurred by incumbents associated with the sharing of resources across business models and the conflict between managers vying for limited resources. While firms’ assets and competitive environments are key drivers of their value, we argue that they also impact adjustment costs. Evidence from the U.S. electric utility industry, which is undergoing a change from a centralized to a decentralized model, offers strong support for our arguments. The greater the level of incumbents’ assets that are specific to the existing model, and the greater the level of competition that they face, the lower are their firms’ valuations when investing in the new model relative to when investing in the existing model. Hence, ironically, those incumbents potentially most threatened by the change seem to be least rewarded for their efforts to renew themselves. However, pursuing the new model via alliances can help mitigate adjustment costs. The study uncovers the challenges that incumbents face as they pursue the new model in tandem with the existing dominant model, and helps explain why some incumbents may successfully navigate the changing industry landscape while others may stumble.
Internationalizing firms often find developing host-country resources challenging as they simultaneously attempt to replicate the resources that worked well in the home country and adapt them to fit the context of the host country. On the basis of a longitudinal study of the expansion of India-domiciled Narayana Health (NH), a tertiary healthcare provider, to the contextually distinct Cayman Islands, we propose a recombination-based internationalization model that allows us to offer a new conception of this replication–adaptation tradeoff. Recombination entails creating anew in the host country by drawing from, adapting, and integrating diverse resources developed earlier in heterogeneous settings in the home country. We theoretically explore the mechanisms underlying such recombination processes in organizational settings.
Prior research suggests that academic scientists who collaborate with firms may experience lower publication rates in their collaborative lines of work because of industry’s insistence on intellectual property protection through patenting or secrecy. In contrast, we posit that university–industry collaboration can sometimes foster specialization and boost academic contribution to open science. Specifically, research lines with both scientific and commercial potential (i.e., in Pasteur’s quadrant) provide an opportunity for a productive division of tasks between academic scientists and their industry counterparts, whereby the former focus on exploiting the scientific opportunities and the latter focus on the commercial ones. The main empirical challenge of examining this proposition is that research projects that involve industry collaborators may be qualitatively different from those that do not. To address this issue, we exploit the occurrence of simultaneous discoveries where multiple scientists make roughly the same discovery around the same time. Following a simultaneous discovery, we compare the follow-on research output of academic scientists who collaborated with industry on the discovery with that of academic scientists who did not. We find that academic scientists with industry collaborators produced more follow-on publications and fewer follow-on patents than did academic scientists without industry collaborators. This effect is particularly salient when the research line has important commercial implications and when the industry partner is an established firm.The online appendix is available at https://doi.org/10.1287/orsc.2018.1235.
Voice, or employees’ upward expression of challenging but constructive concerns or ideas on work-related issues, can play a critical role in improving organizational effectiveness. Despite its importance, evidence suggests that many managers are often hesitant to solicit voice from their employees. We develop and test a new theory that seeks to explain this puzzling reluctance. Voice is a distinctive behavior that involves escalation of opinions, ideas, or concerns by employees to their managers with the expectation that they would respond by making systemic changes in their teams. Hence, we argue that managers are likely to solicit voice more when they perceive requisite discretion and influence (personal control) to effect changes in their teams. Additionally, because voice-driven change can cause short-term disruptions and bring about benefits typically only over time, we propose that managers act on their personal control to solicit more voice when they also possess adequate long-term orientation. We find support for our arguments across four studies using experimental as well as correlational methods. We discuss the conceptual and practical implications of our findings.
This research proposes that individual-level exploration can be promoted by reconfiguring the spatial proximity between organizational members’ workspaces. To test this idea, I exploit a natural experiment in an e-commerce company where the spatial distances between organizational members’ workspaces were reconfigured. Consistent with the theory I develop on learning, results suggest that individuals whose workspaces were moved closer to those of previously separated peers engaged in more individual-level exploration. This pattern was stronger for individuals who had higher prior organizational experience and those who did not have ties with previously separated peers. Finally, I found that the relocated individuals also achieved higher financial performance. Overall, this study highlights the importance of an underexamined organization design element—spatial design—and its implications for organizational learning, individual-level exploration, and firm performance.The online appendix is available at https://doi.org/10.1287/orsc.2019.1291.
This study addresses the oft-debated questions of whether, when, and how corporate board members help shape firm strategy by advancing a new perspective on heterogeneous director influence that introduces the notion of the deep/broad director. Specifically, we take a sociocognitive and sociopolitical perspective on governance to suggest that deep/broad new outside directors, whose expertise results from a blend of depth and breadth of experience, will have an outsized influence in shaping firm strategy. We test our theory using an extensive multiyear data set that tracks all firms that went through an initial public offering in the United States across multiple business cycles (1997, 2001, and 2004) until 2011. Focusing on new outside directors in their first year of service (and controlling for selection issues), our results support our main prediction that those directors whose prior experiences are both deep and broad are the most influential directors for strategic change. We find this result to be robust for multiple indicators of strategic change. We also explore whether the outsized influence of the deep/broad director on strategic change may vary by differences in a board’s openness to strategic change. We conclude by highlighting the relevance of our approach and findings for future research and debates on director selection and director expertise.The supplementary file is available at https://doi.org/10.1287/orsc.2018.1258.
Developmental relationships offer rich opportunities for personal growth, which enables people to operate effectively in complex work environments. Although it is now widely recognized that protégés typically have more than one developmental relationship simultaneously, few researchers have considered the ways in which developmental networks—comprising a protégé’s multiple developers—foster growth. We therefore know little about how protégés grow through their engagement in several concurrent relationships. In this paper, we suggest that understanding growth in the context of developmental networks requires viewing such networks as intertwined assemblages of dyadic relationships. Adopting this perspective, we theorize one way in which the interactions that protégés have across their various relationships may—cumulatively—catalyze growth. In our theorizing, we focus on a specific type of situation: instances when developers offer divergent advice about work-related issues. Our model traces how protégés cope with divergent advice through either an engaged grappling process or an avoidant retreating process. We theorize that whereas grappling activities yield high levels of growth, retreating activities produce no growth. However, we also suggest that protégés may oscillate back and forth between the grappling and retreating processes over time, thereby resulting in varying rather than binary growth outcomes. Our paper contributes to scholarship on developmental relationships and networks while also laying the groundwork for future empirical research.
We study knowledge flows between organizations through secondments, short-term employee assignments at an organization different from the home institution. Secondments allow the sending organization to capture knowledge and network resources from the receiving organization without an organization-level contract, alliance, or colocation, a process we term learning by seconding. We focus on the National Science Foundation (NSF) rotation program, under which the NSF employs academics, called rotators, on loan from their university, to lead peer reviews. We ask how rotators affect the behavior of their academic colleagues after returning from a secondment. Using difference in differences estimations, we show that rotators’ colleagues raise considerably more research funds than similar scientists who do not have a rotator colleague. Additional quantitative and qualitative evidence implies that the treatment effect occurs via knowledge transfer, as rotators help generate ideas, frame proposals, and explain processes, rather than rent-seeking on the part of the rotator. Overall, the results suggest that strong ties and shared social identity play an important role in organizational knowledge acquisition.The online appendix is available at https://doi.org/10.1287/orsc.2018.1245.
Research shows that displaying face time—being observed by others at work—leads to many positive outcomes for employees. Drawing on this prior work, we argue that face time helps employees to receive better work and leads to career advancement because it is a strong signal of their commitment to their job, their team, and their organization. But when employees are geographically distributed from managers who control the assignment of work, they are often unable to display face time. To compensate, employees must engage in other behaviors that signal commitment. Our study of two large, globally distributed, product-development companies demonstrates that employees who engage in certain behaviors can effectively signal their commitment to the organization and, as a consequence, will receive better work assignments. But because they operate in a competitive signaling environment, they have to continually engage in the behaviors that produce desired signals to the point where they often feel that they are sacrificing their personal lives for their job. We induct a model from our data that explains why simple behaviors that signal commitment eventually turn into feelings of sacrifice and why employees at headquarters who have the power to assign better work fail to notice the sacrifice behind the signals. We discuss the implications of this model and the signal misalignment it explains for theories of distributed work and signaling in organizations.The online appendix is available at https://doi.org/10.1287/orsc.2018.1265.
Understanding how organizations operate in different environments has been at the core of organizational research for decades. Three distinct bodies of literature have emerged, with limited cross-pollination among them: routines, heuristics, and improvisation. We add to the existing literature by studying these three types of organizational responses simultaneously via an in-depth longitudinal study of an organization that encountered increasing levels of environmental dynamism. We pay particular attention to explaining how and why routines broke down, prompting the emergence of heuristics or improvisations, as well as when these three responses were used in tandem and when they interacted with each other. Our theoretical model identifies the triggers of heuristics and improvisations and the focal context that led to routines breaking down. We define “focal context” as a constructed temporary reality that encompasses both the objective traits of the environment experienced by the organization at a particular point in time, as well as the subjective perceptions that organizational members had of that reality. We also identify the mechanisms of cognitive search and social convergence that led to the creation of nonroutine responses. Finally, we use our insights to clarify the existing overlaps in the conceptualization of the three organizational responses. Our field study is based on a mountaineering expedition to climb one of the most difficult sides of Mount Everest, the Kangshung face, an archetypical case that is particularly well suited to the development of a new theory in which rich data are required to study the phenomena.
Our paper studies how gender and organizational status affect a university president’s compensation. Similar to previous findings, we hypothesize that women will receive less pay than men. However, we go beyond a dyadic view of individual differences to examine gender’s impact on compensation, and we explicate the importance of institutional forces in understanding the gender pay gap. In doing so, we rely on organizational status and hypothesize that the gender pay gap will be less pronounced as a university’s status rises. Although we find that the gender pay gap persists within the university president context, we also find that as a university’s status rises, the pay gap declines. Moreover, our findings show that the gender pay gap disappears at higher-status universities. Hence, accounting for where the glass ceiling is broken is an important consideration in understanding the gender pay gap. In sum, by integrating a broader institutional perspective to explain gender differences in pay levels, our paper demonstrates the importance of contextualizing gender to better understand its effects on compensation.The online appendix is available at https://doi.org/10.1287/orsc.2018.1266.
Platform-based technology ecosystems are new forms of organizing independent actors’ innovations around a stable product system. This collective organization is proving superior to traditional, vertically integrated systems in many sectors because of greater “generativity”—the ecosystem’s capacity to foster complementary innovation from autonomous, heterogeneous firms—which extends the usage scope and value of the platform to users. However, greater generativity can also lead to greater variance in the way ecosystem members’ contributions satisfy users’ needs, and it could potentially hinder the ecosystems’ value creation. We draw on collective action theory to examine generativity’s impact on user satisfaction and the mechanisms driving it. We argue that products enhancing user satisfaction contribute to a collective, shared asset, the platform system reputation, from which all participants benefit. Thus, generativity has both a positive (system reputation) and negative (free-riding) effect on the ecosystem members’ incentives for developing products that enhance user satisfaction. We argue that the negative free-riding effect prevails as the platform system matures and competition with alternative platform systems increases. Using data from the video game industry, we find supportive evidence for the free-riding effect, which generates an average loss in total revenue for first-rate games of about $36.5 million and a drop of about 3.3% in the console’s market share. By identifying the conditions that exacerbate free riding in platform ecosystems, our study contributes to the understanding of the evolutionary dynamics of platform ecosystems. It also highlights one feedback mechanism governing collective action in ecosystems and its implications for value creation.
In this study, we examine how multiple and sometimes conflicting goals are prioritized and pursued in organizations. Theories of coalitions and political behavior address prioritization among goals and changes in goal emphasis over time but cannot accurately predict the behavior of organizations that pursue conflicting goals. By linking theories of performance feedback theory and variable risk preferences, we show that performance shortfalls relative to aspirations on multiple goals can trigger managerial concerns for organizational failure. In such situations, the goal perceived as more important for survival gets priority and triggers stronger reactions. Empirically, we examine how airlines’ dual focus on safety and profitability affects decisions regarding fleet changes. In the airline industry, safety and profitability have clear conflicts (at least in the short term) owing to the costs of replacing aircraft models with poor safety records. We find evidence that airlines pursue fleet safety goals, but the nature and extent of that pursuit depend on whether the firm’s profitability goals are being met. As predicted, the responsiveness to safety goals is strengthened by low profitability because safety is associated more closely with survival. The study augments existing research on multiple goals by emphasizing the nature of goal interdependencies and its implications for behavior in organizations.
In this study, we examine the nature of Schumpeterian competition between entrants and incumbents. We argue that incumbents may respond to the threat of entry by either attacking the entrant or trying to learn from it, and that entrants, in turn, may react by either reciprocating the incumbent’s advances or retreating from it. Putting these competitive choices together, we develop a framework of four distinct potential scenarios of Schumpeterian competition. In particular, we emphasize a scenario we term creative divergence, wherein incumbents try to learn from entrants and build on their technologies, but their investments to do so cause entrants to retreat, resulting in diminishing returns to learning investments by incumbents. Exploratory analyses of the U.S. cardiovascular medical device industry find patterns consistent with the creative divergence scenario, with incumbent knowledge investments helping them to learn from entrants, but these learning benefits being undermined as entrants move away from incumbents.The online appendices are available at https://doi.org/10.1287/orsc.2018.1264.
Whether, and to what extent, employees learn from their failure experiences remains an unresolved issue for practitioners and scholars alike. On the one hand, failure provides individuals with opportunities for learning, whereas on the other hand, failure can also trigger defensive reactions that stifle learning. The present study expands experiential learning theories by incorporating the social context, thus offering a more comprehensive understanding of employee learning from failure. Specifically, we propose that team contexts that are psychologically safe and exhibit a well-developed transactive memory system provide important socioemotional and informational resources, enabling individual employees to seize the learning opportunities inherent in failure. Analysis of archival data on individual failure and subsequent performance in the domain of workplace creativity from 218 employees working in 42 teams supports our hypotheses. Employees are more likely to learn from their failure experiences if they work in teams with medium-to-high levels of psychological safety. Under these conditions, individual learning from failure is further stimulated by a well-developed transactive memory system. Our results also demonstrate the behavioral pathway linking failure experiences to subsequent outcomes. Interview data from 28 employees further illustrate the processes underlying these findings.
In recent years, scholars have demonstrated that capability theories of firm boundaries are fundamentally intertwined with contractual arguments. A productive use of capability arguments, therefore, is when they are joined with contractual ones in an integrated theory of the firm. However, contractual and capability scholars have traditionally held incommensurable views on the relevance of opportunism safeguards for a theory of the firm. Sponsors of the contractual view treat opportunism safeguards as fundamental, whereas several scholars in the knowledge-based strand of the capabilities camp consider it redundant. Moreover, in several recent integrative efforts, opportunism and safeguarding against it feature as linchpin theoretical ideas. To fully integrate contractual and capability theories, therefore, there is a need to resolve this point of incommensurability. We revisit a specific problem in the international strategy literature where the opportunism debate has been significant—the transfer of tacit know-how by multinational firms—and employ moderator-effect hypotheses to test two alternative mechanisms for why tacit know-how is transferred internally. We test whether tacit know-how is transferred internally to safeguard against opportunism or, alternatively, to avail the coordination benefits of common routines within firms. Our results indicate the former and not the latter, and thereby support a cornerstone notion in recent efforts toward an integrated theory of the firm.
Research on the business case for diversity suggests that organizations may gain important advantages by employing individuals from minority identity groups—those that are historically underrepresented and lower status—such as distinctive perspectives and greater access to minority customers and constituents. Organizations’ ability to capitalize on the promises of diversity ultimately depends on minority employees’ willingness and ability to draw on their distinctive strengths at work. However, little research has explored how employees perceive and act on potential advantages associated with their minority identity at work. Addressing this gap, we draw on in-depth interviews with 47 racial minority (31 Asian American and 16 African American) journalists to develop a conceptual framework of the process of identity mobilization—the steps through which individuals can deliberately draw on or leverage their minority cultural identity as a source of advantage at work and how this process is sustained or disrupted over time. The framework includes four different pathways through which individuals can leverage their minority identity to facilitate progress toward work-related goals and four identity mobilization tensions that can disrupt the identity mobilization process. Our research has significant implications for theory and practice related to diversity, identity, and positive organizational scholarship.The online appendix is available at https://doi.org/10.1287/orsc.2018.1272.
Researchers have endeavored to explain the causes of short organizational time horizons because of the organizational and societal costs of corporate short-termism. These explanations, however, tend to confound cognitive with behavioral explanations, which masks the importance of cognitive biases. We address this oversight by situating our work in prospect theory and organizational search, which underscores the importance of external evaluations on organizational time horizons and the asymmetry of positive and negative evaluations. Specifically, we argue that negative evaluations will shorten organizational time horizons more than positive evaluations will lengthen them. In our research context of financial analysts, this means that “sell” recommendations will shorten time horizons more than “buy” recommendations will lengthen them. Our main thesis can help to explain rising short-termism among some publicly traded companies. We operationalize organizational time horizons by the language managers use during 3,136 quarterly earnings conference calls. We test our main hypothesis and other timing-related moderating effects on 98 extractives firms from 2006 to 2013.
The growing organizational scholarship on authenticity has drawn attention to both its symbolic and material consequences for—among other things—organizational status, identity, consumer ratings, and brand trust. However, our understanding of authenticity has tended to focus on the what—the attributes and content typically associated with authentic products, organizations, and experiences. We still lack a clear understanding of how audiences think about different aspects of authenticity and the mechanisms through which audiences’ perceptions affect outcomes. In this paper we conduct three studies to investigate what people mean when they evaluate an organization as authentic, and what consequences this has for their support for the organization. In study 1 we build on existing theoretical frameworks to empirically derive three dimensions of authenticity: moral, idiosyncratic, and categorical. Using an online survey in the empirical setting of nascent crowdfunding ventures, we test the effects of these dimensions on audience members’ funding decisions. We find that each of the authenticity dimensions proves significant for distinct support outcomes, notably by enhancing the likability (warmth) of the project and/or its creators in the minds of evaluators. Study 2 offers experimental support for the mediating role of likability—but not of assessments of competence—in explaining support for nascent organizations. Finally, study 3 provides evidence that regardless of which dimension of authenticity audiences draw on, the latter are positively related in their minds to an overall notion of authenticity. We draw implications for the study of authenticity as a multidimensional concept in organizations, with both perceptual and real consequences for the support of nascent ventures.
In this study we seek to reconcile diverging dominant views on the relationship between firms and their legal environment by offering a cultural contingency perspective. We begin by accepting the notion that a new law will likely exert a powerful influence on targeted firms and that firms’ strategic responses include efforts to shape the impact of the new law. However, we suggest that the success of such response will be contingent on the degree of cultural consonance of firms’ strategic responses and the dominant cultural context at that time. We elaborate this view in our detailed qualitative and quantitative analyses of the automotive Safety Act of 1966 and the response by targeted firms. We provide evidence showing that the changes in the degree of cultural consonance of firms’ strategic response and the predominant cultural beliefs/values explain both the early failure of firms’ efforts to shape the impact of the law in the mid-1960s and the later success by the end of the 1970s. We highlight how firms’ cultural context provides both a constraint and an opportunity for firms seeking to shape legal environmental pressures, and we conclude by discussing the implications of our dynamic contingency perspective for research on law, culture, and strategy.
There has been a growing interest in the organization of business activities at the public interface as illustrated by the emergent phenomenon of public–private partnerships (PPPs). In this study, we analyze the determinants of private scope in partnering with public actors—that is, the extent to which private actors are involved in multiple, consecutive value-creating activities in the partnership. Based on a unique data set of public–private agreements worldwide over two decades, we find that institutional and capability-based determinants jointly affect the extent of private scope in public–private collaborations. Our results highlight the contingent role of the quality of the institutional environment. Institutions not only facilitate greater private scope directly but also, moderate the effect of public and private capabilities on private scope. We find that prior public experience in PPPs enhances private scope in settings with high-quality institutions while having an opposing effect in low-quality environments. Moreover, public governance capabilities accumulated via units designed to deal with PPPs seem to substitute for the lack of high-quality institutions, suggesting that, even under weak institutional settings, countries can foster high private scope with the creation of pockets of specialized public capabilities. In contrast, private capabilities in PPPs, expressed as firm engagement in recurring government cofunded projects, seem to have a complementary effect: they help to increase private scope in PPPs but only when domestic institutions are of high quality. By highlighting the determinants of private actor involvement in public sector activities, our study offers important implications for the theory and practice of hybrid (cross-sector) organizational forms.The e-companion is available at https://doi.org/10.1287/orsc.2018.1251.
In response to media exposés and activist group pressure to eliminate exploitive working conditions, multinational companies have pushed their suppliers to adopt labor codes of conduct and improve their labor practices to meet the standards set forth in these codes. Yet little is known about the extent to which suppliers are improving their labor practices to conform to codes of conduct, especially in organizations in which legitimacy structures like codes compete with productivity-driving incentive structures. We theorize that the presence of particular internal structures will affect the extent to which suppliers’ labor practices will become more tightly aligned—or coupled—with their formal commitments to adhere to labor codes. Specifically, we theorize high-powered productivity incentives to be associated with less coupling, and being certified to management system standards and having workers’ unions to be associated with more coupling. We also argue that these efficiency and managerial structures will moderate each other’s relationship to coupling, and that certification and unions will each increase the other’s positive association with coupling. Using social audit data on 3,276 suppliers in 55 countries, we find evidence that supports our hypotheses. Our focus on the internal structural composition of suppliers extends the decoupling literature by theorizing and demonstrating conditions under which suppliers’ core organizational functions are likely to be buffered from change by legitimacy structures. Furthermore, our findings suggest important strategic considerations for managers selecting supplier factories and provide key insights for the design of transnational sustainability governance regimes.The online appendix is available at https://doi.org/10.1287/orsc.2018.1261.
Although researchers have devoted considerable attention to assessing how organizations benefit from ascriptions of high status, relatively little research has analyzed the financial costs that organizations may incur in actively managing such ascriptions. In this study, we analyze how and why organizations may pay a relatively steep economic price for the attainment and/or maintenance of social status. Specifically, we advance an original theoretical perspective, which suggests that firms engaged in economic competition are simultaneously engaged in social ceremony and that these dual processes can generate a combination of social gains (in terms of status) and economic losses (in terms of profitability). We theorize and test our perspective in the context of competitive bidding ceremonies using a unique, decade-long data set on repeated competitive market interactions among firms in the U.S. construction industry. We find support for our prediction that firms’ participation in bidding ceremonies can generate divergent outcomes, that is, higher social status and diminished economic performance. We discuss the implications of our theoretical and empirical analysis for the existing literature on social status, competitive bidding, and—more generally—on the role social forces play in competitive market behaviors and outcomes.
Product categories are more than classification devices that organize markets; when reflecting market actors' purposes, they are also judgment devices. Taking stock of the literature on product categories and drawing on the distinction between the faculties of knowing and judging, we elaborate a framework that accounts for how and why market actors include or exclude normative attributes in a product category definition. Based on a field study of the development of socially responsible investment (SRI) funds in France, we describe the phases and conditions of a judgment framework for category definition for both established and nascent categories. We discuss implications for research on product categories and the workings of markets more broadly.
While social capital has been found to play an important role in economic transactions when information is incomplete, it remains unclear how it interacts with human capital in transaction performance. This paper explores the complementarity between social affiliations and human capital in transaction performance, and how affiliations influence the match between qualified professionals and consumers. I argue that human capital is important to professional performance, but that social affiliations lead consumers to increasingly match with lower human capital professionals. Thus, while social and human capital function as complements in transaction performance, social capital can substitute for human capital in the selection process. I test my argument using a novel approach that pairs data from a primary real estate multiple listing service in Utah with hand-collected data on geographically assigned church congregation boundaries. This setting allows me to identify listings for which real estate agents and home sellers share a common church congregation, and to explore the impact of this affiliation, as well as human capital variables, on transaction outcomes. I find that agent performance improves when listing for affiliates, on average, and that gains increase with agent human capital. However, consistent with my theory, I find that sellers are more likely to use inexperienced and underqualified affiliated agents. Human capital deficiencies reduce benefits from social affiliations and lead to inferior transaction outcomes in extreme cases. This suggests a new underexplored dark side to social capital from human capital deficiencies, which is driven by the selection process under incomplete information.
With perceptions of authenticity offering contemporary organizations a key competitive advantage in the marketplace, a growing body of research has investigated “authenticity work”: the diverse ways in which organizational actors fabricate authenticity claims for their audience members. However, claiming authenticity is a challenging and problematic task, because organizations must weigh how much authenticity they can safely project without incurring backfire. This is further complicated by consumers’ fickle and contradictory attitudes regarding authenticity work. This study examines this challenge by asking how organizations can claim authenticity in a way that aligns with their audiences’ variable understandings and expectations. Drawing on a qualitative study of underground restaurants—alternative social dining establishments, also known as “pop-ups” or “supper clubs”—I show that organizers claim authenticity through the coperformance of three illusions: community, transparency, and gift-giving. Instead of rejecting these illusions, most diners and underground organizers knowingly embrace them as authentic. This paper suggests that authenticity work, far from sending a one-way signal that audience members passively accept or reject, involves a continual process that generates the active co-construction of illusions by organizers and their audiences.
Covenants not to compete are often included in employment agreements between firms and employees, justified by each party’s voluntary “freedom to contract.” However, noncompetes may also generate externalities for all individuals in the market, including those who have not signed such agreements. We theorize that enforceable noncompetes increase frictions in the labor market by increasing uncertainty and recruitment costs and by curtailing entrepreneurship. We find that in state-industry combinations with a higher incidence and enforceability of noncompetes, workers—including those unconstrained by noncompetes—receive relatively fewer job offers, have reduced mobility, and experience lower wages. The results offer policymakers a reason to restrict noncompetes beyond axiomatic appeals to a worker’s “freedom of contract” and highlight labor market frictions that may impact firm-level human capital strategies.
We study minority equity partnerships and the representation of investing firms on the boards of directors of their partners. In such alliances, an investor firm owns a minority position in the investee partner and may or may not secure a board seat. Prior alliance governance research has largely focused on the choice between equity and nonequity forms of alliances, and it has paid little attention to the particular administrative features of these organizational forms, notably their governing boards. We extend corporate governance research that has emphasized the monitoring and advisory roles of boards by examining how an investee's concerns about knowledge misappropriation can reduce the likelihood that an investor obtains board representation. We suggest that, although there exist opportunities for an investee to benefit from the investor’s advice, the investor's ability to use an investee's knowledge by itself and indirectly with the help of its other partners negatively affects the likelihood of a board seat for the investor. We further argue that this negative effect is amplified when the investee has particularly valuable technologies at risk of appropriation by the investor. Our evidence from minority equity partnerships in the biopharmaceutical industry indicates why and when investors do not obtain board seats, despite the monitoring and advisory benefits that directors can bring to cooperative commercialization agreements.
We explore the role that contracting plays within the careers of managerial workers. Contracting distances workers from organizational coordination and politics, aspects of organizational life that are often central to the managerial role. Nonetheless, managerial workers make up a substantial proportion of the contracting workforce. Qualitative interviews with managerial contractors indicate that the tension between the natures of contracting and managerial work means that managerial contractors carry out substantially more bounded work than regular employees, and that this boundedness can shape the role that contracting plays in their careers. Examining the employment histories of MBA alumni of a U.S. business school, we show that workers with fewer subordinates and greater personal demands are more likely to enter contracting. We also find that contractors report better work–life balance but receive lower pay both while contracting and in subsequent regular employment. Whereas prior research has highlighted the financial benefits and temporal demands of contracting for highly skilled workers, our findings introduce important boundary conditions to our understanding of high-skill contracting: the nature of the occupation is critical.
Unlike problems requiring new-to-the-world solutions that combine knowledge from multiple sources, operational problems can often be solved by repurposing existing knowledge from other contexts into new-to-the-firm solutions. Firms that seek new-to-the-firm solutions to operational problems face a cost-benefit tradeoff when deciding how many knowledge sources to use. With less need for knowledge recombination than for new-to-the-world solutions, greater knowledge breadth incurs greater screening and implementation costs without concomitant benefits. We study how U.S. manufacturing facilities from 1991 to 2005 improve operational performance by reducing their rate of annual output of toxic chemical waste (i.e., improvements to operational effectiveness). Results show that search involving fewer knowledge sources in a given year is associated with greater improvements in operational performance (greater waste reduction). At the same time, however, using multiple knowledge sources over time helps improve operational performance, suggesting that avoiding satiation from a single source and learning across sources play temporal roles in toxic chemical waste reduction. Overall, the results suggest that the greatest improvements in operational performance arise with a focused search for new-to-the-firm solutions within periods while also exploring multiple sources over time.
This paper examines how employees become simultaneously empowered and alienated by detailed, holistic knowledge of the actual operations of their organization, drawing on an inductive analysis of the experiences of employees working on organizational change teams. As employees build and scrutinize process maps of their organization, they develop a new comprehension of the structure and operation of their organization. What they had perceived as purposively designed, relatively stable, and largely external is revealed to be continuously produced through social interaction. I trace how this altered comprehension of the organization’s functioning and logic changes employees' orientation to and place within the organization. Their central roles are revealed as less efficacious than imagined and, in fact, as reproducing the organization's inefficiencies. Alienated from their central operational roles, they voluntarily move to peripheral change roles from which they feel empowered to pursue organization-wide change. The paper offers two contributions. First, it identifies a new means through which central actors may become disembedded, that is, detailed comprehensive knowledge of the logic and operations of the surrounding social system. Second, the paper problematizes established insights about the relationship between social position and challenges to the status quo. Rather than a peripheral social location creating a desire to challenge the status quo, a desire to challenge the status quo may encourage central actors to choose a peripheral social location.
Although prior literature argues that product adaption is critical to survival in new or dynamic industries, we have very limited information about the antecedents to product adaptation. This paper focuses on start-up top management teams (TMTs) and explores the impact of different types of preentry experience—preentry experience breadth, depth, and type (entrepreneurial and business experience)—on the likelihood that start-ups adapt their products. The hypotheses are tested among the population of U.S. solar photovoltaic start-ups during a period of industry emergence from 1992 to 2007. The analysis suggests that preentry experience breadth and experience in other dynamic settings (entrepreneurial) increases the ability of start-up TMTs to learn quickly about their environment, thereby increasing the likelihood of product adaptation. By contrast, deep preentry experience in the industry, although valuable in many other ways, can lead to narrow learning that decreases the likelihood of adaptation. These results underscore the importance of preentry experience to product adaptation. The results also underscore the role and patterns of product adaptation in firm, industry, and technology evolution.
This paper examines how and why innovations are reshaped as they become implemented and used in locales that are distant and distinct from those where the innovation was initially developed. Drawing on an in-depth field study of the innovation process that produced a mobile money system for Kenya, we contribute an understanding of the particular dynamics that arise when an innovation trajectory interacts with local trajectories that constitute the local conditions and practices of specific places. We identify four distinct patterns of trajectory dynamics—separation, coordination, diversification, and integration—each of which has different implications for the innovation, its implementation, and consequences on the ground. Developing a model of trajectory dynamics in innovation, we theorize the processes through which innovations are transformed over time as they interact with multiple local trajectories and the specific innovation outcomes that are generated as a result. Such theorizing reconceptualizes traditional notions of innovation diffusion by explicating how and why innovations change in multiple and unexpected ways as they move to particular places and engage with local conditions and practices.
Prior studies of academic science have largely focused on researchers in life sciences or engineering. However, while academic researchers often work under similar institutions, norms, and incentives, they vary greatly in how they organize their research efforts across different scientific domains. This heterogeneity, in turn, has important implications for innovation policy, the relationship between industry and academia, the scientific labor market, and the perceived deficit in the relevance of social sciences and humanities research. To understand this heterogeneity, we model scientists as publication-maximizing agents, identifying two distinct organizational patterns that are optimal under different parameters. When the net productivity of research staff (e.g., PhD students and postdocs) is positive, the funded research model with an entrepreneurial scientist and a large team dominates. When the costs of research staff exceed their productivity benefits, the hands-on research approach is optimal. The model implies significant heterogeneity across the two modes of organizing in research funding, supply of scientific workforce, team size, publication output, and stratification patterns over time. Exploratory empirical analysis finds consistent patterns of time allocation and publication in a prior survey of faculty in U.S. universities. Using data from an original survey, we also find causal effects consistent with the model’s prediction on how negative shocks to research staff—due to visa or health problems, for example—differentially impact research output under the two modes of organization.
Most explanations of status dynamics rely on market actor behavior or affiliation to other actors as the primary drivers of change. Yet status is increasingly mediated by third-party intermediaries, which impart status through their ordering of actors. Prior literature suggests that these rankers can affect status orders via changes in the underlying ranking methodology but offers little insight as to whether such changes reflect existing field beliefs or are self-interested. We advance a theory of ranker self-interest, whereby rankers adopt specific behavior to maintain audience attention and increase their chance for survival. We hypothesize that, by threatening audience attention, temporal stability in rankings (an endogenous property of many status systems) induces rankers to self-generate changes in the ranking. We examine the role of stability of rankings in promoting structural changes by rankers using Institutional Investor magazine’s All-America Research Team (all-stars), a widely studied and eminently impactful ranking of equity analysts.
We investigate how higher-ranking organizational members can protect their legitimacy after status loss. We theorize that after status loss, internal stakeholders will scrutinize the behavior of higher-ranking members to determine whether they are still deserving of their high-ranking position (i.e., legitimate) and that those members who display self-control (e.g., persistence, poise, restraint) after status loss signal legitimacy to scrutinizing internal stakeholders. In a laboratory experiment (Study 1), we found that leaders who displayed higher (versus lower) self-control after status loss were judged as more legitimate and were less likely to be challenged. This effect of higher perceived self-control on legitimacy and challenging behavior after status loss was explained by positively influencing internal stakeholders’ instrumental and moral evaluations of the higher-ranking individual. In an online experiment with working adults (Study 2), we constructively replicated these results and found that high self-control is more important for positive legitimacy judgments after status loss than when no status loss has occurred. Finally, in a critical incident study (Study 3), we explored whether the type of perceived self-control influenced the efficacy of the self-control strategy. We found that self-presentation was the most effective “type” of self-control display after status loss, and displaying self-control in multiple ways (e.g., task-related and self-presentation) increased the efficacy of perceived self-control. We discuss the implications of this research for legitimacy judgments, status loss, and self-control.
Protest raises the visibility of a social movement, and this affects all organizations affiliated with the movement’s group identity. Although the mutually beneficial relationship between protest and social movement organizations is well documented, we argue that protest does not necessarily aid other, more mundane types of affiliated organizations in the same manner. Specifically, we expect that increases in protest participation will favor the viability of organizations targeting an audience close to the group identity but not of organizations with an audience in which some members share that identity and others do not. We evaluate these claims using a data set of pro-lesbian, gay, bisexual, transgender, and queer (LGBTQ) protest events and local organizations in U.S. cities using a fixed-effects panel design with instrumental variables. Findings show that increases in protest participation decrease the presence of organizations that engage LGBTQ and non-LGBTQ audiences, especially local businesses that simultaneously bridge multiple groups of owners, customers, and clients.
Strategic management research increasingly examines firms’ strategies for corporate environmental and social disclosures. There are benefits to being perceived as having superior environmental performance, but firms face increasing pressure to provide more complete disclosures, potentially exposing information that will be viewed negatively by external stakeholders. We examine linguistic obfuscation as a means to balance this tension. In particular, we argue that firms may intentionally make their disclosures more complex and harder to understand, thereby blurring the negative content and increasing information processing costs of the recipient. In the context in which an information intermediary actively collects information from firms and evaluates them, we find that firms with unfavorable news to disclose use linguistic obfuscation in information disclosure to manage the tension between the pressure for more complete disclosures and the desire to project a positive image. We further demonstrate that obfuscation lessens the negative impact of reporting negative information on environmental performance ratings given by information intermediaries. This suggests that firms can and do use linguistic tactics to influence environmental ratings.
Platform sponsors typically have both incentive and opportunity to manage the overall value of their ecosystems. Through selective promotion, a platform sponsor can reward successful complements, bring attention to underappreciated complements, and influence the consumer’s perception of the ecosystem’s depth and breadth. It can use promotion to induce and reward loyalty of powerful complement producers, and it can time such promotion to both boost sales during slow periods and reduce competitive interactions between complements. We develop arguments about whether and when a platform sponsor will selectively promote individual complements and test these arguments on data from the console video game industry in the United Kingdom. We find that platform sponsors do not simply promote “best in class” complements; they strategically invest in complements in ways that address complex trade-offs in ecosystem value. Our arguments and results build significant new theory that helps us understand how a platform sponsor orchestrates value creation in the overall ecosystem.
To understand how recipients respond to radical change over time across cognitive, affective, and behavioral dimensions, we conducted a longitudinal study of a mandated language change at a Chilean subsidiary of a large U.S. multinational organization. The engineering-focused subsidiary aiming to facilitate cross-border interactions embedded language-acquisition experts to transition all employees from Spanish to English full time. We gathered survey data and objective fluency scores from the language change recipients at five points over a period of two years. Using variable- and person-centered exploratory analyses, our results suggest that recipients’ negative affective responses to the language change precede their cognitive responses or self-efficacy, predicting their current language learning. Further, we find that recipients’ cognitive and affective responses over time differentially influence two future behavioral outcomes: intention to leave the organization and willingness to adopt the change. Although cognitive rather than affective responses over time drive recipients’ intentions to leave, affective responses influence recipients’ willingness to adopt English. Finally, we show that change recipients followed three trajectories of cognitive responses and two trajectories of affective responses over time. We discuss theoretical and practical implications to the literature on organizational change, emotions, and language in global organizations.
We examine investor responses to board diversity and highlight a previously unexplored mechanism to explain negative market reactions to senior female appointments. Drawing on signaling theory, we propose that an increase in board diversity leads investors to update their beliefs about firm preferences. Specifically, we argue that a gender-diverse board is interpreted as revealing a preference for diversity and a weaker commitment to shareholder value. Consequently, firms with more female directors will be penalized. We test our argument using 14 years of panel data on U.S. public firms. We find that firms that increase board diversity suffer a decrease in market value and that this effect is amplified for firms that have received higher ratings for their diversity practices across the organization. These results suggest that observers respond to the presence of female leaders not simply on their own merit but as broader cues of firm preferences and that firms may counteract any potential signaling effect through careful framing.
New ventures often experience deviations from their plans that oblige them to reorient in pursuit of a better fit between their evolving products and their target customers. Yet, research is largely silent on how managers explain such changes and justify their ventures in the wake of fundamental redirections in strategy. Ventures initially attain legitimacy and amass resources on the strength of aims that audiences find compelling; later, those early claims can complicate course corrections. To shed light on how ventures manage strategic reorientations, we conducted an inductive, comparative case study of ventures in a nascent financial-technology sector. The ventures pursued parallel reorientations and produced comparable end products but diverged conspicuously in managing audiences during transitions. Our process model, inspired by these differences, proposes a sequence of stratagems that may enable entrepreneurs to alter strategy while portraying faithfulness to enduring aims. Our theoretical framework posits that, for ventures, reorientation without penalty may depend on how they anticipate, justify, and stage changes to various audiences.
According to the literature on ambidexterity, organizations can use structural or contextual approaches to simultaneously explore novel opportunities and exploit existing ones. So far, however, we know very little about what induces organizations to focus on structural versus contextual ambidexterity, or how they combine the two approaches to maximize organizational learning. To shed more light on these questions, we investigate how the environment shapes a firm’s use of structural and contextual ambidexterity. Drawing on a comparative, longitudinal case study of the four largest electric utility companies in Germany, we show that firms focused on structural ambidexterity whenever they perceived emerging opportunities in the environment as requiring organizational culture and capabilities fundamentally different from their own. Contextual ambidexterity, on the other hand, became particularly important when opportunities in the environment were both numerous and uncertain, requiring the organization to leverage the distributed attention and expertise of its frontline employees. We show that environments characterized by opportunities that are numerous/uncertain and require novel culture and capabilities lead organizations to invest in initiatives that combine elements of both structural and contextual ambidexterity—an approach we label hybrid ambidexterity. Our theory framework synthesizes and complements existing work that has started to investigate the antecedents of structural versus contextual ambidexterity. We challenge the prevailing understanding of contextual and structural ambidexterity as dichotomous categories and reconceptualize them as two ends of a continuum. In addition, we provide initial evidence that firms’ ambidexterity approaches are influenced by managers’ perceptions of capabilities and opportunities.
To protect themselves against deskilling and obsolescence, professionals must periodically revise their claims to authority and expertise. Although we understand these dynamics in the broader system of professions, we have a less complete understanding of how this process unfolds in specific organizational contexts. Yet given the ubiquity of embedded professionals, this context is where jurisdictional shifts increasingly take place. Drawing on a comparative ethnographic study of human resources (HR) professionals in two engineering firms, we introduce the concept of jurisdictional entrenchment to explain the challenges embedded professionals face when they attempt to redefine their jurisdiction. Jurisdictional entrenchment describes a condition in which embedded professionals have accumulated tasks, tactics, and expertise that enable them to make jurisdictional claims in an organization. We show how such entrenchment is a double-edged sword: instrumental to the ability of professionals to withstand challenges to their authority but detrimental when expertise and skills devalued by the professionals remain in high demand by clients, thus preventing the professionals’ shift to their aspirational jurisdiction. Overall, our study contributes to a better understanding of how embedded professionals renegotiate jurisdictional claims within the constraints of organizational employment.
We examine how a discontinuous increase in the value of an employee’s relational capital influences the employee’s mobility and entrepreneurship decisions in professional and business service contexts. Drawing on the unfolding model of voluntary turnover, we develop a theory proposing that positive shocks to external relational capital will catalyze employees to consider alternative employment options, thereby increasing the probability of exit. We further maintain that exit decisions in response to such shocks will be driven by a desire to appropriate more value, making these shocks strong predictors of employee entrepreneurship, especially when the employee works in an area that is peripheral to the firm’s core capabilities. Empirically, we construct a unique employee-employer linked database that tracks employment of lobbyists in the United States federal lobbying industry. Leveraging plausibly exogenous shocks to the value of an employee’s relational capital and a novel market-based measure of the employee’s position in the firm’s knowledge space, we report two sets of findings. First, an increase in the value of relational capital has a positive effect on the likelihood of mobility to established firms and employee entrepreneurship, with the effect for the latter stronger than the former. Second, the magnitude of the effect on employee entrepreneurship becomes stronger when the employee is peripheral to the firm’s core knowledge. Together, our results are consistent with a value creation-value appropriation rationale, where sudden increases in the value of an employee’s relational capital drive exit as a means to appropriate a greater portion of value created.
Work in organization theory has highlighted that diversification triggers concerns over the newly diversified firm’s capability or commitment to serve its audience. Although this work has shown that perceived lack of commitment may be an important problem for diversifying firms, it has not been established what might resolve these commitment concerns and reduce demand-side penalties for diversifying to serve new customers. We argue that a firm’s ability to signal authenticity will increase perceptions of commitment and resolve ambiguities about commitment generated by diversification. We use a multimethod approach including qualitative evidence from a case in the behavioral health industry and experimental methods to isolate these observed effects. In a qualitative study, we examine a case in which two firms saw divergent outcomes when they tried to engage in the exact same diversification activity and show that when a firm signals that they are highly authentic (i.e., when stakeholders perceive the firm to be willing to fulfill commitments even while sacrificing short-term rewards), diversification does not threaten perceived commitment. However, those who cannot signal authenticity are less likely to be selected in the market because diversification is seen as a threat to perceived commitment. We then test these findings in two experiments using the primary customer audience, addiction recovery therapists, as participants. In a final experiment, we test some key boundary conditions of our argument, finding support in the context of markets for car mechanics, which suggests that our argument may be applicable more broadly than healthcare into markets for various types of credence goods.
The literature on authenticity of cultural production has systematically examined the perceived authenticity of both the producer and the cultural product but not of the creative process. This study aims to address this lacuna, adopting Carroll and Wheaton’s typology of type and moral authenticity to examine how contemporary dance choreographers construct authenticity during the creation of a new choreography. Our analysis of data from 23 contemporary dance companies reveals that the two meanings of authenticity dynamically reconstitute one another in the creative process. First, choreographers construct moral authenticity through transformation of form, deconstructing established artistic dance forms and introducing new movements from a bricolage of techniques. Second, they construct type authenticity through wrapping expression, facilitating the deconstruction of the values attached to the bricolage of techniques into artistic dance aesthetics. Finally, choreographers evoke both moral and type authenticity through a creative process of reconstruction. Our noteworthy finding reveals how the construction of authenticity in the process of creating a new choreography, and the dynamics between the two meanings of authenticity, serve significantly as a means of communication among the involved actors, thereby enabling the creative process.
Firms using stigmatized inputs are likely to experience strong pressures from multiple stakeholders who demand removal of such inputs from firm operations. Consistent with this phenomenon, we propose that greater levels of input stigma decrease firm performance and that firms respond to this threat by reducing the use of such inputs. However, we argue that not all firms are equally able to remove stigmatized inputs. Firms with a high degree of input overlap among their product units will be less likely to remove stigmatized inputs from their operations; as such, a change could be costlier for them. Consequently, we propose that firms facing high input overlap will experience the negative effects of input stigma more strongly. We find support for our theory by looking at toxic chemicals as the inputs that may become stigmatized.
Organizations are getting busier, but can they still learn to get better? This question has urgent practical importance, since competitive pressures in a wide variety of industries have resulted in organizations that increasingly strain their operating limits. This question is deeply connected with organizational learning theory, since organizations operating with constrained capacity may gain experience but lose the ability to digest it—challenging the overall organization’s ability to learn and improve. Some research, though, suggests a seemingly contradictory perspective, with constrained capacity perhaps motivating organizations to adopt more flexible approaches and learn out of necessity. This study integrates the perspectives to examine how constrained capacity impacts organizational learning. To explore this question, the study develops separate theory regarding the amount and timing of capacity crises, suggesting that increasingly constrained capacity tends to detract from learning, but, uniquely, that consistently constrained capacity, rather than periodic spikes, may instead lead to better learning. Hypothesis tests provide support for several of the study’s arguments.
The paper elaborates the concept of temporal multiplexity, defined as the overlaying of ties of different duration, such as transient employment and enduring organizational ties. This concept is instrumental in resolving long-standing challenges in network research, such as capturing the interplay between different levels of analysis or time horizons. This is made possible by longitudinal network and mobility data (1743–2010) from the Palio di Siena—the famous horse race in Siena, Italy. The outcome of interest is Palio-related collective violence. The analysis shows that relationally loaded organizational ties of rivalry or friendship increase the likelihood of incidents, whereas mobility along the same lines reduces it. The results support sociological arguments that symmetrical social space of friendship or rivalry is conducive to conflict. Mobility is a factor of moderation—by connecting employers within the actor and transferring relational content between them, it creates misalignment between the assumption of a role and fulfillment of its expectations. Mobility relaxes the relational constraints of jockeys, reducing their compliance with bellicose demands. The uncertainty resulting from mobility may have a collective benefit that is ignored by employers: the moderation of conflict.
Many contemporary organizations depend on team-based organizing to achieve high performance, innovate services and products, and adapt to environmental turbulence. Significant research focuses on understanding how teams develop, assimilate, and apply diverse information; yet, organizational practices have evolved in new ways that are not fully explored in the teams literature. Individuals with diverse motivations, knowledge, and perspectives are often assigned to teams, creating burdens for members to develop effective ways to work together, learn from each other, and achieve goals amid the complexity of today’s organizational contexts. In this paper, we examine a multilevel model of how team goal orientation affects cross-understanding—the extent to which team members understand the other members’ mental models—which in turn, affects team and individual performance. We examine these effects using 160 teams of 859 participants who completed a semester-long business simulation. Findings show that the more team members are motivated by learning goals, the greater a team’s cross-understanding and subsequent team and individual performance. These effects are dampened when members are motivated by performance goals—to avoid mistakes or prove competence. This study expands the cross-understanding literature, revealing motivational antecedents that explain why some teams develop higher cross-understanding than others. We also contribute to the goal orientation literature by demonstrating that team goal orientation influences members’ learning about other members and in so doing, also affects team and individual performance. Because team motivation can be influenced by organizational practices, our findings also contribute practical insights for organizational leaders.
We tackle the persistent problem of people from specific demographic groups (e.g., women) being undervalued in professional contexts in which traits associated with their group do not align with the traits perceived to be essential for success (the professional prototype). We introduce the concept of balancing professional prototypes such that group membership becomes irrelevant to determining an individual’s prototypicality. Using a novel technique called prototype inversion, we emphasize the importance of professional traits typically associated with an underrepresented group, without dismissing those associated with the currently prototypical group. By balancing the prototype in this way, it becomes easier to recognize the professional potential of members of underrepresented groups, without incurring backlash from the currently prototypical group. We conducted a full-cycle research project to demonstrate the effectiveness of this strategy in the extreme context of women in firefighting using qualitative and quantitative methods and participants from both the laboratory and the field.
We develop and test a theoretical model that explains how collective psychological ownership—shared feelings of joint possession over something—emerges within new creative teams that were launched to advance one person’s (i.e., a creative lead’s) preconceived idea. Our model proposes that such teams face a unique challenge—an initial asymmetry in feelings of psychological ownership for the idea between the creative lead who conceived the idea and new team members who are beginning to work on the idea. We suggest that the creative lead can resolve this asymmetry and foster the emergence of collective psychological ownership by enacting two interpersonal behaviors—help seeking and territorial marking. These behaviors build collective ownership by facilitating the unifying centripetal force of team identification and preventing the divisive centrifugal force of team ownership conflict. Our model also proposes that collective ownership positively relates to the early success of new creative teams. The results of a quantitative study of 79 creative teams participating in an entrepreneurship competition provided general support for our predictions but also suggested refinements as to how a creative lead’s behavior influences team dynamics. The findings of a subsequent qualitative investigation of 27 teams participating in a university startup launch course shed additional light on how collective ownership emerges in new creative teams launched to advance one person’s idea.
Management practices explain an important part of the heterogeneity in firm productivity, but the literature has largely focused on manufacturing, while leaving out research in the industrial setting. A key managerial practice in industrial research projects is the use of autonomy (through the delegation of decision rights). Our paper clarifies the drivers and the effects of autonomy in settings where other managerial instruments are less effective. We discuss that in industrial research projects, autonomy is set for efficiency reasons—autonomy allows researchers to make more competent decisions about a specific problem—as well as for motivational considerations—autonomy motivates researchers to exert greater effort. We also argue that project-relevant capital—the resources that enhance the productivity of researchers on a given project—is a key driver of autonomy. We theorize that the efficiency and motivational channels have opposite implications for the relationship between project-relevant capital and autonomy and find that, empirically, this relationship is U-shaped, which is suggestive evidence of the presence of both channels.
This paper proposes a model to predict when the subunits of a multidivisional firm implement a practice adopted by the firm more or less extensively, focusing on the intraorganizational environment. Drawing on institutional arguments, I propose that a subunit’s extent of practice implementation is a combined result of coercive pressures from its headquarters, imitation of its peer units, and its own perception of the practice’s legitimacy. More specifically, I argue that a subunit will implement new practices related to corporate social responsibility (CSR) more fully (1) when the corporate mandate from the headquarters is more pressing, (2) when its peer subunits have implemented similar actions, and (3) when the practice is perceived as consistent with the subunit’s own values. Regression results further suggest that peers and headquarters influence a subunit’s extent of implementation of a practice only when the subunit perceives it as highly consistent with its own values—a finding that points to the importance of values for practice legitimacy and the need to rethink practice implementation within complex organizations.
Building on work at the individual and organizational levels suggesting that an individual’s self-concept and an organization’s identity are dynamic, we relax the generally held assumption that perceptions of organizational identification are perceived as relatively stable over time and highlight the importance of understanding the perceived dynamism in members’ relationships with their organizations over broader time horizons. We introduce various identification trajectories—a member’s current perception of how his or her identification has evolved and will evolve over time—and investigate the sense of momentum that characterizes these trajectories. We also generate theory about the different action tendencies created by various types of trajectories and examine their influence on cognition, affect, and behavior in the present. Our theoretical model helps to explain why two members of the same organization with similar degrees of identification in the present might think, feel, and behave quite differently. In addition, our theoretical perspective enables us to understand why high (or low) identifiers might display cognition, affect, and behavior typically associated with low (or high) degrees of identification.
Understanding the social landscape at work helps employees accomplish organizational goals. A growing body of evidence, however, suggests that people are fallible perceivers of their work relationships. People do not always know how much others trust (or distrust) them, consider them a friend (or enemy), or rely on them for advice or information at work. Such relational misperceptions may be especially likely in the context of work organizations. Here, we develop theoretical accounts to explain how and why employees misinterpret the nature of their relationships with others at work—and what consequences ensue when they do. We direct attention to five key opportunities for future research on when and why relational misperceptions occur and matter in organizations. Building on the small body of organizational research and larger body of nonorganizational research on relationship misperception, we also identify areas that may be fruitful for exploration, highlighting several topics in the organizational literature that could be enlivened by considering the role of relational misperceptions. For example, we consider how employees’ relational misperceptions may affect how influential they are at work, how effectively they lead others, and how they navigate the social landscape in organizations.
We offer theory and evidence about how the fit between firm experience (supply side) and consumer preferences (demand side) affects postentry performance into a new technology. Specifically, we explore different types of preentry experience (technological and market experience) and use different aspects of postentry performance to draw inferences about consumer heterogeneity. Preentry technological experience (same product and different consumers) helps firms attract a larger share of intensive users (aligning with early adopters) but only if they enter the market early when these adopters make decisions. Preentry market experience (different product and same consumers) helps firms attract a larger share of lighter users, consistent with characterizations of mass market users. Exploiting different components of firm performance in the global second-generation mobile telecommunications industry (average usage intensity and market penetration) allows us to articulate and identify the paths and mechanisms that allow preentry experience to affect postentry performance. The theory as well as important theoretical boundary conditions have implications for research on preentry experience, demand-side heterogeneity, and industry evolution.
Organizations are often conceptualized as systems of interdependent choices that exhibit a core–periphery structure. Research is inconclusive, however, regarding whether organizations should focus their search efforts on their core or peripheral choices. In this paper, we seek to reconcile contradictory arguments and suggest that the efficacy of a search focus depends on the time horizon, environmental change, and how the core and periphery interact. In so doing, we demonstrate that the directionality of interdependence and whether interdependencies occur mostly within the core or between the core and periphery are key determinants of the implications of focus. We discuss the implications of our findings for various streams of research, including research on structural inertia and business model innovation.
Sustaining systems of cooperation in the face of strong self-interest is a subject of long-standing inquiry in the social sciences. Much of this work has focused on understanding the antecedents and outcomes associated with cooperation, noting that the inertial properties of a system should sustain cooperation over time. This paper shifts the focus toward examining how cooperation is maintained in the face of potentially disruptive forces. To advance theory, research, and practice on how to maintain cooperation over time, we examine how systems of cooperation interact with, withstand, or succumb to a potentially disruptive force that is commonplace in organizational contexts: rankings. Using a longitudinal, no-deception, between-groups experimental design, we assess how systems of cooperation respond to the introduction of performance rankings. Examining data from more than 11,000 rounds of decision making from 592 participants clustered in 74 teams, we find that cooperation plummets when performance-rank information is introduced. However, the addition of reputation information—individuals’ histories of prosocial contributions—enables a system of cooperation to withstand the disruptive effects of performance rankings. Actors use reputation information to make decisions that reduce perceived inequity. Our study contributes to theories of cooperation, performance feedback, macrolevel prosocial behavior, and management practice.
We study the behavioral drivers of market entry. An experiment allows us to disentangle the impact on entry across different types of markets of two key behavioral mechanisms: overconfidence and attitude toward ambiguity. We theorize and show that the causal effect of overconfidence on entry is limited to skill-based markets and does not appear in those that are chance based. Moreover, we also find that, independent of confidence levels, individuals exhibit ambiguity-seeking behavior when the result of the competition depends on their skills, which, in turn, leads to higher levels of entry. This preference for ambiguity thus can explain results that have previously been attributed to overconfidence. Our results challenge existing literature that has inferred overconfidence from differential entry levels across types of markets.
We examine how the introduction of a technology that automates research tasks influences the rate and type of researchers’ knowledge production. To do this, we leverage the unanticipated arrival of an automating motion-sensing research technology that occurred as a consequence of the introduction and subsequent hacking of the Microsoft Kinect system. To estimate whether this technology induces changes in the type of knowledge produced, we employ novel measures based on machine learning (topic modeling) techniques and traditional measures based on bibliometric indicators. Our analysis demonstrates that the shock associated with the introduction of Kinect increased the production of ideas and induced researchers to pursue ideas more diverse than and distant from their original trajectories. We find that this holds for both researchers who had published in motion-sensing research prior to the Kinect shock (within-area researchers) and those who did not (outside-area researchers), with the effects being stronger among outside-area researchers.
In the wake of exogenous institutional change, organizational populations often experience a legitimacy shock. As a new institutional logic becomes dominant, old symbols and practices are delegitimated and new ones legitimated. Old symbols and practices persist into the postshock period, however, forming an ecology of diverse cohorts and audience schemas, some divergent and others convergent with the new institutional logic. Because new organizations look to their rivals for knowledge of how to cope, I examine how the shifting alignment of a rival cohort to changing audience schemas influences a new organization’s own alignment and, thus, mortality. I propose that density at founding of divergent preshock organizational cohorts early in the postshock period reduces a new organization’s mortality due to an initial endowment effect and then becomes more mortality-increasing over time as maladaptive imprints take over. Density at founding of convergent postshock organizational cohorts has a U-shaped effect on mortality—similar to that caused by a legitimacy vacuum—but this effect emerges after a delay as legitimation processes begin to dominate delegitimation processes. Also, following Red Queen theory, I argue that competitive experience with divergent organizational cohorts increases mortality, but competitive experience with convergent organizational cohorts decreases mortality. To test these arguments, I use the institutional shock of the American Civil War—during which the firearms industry of the U.S. South underwent a period of government-led command-and-control centralization—as a natural experiment. The findings are consistent with the main arguments, though the overall postshock effect of density at founding appears to be dominated by early stage endowment effects, contrary to assumptions.
Accelerators are entrepreneurial programs that attempt to help ventures learn, often utilizing extensive consultation with mentors, program directors, customers, guest speakers, alumni, and peers. Although accelerators have rapidly emerged as prominent players in the entrepreneurial ecosystem, entrepreneurs, policy makers, and academics continue to raise questions about their efficacy. Moreover, relevant organizational literature suggests that, even if accelerators are associated with better venture outcomes, results could be due to mechanisms other than learning, such as sorting or signaling. Drawing on mixed empirical methods that include proprietary data on the ventures accepted and “almost accepted” to a set of top accelerators, we find evidence that some, but not all, of the early accelerators we study substantially aid and accelerate venture development. We also find some evidence of sorting dynamics. These findings are corroborated using an auxiliary quantitative data set constructed from publicly observable data. Complementary qualitative fieldwork suggests a key driver of these accelerator effects is a novel learning mechanism we label broad, intensive, and paced consultation. The implication of these insights is that the practices of early accelerators represent a beneficial and likely replicable form of intervention that may also have relevance for independent entrepreneurs, educational programs, and corporate innovation.
We study the introduction of the private logic into a mature Italian hospital that was governed previously as a hybrid of professional and public logics. Intriguingly, the reconstituted hospital was for several years widely praised for its strong clinical and financial performance, but quickly and with little warning, it became riven by political differences that led to its demise. Through our case analysis, we develop a multilevel model that reveals the destabilizing process that can unfold when a new logic enters an established organization. We contribute to the hybrids literature by explaining the puzzle of how a new logic can become accepted and then rejected in organizations, emphasizing the critical importance of the interaction between the audience, organization, and practice levels. Crucially, we reveal that positive feedback from multiple audiences may be a mixed blessing for hybrids: although it offers resource and legitimacy advantages, it can induce internal tensions with severe destabilizing consequences. Our findings and model also run counter to two core assumptions within the institutional literature: that social endorsement is advantageous and that alignment with institutional expectations results in stabilization. We qualify these assumptions and indicate the circumstances under which they are unlikely to hold.
This study examines data from 35 countries and 24 industries to understand the relationship between gender diversity and firm performance. Previous studies report conflicting evidence: some find that gender-diverse firms experience more positive performance, and others find the opposite. However, most research to date has focused on a single country or industry and has not accounted for possible variation across social contexts. This paper advances an institutional framework and predicts that gender diversity’s relationship with performance depends on both its normative and regulatory acceptance in the broader institutional environment. Using a unique longitudinal sample of 1,069 leading public firms around the world, I find that the relationship between gender diversity and firm performance varies significantly across countries and industries owing to differences in institutional context. The more that gender diversity has been normatively accepted in a country or industry, the more that gender-diverse firms experience positive market valuation and increased revenue. These findings underscore the importance of the broader social context when considering the relationship between gender diversity and firm performance.
Leader exits at the work-unit level are prevalent, yet little attention has been devoted to understanding the impact of leader succession on employee turnover. In this paper, we draw from uncertainty-management theory to specify and test conditions under which leader exits are followed by increases (or decreases) in the turnover rates of remaining members. We theorize that leader exits disrupt the status quo and heighten remaining members’ feelings of uncertainty and propose that characteristics of the outgoing and incoming leaders help members forecast their future work situation and influence their decisions to stay or leave. Leveraging longitudinal data from 287 locations of a U.S. hospitality organization (n = 6,357 unit-month observations), we test several attributes of the succession context that moderate the effects of leader departures on both the initial change in turnover levels and the longer-term change in turnover trends. Discontinuous growth models revealed both an initial spike and a longer-term rise in voluntary turnover rates following the departure of a high-performing leader. In addition, the longer-term turnover trajectory was found to trend upward when replacements lacked role experience, when replacements were internally promoted, and when post-succession involuntary termination rates were high. Overall, we conclude that the magnitude and direction of leader-succession effects on unit turnover rates depends on uncertainty-reducing characteristics associated with both outgoing and incoming leaders.
There is a prevailing view in both the academic literature and the popular press that firms need to behave more entrepreneurially. This view is reinforced by a stylized fact in the innovation literature that research and development (R&D) productivity decreases with size. A second stylized fact in the innovation literature is that R&D investment increases with size. Taken together, these stylized facts create a puzzle of seemingly irrational behavior by large firms—they are increasing spending despite decreasing returns. There have been a number of proposals to resolve the puzzle. However, to date none of these proposals has been fully validated, so the puzzle remains. Accordingly, this paper empirically tests the proposals to see whether any resolves the puzzle. We found one proposal (use of alternative measures) was able to resolve the puzzle. When using a recent measure of firms’ R&D productivity, RQ, we found that both R&D spending and R&D productivity increase with firm size. Thus, large firms seem to be acting rationally in their increasing R&D investments, as one would expect.
Client capture is the process by which professionals become so dependent on certain clients that their professional judgment is compromised. We explore whether there are systematic differences across professionals in their likelihood of improperly biasing their judgment in the interests of clients on whom they highly depend. To do so, we examine the disclosure of prior art by patent lawyers when representing client patent applications submitted to the U.S. Patent and Trademark Office (USPTO). Lawyers are obligated professionally to disclose all relevant prior art of which they are aware even if, in doing so, their clients receive narrower intellectual property rights. We suggest that patent lawyers are generally more dependent on clients with whom they repeatedly engage and when they have numerous similar clients. We find, however, that the influence of such dependency on lawyers’ withholding prior art is greater when they have entered the legal profession through a regulatory employment revolving door. Specifically, regulatory experience as a USPTO patent examiner provides patent lawyers with unique insight that enables them to compromise their judgment on behalf of clients on whom they depend. Further, patent lawyers who are associates in their law firms are more inclined than are partners to withhold prior art on behalf of clients with whom they repeatedly engage. Because associates’ employment with their professional service firms is relatively insecure, compromising their professional judgment on behalf of clients with whom they repeatedly engage is more alluring in their efforts to enhance future employment prospects.
Innovative technology may reduce organizations’ reliance on professionals in the performance of expert tasks, weakening professions’ control over work. However, professions resist and challenge such innovation, framing it as unsafe and immoral. This paper theorizes a process by which innovative nonprofessional firms overcome the resistance, enter professionalized markets, and weaken professional control over work. It analyzes the rise of a new organizational form—retail health clinics—that deprofessionalized some medical tasks in U.S. primary healthcare. An analysis of newspaper articles, archival documents, and interviews with key industry participants suggests that retail clinic chains capitalized on long-standing jurisdictional tensions between the physician and nursing professions. The clinics operated by relying on nurses’ legal rights to perform physicians’ tasks and defended retail medicine as a safe and morally justified innovation by using the nursing profession’s established repertoire or frames and arguments. Sentiment analysis of over 1,600 newspaper articles suggests that the legitimacy of retail clinic chains in public discourse improved with the proliferation of the clinics, but the legitimacy of nurse practitioners did not. Nonprofessional firms thus introduced an innovation that weakened professional control over medical work by capitalizing on interprofessional tensions and repurposing professions’ own jurisdictional claims.
Most innovation builds closely on existing knowledge and technology, delivering incremental advances on existing ideas, products, and processes. Sometimes, however, inventors make discoveries that seem very distant from what is known and well understood. How do individuals and firms explore such uncharted technological terrain? This paper extends research on knowledge networks and innovation to propose three main processes of knowledge creation that are more likely to result in discoveries that are distant from existing inventions: long search paths, scientific reasoning, and distant recombination. We explore these processes with a combination of a large and unique data set on outlier patents filed at the U.S. Patent and Trademark Office and interviews with inventors of outlier patents. Our exploratory analysis suggests that there are significant differences in the inventor teams, assignees, and search processes that result in outlier patents. These results have important implications for managers who wish to encourage a more exploratory search for breakthrough innovation.
Although societies are becoming increasingly secularized, religion continues to play an important role worldwide. However, few studies have focused on how religion affects the entrepreneurial emergence novel markets. To address this gap, I examine the impact of Islam, as a decentralized belief system, on entrepreneurship in the context of developing Islamic investment fund markets across countries. I focus on religious diversity within Islam as an instance of intrainstitutional complexity and analyze a country-level panel dataset of Islamic investment funds in addition to complementary qualitative data. Intriguingly, I find that religious diversity within Islam plays a paradoxical role: it promotes the entrepreneurial supply of Islamic investment funds in a country, but it also reduces the investor demand for these funds. This complex effect is moderated by interinstitutional forces: the market logic positively moderates the effect on supply dynamics, whereas the state logic negatively moderates the effect on supply and positively that on demand. This study contributes to the research on religion and market emergence, institutional complexity, and Islamic finance.
We ask how social similarity between start-up founders and venture capitalists (VCs) influences VCs’ pricing decisions and returns on investments. We conceptualize how regional and caste similarity, two salient aspects of social similarity in India, affect two distinct aspects of deal pricing: premoney valuation and investors’ downside risk protection in the Indian venture capital market. We theorize that VCs reflect the benefits and costs of social similarity by setting higher premoney valuation when investing in companies led by socially similar founders while also minimizing their downside risks in these investments. We expect that social similarity’s impact on pricing is amplified when VCs face greater subjective uncertainty, such as for early-stage deals or if the VCs lack expertise in the start-up company’s product market. Finally, we claim that VCs achieve superior returns on investments when their deal pricing accurately reflects the impact of social similarity. We tested our conceptual model using both parametric and nonparametric methods on a hand-collected data set of all deals that occurred during 2005–2012, and we supplemented our analyses with in-depth, qualitative interviews that contextualize our findings. The pattern of findings on regional similarity are consistent with our model, but the effects of caste in our data are theoretically anomalous. Post hoc analyses to resolve the anomaly suggest an “intrinsic quality” mechanism, whereby higher-caste VCs set higher valuations when matching with lower-caste founders that signal high quality. Overall, our findings offer evidence that VCs incorporate social attributes into deal pricing in nuanced yet boundedly rational ways.
New entrants in established markets face competing recommendations over whether it is better to establish their legitimacy by conforming to type or to differentiate themselves from incumbents by proposing novel contributions. This dilemma is particularly acute in cultural markets in which demand for novelty and attention to legitimacy are both high. We draw upon research in organizational theory and entrepreneurship to hypothesize the effects of pursuing narrow or broad appeals on the performance of new entrants in the music industry. We propose that the sales of novel products vary with the distance perceived between the classes being combined and that this happens, in part, because combinations that appear to span great distances encourage consumers to adopt superordinate rather than subordinate classes (e.g., to classify and evaluate something as a “song” rather than a “country song”). Using a sample of 144 artists introduced to the public via the U.S. television program The Voice, we find evidence of a U-shaped relationship between category distance and consumer response. Specifically, consumers reward new entrants who pursue either familiarity (i.e., nonspanning) or distinctive combinations (i.e., combine distant genres) but reject efforts that try to balance both goals. An experimental test validates that manipulating the perceived distance an artist spans influences individual evaluations of product quality and the hierarchy of categorization. Together these results provide initial evidence that distant combinations are more likely to be classified using a superordinate category, mitigating the potential confusion and legitimacy-based penalties that affect middle-distance combinations.
We examine the managerial delegation decisions of foreign entrepreneurs and assess how these decisions are shaped by characteristics of the local product and labor market environment. We argue that actual or perceived home bias in court proceedings leads foreign entrepreneurs to place little reliance on formal contracts in their dealings with local agent-managers. Adopting the lens of relational contract theory, we develop hypotheses linking managerial delegation decisions to market conditions associated with stable self-enforcing agreements and test the hypotheses in the context of post-Soviet Russia. Consistent with our arguments, we find that foreign entrepreneurs are more likely to hire an agent-manager in local markets where industry growth creates a substantial “shadow of the future,” where managers’ outside employment options are relatively limited, and where competition and the variability of returns are not so high as to induce defection from an informal agreement. Similar observations on a sample of Russian-owned entrepreneurial firms suggest that these delegation decisions are relatively insensitive to local market conditions but that they are influenced by the density of local reputation networks. Our study thus contributes to understanding of the distinctive features of foreign entrepreneurs’ managerial delegation decisions and reinforces the view that contracting impediments constitute one important aspect of the “liability of foreignness” for entrepreneurial firms.
The relationship between regionally tied institutional logics and the location of organizations is an important issue in organization theory. Recent work highlights how supportive regional logics can give rise to products or organizations that resonate with these logics and how the geographic patterns that underlie industries may be understood by examining such relationships. This literature has not, however, offered deep attention to the ways in which features of technology—specifically, its inherent uncertainty—may interact with such dynamics. In this paper, we tackle the challenge. Our work examines how the level of support for an environmental-conservation logic within a region is associated with the number of wind and solar equipment manufacturers in that region in the years 1978–2006. By simultaneously exploring the effects of this logic on two similar technologies, our work not only reinforces how logics may interact with organizational activity but also shows how the magnitude and mechanisms of this effect depend on the technology in question. We build on these findings to discuss the importance of examining technologies in detail, including their dimensions of uncertainty, the role of timing in examining the effect of regionally tied logics, and the links between public policy and logics.
To investigate how firms engage in forward-looking action, we examined the processes by which a pioneering firm actively influenced the future of its industry over five decades. From our longitudinal field study, we generated a process model of strategy making that helps to explain how firms work to shape the future in some preferred fashion. Specifically, we describe our findings on shaping-oriented forward-looking strategy making in terms of “artificial evolution” processes—interventions by which a firm’s leaders challenge the status quo and leverage the internal ecology of the organization to nudge the evolution of the business landscape toward a preferred direction. This is distinct from the more conventional and commonly invoked natural selection processes that describe how firms adapt to markets or unintentionally shape them. These findings on strategy making as akin to artificial evolution complement and extend the traditional view of strategic management, which has historically focused on processes anchored in models of search and adaptation. Our findings also shed light on an exceptional mode of strategy making—one that goes beyond concerns of firm survival and competitive advantage, and tackles societal grand challenges. By accounting for constructivist, forward-looking dimensions of strategic agency, our findings also contribute to the microfoundations of strategic decision making and to organization theories, more generally.
We argue that strong indirect ties are conducive to the transfer of private information, which provides an advantage in identifying profitable investment opportunities. In our context, a strong indirect tie is generated between an investor and a focal firm if the investor was a limited partner of the focal firm’s lead venture capital fund. We suggest that an investor can access private information on the focal firm’s underlying value through its strong indirect tie to the focal firm via the focal firm’s lead venture capitalist. Supporting our theory, we show that after the focal firm’s initial public offering, the investor with a strong indirect tie to the focal firm receives high risk-adjusted return when the investor chooses to invest in the focal firm’s stock in the stock exchange market. We also show that the investor’s private information attained through its strong indirect tie to the focal firm is more valuable (i) when there is higher exogenous market uncertainty and (ii) when the investor faces higher information asymmetry.
Research examining board efficacy often focuses on oversight and monitoring, particularly as evidenced by the sensitivity of chief executive officer (CEO) compensation to prior firm performance. In this study, we adopt an alternative perspective on CEO compensation—specifically over/underpayment, or the extent to which a CEO’s initial compensation is above or below prevailing market norms—that allows us to assess a board’s efficacy via the accuracy of its initial CEO selection and compensation decisions. We build on and extend human capital theory to argue that boards make initial CEO compensation decisions based a range of manifestations of CEO human capital (that are both observable and unobservable to outsiders) and that initial over/underpayment represents an implicit assessment of underlying CEO quality. Using a sample of 766 CEOs, we relate initial over/underpayment to subsequent CEO career performance. Our results show that this core relationship is positively significant and economically meaningful. Thus, U.S. public company boards, as a group, do tend to be making broadly accurate initial predictions regarding the underlying capabilities of new CEO hires. This relationship is amplified in situations where board assessments of CEO human capital are more unequivocal (greater current versus prospective compensation) and when CEO human capital can be expressed most comprehensively (high managerial discretion). In supplemental analyses we show that these relationships fundamentally changed following the implementation of the Sarbanes–Oxley Act, suggesting that boards may be performing this important aspect of their governance role more effectively in recent times. We also find that our results are not symmetric—rather, they are strongest in situations where initial compensation is midrange or lower; high levels of initial overpayment are not associated with commensurate levels of career performance. Finally, we consider and account for a range of alternative explanations for our central finding.
We investigate the impact of financial restatements as critical events that influence board interlock formation among Fortune 500 firms during the 2009–2013 period. Our empirical study is based on a longitudinal analysis of tie formation while accounting for dynamic changes in the behavior and characteristics of network nodes using stochastic actor-oriented models. We find that firms facing financial restatements experience disruption in network ties. However, social status helps mitigate these effects, and restating firms build new ties through socially embedded processes, such as reciprocity and transitivity. Our work contributes to the understanding of how interorganizational relationships are altered as a result of financial restatement events.
We theorize that vicarious learning theory provides a framework for understanding how small- and medium-sized start-ups can learn from the activity of a variety of regional actors, not just from the activity of colocated peer firms (i.e., other start-ups). Furthermore, we suggest that the magnitude of the impact of vicarious learning is influenced by a firm’s own specific experience with a variety of actors. We use longitudinal data of the population of German biotechnology start-ups and pharmaceutical multinational corporations (MNCs) between 1996 and 2015 across 19 German biotechnology regions. We show that colocated start-ups’ international expansion is positively impacted by the regional network centrality of colocated MNCs and that this relationship is moderated by a start-up’s direct alliance experience with these entities. Our results highlight how important it is for researchers to differentiate the distinct and separate influences a wide variety of actors have on vicarious learning to more clearly identify outcomes of this influence. We also provide evidence that the influence of MNCs is heterogeneous and depends on whether MNCs are domestic or foreign and on their R&D intensity, yet find that country of origin has no significant influence. Our study makes a number of contributions, one of which is research on alliances, supporting conflicting arguments on the subsequent impacts of experience. We further find that certain types of alliance experience may not be transferrable to induce start-ups’ future international expansion, and in some cases may even hinder it.
Studies have demonstrated that foreign firms locate where immigrants from their home countries reside and have suggested that doing so can improve performance. We argue that to properly assess how immigrants impact the performance of co-national firms, research must account for heterogeneity in how independent foreign firms (owned by individual foreigners) versus multinational corporation (MNC) subsidiaries (owned by a foreign corporate parent) benefit from immigrant communities. Independent firms have a greater need for resources from the immigrant community and depend more on their individual managers’ personal connections within the community to obtain such resources. Subsidiaries of MNCs can instead rely on the impersonal organizational resources of their parent firm (e.g., brand, reputation, channels) to access valuable immigrant community resources. Using data on foreign firms in Russia during 2006–2011, we find that immigrants improve the profitability of co-national independent firms only if they are managed by immigrant chief executive officers (CEOs), whereas co-national MNC subsidiaries profit from immigrants regardless of their CEOs’ nationality. Our study suggests that although organizations benefit from the resources of co-national immigrant communities in foreign markets, the means by which they activate them—personal or impersonal—systematically vary across different types of firms.
Early-stage experiments are central to the design-thinking approach to organizational innovation, and they are also a core practice in evidence-based management. Organizations use experiments to test new strategies in a low-stakes setting before escalating their commitment to a new initiative. Yet organizations also use experiments to evaluate managers who will implement these strategies in a high-stakes setting. I develop a formal model to demonstrate that these two types of evaluations are fundamentally incompatible. Managers who fear replacement in response to a poor experimental outcome pervert their experiments to maximize the likelihood that they succeed. This saps early experiments of much of their informational value. I show that if an organization can observe a manager's experimental strategies and the experiment’s outcomes and can commit ahead of time to an evaluation and replacement rubric, then it can resolve this agency problem. However, if a manager's experimental strategy cannot be credibly communicated, the organization will either replace good managers to induce a desirable experimental strategy or retain bad managers to alleviate the fear of replacement. I show that in low-uncertainty environments, the control exerted by the former replacement regime offers the best results, whereas, in high-uncertainty environments, commitment to retention regardless of experimental outcomes is best.
This study examines how pluralistic organizations confronting fundamental differences in values can proceed with strategic change. By drawing on a longitudinal case analysis of strategic change in a Nordic city organization, we show how the proponents and challengers play a “rhetorical game” in which they simultaneously promote their own value-based interests and ideas and seek ways to enable change. In particular, we identify a pattern in which the discussion moved from initial contestation through gradual convergence to increasing agreement. In addition, we elaborate on four rhetorical practices used in this rhetorical game: voicing own arguments, appropriation of others’ arguments, consensus argumentation, and collective we argumentation. By so doing, our study contributes to research on strategic change in pluralistic organizations by offering a nuanced account of the use of rhetoric when moving from contestation to convergence and partial agreement. Furthermore, by detailing specific types of rhetorical practices that play a crucial role in strategy making, our study advances research on the role of rhetoric in strategy process and practice research more generally.
Irony is an effective means of dealing with controversy in organizations, but there is a paucity of knowledge of the various ways in which irony helps managers to do so without necessarily ‘solving’ those issues. By drawing on discursive incongruity theory, we examine the use of irony when managers are confronted with controversial issues in a multinational company. As a result, we identify and elaborate on four distinctively different pathways of how irony helps participants to move on: ‘acquiescing’ (framing understanding as having no alternative because of environmental constraints), ‘empowering’ (synthesizing a view through broad inputs from different individuals), ‘channelling’ (subsuming other interpretations under a single and often dominant view) and ‘dismissing’ (rejecting alternative interpretations and often reinforcing the status quo). On this basis, we develop a theoretical model that elucidates the process dynamics in dealing with and moving on with controversial issues and elaborates the specific characteristics of each of these four pathways. Our analysis also leads to a fuller understanding of the discursive underpinnings and intersubjective dynamics in irony use in organizations.
This research addresses the important question of how organizations can use financial incentives to influence the work tasks of their professional workforce—a constituency that is notoriously difficult to manage because of their specialized knowledge, considerable autonomy, strong socialization, and powerful professional norms. In particular, I explore how a baseline incentive effect is moderated by two features of professionals’ tasks and jurisdictions: jurisdictional dominance (i.e., how much the profession controls the provision of the task relative to other professions) and jurisdictional prominence (i.e., how commonly provided the task is within a profession relative to other tasks). Using data on thousands of physician tasks from Ontario, Canada, and a difference-in-differences empirical design, I find that professionals’ incentive responses are smaller when a profession has higher jurisdictional dominance over a task, but are larger when the task has higher jurisdictional prominence within the profession. This research contributes to the literature on professions and professionals in multiple ways. First, I introduce the concepts of jurisdictional dominance and jurisdictional prominence, distinguishing them from each other and from existing conceptions of professional control. Second, this study shows that financial incentives can be an effective tool for influencing professionals, but highlights that their efficacy is shaped by a task’s jurisdictional dominance and jurisdictional prominence. Finally, I show that these new conceptions of jurisdictional control influence professionals’ behaviors in meaningful ways and should therefore be considered in future studies of professions.
In this article, we develop a process model that specifies how managers come to understand and approach the evaluation of merit in the workplace. Interviews from a diverse sample of managers and from managers at a U.S. technology company, along with supplemental qualitative online review data, reveal that managers are not blank slates: we find that individuals’ understandings of merit are shaped by their (positive and negative) experiences of being evaluated as employees prior to promotion to management. Our analysis also identifies two distinct managerial approaches to applying merit when evaluating others: the focused approach, in which managers evaluate employees’ work actions quantitatively at the individual level; and the diffuse approach in which managers assess both employees’ work actions and personal qualities, quantitatively and qualitatively, at both the individual and team levels. We further find that, as a result of their different past experiences as subjects of evaluation, individuals who experience mostly negative evaluation outcomes as employees are more likely to adopt a focused approach to evaluating merit, whereas individuals who experience mostly positive evaluation outcomes are more likely to adopt a diffuse approach. Our study contributes to the scholarship on meritocracy and workplace inequality by showing that merit is not an abstract concept but a guiding principle that is produced and reproduced over time based on individuals’ evaluation experiences in the workplace.
We study how the cognitive complexity of chief executive officers (CEOs) changes during their tenures. Drawing from prior theory and research, we argue that CEOs attain gradually greater role-specific knowledge, or expertise, as their tenures advance, which yields more complex thinking. Beyond examining the main effect of CEO tenure on cognitive complexity, we consider three moderators of this relationship, each of which is expected to influence the accumulation of expertise over a CEO’s time in office: industry dynamism, industry jolts, and CEO positional power. We conduct our tests on a sample of 684 CEOs of public corporations. The analytic centerpiece of our study is a novel index of CEO cognitive complexity based on CEOs’ language patterns in the question-and-answer portions of quarterly conference calls. As part of our extensive theory of measurement, we provide evidence of the reliability and validity of our index. Our results indicate that CEOs, in general, experience substantial increases in cognitive complexity over their time in office. Examined moderators somewhat, but modestly, alter this general trajectory, and nonlinearities are not observed. We discuss the implications of our findings.
We examine how entrepreneurs acquire financial resources for their early-stage ventures from distributed non-professionals via crowdfunding. Through an inductive analysis of entrepreneurs’ successful and unsuccessful non-equity crowdfunding campaigns, we derive a holistic framework of community-based resource mobilization. Our framework consists of three distinct processes entrepreneurs use to attain financial capital from non-professional resource providers over time: community building to establish psychological bonds with individuals possessing domain-relevant knowledge, community engaging to foster social identification with existing resource providers, and community spanning to leverage proofpoints with intermediaries who can help orchestrate resource mobilization with broader audiences. Entrepreneurs’ enactment and temporal sequencing of these three processes distinguish successful versus unsuccessful resource mobilization efforts in a crowdfunding setting. Community building is used by successful entrepreneurs primarily prior to a campaign’s launch, community engaging is used throughout a campaign, and community spanning is most effectively used after achieving a campaign’s initially-stated funding goal. This study empirically illustrates and theoretically conceptualizes the dynamics of resource mobilization in a crowdfunding setting.
Little is known about when and where hybrid organizations diffuse. We argue that neo-institutional perspectives, which stress the constraining role of market categories and institutional logics, have to be complemented with demand-side perspectives that stress the enabling force of economic incentives to explain the origins of hybrids. We develop theory to predict the country-level diffusion of hybrid forms in Islamic banking in the 1975–2017 period, during which many conventional banks invaded the domain of Islamic banking by starting to sell Islamic banking services, or so-called “Islamic windows.” Our findings underscore the relevance of simultaneously studying the impact of constraining and enabling forces. Consistent with neo-institutional theory, we find strong evidence that a lack of constitutive legitimacy of the window form—only in countries where Muslims make up a large share of the population—and the ideological polarization of local audiences reify the ideological boundaries between the oppositional banking logics, which in turn hampers the diffusion of windows in the focal country. At the same time, however, it appears that the failure of local credit markets and country-level economic globalization, the latter even more in countries with a Muslim majority, provide potent economic incentives for the diffusion of windows. By stressing the role of utilitarian incentives and material exchange as drivers of hybridization, we bridge the gap between neo-institutional and more rationalist approaches of institutional change.
Although the relationship between competition and firm innovation has long been of scholarly interest, prior research has predominantly considered changes in internal research and development (R&D) as a strategic response to competitors’ actions. In this study, we focus on one of the most important and commonly observed contractual mechanisms used to acquire external technologies: technology licensing. Surprisingly, licensing has been mostly overlooked by prior studies examining the effect of competition on firms’ allocation of R&D. We take into account the unique properties of licensing and systematically link them to the demands arising from the competitive pressure caused by rivals’ launches of new products. Furthermore, we discuss how licensing-in decisions ultimately shape a firm’s subsequent innovation in areas where they are threatened by competitors and how such innovation depends on the cumulative R&D investments inside the organization into which licensed knowledge is added. We test our theoretical model through a longitudinal design that tracks the licensing-in and innovation outcomes of firms in the global biopharmaceutical industry. Accounting for the endogenous selection of firms into licensing, our findings illustrate that licensing-in is motivated by competitive pressures. We also find that licensing-in increases a firm’s capacity to innovate in areas where competitors have exerted pressure, particularly in the presence of cumulative R&D investments. In so doing, the paper anchors technology licensing as a key organizational action that helps increase our understanding of the important relationship between competition and innovation.
This virtual special issue (VSI) collects together 19 papers published in Organization Science that explore how organizations learn from crises. The objective is to discuss insights that can help us understand the COVID-19 pandemic crisis, implications that existing research carries for organizations’ abilities to keep hard-earned lessons after the storm passes, and opportunities that the current phenomenon offers for future inquiry in this domain. Organizations, large and small, in scores of countries, have suspended normal operations. To survive, many organizations have adapted by shifting almost all human-to-human interactions online while facing an ethical dilemma and a tense tradeoff between public health and economic well-being. We take stock of the research on organizational learning from crises, summarize useful knowledge for managing the current crisis, and provide directions for future research.
Several case studies suggest that firms targeting mass market services can align profitability with jobs offering a living wage, stable schedules, and engaging work. Yet, few do. To understand this puzzle, we draw on theories of firms as systems of interdependent choices. Building on a few cases, we map the processes connecting managerial choice to performance and formalize the resulting performance landscape. In a strategy space defined by two dimensions—task richness and compensation—two local profitability peaks emerge: one with low compensation and low task richness and one with high compensation and high task richness. The bimodal landscape results from complementarity among choices and is robust when the strategy space is expanded from two to six dimensions and under many alternative parameterizations. Exploring how firms discover, move to, and remain at the high-compensation–high-task richness peak, we find three challenges to this strategy: (a) contextuality—adoption, imitation, and replication are harder for strategies that rely on interdependences among components and thus, require significant customization for each context; (b) temporal complexity—strategies depending on long-term and synergistic investments and slow-moving reinforcing feedbacks are hard to learn owing to misleading performance feedback; and (c) variable demand with no inventory buffers—efforts to adjust labor supply to highly variable demand in services often lead to unstable schedules given with short notice that drive quality employees away and compromise the strategy. These mechanisms can undermine promising strategies even if the actual performance landscape includes a small number of local peaks.
This study examines how constraining a firm’s ability to adjust resources affects innovation and underscores a firm’s competitive position as a critical contingency to the competing demands for flexibility and efficiency. In response to losing competitiveness, lagging firms must release obsolete resources and increase experimentation with new resources. Limiting the pace and efficiency at which they can do so impedes their ability to innovate and challenge leading firms. I explore these ideas empirically by exploiting the staggered adoption of employment protection laws. Employment protection indeed reduces innovation by lagging firms, driven by a decrease in radical innovations. In contrast, I find a small yet positive effect on leading firms in low-velocity sectors, which prioritize the efficient use of existing resources. This study extends the prior contingency approach based on industry and task characteristics to incorporate a firm’s competitive position and provides a much more dynamic account of when and how firms experiment with new inventors, resources, and radical innovations.
Over the past two decades, organizations have established sanctioning systems as an important component of their ethical infrastructures to detect and punish wrongdoing. However, empirical knowledge about the overall effectiveness of such systems remains limited. Existing studies have mostly adopted a single-party perspective even though many wrongdoing situations involve dynamic multiparty interactions between actors, recipients, and observers of wrongdoing. Moreover, most existing research has emphasized an economic perspective—that sanctioning systems only affect behavior because of economic considerations while crowding out ethical ones. In this research, we develop a moral and normative perspective of sanctioning systems. Using a novel experimental game design, our study focuses on the investigative dimension of sanctioning systems to examine their psychological and behavioral effects in actor–recipient–observer wrongdoing interactions. Findings reveal that investigative sanctioning systems influence wrongdoing, reporting, and helping behaviors as well as alter ethical and normative considerations, such that as systems become stronger, wrongdoing behaviors are judged as more unethical and perceived as less typical than when weaker systems are in place. These moral judgments and norm perceptions mediate the effect of investigative sanctioning system strength on wrongdoing behavior. Our research extends previous empirical and theoretical work on sanctioning systems by applying a more holistic perspective and by demonstrating that highly effective systems can serve as important behavioral guides because they activate and alter moral and normative considerations about wrongdoing.
We investigated how abusive supervision influences interactions between third-party observers and abused victims and hypothesized when and why third parties react maliciously toward victims of abusive supervision. Drawing on the theory of rivalry, we predicted that third-party observers would experience an “evil pleasure” (schadenfreude) when they perceive a high level of rivalry with the victims of abusive supervision and that the experienced schadenfreude then would motivate third parties to engage in interpersonal destructive behaviors (i.e., undermining, incivility, and interpersonal deviance) toward the victims. We further proposed that such malicious reactions would be attenuated if groups have a high level of cooperative goals. Results based on one experimental study and two time-lagged field studies lend support to our propositions.
Despite the common portrait of leadership as a worthy, needed, and frequently rewarded endeavor, individuals do not always step up to lead as informal leaders in their teams. In the present research, we introduce the idea of leadership risk perceptions, arguing that individuals sometimes see risks for themselves if they step up to lead. We conceptualize three types of leadership risk perceptions (interpersonal, image, and instrumental) and investigate how changes in these risk perceptions over time impact the overall level of informal leadership that individuals contribute in their teams, as well as how these risk perceptions’ change trajectories are shaped by the level of conflict in a team. To address these issues, we conducted a series of studies, exploring the relevance of the three risk perceptions qualitatively, developing measures for them, and then testing our hypotheses in a field study following 454 individuals working in 89 master of business administration (MBA) consulting teams. We found that a decrease in an individual’s leadership risk perceptions over time was related to the individual’s overall informal leadership contributions in the team, though the pattern of relationship was not the same for all three risk perceptions. Furthermore, in teams with higher levels of relationship conflict, interpersonal and image risk perceptions decreased less over time, suggesting the importance of the social context in shaping perceived risks. Overall, this research calls attention to the much under-investigated risky side of leadership and highlights a temporal approach to understanding the impact of leadership risk perceptions.
Transaction costs strongly influence diversification dynamics, as predicted by resource theory. A mainstream view links the profitability of diversification with the existence of transaction costs that prevent a firm from trading in fungible, scale-free resources. This study applies a neo-Penrosian perspective to transaction costs, with the notion that diversification may be driven by the redeployment of non-scale-free resources. An empirical analysis, using tax changes in the drink sector as a measure of exogenous demand variation, offers results consistent with the prediction that redeployment is particularly relevant when retailing is concentrated and single-product competition within a focal product niche (e.g., beer) is fragmented. This study also measures redeployment across a portfolio of a multiniche firm when changes in its sales-growth rates for a particular product niche might imply contrasting changes in other product niches. The resulting evidence is consistent with predictions that demand uncertainty, and transaction costs create viable redeployment opportunities for multiniche companies.
When outsourcing design tasks, firms want their suppliers to be both efficient and adaptive. Whereas efficiency is necessary to reap economic gains from outsourcing, adaptation is required to deal with interdependencies as the design evolves. Achieving both objectives simultaneously, however, is difficult because procurement contracts require a trade-off between providing incentives for efficiency and facilitating adaptation. In the presence of formal contracts that provide strong incentives for efficiency, ensuring adaptation thus requires effective relational contracts between the buyer and the supplier. But because the focus of prior research has been on dyadic buyer–supplier relationships, it is unclear how the efficiency–adaptation trade-off can be mitigated in the multitier supplier systems that are common in many industries. Addressing this gap, we argue that in hierarchical supplier systems, relational contracts between contractual partners become more important, but at the same time harder to establish, than in single-tier supplier systems. An in-depth case study of the adaptive frictions that arose in the Airbus A350 program allows us to illustrate this challenge of tiered outsourcing. Moreover, we show how Airbus came to resolve the frictions by leveraging skip-level ties—direct informal contacts to lower-level suppliers with which no contractual relationship existed, thus replacing the archetypal notion of a supplier hierarchy by a more complex relationship structure. We discuss the boundary conditions of our findings and suggest propositions for the emergence of skip-level ties in tiered outsourcing.
A long-standing question in the organizations literature is whether firms are better off by using simple or complex representations of their task environment. We address this question by developing a formal model of how firm performance depends on the process by which firms learn and use representations. Building on ideas from cognitive science, our model conceptualizes this process in terms of how firms construct a representation of the environment and then use that representation when making decisions. Our model identifies the optimal level of representational complexity as a function of (a) the environment’s complexity and uncertainty and (b) the firm’s experience and knowledge about the environment’s deep structure. We use this model to delineate the conditions under which firms should use simple versus complex representations; in doing so, we provide a coherent framework that integrates previous conflicting results on which type of representation leaves firms better off. Among other results, we show that the optimal representational complexity generally depends more on the firm’s knowledge about the environment than it does on the environment’s actual complexity. We also show that the relative advantage of heuristics vis-à-vis more complex representations critically depends on an unstated assumption of “informedness”: that managers can know what are the most relevant variables to pay attention to. We show that when this assumption does not hold, complex representations are usually better than simpler ones.
This paper explores the everyday practices, forms, and means by which employees mobilize national identity as a tool of resistance in opposing managerial demands of their dual, global/Western and local/Japanese, organizational identity. Drawing on an ethnographic study of a Japanese subsidiary of a multinational corporation, we show how employees use national identity to invoke three forms of othering in constructing various resistant identities: individual employees’ resistant identities through verbal othering, expressed in employees’ talk; departmental resistant identities through spatial othering, referring to employees’ use of space; and subsidiary resistant identity through ritual othering, illustrating employees’ collective use of ritual practices and symbolic artifacts. Our study makes three significant theoretical contributions: First, by illustrating the ways and means by which employees take on different national identities to construct diverse and often contradictory resistant identities to their expected dual organizational identity, we highlight the changeable nature of national identity. Second, this study contributes to our understanding of contextual constituents that shape individuals’ identity-related resistance. By unraveling employees’ various resistance forms, we show how resistance dynamically takes on assorted manifestations according to the organizational level in which it occurs and the managerial demands being resisted. Third, we illustrate the constitutive resources of resistance by highlighting the diverse means used by employees to construct their resistant identities.
Because new technologies allow new performances, mediations, representations, and information flows, they are often associated with changes in how coordination is achieved. Current coordination research emphasizes its situated and emergent nature, but seldom accounts for the role of embodied action. Building on a 25-month field study of the da Vinci robot, an endoscopic system for minimally invasive surgery, we bring to the fore the role of the body in how coordination was reconfigured in response to a change in technological mediation. Using the robot, surgeons experienced both an augmentation and a reduction of what they can do with their bodies in terms of haptic, visual, and auditory perception and manipulative dexterity. These bodily augmentations and reductions affected joint task performance and led to coordinative adaptations (e.g., spatial relocating, redistributing tasks, accommodating novel perceptual dependencies, and mounting novel responses) that, over time, resulted in reconfiguration of roles, including expanded occupational knowledge, emergence of new specializations, and shifts in status and boundaries. By emphasizing the importance of the body in coordination, this paper suggests that an embodiment perspective is important for explaining how and why coordination evolves following the introduction of a new technology.
Network affiliations have been extensively investigated as a way for new entrants to establish a foothold in markets. A commonly invoked mechanism is that of signaling, whereby affiliations provide exposure and improve a newcomer's odds of success. Our paper highlights a second mechanism that we argue is especially relevant in cultural markets: how a new entrant's perceived distinctiveness varies based on its affiliations. Leveraging data on music concerts, interviews of musicians, and biographical and genre information from archival sources, we investigate the effects of early affiliations for 1,385 bands formed between 2000 and 2005. In particular, we consider whether a new band benefits from appearing with a high-status act. If the main value is signaling, then a newcomer would be better off opening for high status bands, because doing so maximizes both legitimacy and exposure. However, in such conditions entrants run the risk of not being allocated enough attention, and thus not being seen as sufficiently distinctive. While the literature typically emphasizes the positive role of signaling, we find that our results more strongly support the notion of distinctiveness: new bands that frequently appeared with high-status artists made less money and were more likely to subsequently dissolve. This suggests that social network approaches to cultural markets need to better incorporate how network position affects a newcomer's opportunity to be recognized as distinctive.
The mechanisms by which social networks and organizational vocabularies combine jointly to affect communication patterns across organizational boundaries remain largely unexplored. In this paper, we examine the mutually constitutive relation between the network ties through which organizational members communicate with each other and the vocabularies that they use to describe their organization. We suggest that the dynamic structure of social networks and organizational vocabularies is contingent on the formal design of organizational subunits. Within subunit boundaries, members who interact with each other are more likely to develop similar vocabularies over time. Interestingly, between subunits, the more two members share similar organizational vocabularies, the more likely they are to form a tie over time. We find empirical evidence for these arguments in a longitudinal study conducted among the managers of a multiunit organization. Organizational vocabularies, we suggest, may sustain communication patterns across organizational boundaries, thus bridging cultural holes within organizations.
We explore how minority- and women-owned suppliers lacking hard power manage asymmetric relationships with larger, more powerful buyers in the context of supplier diversity relationships. We examine how these suppliers create and use soft power to manage the opportunities and challenges they encounter trying to maintain their positions in large buyers’ supply chains. We find that these easily substitutable firms use a variety of information sources to identify and make themselves cognitively central to individuals inside and outside the buyer organizations who can serve as functional and political influencers. They then employ these influencers to affect the buyer’s decisions when their position in the supply chain is threatened, largely without the buyer noticing. Our study contributes to the literatures on the use of soft power buyer-supplier power relationships and supplier diversity.
Viewing entrepreneurship as a form of collective action, this paper investigates the tension between an entrepreneurial team’s reliance on collective efforts for achieving success and individual members’ tendencies to withhold their personal resources. We argue that the precarious nature of the early founding stage and the difficulty of redeploying some resources for other uses amplify the risk of early-stage resource contributions and may lead to team members withholding resources or even free riding. Two conditions may help overcome such collective action problems: adopting a formal contract to specify rewards and sanctions and encouraging reciprocal exchange among team members through the lead entrepreneur’s voluntary contributions. Analyzing a nationally representative multiwave panel study of entrepreneurial teams in the United States, we show that early-stage team members are reluctant to provide resources tailored to the business, even though such resources are critical to venture survival. We find that presigned formal contracts and founding entrepreneurs’ initial contributions make members’ contributions of such resources much more likely. Lead entrepreneurs’ voluntary contributions to their businesses, signified by their provision of resources that impose high risks on themselves but increase the viability of the business, help mitigate collective action problems within entrepreneurial teams.
Although the diversity of cultural expectations for different corporate governance practices has been acknowledged, our understanding of how companies use cultural differences to legitimize their governance practice choice and facilitate their resource acquisition remains limited. Building on the literature on cultural entrepreneurship, we theorize how foreign-listed firms engage in global framing in tailoring the description of their new governance practice to the host country investors. Our empirical study examines the relative emphasis of monitoring over resource-providing roles by the U.S.-listed Chinese companies’ independent directors when their roles are understood differently in the home and host countries. Our study finds that exposure to the alternative cultural repertoire of a host country via overseas education of board members and foreign institutional ownership enhance a firm’s organizational resources available to engage in global framing. The likelihood of global framing, however, is constrained by the home country’s institutional environment, which is characterized by local business history and connections to strong local resource providers such as the state. We also find that the effectiveness of global framing to obtain investor recognition in the host country is restricted when the company lacks the capacity to implement the declared role of independent director-as-monitor in its home country.
Novel external partnerships are valuable but risky, and scholars have examined the organization- and individual-level determinants of firms’ decisions to pursue these new relationships. Yet, in organizations performing complex and knowledge-intensive work, decisions about interorganizational relationships are often made within teams. We characterize these decisions as a two-stage process in which a team member proposes a partner and other team members respond, supporting or challenging the proposal. As novel partnerships are risky, and power is a key determinant of risk-taking propensity, we argue that the power of team members—both those who initiate proposals and those who respond—will shape the likelihood that the team will pursue a novel external partnership. Using personnel data from project teams in an automated equipment design and build firm, we find that the effect of power on the likelihood of novel partner adoption depends on both the type of power and the role of the person in the decision process. Novel partner selection is more likely when those initiating proposals hold formal structural power but less likely when initiators hold informal power. Both the formal and informal power of the initiator’s teammates attenuate the effect of initiator power, such that the more power one’s teammates have, the less one’s own power will affect the likelihood of novel external partner selection. Finally, we provide evidence that these effects on likelihood of novel partner adoption are as materially consequential for project outcomes as other strategic choices available to project teams. These findings have implications for the intraorganizational determinants of interorganizational networks.
A key assumption in past literature has been that human services workers become emotionally distant from their charges (such as clients or patients). Such distancing is said to protect workers from the emotionally draining aspects of the job but creates challenges to feeling and behaving compassionately. Because little is known about when and how compassion occurs under these circumstances, we conducted a multiphased qualitative study of 119 correctional officers in the United States using interviews and observations. Officers’ accounts and our observations of their interactions with inmates included cruel, disciplinary, unemotional, and compassionate treatment. Such treatment varied by the situations that officers faced, and compassion was surprisingly common when inmates were misbehaving—challenging current understanding of the occurrence of compassion at work. Examining officers’ accounts more closely, we uncovered a novel way that we theorize human services workers can be compassionate, even under such difficult circumstances. We find that officers describe engaging in practices in which they (a) relate to others by leveling group-based differences between themselves and their charges and (b) engage in self-protection by shielding themselves from the negative emotions triggered by their charges. We posit that the combined use of such practices offsets different emotional tensions in the work, rather than only providing emotional distance, and in doing so, can foster compassionate treatment under some of the most trying situations and organizational barriers to compassion.
To better understand the origins of sustained superior firm performance, we consider processes of entrepreneurial creation and discovery. Discovering new opportunities requires speed and flexibility, and although many discoveries are easy to implement, this makes them easy to imitate. By comparison, creation is more complex, iterative, and cumulative, making these opportunities slower to implement yet harder to copy. In formal simulation modeling and analyses of the U.S. Compustat population from 1966 to 2015, we found a bimodal pattern of entry into golden eras of sustained superiority. One peak consisted of an initial spike as firms went public, and evidence suggests this was driven by entrepreneurial discovery. The other peak involved a long-term inverted U-shaped trajectory that reached its high point about 17 years later. Evidence suggests this was driven by entrepreneurial creation, which first enabled then later constrained durable superiority. Our results have implications for research on sustained superiority, entrepreneurial opportunities, and strategic renewal.
Becoming a manager is generally seen as a highly coveted step up the career ladder that corresponds to a gain in responsibility. There is evidence, however, that some individuals experience “managerial blues,” or disenchantment with their managerial jobs after being promoted. Although past scholarship points to individual differences (such as skills inadequacy) or the promotion circumstances (such as involuntary) as possible explanations for such blues, less is known as to how the expectations that people carry with them from past jobs—such as expectations about what responsibility entails—may shape their first managerial experience. To answer this question, we compare the experiences of supervisors coming from different jobs—that is, former Paris subway drivers (working independently and impacting the lives of others) and station agents (working interdependently with limited impact on others’ lives)—that left them with distinct sets of expectations around responsibility. Drawing on interviews and observations, we find that former drivers developed a deep sense of “personal” responsibility. After promotion, their perceived managerial responsibility paled in comparison with their expectations of what it felt like to have personal responsibility, leading the majority to experience managerial blues. In contrast, former agents had few expectations of what responsibility entailed and reported no disenchantment once they joined the managerial ranks. Overall, we show how imprinted expectations shape people’s future managerial experiences, including their managerial blues, and discuss the implications of our findings for literatures on job mobility and job design.
We develop and test a theoretically informed and generalizable empirical framework for evaluating the performance gap between internally and externally hired workers. First, human capital theory predicts that internal hires will be immediately more productive than external hires. Second, contextual learning predicts that internal hires will be more productive with time. Finally, theories of commitment, which are rarely applied to this literature, predict that internal advancement enhances retention among high performers (“positive retention”). Applying a general empirical framework for quantifying the relative contributions of these mechanisms to a retailer with 109,063 commissioned salespeople and their 12,931 managers, we find that the gap in our setting is primarily driven by positive retention: High performers and internal hires are less likely to quit, and crucially, high-performing internal hires are especially unlikely to quit. When high-performing internal hires do quit, they tend to cite reasons unrelated to work rather than advancement opportunities. By typically examining performance and retention in isolation, researchers and organizations may be underestimating the importance of internal advancement as a means of retaining of high performers.
A crucial challenge for organizations is to pool and aggregate information effectively. Traditionally, organizations have relied on committees and teams, but recently many organizations have explored the use of information markets. In this paper, the authors compared groups and markets in their ability to pool and aggregate information in a hidden-profiles task. In Study 1, groups outperformed markets when there were no conflicts of interest among participants, whereas markets outperformed groups when conflicts of interest were present. Also, participants had more trust in groups to uncover hidden profiles than in markets. Study 2 generalized these findings to a simple prediction task, confirming that people had more trust in groups than in markets. These results were not qualified by conflicts of interest. Drawing on experienced forecasters from Good Judgment Open, Study 3 found that familiarity and experience with markets increased the endorsement and use of markets relative to traditional committees.
Despite substantial scholarly attention to workforce demographic diversity, existing research is limited in understanding whether or in what contexts firm-level racial diversity relates to performance and workforce outcomes of the firm. Drawing on social interdependence theory along with insights from social exchange and psychological ownership theories, we propose that the use of broad-based stock options granted to at least half the workforce creates the conditions supporting a positive relationship between workforce racial diversity and firm outcomes. We examine this proposition by analyzing panel data from 155 companies that applied for the “100 Best Companies to Work For” competition with responses from 109,314 employees over the five-year period from 2006 to 2010 (354 company-year observations). Findings revealed that racial diversity was positively related to subsequent firm financial performance and individual affective commitment and was not significantly associated with subsequent voluntary turnover rates, when accompanied by a firm’s adoption of broad-based stock options. However, under the nonuse of broad-based stock options, racial diversity was significantly related to higher voluntary turnover rates and lower employee affective commitment, with no financial performance gains. By documenting the beneficial effects of financial incentives in diverse workplaces, this paper extends theory asserting the value of incentives for performance.
In this paper, we apply a core/periphery framework to an intraorganizational context to study the interplay between formal and informal core/periphery structures. Specifically, we consider how core positions occupied by inventors in the corporate research and development division of a large multinational high-tech company affect their ability to generate incremental innovations. We theorize and empirically observe that formal and informal core positions have positive and independent effects on the generation of incremental innovations. These effects have a multiplicative impact on innovative productivity when inventors who are core in the informal knowledge-sharing network are also affiliated with a core organizational unit. We also observe, however, that the positive effect of being located at the core of both the informal and formal structures is negatively moderated by individuals’ distribution of knowledge ties when these reach outside the core of their informal knowledge-sharing network.
Socially and educationally disadvantaged entrepreneurs often lack the knowledge and prior experience to develop and scale their businesses. Owing to limited educational and employment opportunities, poverty, and discrimination, these entrepreneurs frequently experience low business growth and performance. What factors influence the effectiveness of early-stage venture incubation and mentoring for promoting learning, scaling, and profitability among these entrepreneurs? Two studies in a business incubator serving low-income, underprivileged entrepreneurs in South Africa evaluate this question. Study 1 uses a matched, two-period case-control design to investigate the effects of incubation on business growth by comparing selected and incubated companies to similar also-selected but not incubated ones. The findings show that incubated companies grew 22% more in revenue and 15% more in employment than not incubated companies over the six months between applying to and graduating from the incubator. Study 2 uses instrumental-variable models to evaluate the role that mentoring played in improving business performance by analyzing data from seven cohorts of participants in the incubator randomly assigned to mentors. The findings show that participants assigned to high-ability (versus low-ability) mentors had 3.2% higher revenue and 3.5% higher profits one year after incubation. Further, the benefits of being mentored were more significant for businesses whose entrepreneurs had less pre-entry knowledge and experience, suggesting that mentoring supplemented gaps in human capital. These findings have implications for ways to support disadvantaged entrepreneurs and their businesses through mentoring and early-stage venture incubation.
In this study, we build on the foundational observations of Selznick and Stinchcombe that organizations bear the lasting imprint of their founding context and explore how characteristics shaped during founding are coherently carried forward through time. To do so, we draw on an ethnography of a social venture where the entrepreneurs left soon after founding. In examining how an initial organizational imprint evolves beyond a venture’s founding phase, we focus on the actions and interactions of organizational members, the founders’ imprint, the venture’s new leadership, and the external environment. The process model we develop shows how the organizational imprint evolves as a consequence of the interplay between top-down and bottom-up forces. We first find that the initial imprint is transmitted through a bottom-up mechanism of imprint reinforcement, and second, that the venture is reimprinted after the founding period through two processes which we call imprint reforming and imprint coupling. The result of this is the formation of a sedimented imprint. Our findings further illuminate that, although the initial imprint sticks, its function and manifestation changes over time.
This study advances understanding of the conditions under which a new worker improves organizational performance. We argue that the extent to which new group members have experience working as specialists or generalists is a critical factor in explaining performance after the new member joins. We conceptualize specialists as those who concentrate on a particular component of an organization’s task, whereas generalists perform all components of the task. As such, a specialist must coordinate with other group members to complete the group’s task, which makes a specialist more interdependent with other members and in possession of more organization-specific knowledge than a generalist. We predict that (1) groups receiving specialist new members do not perform as well after the new member joins as compared with groups receiving generalist new members and (2) groups with new members whose work experience and recipient group structure are aligned (i.e., generalist movers into generalist groups and specialist movers into specialist groups) perform better than groups with new members whose experience and recipient group structure are not aligned. We test our hypotheses using a laboratory study in which we manipulate the extent to which new members and incumbent members of recipient groups work as specialists or generalists. Participants work as generalists or specialists in three-person groups and receive a new member who acquired experience as a specialist or generalist in another group. We find support for our hypotheses and provide evidence on mechanisms through which potential new members’ backgrounds enable them to contribute significantly to their recipient groups. New members who acquire experience in a structure similar to that of their recipient organizations report that they experience greater fit with their new groups, which enables their recipient groups to perform better than groups where new members’ experience and recipient group structure are not aligned. Additionally, our results suggest generalists may be more likely than specialists to transfer knowledge to their new groups.
Team production is ubiquitous in the economy, but managing teams effectively remains a challenge for many organizations. This paper studies how familiarity among teammates influences the performance of specialist teams, relative to nonspecialist teams. Applying theories of team production to contexts where team members coordinate interdependent activities extemporaneously, we develop predictions about factors that shift the marginal returns to specialization along two dimensions of familiarity: social familiarity and functional familiarity. We test our hypotheses in the context of Defence of the Ancients 2 (DOTA2), a major e-sports game where, in some formats, players are exogenously assigned to five-person teams. After analyzing nearly 6.5 million matches, we find that specialist teams are relatively more successful when members are more socially and functionally familiar with one another. The results suggest that the plug-and-play perspective on specialist teams is incomplete; rather, specialization and familiarity are complements in dynamic environments where team members coordinate extemporaneously.Funded: Financial support from the UCL School of Management and Worcester Polytechnic Institute is gratefully acknowledged.
When actors deem technological change undesirable, they may act symbolically by pretending to comply while avoiding real change. In our study of the introduction of an algorithmic technology in a sales organization, we found that such symbolic conformity led unintendedly to the full implementation of the suggested technological change. To explain this surprising outcome, we advance a regime-of-knowing lens that helps to analyze deep challenges happening under the surface during the process of technology introduction. A regime of knowing guides what is worth knowing, what actions matter to acquire this knowledge, and who has the authority to make decisions around those issues. We found that both the technologists who introduced the algorithmic technology, and the incumbent workers whose work was affected by the change, used symbolic actions to either defend the established regime of knowing or to advocate a radical change. Although the incumbent workers enacted symbolic conformity by pretending to comply with suggested changes, the technologists performed symbolic advocacy by presenting a positive side of the technological change. Ironically, because the symbolic conformity enabled and was reinforced by symbolic advocacy, reinforcing cycles of symbolic actions yielded a radical change in the sales' regime of knowing: from one focused on a deep understanding of customers via personal contact and strong relationships, to one based on model predictions from the processing of large datasets. We discuss the theoretical implications of these findings for the introduction of technology at work and for knowing in the workplace.
Legitimacy is critical to the formation and expansion of nascent fields because it lends credibility and recognizability to once overlooked actors and practices. At the same time, legitimacy can be a double-edged sword precisely because it facilitates field growth, attracting actors with discrepant practices that may lead to factionalization and undermine the coherence of the field’s collective identity. In this paper, we investigate how organizations can mitigate the downside of legitimation by eliciting emotions that align increasingly discrepant actors and celebrate an inclusive collective identity. We leverage fieldwork and computational text analysis to examine the relationship between legitimation, collective identity coherence, and emotions in the context of the Makers, a nascent field of do-it-yourself hobbyists and technology hackers. In our quantitative analysis we show that legitimation was associated with increased field heterogeneity, but that collective events countered the diluting effects of legitimation. In the qualitative analysis of our interview data we demonstrate that activities at these events—demonstrations and hands-on experiences—elicited emotional contagion and empathy among actors. These emotions reconciled tensions among increasingly heterogeneous actors and bolstered the coherence of the Maker collective identity. We conclude by discussing our contribution to research on legitimacy, collective identity, and field-configuring events.
Across the globe, every workday people commute an average of 38 minutes each way, yet surprisingly little research has examined the implications of this daily routine for work-related outcomes. Integrating theories of boundary work, self-control, and work-family conflict, we propose that the commute to work serves as a liminal role transition between home and work roles, prompting employees to engage in boundary management strategies. Across three field studies (n = 1,736), including a four-week-long intervention study, we find that lengthy morning commutes are more aversive for employees with lower trait self-control and greater work-family conflict, leading to decreased job satisfaction and increased turnover. In addition, we find that employees who engage in a specific boundary management strategy we term role-clarifying prospection (i.e., thinking about the upcoming work role) are less likely to be negatively affected by lengthy commutes to work. Results further show that employees with higher levels of trait self-control are more likely to engage in role-clarifying prospection, and employees who experience higher levels of work-family conflict are more likely to benefit from role-clarifying prospection. Although the commute to work is typically seen as an undesirable part of the workday, our theory and results point to the benefits of using it as an opportunity to transition into one’s work role.
Given the growing corporate social responsibility (CSR) pressures to increase board gender diversity and the scrutiny afforded to firms that fail to appoint female directors, one may expect shareholders to vote with greater support for women (than for men) nominated to boards. However, diversity management research suggests that pressures to improve female representation in organizations and in leadership roles may also backfire. We propose a threat-contingency model of shareholder dissent against female director candidates to explain when shareholders will be more or less likely to dissent against female (relative to male) directors. Specifically, we advance CSR legitimacy threats and agency threats as conditions contextualizing shareholder dissent against female director candidates. Using a sample of 50,202 director elections at 1,104 public firms from 2003 to 2015, we find that female directors receive less dissent from shareholders; further, low female board representation intensifies this leniency as CSR legitimacy threats become more salient. However, when firm-related agency threats occur (e.g., firm underperformance and media controversies), shareholders’ leniency toward female director candidates dissipates, and when directors themselves present agency threats (e.g., director attendance problems and nonindependence), shareholders evaluate female directors more harshly than male directors. Underlining the relevance of our theory, our supplementary analyses show that shareholder dissent increases the probability of director turnover. These findings contribute to theory and research on women on boards, firm responses to institutional pressures, and shareholder dissent.
Previous research has demonstrated that the size and reach of people’s social networks tend to be positively related to their social status. Although several explanations help to account for this relationship—for example, higher-status people may be part of multiple social circles and therefore have more social contacts with whom to affiliate—we present a novel argument involving people’s beliefs about the relationship between status and quality, what we call status-quality coupling. Across seven separate studies, we demonstrate that the positive association between social status and network-broadening behavior (as well as social network size) is contingent on the extent to which people believe that status is a reliable indicator of quality. Across each of our studies, high- and low-status people who viewed status and quality as tightly coupled differed in their network-broadening behaviors, as well as in the size of their reported social networks. The effect was largely driven by the perceived self-value and perceived receptivity of the networking target. Such differences were significantly weaker or nonexistent among equivalently high- and low-status people who viewed status as an unreliable indicator of quality. Because the majority of participants—both high- and low-status—exhibited beliefs in status-quality coupling, we conclude that such a belief marks an important and previously unaccounted-for driver of the relationship between status, network-broadening behaviors, and social networks. Implications for research on social capital, advice seeking, and inequality are highlighted in the discussion section.
Authenticity is a valuable attribution for organizations, but one that raises a challenge of audience acceptance for innovative entrepreneurs. In particular, organizations that depart from an established type risk being judged as inauthentic. However, entrepreneurs may be able to overcome this challenge by basing their authenticity on notions of craft—such as skilled hands-on techniques, sophisticated ingredients, and small-scale artistry rather than mass industrial manufacturing—that better support innovation. We propose that communities vary in the extent to which they embrace craft production as an evolved understanding of authenticity that is less concerned with conformity to type. This local context, in turn, conditions the likelihood of entrepreneurs creating innovative ventures that rely on perceptions of craft authenticity. We develop this argument through a mixed-methods study of the spatially uneven emergence of gourmet food trucks across the United States. Our findings contribute to research on authenticity and the geography of entrepreneurship and innovation.
This paper focuses on how digital innovation develops in ecologies of distributed heterogeneous actors with contesting logics, diverse technologies, and various forms of orchestrations. Drawing on the insights from emerging theories of digital innovation augmented by an institutional logics perspective, we examine a case study of how residential internet infrastructure was shaped over 20 years by the interplay of self-organized residential communities, corporate internet service providers (ISPs), and a state ISP. Our analysis of this case leads to the identification of four types of interactions that shape the trajectories of digital infrastructure development beyond direct actor interplays and competitive or collaborative relationships. We label these interactions symbiotic generative, symbiotic mutualistic, parasitic complementary, and parasitic competitive and explain the processes and conditions of their development and their innovation outcomes. Drawing on these findings, we develop a model of symbiotic and parasitic interactions shaping digital infrastructure development and identify key characteristics of the ecologies where these emerge. The case study and the model that emerged aim to contribute to the growing field of research on complex and nonlinear paths of digital innovation development constituted by the dynamics of its distributed agency. The article concludes by highlighting avenues for future research.
We explore how members of a community of practice learn new tools and techniques when environmental shifts undermine existing expertise. In our 20-month comparative field study of medical assistants and patient-service representatives learning to use new digital technology in five primary care sites, we find that the traditional master-apprentice training model worked well when established practices were being conferred to trainees. When environmental change required introducing new tools and techniques with which the experienced members had no expertise, third-party managers selected newer members as trainers because managers judged them to be agile learners who were less committed to traditional hierarchies and more willing to deviate from traditional norms. This challenged community members’ existing status, which was based on the historical distinctions of long tenure and expertise in traditional tasks. In three sites, the introduction of this illegitimate learning hierarchy sparked status competition among trainees and trainers, and trainees collectively resisted learning new tools and techniques. In the other two sites, managers paired the new, illegitimate learning hierarchy with the opportunity for trainee status mobility by rotating the trainer role; here, trainees embraced learning in order to exit the lower-status trainee group and join the higher-status trainer group. Drawing on ideas of status group legitimacy and mobility, we suggest that managers’ pairing of an illegitimate learning hierarchy with the opportunity for trainee status mobility is a mechanism for enabling the situated learning of new techniques when traditional expertise erodes.
This paper investigates whether firms from developing countries that lag the global technological frontier can learn from technologically successful peers as a means of closing the technological gap with leaders from developed countries. We define technologically successful peers as those that hail from similar home countries, operate in the same industry, and have achieved a certain degree of success in closing the gap with the global technological frontier. We argue that technologically successful peers represent an important reference group for lagging firms and, as such, offer opportunities for lagging firms from developing countries to hasten technological development. We find that lagging firms from developing countries that build upon the knowledge of technologically successful peers achieve higher rates of technological improvement. Moreover, learning from technologically proximal successful peers helps even further with technological improvement. However, there are limits to such learning, with diminishing marginal returns to lagging firms that over rely on successful peers.
We analyze the relationship between the actions and interactions of secondary stakeholders with an interest in corporate social performance (CSP) and variation in firm-level CSP across countries. Our work represents a significant theoretical shift in research exploring comparative CSP, which, to date, has focused on cross-national variation in institutions. We propose that stakeholders can also drive cross-country heterogeneity in CSP by influencing the salience of the issues for which they advocate. Stakeholders raise salience of CSP issues through their interactions with important sociopolitical actors within a country, signaling their collective ability to change expectations on CSP. CSP issue salience is also heightened where heterogeneous stakeholder groups advocate for CSP issues, signaling that issues have garnered widespread acceptance or legitimacy. Managers are also more attuned to the urgency of issues through the direct actions that stakeholders take against firms in the country. We also argue and find that these effects are moderated by interstakeholder interactions, which signal the degree of consensus among stakeholders on issues and their ability to mobilize repeatedly against firms. We draw on a novel data set of 250 million media-reported events to identify secondary stakeholders with interests in the environmental and social issues that constitute CSP, their direct actions against firms, and their interactions with important sociopolitical actors and each other. We show empirically that variation in secondary stakeholder actions and interactions between countries, and within countries over time, is associated with differences in firm-level CSP among a sample of 2,852 firms spanning 36 countries from 2004 to 2013.
This paper studies how audience members categorize and evaluate ambiguous offerings. Depending on whether audience members categorize ambiguous offerings based on prototypes or goals, they activate two distinct cognitive mechanisms and evaluate differently ambiguous offerings. We expect that when audiences engage in goal- versus prototype-based categorization, their evaluation of ambiguous products increases. We theorize that, under goal-based categorization, the perceived utility of unclear attributes increases for audiences, which leads them to evaluate more positively ambiguous product offerings. We test and find support for these direct and mediated relationships through a series of laboratory, online, and field experiments. Overall, this study offers important implications for research on product and market categories, optimal distinctiveness, and market agents’ cognitive ascription of value.
Using three years’ data from more than 1,000 employees at a large professional services firm, we find that adopting an expertise search tool improves employee work performance in billable revenue, which results from improvements in network connections and information diversity. More importantly, we also find that adoption does not benefit all employees equally. Two types of employees benefit more from adoption of digital collaboration tools than others. First, junior employees and women benefit more from the adoption of digital collaboration tools than do senior employees and men, respectively. These tools help employees overcome the institutional barriers to resource access faced by these employees in their searches for expertise. Second, employees with greater social capital at the time of adoption also benefit more than others. The tools eliminate natural barriers associated with traditional offline interpersonal networks, enabling employees to network even more strategically than before. We explore the mechanisms for these differential benefits. Digital collaboration tools increase the volume of communication more for junior employees and women, indicating greater access to knowledge and expertise than they had before adoption. The tools also decrease the volume of communication for people with greater social capital, indicating more efficient access to knowledge and expertise. An important implication of our findings is that digital collaboration tools have the potential to overcome some of the demographic institutional biases that organizations have long sought to change. It does so, however, at the expense of potentially creating new biases toward network-based features—a characteristic we call “network-biased technical change.”
Organizations that desire creativity often use groups like task forces, decision panels, and selection committees with the primary purpose of evaluating novel ideas. Those groups need to keep at least some novel ideas alive while also assessing the usefulness of ideas. Research suggests, however, that such groups often prefer proven ideas whose usefulness can be easily predicted and reject novel ideas early in the course of discussion. How those groups deal with the tension between novelty and the predictability of idea usefulness in the process of overcoming a bias against novelty is therefore an important question for understanding organizational creativity and innovation. We explore that question with a qualitative study of the discussions of four healthcare policy groups who confronted the tension early in the process of evaluating ideas. Unlike prior work that emphasizes how groups integrate tensions to build consensus around ideas, our study showed that overcoming a bias against novelty involved maintaining tension by fracturing a group’s shared understanding of usefulness and retaining those divergent perspectives alongside moments of consensus. We describe this as a diverging consensus model of overcoming a bias against novelty. Our work contributes to the literature examining how groups can productively engage with tensions and provides a dynamic process for how groups might overcome the bias against novelty and therefore keep some novel ideas alive to fuel organizational creativity and innovation.
Organizations increasingly offer work-family programs to assist employees with balancing the competing demands of work and family life. Existing scholarship indicates that the consequences of work-family programs are heterogeneous across different socio-demographic groups, but limited research examines when and why workers within the same group may benefit differently from such programs. Understanding these differences may illuminate important mechanisms driving the effectiveness of work-family policies. We theorize that one key driver of within-group variation in the effectiveness of work-family programs is the extent to which workers’ nonwork social networks activate resources to support them. Specifically, we argue that workers whose nonwork networks are less likely to activate supportive resources will benefit more from organizational programs. We further posit that the status characteristics of workers’ dependents may shape the activation of resources among nonwork networks. Drawing on novel data from an Indian garment factory and a quasi-experimental research design, we examine how a work-family program, employer-sponsored childcare, affects the daily work attendance of a socio-demographically homogenous group of working mothers. We find that women whose nonwork networks are less likely to activate informal childcare support—specifically, women with daughters—benefit more from employer-sponsored childcare. Supplemental interview data supports our theoretical claims. We conclude by discussing the contributions of our argument to scholarship on work-family policies and social networks.
It is well established that the effectiveness of pay-for-performance (PfP) schemes depends on employee- and organization-specific factors. However, less is known about the moderating role of external forces such as market competition. Our theory posits that competition generates two counteracting effects—the residual market and competitor response effects—that vary with competition and jointly generate a curvilinear relationship between PfP effectiveness and competition. Weak competition discourages effort response to PfP because there is little residual market to gain from rivals, whereas strong competition weakens incentives because an offsetting response from competitors becomes more likely. PfP hence has the strongest effect under moderate competition. Field data from a bakery chain and its competitive environment confirm our theory and let us refute several alternative interpretations.
Strategy scholars have long studied the strategic implications of firm-specific human capital but have almost completely ignored their conceptual dual: firm-specific worker incentives. This paper proposes that firm-level incentives can also vary in firm specificity, and accordingly, firm-specific incentives may help to explain advantages independent of the firm specificity of human capital. Results from a proprietary data set, including data from 284 software development firms and matched employee-level compensation data for 8,208 software developers in 99 of those firms, suggest that firms with higher firm specificity in their incentive bundles may have lower dysfunctional employee turnover rates as well as lower wage–tenure slopes. In other words, these firms may lose fewer employees who they would prefer to keep and may be able to do so while still offering lower wage increases over time than their competitors in the labor market. Thus, firm-specific incentives may provide a viable alternative pathway to human capital–based competitive advantages.
In this paper, we contribute a temporal perspective on work coordination across collaborating occupations. Drawing on an ethnographic study of medical specialists—surgeons, pathologists, oncologists, and radiologists—we examine how their temporal orientations are shaped through the temporal structuring of occupational work. Our findings show that temporal structuring of occupational practices develop in relation to the contingencies and materialities of their work and that this shapes and is shaped by specialists’ temporal orientations. Further, we show that differences in occupations’ temporal orientations have important implications for coordinating work. More specifically, our study reveals how the domination of one temporal orientation can lead to recurrent strain, promoting a competitive trade-off between the different temporal orientations in guiding interaction. This temporal orientation domination is accompanied by a persistent emotional strain and potential conflict. Finally, we suggest that, alternatively, different temporal orientations can be resourced in solving coordination challenges through three interrelated mechanisms, namely juxtaposing, temporal working, and mutual adjusting. In so doing, we show how temporal resourcing can be productive in coordinating work.
Existing research at the nexus of institutional theory and entrepreneurship suggests that lowering institutional barriers to forming, growing, and exiting new firms can affect the types of start-ups that entrepreneurs found in a region. These institutional changes could influence entrepreneurs’ perceptions of the value of partnering with venture accelerators and potentially improve these sponsors’ capacity to select high-growth start-ups to fund and develop. This study evaluates these ideas by developing and testing three hypotheses. First, institutional reforms improve entrepreneurs’ perceived value of venture accelerators for resources that affect new venture development. Second, they reduce the average probability of being selected for new applicants, due to a surge in the number and heterogeneity of new applicants within accelerators’ local ecosystems. Third, institutional reforms increase the quality of selected cohorts for accelerator managers due to increases in the average quality and human capital of new applicants. To evaluate these hypotheses, I analyze data from 13,770 applicants to venture accelerators over multiple application cycles between 2016 and 2018 in 170 countries. I use a differences-in-differences design to estimate the effects of institutional changes on start-up selection after regulatory reforms that reduced the time and procedures to start new firms, obtain credit, and resolve bankruptcy for entrepreneurs. The findings have valuable implications for how governments, especially those in emerging and developing economies, can support high-growth entrepreneurship.
While there is a growing literature on moral markets that aim to create social value through market exchange, much of it has focused on how producer activism is able to legitimate new, institutionally complex, organizational and economic forms that are inscribed with competing market and social/community logics. Much less attention has been directed towards understanding how moral markets are scaled by the entry of large, established organizations. While the scaling of moral markets entails the risk of conservative goal transformation, we still know relatively little about how moral values become embedded in markets, providing an ongoing catalyst for social value creation. Based on a five-year ethnographic study, we show how cultural entrepreneurship associated with the creation of a cross-sector partnership, legitimated local food procurement by large, established organizations, enabling the scaling of the overall market. We argue that a key aspect of their success had to do with bridging the institutional void segregating local and industrial food logics. Based on our study, we highlight how this institutional void bridging was facilitated by cultural entrepreneurship that initially focused on communications that decoupled the values and practices associated with the local food logic, and subsequently, reinfused locavore values by valorizing stories and activities that recoupled those values to food procurement practices after the institutional void was diminished. We discuss the implications of our study for research on moral markets and cultural entrepreneurship.
Although it is well established that vertical integration decisions have important consequences for firms, direct evidence on how vertical integration matters to firm innovation has been scarce. This study draws from seminal research on organizing for innovation and recent synthesis of transaction cost and capabilities theories to examine how vertical integration affects the rate and types of firm innovation pursued. To strengthen identification of causal effects, we exploit a quasi-experimental design to compare firms that announced and completed a vertical merger and acquisition (M&A) with those announcing but not completing the transaction. We show that firms completing vertical M&As experience a growth in their rate of innovation; in addition, such firms witness an increase in systemic innovation but a drop in autonomous innovation. Our study contributes important empirical evidence to bear on the literature on the organization of innovation, highlighting that organizational mode choices are a critical determinant of the rate and direction of inventive activity.
Despite the large literature on alliance contract design, we know little about how transacting parties change and amend their underlying contracts during the execution of strategic alliances. Drawing on existing research in the alliance contracting literature, we develop the empirical question of how contract detail and prior ties influence the amount, direction, and type of change in such agreements during the collaboration. We generated a sample of 115 joint ventures (JVs) by distributing a survey to JV board members or top managers and found that the amount of contract change is negatively associated with the level of detail in the initial contract but is positively associated with the number of prior ties between alliance partners. In relation to the direction of contract change, we find that the level of detail of the initial agreements negatively correlates with the likelihood of removing or weakening existing provisions and that prior collaborative experience positively correlates with the likelihood of strengthening of existing provisions or adding of new ones. We also find that prior ties affect the type of change in that JV parents prefer to change enforcement provisions more so than the coordination provisions in the contract. Our paper generates new insights on the complementarities between relational governance and transaction cost economics perspectives on alliance contracting.
The recent emergence of blockchains may be considered a critical turning point in organizing collaborations. We outline the historical background and the fundamental features of blockchains and present an analysis with a focus on their role as governance mechanisms. Specifically, we argue that blockchains offer a way to enforce agreements and achieve cooperation and coordination that is distinct from both traditional contractual and relational governance as well as from other information technology solutions. We also examine the scope of blockchains as efficient governance mechanisms and highlight the tacitness of the transaction as a key boundary condition. We then discuss how blockchain governance interacts with traditional governance mechanisms in both substitutive and complementary ways. We pay particular attention to blockchains’ social implications as well as their inherent challenges and limitations. Our analysis culminates in a research agenda that explores how blockchains may change the way to organize collaborations, including issues of what different types of blockchains may emerge, who is involved and impacted by blockchain governance, why actors may want blockchains, when and where blockchains can be more (versus less) effective, and how blockchains influence a number of important organizational outcomes.
In the Carnegie School tradition of experiential learning, learning processes are driven by the encoding of performance outcomes as a success or failure relative to a goal. We expand this line of inquiry by highlighting how conflicting and thus ambiguous outcomes across multiple goals make interpretation a critical aspect of organizational learning processes. In early work in the Carnegie tradition, interpretation played a role in the demarcation between what constituted success or failure on a given outcome metric. However, in March’s latter writings, learning and decision making produce an arena or even an opportunity for generating interpretations and broader meanings regarding roles, values, and identities. We explore how the two interpretive approaches in March’s work play out across three modes of responses to ambiguity. First, the process of self-enhancement whereby participants interpret conflictual outcomes so that they, the participants, appear in a positive light. Second, an explicit political process regarding the contestation of how to interpret conflicting outcomes. Third, from the perspective of the organizations’ literature on wisdom, participants may embrace ambiguity either to enhance learning or simply to enrich individuals’ interpretation of their experiences. Although these three modes of response do not offer a complete set of responses for learning in a world of ambiguity, they constitute valuable touchstones for the perspective we wish to put forward and, collectively, help enrich our understanding of the role of learning, ambiguity and interpretation within the Carnegie School.
Organizations are often pressured to adopt and maintain institutionally supported practices. Why do some companies remain committed to these practices, despite high operational cost and widespread frustration with them? Although prior theorists have emphasized the importance of institutional pressure at the broader population level, less research attention has been paid to the abandonment of a practice as a result of resource dependence between a firm and market intermediaries. In this paper, we theorize intermediary coverage breadth and depth as two important structural indicators of resource dependence. Firms lacking in coverage breadth (as indicated by the degree of reporting by market intermediaries) and firms with deeper coverage (as evidenced by a prolonged relationship with market intermediaries) are less likely to abandon a practice due to an increase in power imbalance and mutual dependence within the firm-intermediary relationships. We also theorize how a firm’s resource dependence, as determined by coverage structure, moderates the firm’s sensitivity to (1) observed peer support for the practice, (2) the intermediary’s expectation regarding the continued use of the practice, and (3) performance deviations that fail to meet intermediary expectations. Our empirical study of the abandonment of quarterly earnings guidance by U.S. public companies during 2001–2010 provides overall support for our theoretical arguments.
We connect two distinct streams of research on categories to study the role of within-category typicality in the context of legitimacy shocks. We argue that, following a legitimacy shock, member organizations of the tainted, focal category suffer equally, irrespective of their typicality. However, only the typical members of the newly favored, oppositional category benefit. Therefore, the effects of legitimacy shocks are asymmetrically influenced by typicality. We argue this pattern is the result of a two-stage process of categorization by audiences, whereby audiences prioritize distinctions between organizations in a newly favored category and spend limited efforts considering distinctions in the tainted, focal category. We examine our theory in the context of the U.S. financial services industry, where four different kinds of organizations engage in competition: traditional commercial banks, community banks, single-bond credit unions, and multibond credit unions. Consistent with our theory, we show that both traditional commercial banks and community banks suffer in terms of deposit market share following the legitimacy shock of the 2007 financial crisis, but the relative gains to credit unions are strongest for single-bond credit unions.
Evaluating the attractiveness of startup employment requires an understanding of both what startups pay and the implications of these jobs for earnings trajectories. Analyzing Danish registry data, we find that employees hired by startups earn roughly 17% less over the next 10 years than those hired by large, established firms. About half of this earnings differential stems from sorting—from the fact that startup employees have less human capital. Long-term earnings also vary depending on when individuals are hired. Although the earliest employees of startups suffer an earnings penalty, those hired by already-successful startups earn a small premium. Two factors appear to account for the earnings penalties for the early employees: Startups fail at high rates, creating costly spells of unemployment for their (former) employees. Job-mobility patterns also diverge: After being employed by a small startup, individuals rarely return to the large employers that pay more.
We investigate why employee-friendly firms often benefit from lower costs of debt financing. We theorize that banks use employee treatment as a screen to assess firms’ trustworthiness, which encompasses not only confidence in firms’ ability to perform well but also the belief that they will act with good intent toward their creditors. We integrate screening theory and stakeholder theory to explain the—oftentimes unintended—consequences that firms’ actions toward employees have on their relationships with other stakeholders. An analysis of U.S. firms between 2003 and 2010 shows that favorable employee treatment reduces the cost of bank loans, and this relationship is stronger when banks cannot infer firms’ intent from their relations with stakeholders other than employees. A policy-capturing study provides further support that employee treatment serves as a screen for intent. We discuss the implications of our stakeholder-screening perspective as a novel way to understand the second-order, unintended effects of a focal stakeholder relationship on firms’ relations with other stakeholders.
Many studies have examined the relationship between employee ownership and firm productivity. However, research is lacking on how this relationship is strengthened or weakened by environmental characteristics. This is a critical oversight in the employee ownership literature because industry characteristics can significantly influence employees’ expected gains from their firm ownership. Thus, based on agency theory and expectancy theory, we develop a multilevel contingency model of employee ownership with industry growth and instability as boundary conditions. We test the proposed model with a sample of 573 firms in South Korea (Study 1: 1,415 firm years) and a sample of 892 firms in 28 European countries (Study 2: 4,768 firm years). In both studies, we find that employee ownership does not significantly contribute to firm productivity on its own. However, we find a significant three-way interaction effect of employee ownership, industry growth, and industry instability on firm productivity. Specifically, employee ownership is most effective at improving firm productivity when both industry growth and industry instability are high. We discuss the theoretical and practical implications of the findings.
We propose a new mechanism explaining why companies may remain silent about their positive corporate behaviors, such as socially responsible activities. We examine such strategic silence in the context of corporate philanthropy. Building on and extending the literature on legitimacy and stakeholder management, we argue that when a firm mistreats primary stakeholders, it is more likely to keep quiet about its philanthropic acts to avoid backlash from stakeholders. We also propose that long-term orientation among stakeholders mitigates the positive relationship between mistreating primary stakeholders and quiet giving, which allows stakeholders to appreciate the long-term value of corporate philanthropy. Data from listed Chinese firms show that firms are more likely to give quietly when they underpay their employees and/or investors. Moreover, research and development expenditures and institutional shareholding, as indicators of stakeholder long-term orientation, attenuate this relationship.
This paper investigates how industry research and development (R&D) may shape the rate and direction of research at geographically proximate universities and explores the mechanisms leading to such geographical localization of industry influence. Endogeneity concerns related to the selection of industry location were addressed by using the unique setting of the agricultural biotechnology revolution of the 1980s. The emergence of plant biotechnology spurred the entry of nonbiotechnology agribusiness incumbents into genetic engineering, creating a quasi-natural experiment for universities located near the company R&D headquarters. Relative to the control universities identified by coarsened exact matching, the universities within a 50-mile radius of the incumbent R&D headquarters show an approximate 28.5% increase in industry-relevant research output after the company’s entry into agricultural biotechnology R&D. The effect of industry R&D is stronger for research on major cash crops. Additional analysis suggests that local university-industry coauthorships, which proxy for local industry funding, may be a leading mechanism behind the effect. Overall, the results highlight the roles of R&D-intensive anchor tenants in the geography of academic knowledge production. The findings are relevant for regional policymakers and managers considering strategic placements of corporate R&D locations.
Organizations often learn vicariously by observing what other organizations do. Our study examines vicarious learning–related communication through which individuals share their observations with other organizational members. Most students and members of present-day organizations would expect that this communication is driven by a prodevelopment logic—that communication serves the purpose of organizational improvement and competitiveness. Our unique historical evidence on learning-related communication over multiple decades shows that the subjective and collective attitude toward prodevelopment communication may be ideologically conditioned. Prodevelopment communication is the norm in capitalist organizations, but competing ideologies may emphasize other goals higher than organizational development. Consequently, increasing challenges to capitalism as the ideological basis of economic organization can have deep impacts on how organizations learn and produce innovations in the future.
Members of global teams are often dispersed across time zones. This paper introduces the construct of temporal brokerage, which we define as being in a position within a team’s temporal structure that bridges subgroups that have little or no temporal overlap with each other. Although temporal brokerage is not a formal role, we argue that occupying such a position makes an individual more likely to take on more coordination work than other members on the team. We suggest that, while engaging in such coordination work has advantages in the form of enhanced integrative complexity, it also comes with costs in the form of a greater workload relative to other members. We further argue that the increased integrative complexity and workload that result from occupying a position of temporal brokerage have implications that go beyond the boundaries of the focal team, spilling over into other projects the individual is engaged in. Specifically, we predict that being in positions of temporal brokerage on global teams decreases the quantity but increases the quality of an individual’s total productive output. We find support for these predictions across two studies comprising 4,553 individuals participating in global student project teams and 123,586 individuals participating in global academic research teams, respectively. The framework and findings presented in this paper contribute to theories of global teamwork, pivotal roles and leadership emergence in global teams, and social network theory.
Where do valuable contributions originate from in online innovation communities? Prior research provides conflicting answers. One view, consistent with a community of practice perspective, is that valued knowledge contributions are primarily provided by central participants at the core of a community. In contrast, other research—including work adopting an open innovation perspective—predicts that valuable ideas primarily emerge from peripheral participants, those at the margins of a field of knowledge who provide novel ideas and viewpoints. We integrate these contrasting perspectives by considering two distinct forms of position: social embeddedness (a core social position within the social network of participants interacting within a community) and epistemic marginality (a peripheral epistemic position based on the network of topics discussed by a community). Analyzing contributions by 697,412 participants of 52 Stack Exchange online innovation communities, we find that both participants who are socially embedded and participants who are epistemically marginal provide knowledge contributions that are highly valued by fellow community participants. Importantly, among epistemically marginal participants, those with high social embeddedness are more likely to provide contributions valued by the community; by virtue of their epistemic marginality, these participants may offer novel ideas while by virtue of their social embeddedness they may be able to more effectively communicate their ideas to the community. Thus, the production of knowledge in an online innovation community involves a complex interaction between the novelty emanating from the epistemic periphery and the social embeddedness required to make ideas congruent with existing social and epistemic norms.
Despite growing engagements between firms and nonmarket stakeholders—such as local communities and nongovernmental organizations —research has yet to examine the emergence of formal contracts between them. Given that a very large number of such contracts are theoretically possible but only a small number exist, we seek to understand what factors explain the use of contracts to govern some relationships between firms and nonmarket stakeholders but not others. We draw on transaction cost economics to study transactions wherein a nonmarket stakeholder provides a firm access to a valuable resource and to understand when these transactions are governed by formal contracts. We propose that, when a firm makes site-specific investments, a stakeholder’s use rights to the resource sought by the firm, the negative externalities generated by its use, and the stakeholder’s capacity for collective mobilization increase holdup risk for the firm and therefore the probability of a contract. We collect novel data on the location of indigenous communities and mines in Canada to identify a plausible exhaustive set of indigenous communities “at risk” of signing a contract with a mining firm. We test our hypotheses by relying, respectively, on historically assigned property rights over lands, the mine-community colocation in a watershed and proximity on transportation routes, and archival records of community mobilization events. We find support for our propositions by examining which of the 5,342 dyads formed by 459 indigenous communities and 98 firms signed 259 contracts between 1999 and 2013.
Business models increasingly depend on inputs from outside traditional organizational boundaries. For example, platforms that generate revenue from advertising, subscription, or referral fees often rely on user-generated content (UGC). But there is considerable uncertainty on how UGC creates value—and who benefits from it—because voluntary user contributions cannot be mandated or contracted or its quality assured through service-level agreements. In fact, high valuations of these platform firms have generated significant interest, debate, and even euphoria among investors and entrepreneurs. Network effects underlie these high valuations; the value of participation for an individual user increases exponentially as more users actively participate. Thus, many platform strategies initially focus on generating usage with the expectation of profits later. This premise is fraught with uncertainty because high current usage may not translate into future profits when switching costs are low. We argue that the type of user-generated content affects switching costs for the user and, thus, affects the value a platform can capture. Using data about the valuation, traffic, and other parameters from several sources, empirical results indicate greater value uncertainty in platforms with user-generated content than in platforms based on firm-generated content. Platform firms are unable to capture the entire value from network effects, but firms with interaction content can better capture value from network effects through higher switching costs than firms with user-contributed content. Thus, we clarify how switching costs enable value for the platform from network effects and UGC in the absence of formal contracts.
An important and underexamined topic in the growing literature on community-embedded organizing concerns situations in which dramatic shifts in the environment require the time-sensitive re-establishment of both communities and organizations to address urgent needs. We conduct a qualitative study of emergent community-organization trajectories in the aftermath of the 2010 Haiti earthquake and explore differences in the processes and interactions between emerging organizations and communities. Despite all organizations in our data facing the same external shock, they differed in how they interpreted the nature of crisis-induced voids, established boundaries to build and organize communities, and created connections to bind themselves to their communities. We compare and contrast these differences to reveal three trajectories of community-organization emergence, explain why these trajectories initially formed in the ways they did, and identify unique mechanisms that led to these trajectories’ divergence. Our findings contribute to the literature on community-embedded organizing by demonstrating how organizations re-establish communities while simultaneously emerging within those communities.
Across many fields of social science, machine learning (ML) algorithms are rapidly advancing research as tools to support traditional hypothesis testing research (e.g., through data reduction and automation of data coding or for improving matching on observable features of a phenomenon or constructing instrumental variables). In this paper, we argue that researchers are yet to recognize the value of ML techniques for theory building from data. This may be in part because of scholars’ inherent distaste for predictions without explanations that ML algorithms are known to produce. However, precisely because of this property, we argue that ML techniques can be very useful in theory construction during a key step of inductive theorizing—pattern detection. ML can facilitate algorithm supported induction, yielding conclusions about patterns in data that are likely to be robustly replicable by other analysts and in other samples from the same population. These patterns can then be used as inputs to abductive reasoning for building or developing theories that explain them. We propose that algorithm-supported induction is valuable for researchers interested in using quantitative data to both develop and test theories in a transparent and reproducible manner, and we illustrate our arguments using simulations.
This study proposes and tests a new theoretical model explaining whether, and how, supervisors socialize “temporary newcomers,” defined as new organizational members who join an organization on a temporary basis, with a potential, but uncertain, opportunity of receiving a long-term job offer in the future. We suggest that under specific conditions, supervisors first evaluate temporary newcomers’ proactivity based on whether they positively stand out by proposing new feasible ideas and by promoting their achievements. On the basis of these initial evaluations, supervisors then decide whether to increase their support of newcomers’ creativity (using an investiture approach) or to intensify newcomers’ socialization by attempting to change their behavior (using a divestiture approach). When supervisors adopt an investiture approach, it positively influences temporary newcomers’ socialization adjustment outcomes, as indicated by increased newcomer job satisfaction, social integration, task performance, organizational and task socialization, challenge stress, and reduced hindrance stress. When supervisors instead adopt a divestiture approach, it has an opposite (thus negative) effect on the same socialization outcomes. We tested our theoretical model using a mix-method design, based on a three-wave longitudinal sample of 325 newcomer–supervisor dyads spanning a wide range of companies and industries, complemented with interviews of 41 supervisors.
Existing literature shows that corporate investment relationships play an important role in the development of startups. Although startups are relevant sources of innovations, especially those that radically depart from existing technologies, they often have limited access to resources. Corporate investment relationships are relevant to startups because they help them access resources of their corporate partners, especially those that are necessary for innovations to eventually achieve commercial success. This study examines the possibility that these relationships might also affect how startups search for innovations, producing greater alignment with the technologies of incumbents. Investigating this possibility is important because it can partly offset startups’ distinctiveness in technological domains of search and accordingly undercut learning opportunities available to incumbents. We argue that, following the formation of a corporate investment relationship, considerations related to capabilities and incentives result in a startup shifting the search for innovations toward technological domains of its corporate partner. We also argue that the radicalness of a startup’s innovations and the corporate partner’s commercial success exacerbate this search shift. We test these propositions in the context of biotech startups. Our difference-in-differences analysis shows that startups forming corporate investment relationships increase search in the domains of their corporate partners relative to analogous change observed among matching counterfactual startups without such relationships. We discuss implications for understanding of the influences of interorganizational relationships on startups’ technological trajectories and on incumbents’ learning and adaptation.
Extant research shows that entrepreneurs are typically overly optimistic about their ventures’ prospects and that such optimism hampers performance. We analyze how dispositional optimism affects the adjustments to entrepreneurs’ expectations after they receive negative feedback on their task performance. We then explore the relationship between optimism and the effectiveness of innovation. Exploiting unique firm-level data and a laboratory experiment involving 205 entrepreneurs, we find that dispositional optimism is negatively associated with both the likelihood and extent of belief updating in response to negative feedback. Furthermore, dispositional optimism triggers a discrepancy—between innovation inputs and outputs—that reduces a firm’s innovation effectiveness.
We assess the heterogeneous impact of economic downturns on individuals’ decisions to bring high-technology ideas to the market in the form of new ventures. We thereby examine how worsening labor market conditions influence individuals’ opportunity costs of starting new ventures, the resulting composition of the entrepreneurial pool, and start-up performance outcomes. Using a rich data set of start-up founders in the biotechnology and medical device sectors, we find that an increase in the unemployment rate is associated with a substantial rise in the share of entrepreneurs who are most sensitive to worsening labor market conditions. Additionally, we find that start-ups founded by these entrepreneurs display lower financial and innovative performance than start-ups founded by entrepreneurs who are relatively insensitive to business cycles. Finally, we provide suggestive evidence that individuals’ heterogeneous response to worsening labor market conditions is a relevant factor in explaining the negative relationship between unemployment and start-up performance outcomes.
We examine how groups fall prey to the sequence effect when they make choices based on informed assessments of complex situations, for example, when evaluating research and development (R&D) projects. The core argument is that the temporal sequence of selection matters because projects that appear in a sequence following a funded project are themselves less likely to receive funding. Building on the idea that selecting R&D projects is a demanding process that drains participants’ mental and emotional resources, we further theorize the moderating effect of the influence of the timing of the panel meeting on the sequence effect. We test these conjectures using a randomization in sequence order from several rounds of R&D project selection at a leading professional service firm. We find robust support for the existence of a sequence effect in R&D as well as for the moderating effect. We further explore different explanations for the sequence effect and how it passes from the individual to the panel. These findings have broader implications for the literature on innovation and search in general and on group decision making for R&D, specifically, as they suggest that a previously overlooked dimension affects selection outcomes.
The current paper complements and extends traditional Penrosean theories of firm growth by examining how a (supplier) firm’s relational embeddedness with its portfolio of existing buyers affects its business growth. Our theorizing rests on the foundation that a firm’s business growth stems from its breadth (or volume) of opportunities for creating added value with buyers, which more fully realizes the Penrosean vision that firm growth can be explained by a dynamic interaction between productive resources and demand-side market opportunities. Although relational embeddedness may give a supplier dyadic advantages with focal buyers, which supports business growth, we theorize that it can also lead to narrower added value business opportunities with the supplier’s entire portfolio of buyers. Critically, we hypothesize that the effect of relational embeddedness on business growth is moderated by a set of relational and demand-side attributes. These hypotheses are tested on a panel data set of patent law firms (suppliers) and their relationships with corporate clients (buyers). We find that greater relational embeddedness is associated with slower supplier business growth, and consistent with our hypotheses, this negative effect is alleviated when these firms have greater cross-servicing ability and receive more relational commitment from buyers but exacerbated when suppliers hold more buyer-specific knowledge and when buyers undertake more (internal) concurrent sourcing. In turn, our research demonstrates how the attributes of a supplier’s relationships with its portfolio of buyers can impact access to new business opportunities and thus opens up new directions for research on firm growth, demand-side strategy and buyer-supplier relationships.
Social movement organizations (SMOs) are increasingly using collaborative tactics to engage firms. Implications of this are not well understood by researchers. This study investigates one risk that looms over such collaborations: if the corporate partner is later implicated in an industry scandal. Ideas are investigated in the context of the Deepwater Horizon oil spill. First, we find that industry scandals differentially affect contentious and collaborative SMOs’ ability to mobilize resources. SMOs that had collaborated with the oil and gas industry before the spill suffered from reduced public support after the spill, and those that had contentiously interacted with the industry enjoyed increased contributions. Second, we find that industry scandals affect SMOs’ willingness to collaborate with firms in the future. We show that the Horizon oil spill produced a broad chilling effect on environmental SMOs’ collaborations with firms both within and outside of the oil and gas industry. Our findings show that there are risks inherent to a collaborative strategy that cannot be fully mitigated. Further, we demonstrate that industry scandals represent critical exogenous events that affect social activists’ tactical repertoires for engaging in private politics.
External actors often advocate for organizations to address a wide range of societal concerns, such as diversity, equality, and sustainability, and organizations have frequently responded by establishing new positions to oversee these demands. However, calls to address social problems can be broad and unrelated to an organization’s primary objectives, so the external mandates that underpin these new positions do not easily translate to clear task jurisdictions inside organizations. Furthermore, previous studies have found that the tasks that are pursued by occupations established through external pressure often diverge from what external groups had envisioned for these new roles. This study addresses the question of why this divergence occurs. It does so by examining the formation of the occupational group of sustainability managers in higher education. Through fieldwork, interviews, and analyses of longitudinal archival data, this paper uncovers the dynamics of jurisdictional drift and shows how jurisdictional drift unfolded first through sustainability managers’ confrontation of their jurisdictional ambiguity, and then through their efforts at performing neutrality, in particular by trading external Politics for internal politics and trading values for standards. Additionally, it uncovers how the sustainability managers attempted to partially realign their jurisdiction with their external mandate, but did so in a concealed manner. This study illuminates the process of how jurisdictions can come to drift away from mandates, highlights the importance of studying how mandates are translated into jurisdictions, and also furthers our understanding of the formation of externally mandated occupational groups.
When does market success indicate superior merit? We show that when consumer choices between products with equal prices depend on quality but also on past popularity, more popular products are not necessarily of higher quality. Rather, a medium level of popularity may be associated with lower quality than lower levels of popularity. Using a formal model, we show that this kind of nonmonotonic association occurs when reinforcing processes are strong. More generally, a dip can occur when outcomes depend on both quality and resources and the latter are allocated bimodally, with some being given a lot of resources and most receiving little. Empirically, we illustrate that such a dip occurs in the association between movie theater sales and ratings. The presence of a dip in the outcome-quality association complicates learning from market outcomes and evaluation of individuals and new ventures, challenges the legitimacy of stratification systems, and creates opportunities for sophisticated evaluators who understand the dip.
This study develops a theory of due diligence in corporate acquisitions. Using a formal model, the study situates due diligence in the context of economies of scope, which are often sought by acquiring organizations that have incomplete information about such economies. Relatedness, the key determinant of economies of scope, and ambiguity, the key determinant of incomplete information, are used to derive the optimal due diligence effort and the returns to an acquiring organization that result from that effort. The derived predictions qualify both the general appeal to extensive due diligence and the general recognition of the costliness of due diligence. These predictions can be tested in future empirical research on corporate acquisitions and may guide corporate acquirers on the optimal allocation of their due diligence efforts in the mergers and acquisitions market.
This paper explores how social psychological biases impede the social exchange relations of executives who ascend to high-status corporate leadership positions. We theorize that a combination of self-serving attribution biases among executives who gain status and egocentric biases of their prior benefactors cause a systematic difference in perceptions about the relative importance of that help to the beneficiary’s success. This leads to the perception among prior benefactors that the high-status executives have not adequately reciprocated their help. We then extend this argument to explain why perceptions of under-reciprocation are heightened when the high-status executive is a racial minority or a woman but reduced when the prior benefactor is a racial minority or a woman. The final element of our theoretical framework examines the social consequences of perceived under-reciprocation for corporate leaders. We suggest that the high-status leaders’ access to strategic help is reduced, and they may become the target of social undermining that can damage their broader reputation. The findings support our social psychological perspective on social exchange in corporate leadership, revealing how and why executives who have ascended to high-status positions not only may encounter difficulty obtaining assistance from fellow leaders but also may experience adverse reversals in their social exchange ties such that managers who previously aided them engage in socially harmful behavior toward them.
Although a substantial body of work has investigated drivers of tie formation, there is growing interest in understanding why relationships decay or dissolve altogether. The networks literature has tended to conceptualize tie decay as driven by processes similar to those underlying tie formation. Yet information that is revealed through ongoing interactions can exert different effects on tie formation and tie decay. This paper investigates how tie decay and tie formation processes differ by focusing on contentious practices. To the extent that information about dissimilarities in contentious practices is learned through ongoing interactions, it can exert diverging effects on tie formation and tie decay. Using a longitudinal data set of 141,543 physician dyads, we find that differences in contentious prescribing led ties to weaken or dissolve altogether but did not affect tie formation. The more contentious the practice and the more information available about the practice, the stronger the effect on tie decay and dissolution. Collectively, these findings contribute to a more nuanced understanding of relationship evolution as an unfolding process through which deeper-level differences are revealed and shape the outcome of the tie.
In many facets of life, individuals make evaluations that they may update after consulting with others in their networks. But not all individuals have the same positional opportunities for social interaction in a given network or the ability and desire to make use of those opportunities that are available to them. The configuration of a person’s network can also alter how information is spread or interpreted. To complicate matters further, scant research has considered how positions in social networks and the valence of network content interact because of the difficulty of (a) separating the “player” from the position in networks and (b) measuring all germane content in a particular network. This research develops a novel experimental platform that addresses these issues. Participants viewed and evaluated an entrepreneurial video pitch and were then randomly assigned to different networks, and positions within networks, and thus various opportunities for peer influence that were orthogonal to their network history, inclinations, attributes, or capabilities. Furthermore, all the content of social interaction, including its valence, was recorded to test underlying assumptions. Results reveal that those assigned to a position with brokerage opportunities in a network updated their evaluations of the entrepreneurial video considerably more negatively.
The persistent failure of organizations to engage diversity—to employ a diverse workforce and fully realize its potential—is puzzling, as it creates labor-market inefficiencies and untapped opportunities. Addressing this puzzle from a behavioral strategy as arbitrage perspective, this paper argues that attractive opportunities tend to be protected by strong behavioral and social limits to arbitrage. I outline four limits—cognizing, searching, reconfiguring, and legitimizing (CSRL)—that deter firms from sensing, seizing, integrating, and justifying valuable diversity. The case of Moneyball is used to illustrate how these CSRL limits prevented mispriced human resources from being arbitraged away sooner, with implications for engaging cognitive diversity that go beyond sports. This perspective describes why behavioral failures as arbitrage opportunities can persist and prescribes strategists, as contrarian theorists, a framework for formulating relevant behavioral and social problems to solve in order to search for and exploit these untapped opportunities.
The fatal August 9, 2014, officer-involved shooting of a Black teenager in Ferguson, Missouri, sparked a series of local protests that culminated in a national social movement: Black Lives Matter. In this study, through a minute-by-minute analysis of crowd dynamics, I find that the eventual social movement strategy emerged from spontaneous acts of anger in protest crowds within the first 48 hours of the shooting. This finding is surprising in light of social movement scholarship, in which strategy is thought to follow from rationality and decision making within formal organizations, not emotionality and spontaneous action within informal crowds. By coupling a historical analysis of protest and policing practices with a comparison of prominent theories of crowds, emotion, and strategy, I theorize how strategy can emerge from spontaneous acts of anger as part of a distributed sensemaking process in crowds, rather than conflating strategy with rationality and deliberate planning in organizations. Taken in sum, this study challenges prevailing ideas about the wisdom of crowds and exemplifies the immanent potential for change, in which our seemingly “micro” actions are not trivial but can influence even the most “macro” of strategic outcomes.
This paper addresses the recognized need for connecting scholarship on materiality and evaluation by conceptualizing how materiality provides grounds for “valuation entrepreneurship.” It extends the scope of materiality scholarship by considering an ignored organizational outcome while offering stronger evidence for the role of supply-side factors in social evaluation. The theoretical model posits that materiality affords opportunities for identity construction and social organization that can lead to the emergence of a new theory of value contesting the evaluative regime. This framework is applied to the reanalysis of a famous case: Impressionism. The analysis shows that new materials and methods of painting served as a “focus” for the social organization of artists with a shared identity of craftsmen. These artists espoused a new theory of value that advocated the “unfinishedness” of artworks and used natural perception as an objective basis for contestation of the “subjective” evaluative regime at the salons. The contestation had political overtones, drawing on cultural resources and scientific tenets to justify the valorization of individuality and decentralization of art appraisal. An endogenous account of culture in action presents materiality as a natural counterpoint to the emphasis on conceptualization.
Long-standing wisdom holds that criticism is antithetical to effective brainstorming because it incites intragroup conflict. However, a number of recent studies have challenged this assumption, suggesting that criticism might actually enhance creativity in brainstorming by fostering divergent thinking. Our paper reconciles these perspectives with new theory and a multimethod investigation to explain when and why criticism promotes creativity in brainstorming. We propose that a cooperative social context allows criticism to be construed positively, spurring creativity without inciting intragroup conflict, whereas a competitive social context makes criticism more divisive, leading to intragroup conflict and a corresponding reduction in creativity. We found support for this theory from a field experiment involving 100 group brainstorming sessions with actual stakeholders in a controversial urban planning project. In a cooperative context, instructions encouraging criticism yielded more ideas and more creative ideas, whereas in a competitive context, encouraging criticism yielded fewer ideas and less creative ideas. We replicated this finding in a laboratory study involving brainstorming in the context of a union-management negotiation scenario, which allowed us to hold constant the nature of the criticism. Taken together, our findings suggest that the optimal context for creativity in brainstorming is a cooperative one in which criticism occurs but is interpreted constructively because the brainstorming parties perceive their goals as aligned.
In this paper, we investigate the attentional engagement of chief executive officers (CEOs) of large healthcare organizations in England. We study attention ethnographically as something managers do—at different times, in context, and in relation to others. We find that CEOs match the challenges of volume, fragmentation, and variety of attentional demands with a bundle of practices to activate attention, regulate the quantity and quality of information, stay focused over time, and prioritize attention. We call this bundle of practices the CEO’s attentional infrastructure. The practices that compose the attentional infrastructure work together to ensure that CEOs balance paying too much with paying too little attention, sustain attention on multiple issues over time, and allocate attention to the issues that matter, while avoiding becoming swamped by too many other concerns. The attentional infrastructure and its component practices are constantly revised and adapted to match the changes in the environment and ensure that managers remain on top of the things that matter to them. The idea of a practice-based attentional infrastructure advances theory by expanding and articulating the concept of attentional engagement, a central element in the attention-based view of the firm. We also demonstrate the benefits of studying attention as practice, rather than as an exclusively mental phenomenon. Finally, we contribute to managerial practice by introducing a set of categories that managers can use to interrogate their existing attentional practices and address attentional traps and difficulties.
This paper develops a new understanding about how “client managers”—those using platform labor markets to hire and manage workers—attempt to maintain control when managing skilled contractors. We conducted an inductive field study analyzing interactions between client managers and contractors in software development “gigs” mediated by a platform labor market. The platform provided multiple tools client managers could use for control, including in response to unexpected events. We found that, when managers used the tools to exert coercive control over contractors acting unexpectedly, it backfired and contributed to uncompleted project outcomes. In contrast, when they refrained from using the tools for coercive control in such circumstances and instead engaged in what we call collaborative repair, their actions contributed to completed project outcomes. Collaborative repair refers to interactions that surface misaligned interpretations of a situation and help parties negotiate new, reciprocal expectations that restore trust and willingness to continue an exchange. Client managers’ attempts at collaborative repair yielded fuller understanding of project-related breakdowns and shared investment in new expectations, facilitating effective control and completed projects. This study extends prior theories of control by characterizing the new client manager role created by platforms and demonstrating how initiating repair is integral for managers’ capacity to accomplish control in these comparatively brittle work relationships.
This study investigates the negative relationship between firm risk and accounting performance known in the strategy field as the Bowman paradox, which has been generally attributed to differences across firms in their willingness to take risk. Most research to date relies on the behavioral theory of the firm to suggest that underperformers take greater risks to increase their performance to their reference point. As an alternative explanation, we suggest that the Bowman paradox may result from the inherent vulnerability of low performers to negative external shocks. Our panel analysis of 2,681 U.S. firms from 1980 to 2010 confirms that firms with lower performance within their industry are more affected by negative shocks to the economy. The asymmetric vulnerability of low performers to external events makes their overall accounting performance more volatile and difficult to predict by market analysts, even if all firms have a similar attitude toward risk taking and capabilities to manage change. Our vulnerability explanation is also supported by our empirical analysis of the 2008 global financial crisis as a natural experiment. Furthermore, we find strong evidence of a negative risk–return relationship using different methods to control for their endogeneity.
How a firm views its competitors affects its performance. We extend the networks literature to examine how a firm’s positioning in competition networks—networks of perceived competitive relations between firms—relates to a significant organizational outcome, namely, product innovation. We find that when firms position themselves in ways that allow them to see differently than rivals, new product ideas emerge. Simply put, firms with an unusual view of competition are more innovative. We situate our analysis in the U.S. enterprise infrastructure software industry, examining the relationship between the firm’s position in competition networks and its innovation over the period of 1995–2012. Using both archival and in-depth field data, we find that two factors—the focal firm’s spanning of structural holes in the network and the perception of peripheral firms as competitors—are positively associated with its product innovation. At the same time, turnover in firms perceived as competitors has an unexpected negative association with innovation. Overall, the findings suggest that firms benefit when they see the competitive landscape differently than their competitors. The findings also show that what we know about innovation-enhancing positioning in collaboration networks does not necessarily hold in competition networks.
In this special issue, we review 14 articles published in Organization Science over the past 25 years examining large-scale collaborations (LSCs) tasked with knowledge dissemination and innovation. LSCs involve sizeable pools of participants carrying out a common mission such as developing open-source software, detector technologies, complex architecture, encyclopedias, medical cures, or responses to climate change. LSCs depend on technologies because they are often geographically distributed, incorporate multiple and diverse epistemic perspectives. How such technologies need to be structured and appropriated for effective LSC collaborations has been researched in piecemeal fashion by examining a single technology used in a single collaboration context with little opportunity for generalization. Studies have tended to black box technology use even though they acknowledge such uses to be critical to the LSC operation. We unveil the black box surrounding LSC collaboration technologies by identifying three challenges that LSCs face when they pursue an LSC effort: (1) knowledge exchange challenges, (2) knowledge deliberation challenges, and (3) knowledge combination challenges. We examine how technology was used in responding to these challenges, synthesizing their use into three socio-technical affordances to improve knowledge dissemination efficiency and innovation effectiveness: knowledge collaging, purposeful deliberating, and knowledge interlacing. We demonstrate the intellectual benefit of incorporating socio-technical affordances in studies of LSCs including what small group collaboration research can learn from LSCs.
Actors engaged in learning from rare events must trade off between two different criteria for effective learning: validity—the extent to which learning can be used for understanding, prediction, and control—and reliability—the extent to which understandings of experience are public, stable, and shared. Existing models of learning from rare events have elided conflict and politics by assuming that individuals and organizations always seek new valid knowledge that then becomes public, stable, and shared across actors. Here we examine the politics of learning in a historical analysis of population-level learning by four different actors following the 1994 sinking of the ferry Estonia. We show how politics shaped the trade-off between reliability and validity and, in turn, shaped the nature of the learning. Whereas the new knowledge was sometimes both valid and reliable, the more common outcome was knowledge that was only partly valid and reliable. Rather than treat these outcomes as substandard, we show how they are important to the dynamics of learning, as different population-level actors take into account different aspects of experience. The result is a model that makes conflict and contestation—and hence politics—essential to effective learning.
How do we reconcile misalignments between a system’s existing normative and cognitive elements and novel regulatory change? Prior work either largely focuses only on regulatory change or analyzes normative and cognitive barriers in parallel to rather than in interaction with regulatory change. Moreover, the institutional entrepreneurship literature that focuses on reconciling such misalignments is predominantly centered on the tactics of entrepreneurs rather than the support provided by institutional carriers. We, therefore, use the case of the Chinese Academy of Sciences (CAS) Knowledge Innovation Program (KIP) to better understand these neglected facets of institutional change. Through a mixed methods approach, we posit and find support for two key mechanisms that support regulatory change. First, institutional carriers (e.g., CAS institutes) clarify the market relevance of technical knowledge, linking cognitive support to regulatory change. Second, institutional carriers (e.g., science parks) create shared standards that could not occur otherwise, linking normative support to regulatory change. Finally, these changes to institutions seem particularly associated with more nascent clusters. Our study contributes to studies at the nexus between institutional change and entrepreneurship by highlighting the role of linking cognitive and normative support to regulatory changes aimed at increasing entrepreneurship.
How organization designs evolve between adaptation to changing conditions and the pressures toward persistence of the designs adopted at founding remains an understudied phenomenon. To fill this lacuna, we conducted a longitudinal, multicase study of eight young ventures. We find that, in these ventures, specific organization design solutions changed frequently, triggered by various internal and external developments, although the changes were typically incremental and myopic. However, the more abstract principles of design, captured in the founders’ logics of organizing, were less amenable to change. This explains why observations of imprinting effects in logics of organizing may be consistent with observations of dynamic change to organization designs.
Business elites influence the allocation of resources to a range of causes related to the social good, such as to corporate community or environmental programs. We extend research on executive influence on corporate attention to alternative causes by showing how chief executive officers’ (CEOs’) engagement in two distinct institutional domains, corporate social responsibility (CSR) and independent foundation philanthropy, are interrelated. We draw on the psychology of moral accounting to refine the assumption of personal consistency prevalent in studies of executives’ corporate influence. Specifically, we show that executives use flexible means to realize an overall aspiration of doing good, resulting in divergent emphases in their CSR and philanthropic causes. Evidence comes from a panel of 677 corporations linked to 309 foundations through 1,109 CEOs during the period 2003–2011. CEOs compensated for deficits in their firms’ CSR record by joining the board of trustees of specific nonprofit foundations, but subsequently advanced divergent cause priorities in the corporation and the foundation. Our work suggests that studies of CSR and of executive influence on organizations benefit from taking into account executives’ cross-domain engagement.
Over the past two decades, women have increased their representation among multiboard directors—corporate directors who simultaneously hold seats on two or more firms. Traditionally, multiboard directors exercised greater power and influence in corporate governance. As a consequence, women’s increased representation among this “inner circle” could signal women’s increased influence in corporate governance. However, women’s access to these elite positions comes at a time when multiboard holding has declined. This paper investigates gendered patterns in access to and outcomes of multiboard holding. I argue that these patterns reflect gendered logics in director appointment practices such that firms increasingly recruit and appoint highly boarded female directors, but multiboard women continue to lag in substantive influence in the boardroom. Analyses of nearly two decades of data on S&P 1500 boards demonstrate female directors’ increased access to the corporate inner-circle, but this access is decoupled from increased participation in board committees and interorganizational social influence. I discuss implications for theory on gender tokenism, corporate networks, and board processes.
A new design can be compared with its contemporaries or older designs. In this study, we argue that the temporal distance between the new design and its comparison play an important role in understanding how a new design’s similarity with other designs contributes to its valuation. Construing the value of designs as a combination of their informational value and their expressive value, we propose the “anchored differentiation” hypothesis. Specifically, we argue that expressive value (which is enhanced by how much the new design appears different from others) is emphasized more than informational value (which is enhanced by how much the new design appears similar to others) compared with contemporary designs. Informational value, however, is emphasized more than expressive value when compared against designs from the past. Therefore, both difference from other contemporary designs (contemporary differentiation) and similarity to other past designs (past anchoring) help increase the value of a new design. We find consistent evidence for our theory across both a field study and an experimental study. Furthermore, we show that this is because temporal distance changes the relative emphasis on expressive and informational values. We discuss our contribution to the growing literature on optimal distinctiveness and design innovation by offering a dynamic perspective that helps resolve the tension between similarities and differences in evaluating new designs.
Existing academic literature has discussed contracts and relational governance as the key mechanisms that help alliance partners address problems of cooperation and coordination. However, when an alliance undergoes disruption, the nature and extent of such problems may change and therefore the value of these mechanisms may change. This study advances a dynamic perspective on alliance governance by examining the impact of disruption and subsequent adjustment on the value of alliance governance mechanisms. To this end, we longitudinally studied a revelatory case of a research and development alliance in the veterinary drug industry that experienced disruption triggered by an internal restructuring at one of the partner companies. We approached the evidence with a fine-grained typology that builds on two dimensions that underlie governance mechanisms: the means to enforce their ruling principles (contractual versus relational) and the level of codification of these principles (formal versus informal). Based on our findings, we (1) show the significance of this revised typology, which suggests that contractual governance is not necessarily formal and relational governance is not necessarily informal; (2) provide a more systematic discussion of the tradeoffs that the various mechanisms entail and how these are altered through disruption and adjustment dynamics; and (3) analyze how the interplay between different types of governance mechanisms evolves following disruption and adjustment. Overall, our study brings the concept of disruption to the dynamic perspective of alliance governance and highlights the contingent value of alliance governance mechanisms.
Despite increasing studies of information technology (IT) monitoring, our understanding of how IT-mediates relations between the watcher and watched remains limited in two areas. First, either traditional actor-centric frameworks assuming predefined watcher-watched relationships (e.g., panopticon or synopticon) are adopted or monitoring actors are removed to focus on data flows (e.g., dataveillance, assemblages, panspectron). Second, IT monitoring research predominantly assumes IT artifacts to be stable, bounded, designed objects, with prescribed uses which provides an oversimplified view of actor relationships. To redress these limitations, a conceptual framework of veillance applicable to a variety of possible IT or non-IT-mediated relationships between watcher and watched is developed. Using the framework, we conduct a conceptual review of the literature, identifying IT-enabled monitoring and transformations of actors, goals, mechanisms and foci and develop an action net model of IT veillance where IT artifacts are theorized as equivocal, distributable and open for diverse use, open to edits and contributions by unbounded sets of heterogenous actors characterized by diverse goals and capabilities. The action net of IT veillance is defined as a flexible decentralized interconnected web shaped by multidirectional watcher-watched relationships, enabling multiple dynamic goals and foci. Cumulative contributions by heterogenous participants organize and manipulate the net, having an impact through influencing dispositions, visibilities and the inclusion/exclusion of self and others. The model makes three important theoretical contributions to our understanding of IT monitoring of watchers and watched and their relationships. We discuss implications and avenues for future studies on IT veillance.
Technologies are changing at a rapid pace and in unpredictable ways. The scale of their impact is also far-reaching. Technologies such as artificial intelligence, data analytics, robotics, digital platforms, social media, blockchain, and 3-D printing affect many parts of the organization simultaneously, enabling new interdependencies within and between units and with actors that many organizations have typically considered to be outside their boundaries. Consequently, today’s emerging technologies have the potential to fundamentally shape all aspects of organizing. This article introduces the special issue “Emerging Technologies and Organizing.” We treat these new technologies as “emerging” because their uses and effects are still varied and have yet to stabilize around a recognizable set of patterns and because the technologies themselves are, by design, always changing and adapting. To theorize the relationship between emerging technologies and organizing, we draw on relational thinking in philosophy and sociology to develop a relational perspective on emerging technologies. Our goal in doing so is to create a new way for organizational scholars to incorporate the ever-increasing role of technology in their theorizing of key organizational processes and phenomena. By developing a relational perspective that treats emerging technologies not as stable entities, but as a set of evolving relations, we provide a novel way for organizational scholars to account for the role of technology in their topics of interest. We sketch the outlines of this relational perspective on emerging technologies and discuss the implications it has for what organizational scholars study and how we study it.
Data are no longer simply a component of administrative and managerial work but a pervasive resource and medium through which organizations come to know and act upon the contingencies they confront. We theorize how the ongoing technological developments reinforce the traditional functions of data as instruments of management and control but also reframe and extend their role. By rendering data as technical entities, digital technologies transform the process of knowing and the knowledge functions data fulfil in socioeconomic life. These functions are most of the times mediated by putting together disperse and steadily updatable data in more stable entities we refer to as data objects. Users, customers, products, and physical machines rendered as data objects become the technical and cognitive means through which organizational knowledge, patterns, and practices develop. Such conditions loosen the dependence of data from domain knowledge, reorder the relative significance of internal versus external references in organizations, and contribute to a paradigmatic contemporary development that we identify with the decentering of organizations of which digital platforms are an important specimen.
Existing literature examines control and resistance in the context of service organizations that rely on both managers and customers to control workers during the execution of work. Digital platform companies, however, eschew managers in favor of algorithmically mediated customer control—that is, customers rate workers, and algorithms tally and track these ratings to control workers’ future platform-based opportunities. How has this shift in the distribution of control among platforms, customers, and workers affected the relationship between control and resistance? Drawing on workers’ experiences from a comparative ethnography of two of the largest platform companies, we find that platform use of algorithmically mediated customer control has expanded the service encounter such that organizational control and workers’ resistance extend well beyond the execution of work. We find that workers have the most latitude to deploy resistance early in the labor process but must adjust their resistance tactics because their ability to resist decreases in each subsequent stage of the labor process. Our paper, thus, develops understanding of resistance by examining the relationship between control and resistance before, during, and after a task, providing insight into how control and resistance function in the gig economy. We also demonstrate the limitations of platforms’ reliance on algorithmically mediated customer control by illuminating how workers’ everyday interactions with customers can influence and manipulate algorithms in ways that platforms cannot always observe.
This paper presents research on how knowledge brokers attempt to translate opaque algorithmic predictions. The research is based on a 31-month ethnographic study of the implementation of a learning algorithm by the Dutch police to predict the occurrence of crime incidents and offers one of the first empirical accounts of algorithmic brokers. We studied a group of intelligence officers, who were tasked with brokering between a machine learning community and a user community by translating the outcomes of the learning algorithm to police management. We found that, as knowledge brokers, they performed different translation practices over time and enacted increasingly influential brokerage roles, namely, those of messenger, interpreter, and curator. Triggered by an impassable knowledge boundary yielded by the black-boxed machine learning, the brokers eventually acted like “kings in the land of the blind” and substituted the algorithmic predictions with their own judgments. By emphasizing the dynamic and influential nature of algorithmic brokerage work, we contribute to the literature on knowledge brokerage and translation in the age of learning algorithms.
Technologies are known to alter social structures in the workplace, reconfigure roles and relationships, and disrupt status hierarchies. However, less attention has been given to how an emerging technology disrupts the meaning and moral values that tether people to their work and render it meaningful. To understand how workers respond to such an emerging technology, we undertook an inductive, qualitative study of military personnel working in unmanned aerial vehicles, or drone operations, for the U.S. Air Force. We draw on multiple data sources, including personal diaries kept by personnel involved in drone operations. We identified three characteristics of drone technology: remote-split operations, remote piloting of unmanned vehicles, and interaction through iconic representations. Our analysis suggests that drone technology has revolutionized warfare by (1) creating distanciated intimacy, (2) dissolving traditional spatio-temporal boundaries between work and personal life, and (3) redefining the legal and moral parameters of work. Drone program workers identified with these changes to their working environment in contradictory ways, which evoked emotional ambivalence about right and wrong. However, their organization gave them little help in alleviating their conflicting feelings. We illuminate how workers cope with such ambivalence when a technology transforms the meaning and morality of their work. We extend theory by showing that workers’ responses to a changed working environment as a result of a remote technology are not just based on how the technology changes workers’ tasks, roles, and status but also on how it affects their moral values.
This paper develops and deploys a theoretical framework for assessing the prospects of a cluster of technologies driving what is often called the digital transformation. There is considerable uncertainty regarding this transformation’s future trajectory, and to understand and bound that uncertainty, we build on Schumpeter’s macro-level theory of economy-wide, technological revolutions and on the work of several scholars who have extended that theory. In this perspective, such revolutions’ trajectories are shaped primarily by the interaction of changes within and between three spheres—technology, organization, and public policy. We enrich this account by identifying the critical problems and the collective choices among competing solutions to those problems that together shape the trajectory of each revolution. We argue that the digital transformation represents a new phase in the wider arc of the information and communication technology revolution—a phase promising much wider deployment—and that the trajectory of this deployment depends on collective choices to be made in the organization and public policy spheres. Combining in a 2 × 2 matrix the two main alternative solutions on offer in each of these two spheres, we identify four scenarios for the future trajectory of the digital transformation: digital authoritarianism, digital oligarchy, digital localism, and digital democracy. We discuss how these scenarios can help us trace and understand the future trajectory of the digital transformation.
Artificial intelligence (AI) technologies promise to transform how professionals conduct knowledge work by augmenting their capabilities for making professional judgments. We know little, however, about how human-AI augmentation takes place in practice. Yet, gaining this understanding is particularly important when professionals use AI tools to form judgments on critical decisions. We conducted an in-depth field study in a major U.S. hospital where AI tools were used in three departments by diagnostic radiologists making breast cancer, lung cancer, and bone age determinations. The study illustrates the hindering effects of opacity that professionals experienced when using AI tools and explores how these professionals grappled with it in practice. In all three departments, this opacity resulted in professionals experiencing increased uncertainty because AI tool results often diverged from their initial judgment without providing underlying reasoning. Only in one department (of the three) did professionals consistently incorporate AI results into their final judgments, achieving what we call engaged augmentation. These professionals invested in AI interrogation practices—practices enacted by human experts to relate their own knowledge claims to AI knowledge claims. Professionals in the other two departments did not enact such practices and did not incorporate AI inputs into their final decisions, which we call unengaged “augmentation.” Our study unpacks the challenges involved in augmenting professional judgment with powerful, yet opaque, technologies and contributes to literature on AI adoption in knowledge work.
Past research offers mixed perspectives on whether domain experience helps or hurts algorithm-augmented worker performance. Reconciling these perspectives, we theorize that intermediate levels of domain experience are optimal for algorithm-augmented performance, due to the interplay between two countervailing forces—ability and aversion. Although domain experience can increase performance via increased ability to complement algorithmic advice (e.g., identifying inaccurate predictions), it can also decrease performance via increased aversion to accurate algorithmic advice. Because ability developed through learning by doing increases at a decreasing rate, and algorithmic aversion is more prevalent among experts, we theorize that algorithm-augmented performance will first rise with increasing domain experience, then fall. We test this by exploiting a within-subjects experiment in which corporate information technology support workers were assigned to resolve problems both manually and using an algorithmic tool. We confirm that the difference between performance with the algorithmic tool versus without the tool was characterized by an inverted U-shape over the range of domain experience. Only workers with moderate domain experience did significantly better using the algorithm than resolving tickets manually. These findings highlight that, even if greater domain experience increases workers’ ability to complement algorithms, domain experience can also trigger other mechanisms that overcome the positive ability effect and inhibit performance. Additional analyses and participant interviews suggest that, even though the highest experience workers had the greatest ability to complement the algorithmic tool, they rejected its advice because they felt greater accountability for possible unintended consequences of accepting algorithmic advice.
Organizational accountability is considered critical to organizations’ sustained performance and survival. Prior research examines the structural and rhetorical responses that organizations use to manage accountability pressures from different constituents. With the emergence of social media, accountability pressures shift from the relatively clear and well-specified demands of identifiable stakeholders to the unclear and unspecified concerns of a pseudonymous crowd. This is further exacerbated by the public visibility of social media, materializing as a stream of online commentary for a distributed audience. In such conditions, the established structural and rhetorical responses of organizations become less effective for addressing accountability pressures. We conducted a multisite comparative study to examine how organizations in two service sectors (emergency response and hospitality) respond to accountability pressures manifesting as social media commentary on two platforms (Twitter and TripAdvisor). We find organizations responding online to social media commentary while also enacting changes to their practices that recalibrate risk, redeploy resources, and redefine service. These changes produce a diffractive reactivity that reconfigures the meanings, activities, relations, and outcomes of service work as well as the boundaries of organizational accountability. We synthesize these findings in a model of crowd-based accountability and discuss the contributions of this study to research on accountability and organizing in the social media era.
We offer a path-centric theory of emerging technology and organizing that addresses a basic question. When does emerging technology lead to transformative change? A path-centric perspective on technology focuses on the patterns of actions afforded by technology in use. We identify performing and patterning as self-reinforcing mechanisms that shape patterns of action in the domain of emerging technology and organizing. We use a dynamic simulation to show that performing and patterning can lead to a wide range of trajectories, from lock-in to transformation, depending on how emerging technology in use influences the pattern of action. When emerging technologies afford new actions that can be flexibly recombined to generate new paths, decisive transformative effects are more likely. By themselves, new affordances are not likely to generate transformation. We illustrate this theory with examples from the practice of pharmaceutical drug discovery. The path-centric perspective offers a new way to think about generativity and the role of affordances in organizing.
Emerging digital technologies give rise to digital entrepreneurship and the widespread phenomenon of open source collaboration (OSC) on GitHub for entrepreneurial pursuits. Although openness is a common theme in digital entrepreneurship, it is unclear how digital startups—that is, startups that that have digital artifacts at the core of their business model for value creation and capture—actually realize value from their OSC engagement. We develop a theoretical framework to explain how the engagement in OSC may affect the value of digital startups and how the effect is contingent on the stage of venture maturity (conception, commercialization, or growth) and the mode of OSC engagement (inbound or outbound). In analyses that pool 17,552 matched digital startups with monthly panel observations between 2008 and 2017, we find digital startups in the conception and commercialization stages benefit more from inbound OSC whereas the ones in the growth stage benefit more from outbound OSC. As digital startups increasingly use OSC for ideation, experimentation, and scaling, our contribution is to show whether, when, and how knowledge flows through OSC might affect the value of digital startups. We discuss implications for research on organizing for digital entrepreneurship as well as open innovation.
On-demand or “gig” workers show up to a workplace without walls, organizational routines, managers, or even coworkers. Without traditional organizational scaffolds, how do individuals make meaning of their work in a way that fosters engagement? Prior literature suggests that organizational practices, such as recruitment and socialization, foster group belonging and meaningfulness, which subsequently leads to engagement, and that without these practices alienation and attrition ensue. My four-year qualitative study of workers in the largest sector in the on-demand economy (ridehailing) suggests an alternative and more readily available mechanism of engagement—workplace games. Through interactions with touchpoints—in this context, the customer and the app—individuals turn their work into games they find meaningful, can control, and “win.” In the relational game, workers craft positive customer service encounters, offering gifts and extra services, in the pursuit of high customer ratings, which they track through the app’s rating system. In the efficiency game, workers set boundaries with customers, minimizing any “extra” behavior, in the pursuit of maximizing money per time spent driving and they create their own tracking tools outside the app. Whereas each game resulted in engagement—as workers were trying to “win”—games were associated with two divergent stances or relationships toward the work, with contrasting implications for retention. My findings embed meaning-making in what is fast-becoming the normal workplace, largely solitary and structured by emerging technologies, and holds insights for explaining why people remain engaged in a line of work typically deemed exploitative.
Transferring individuals who possess relevant knowledge from one organizational unit to another—a form of resource redeployment—may help to overcome impediments to knowledge transfer. Despite the promise of this mechanism, which often occurs through intrafirm geographic mobility, relatively little research has examined how the knowledge and expertise of individuals interacts with the organizational resources of the units to which individuals move. This study examines whether intrafirm geographic mobility improves organizational performance by providing a conduit for the transfer of knowledge while accounting for the interaction between individual knowledge and factors at the organization-unit level of analysis. We analyze the performance effects of the transfer of engineers who have expertise in innovative process technologies. The results from a large multinational company show that the innovative process technology-related expertise of an individual engineer who moves to a new organizational unit is positively associated with the performance of that unit, suggesting that intrafirm geographic mobility improves organizational performance by providing a conduit for the transfer of knowledge. The results also show that the technology-related knowledge of engineers is a substitute for organization-level factors when a unit uses only technologies with which it is already familiar, whereas the technology-related knowledge of engineers is a complement to organization-level factors when units introduce new technologies. Thus, individuals who bring novel expertise to their organizational units through intrafirm mobility may be important vehicles for organizational learning and building new competences, helping to diffuse best practices.
Product innovation can result from the novel design and combination of product components as well as from changing the underlying architecture: that is, the way components interact with each other. Even though previous studies have shown that architectural change can constitute a powerful source of innovation, little insight exists on how organizations should engage in architectural search itself. In this paper, using computer simulation, we explore underlying mechanisms of architectural search. We find that contrary to search for component combinations, architectural search provides greater performance improvements the narrower the search scope, regardless of product complexity. Moreover, our theory and findings suggest a more differentiated typology of architectural innovation. Although narrow architectural search often leads to pure architectural innovations that do not require substantial component changes, broader architectural search often leads to composite architectural innovation (i.e., architectural innovations that typically render existing component designs suboptimal but allow for new high-performing component combinations to arise). Lastly, although narrow architectural search outperforms broad architectural search in the long run, in the short run broad architectural search can have performance advantages.
Although property-rights theory has long been used to explain firms’ ownership of resources, research on the channels through which property rights affect heterogeneous firms’ investment in building resources remains scarce. Leveraging a property-law enactment in China, we find that strengthening property-rights protection leads private firms to make greater intangible and tangible asset investment compared with state-owned firms and that these effects are mediated by external equity and debt financing. Further, we unpack resource heterogeneity by explicating key differences between intangible and tangible assets, and we document an alignment between asset intangibility and financing approaches such that for intangible asset investment, equity plays a larger mediation role, whereas for tangible asset investment, debt’s mediating effect is greater. We contribute to the strategy literature by using property-rights theory to link together asset intangibility and financing approaches and by showing that the strength of property-rights protection affects firms’ resource investment and shapes firm heterogeneity.
Multiteam structures are increasingly used to coordinate complex tasks between different groups. To realize this potential, however, the members of a multiteam structure must manage a complex set of boundary relations within, between, and beyond the various constituent teams—boundary relations that can be cooperative, competitive, or some combination of both at the same time. This multimethod study provides insight into how multiteam structures can meet this challenge. Specifically, we examined how the different organizations that utilize and support the Dutch railway system learned to manage boundaries as they transitioned from a centralized, arms-length structure to a colocated, multiteam structure for coordinating disruption responses (i.e., the Rail Operations Control Center (ROCC)). In part 1 of our study, qualitative analyses of interview, observational, and archival data suggested that learning to manage boundaries within the ROCC was not simple or linear but evolved through trial and error during various phases. Ultimately, the ROCC developed an approach we call “integrated pluralism,” establishing a dynamic balance that combines both collaborative and competitive approaches to boundary management. In this manner, the ROCC teams were able to attain integrated solutions and coordinated task accomplishment while simultaneously defending internal team operations and home organization interests. In part 2, we employed an interrupted time series analysis to demonstrate that the implementation of the ROCC resulted in significant performance improvements. Consistent with the results of part 1, we found that these improvements emerged gradually over time as teams learned to work out their boundary relations and transitioned to integrated pluralism. These findings provide new insights into how individuals and teams can work together to tackle the unique boundary management challenges presented by multiteam structures and illuminate the dynamic trial and error process by which component teams can learn to both cooperate and compete.
We advance understandings of knowledge transfer by showing the central role of symbolic action, taking the form of ritual, in contexts characterized by worldview differences. Using qualitative data from interactions between farming communities in rural Ghana and agriculture development specialists, we examine how rituals do relational work that enables informational work. We find that rituals (i.e., visits, value affirmations, gift-giving, prayer, performing, storytelling) do so by means of their functions–bracketing worldview differences, modeling collaboration between farmers and agriculture development specialists, and packaging new knowledge in displays of compatibility. Our work also expands scholarship on the role of rituals in organizations and on management practices in Africa. Overall, our paper offers a complex, comprehensive view of knowledge transfer as involving both relational and informational work and relying on both symbolic action and tangible elements.
We hypothesize that employee mobility between organizations will be lower when the organizations’ managers share affiliation ties. We test this idea by examining interorganizational employee mobility between large corporate law practices. We find that a practice area is less likely to hire attorneys from a rival practice area when the leaders of the two practice areas attended the same law school at the same time, our proxy for the presence of an affiliation tie. The negative relationship is stronger for hiring higher-ranked attorneys, and it is driven by practice leaders from the same law school class. Exploiting appointments of new practice leaders, we find a sharp and immediate decline in interorganizational mobility following an appointment that creates an affiliation tie between the leadership of the practice areas. Although we cannot rule out the possibility that job seekers’ preferences drive the results, we conclude that rival managers’ ties deserve further scrutiny because they might limit the outside employment opportunities of their subordinates.
This study advances and tests the notion that the phenomenon of guilt by association-- whereby innocent organizations are penalized due to their similarity to offending organizations-- is shaped by two distinct forms of generalization. We analyze how and why evaluators’ interpretative process following instances of corporate misconduct will likely include not only inductive generalization (rooted in similarity judgments and prototype-based categorization) but also deductive generalizing (rooted in evaluators’ theories and causal-based categorization). We highlight the role and relevance of this neglected distinction by extending guilt-by-association predictions to include two unique predictions based on deductive generalization. First, we posit a recipient effect: if an innocent organization falls under a negative stereotype that causally links the innocent firm with corporate misconduct, then that innocent firm will suffer a greater negative spillover effect, irrespective of its similarity to the offending firm. Second, we also posit a transmission effect: if the offending firm falls under the same negative stereotype, then the negative spillover effect to other similar firms will be lessened. We also analyze how media discourse can foster negative stereotypes, and thus amplify these two effects. We find support for our hypotheses in an analysis of stock market reactions to corporate misconduct for all U.S. and international firms using reverse mergers to gain publicly traded status in the United States. We discuss the implications of our theoretical perspective and empirical findings for research on corporate misconduct, guilt by association, and stock market prejudice.
The extent to which men and women sort into different jobs and organizations—namely, gender differences in supply-side labor market processes—is a key determinant of workplace gender composition. This study draws on theories of congruence to uncover a unique organization-level driver of gender differences in job seekers’ behavior. We first argue and show that congruence between leadership gender and organizational claims is a key mechanism that drives job seekers’ interest. Specifically, many organizational claims are gender-typed, such that social claims activate the female stereotype, whereas business claims activate the male stereotype. Thus, whereas female-led organizations making social claims are gender-congruent, male-led firms making the same claims are gender-incongruent. Beyond demonstrating a general preference among job seekers for congruence, we also find that female job seekers are most interested in working for organizations that are simultaneously congruent and provide credible signals that they are fair and equitable employers. The congruence of leadership gender and organizational claims thus affects the gender composition of applicant pools for otherwise identical jobs.
The extant research has often examined the work-related experiences of corporate executives, but their off-the-job activities could be just as insightful. This study employs a novel proxy for the risky hobbies of chief executive officers (CEOs)—CEOs’ hobby of piloting a private aircraft—and investigates its effect on credit stakeholders’ evaluation of the firms led by the CEOs as reflected in bank loan contracting. Using a longitudinal data set on CEOs of large United States-listed firms across multiple industries between 1993 and 2010, we obtain strong evidence that bank loans to firms steered by CEOs who fly private jets as a hobby tend to incur a higher cost of debt, to be secured, to have more covenants, and to be syndicated. These effects are mainly driven by banks, which perceive such firms as having a higher default risk. These relationships become stronger when the CEO is more important to the firm and/or can exercise stronger control over decision making. Supplemented by field interviews, our results are also robust to various endogeneity checks using different experimental designs, the Heckman two-stage model, a propensity score-matching approach, a difference-in-differences test, and the impact threshold of confounding variables.
We advance research on the antecedents of business model design by integrating institutional and imitation theories to explore how the business model of new ventures evolves in a weak institutional environment. Based on a case study of Jumia—an online retailing company in Africa established with the aim to emulate the success of Amazon.com—we propose a process model entitled “imitate-but-modify” that explains how business models evolve through four distinct phases (i.e., clarification, legitimacy, localization, and consolidation). In essence, this model explains how new ventures surrounded by considerable uncertainty deliberately seek to learn vicariously by imitating the business model template of successful firms. However, because of significant institutional voids, the ventures’ intentional imitation is progressively replaced by experiential learning that blends business model imitation with innovation.
Formal hierarchies may be presumed to reduce uncertainty about the status ordering of employees as they imply a consistent global ranking. However, formal hierarchies in organizations are not merely linear, but are characterized by branching and nesting (i.e., they comprise subunits within the organization and subunits within other subunits), which creates a local ranking of individuals within each subunit. This can create tension between global and local formal ranks as status cues. Moreover, individuals may also draw on informal status cues that are inconsistent with formal ranks. Consequently, organizational members may experience upward status disagreement (USD), whereby each assumes they have higher status than the other. We offer a theoretical model that identifies important conditions under which cues arising from the structure of the formal hierarchy—either on their own or in conjunction with informal status cues—can be a source of USD. We also explore when USD can result in status conflict and identify moderators of this relationship. Our research has implications for how the frequency of USD can be mitigated as organizational structures become more complex and the workforce becomes increasingly diverse.
Responsibility is an important issue in organizations and society. Employees, managers, and owners can behave responsibly in the workplace and beyond. In addition, these individuals can be influenced by the propensity of the organization to behave responsibly. Organizations can pursue strategies that take into account responsibility at the product, firm, industry, and societal levels. This virtual special issue examines 19 articles published in Organization Science that consider responsibility at multiple levels of analysis. An important theme that emerges is that although some studies have crossed levels of analysis, future research would benefit from cross-level or more meso-based approaches.
It is well known in economics, law, and sociology that reputation costs in a closed network give insiders a feeling of being protected from bad behavior in their relations with one another. A person accustomed to doing business within a closed network is, therefore, likely to feel at unusual risk when asked to cooperate beyond the network because of absent reputation-cost security. It follows that business leaders in more closed networks should be less likely to cooperate beyond their network (Hypothesis 1). Success reinforces the status quo. Business leaders successful with a closed network associate their success with the safety of their network, so they should be even less likely to cooperate with a stranger (Hypothesis 2). We combine network data from a heterogeneous area probability survey of Chinese CEOs with a behavioral measure of cooperation to show strong empirical support for the two hypotheses. CEOs in more closed networks are less likely to cooperate beyond their network, especially those running successful businesses: successful CEOs in closed networks are particularly likely to defect against people beyond their network. The work contributes to a growing literature linking network structure with behavior: here, the closure that facilitates trust and cooperation within a network simultaneously erodes the probability of cooperation beyond the network, thereby reinforcing a social boundary around the network. Taking our results as a baseline, we close sketching new research on personality, homophily, network dynamics, and variation in the meaning of “beyond the network.”
Collaborations between individuals in firms have important implications for the development of relational and human capital. In knowledge-intensive contexts where collaborations are formed to deliver services to clients, collaboration decisions can involve nontrivial tradeoffs between short-term and long-term benefits: individuals and firms must carefully manage the tradeoffs between leveraging existing relational and human capital for the reliable performance of repeat collaboration and creating new relational and human capital through new collaboration. Building from the premise that servicing clients is central to collaboration decisions in human asset–intensive firms, we examine how client-related factors shape collaboration decisions among lawyers (partners) in UK law firms providing M&A legal advisory services. We focus on three key client-related dimensions that we predict govern collaboration decisions: the depth of individual- and firm-level relationships with the focal client, key client attributes that reflect the client’s status and its use of different firms to undertake its outsourced work, and client-driven individual- and firm-level resource constraint. Our empirical findings support our proposition that client-related factors influence the pattern of collaborations between individuals in firms. We also reveal how client-related factors at the individual level can have opposite effects on collaboration decisions from those at the firm level. Overall, our findings contribute to research on relational capital, strategic human capital, team formation, professional service firms, and the microfoundations of strategy.
Despite organizational psychologists’ long-standing caution against monitoring (citing its reduction in employee autonomy and thus effectiveness), many organizations continue to use it, often with no detriment to performance and with strong support, not protest, from employees. We argue that a critical step to resolving this anomaly is revisiting researchers’ fundamental assumptions about access to gathered data. Whereas previous research assumes that access resides nearly exclusively with supervisors and other evaluators, technological advances have enabled employee access. We hypothesize that with employee access, the psychological effects of monitoring may be far more complex than previously acknowledged. Whereas multiparty access may still decrease employee autonomy, it may also trigger an important psychological benefit: alleviating employees’ perceptions of polarization—the increasing social and ideological divergence between themselves and their evaluators. Access gives employees unprecedented opportunities to use the “objective” footage to show others their perspective, address evaluators’ erroneous assumptions and stereotypes, and otherwise defuse ideological tensions. Lower perceived polarization, in turn, attenuates the negative effects that low autonomy would otherwise have on employee effectiveness. We find support for these hypotheses across three field studies conducted in the law enforcement context, which has been a trailblazer in using technological advances to grant broad access to multiple parties, including employees. Overall, our studies shed light on the conflicting (and ultimately more innocuous) impact of monitoring and encourage scholars to break from prior approaches to account for its increasing egalitarianism.
This 1.5-year ethnographic study of a U.S. medical center shows that avoiding loss of autonomy and work intensification for less powerful actors during digital technology introduction and integration presents a multisited collective action challenge. I found that technology-related participation problems, threshold problems, and free rider problems may arise during digital technology introduction and integration that enable loss of autonomy and work intensification for less powerful actors. However, the emergence of new triangles of power allows for novel coalitions between less powerful actors and newly powerful third-party actors that can help mitigate this problem. I extend the political science perspective of experimentalist governance to examine how a digital technology-focused, iterative collective action process of local experimentation followed by central revision can facilitate mutually beneficial role reconfiguration during digital technology introduction and integration. In experimentalist governance of digital technology, local units are given discretion to adapt digital technologies to their specific contexts. A central unit composed of diverse actors then reviews progress across local units integrating similar digital technology to negotiate a new shared understanding of mutually beneficial technology-related tasks for each group of actors. The central unit modifies both local routines and the technology itself in response to problems and possibilities revealed by the central revision process, and the cycle repeats. Here, accomplishing mutually beneficial role reconfiguration occurs through an experimentalist, collective action process rather than through a labor-management bargaining process or a professional-led tuning process.
Team creative processes of generating and elaborating ideas tend to be laden with emotional expressions and communication. Yet, there is a noticeable lack of theory on how differences in teams’ management and support of affect expressions influence their ability to produce creative outcomes. We investigate why and when team authentic affect climates, which encourage members to share and respond to authentic affect, generate greater creativity compared with more constrained affect climates where members suppress or hide their genuine feelings. We propose that authentic affect climate enhances team creativity through greater information elaboration by the team and that these informational and creative benefits are more likely in functionally diverse teams. Results from three complementary studies—one multisource field study of management teams and two experiments—provide support for our predictions. In our experiments, we also examine the theorized affective mechanisms and find that authentic affect climate increases information elaboration and creativity through members’ affect expressions (Study 2) and empathic responses to each other’s expressed affect (Studies 2 and 3). We discuss the implications of our findings for the team creativity, diversity, and affect literatures.
Organizational involvement in stigmatized practices, that is, practices that attract substantial societal condemnation, is often challenging, inasmuch as it requires the successful management of stakeholder disapproval. In this regard, existing work on organizational stigma has highlighted the advantages of situating stigmatized practices within large, generalist organizations, because doing so allows for stigma dilution—that is, organizations can reduce stakeholder disapproval by increasing their relative engagement in uncontested practices, thereby straddling multiple categories in the eyes of audiences. This line of argument, however, runs counter to the empirical observation that stigmatized practices often remain overwhelmingly concentrated within smaller, specialist organizations, even though these are often not optimally positioned to cope with stigma. In this paper, therefore, we undertake an in-depth historical analysis of a revelatory case—abortion provision in the United States following the landmark Roe v. Wade U.S. Supreme Court decision—to build theory of how stigmatized categories can come to be populated predominantly by specialists. Building on primary and secondary archival materials, we identify three mechanisms that shaped category evolution and resulted in the de facto segregation of abortion into specialist organizations: the founding of freestanding facilities by values-driven providers, the exit of generalist organizations from the category, and the involuntary specialization of remaining providers, as customers no longer frequented them for other services and they soon became labeled simply as “abortion clinics.” We conclude by discussing the implications of our findings for the stigma literature and the generalizability of our theorizing to other settings.
We examine how the presence of a firm’s political connections in a candidate location affects the firm’s likelihood of choosing that location over unconnected but otherwise comparable ones to establish a new subsidiary. First, because of various benefits that political connections can generate for firms, all else equal, firms are more likely to choose the locations in which they have connections with local political leaders. Second, this effect is dampened when local economic conditions may drive local politicians to demand that connected firms engage in economically inefficient but politically desirable tasks, such as hiring superfluous labor. As a result, firms are less likely to choose a politically connected location that also suffers from higher unemployment. Moreover, this dampening effect exists (and becomes stronger) when the connected politicians hold political positions that shoulder greater responsibility for resolving local unemployment issues. Using data on all new subsidiaries established by Chinese listed firms from 2003 to 2009, we obtain empirical evidence that corroborates the hypotheses. Therefore, whether and how firms use their political connections in making location choice is strategic in that it is highly dependent on the economic and political context.
This study explores heterogeneity in the efficacy of stretch goals for engaging employees in innovation, as stretch goals may both boost norm-breaking creativity and hamper fruitful ideation by overwhelming employees. Through a multilevel perspective, we demonstrate that stretch goals motivate more capable employees (successful, experienced, senior) to submit useful innovative ideas by combining the motivation of stretch goals with these employees' ability to discern fruitful from futile ideas. Other employees, meanwhile, may “spin their wheels” and submit lower-quality ideas based on their inability to apply useful knowledge. Empirically, we leverage idea generation data from a Fortune 500 firm. We contribute to stretch goals research by demonstrating both the intended and the unintended consequences that shape employee behavior and to the innovation literature by articulating when stretch goals can and cannot motivate valuable innovation from employees.
We study how becoming an entrepreneur affects academic scientists’ research. We propose that entrepreneurship will shift scientists’ attention away from intradisciplinary research questions and toward new bodies of knowledge relevant for downstream technology development. This will propel scientists to engage in exploration, meaning they work on topics new to them. In turn, this shift toward exploration will enhance the impact of the entrepreneurial scientist’s subsequent research, as concepts and models from other bodies of knowledge are combined in novel ways. Entrepreneurship leads to more impactful research, mediated by exploration. Using panel data on the full population of scientists at a large research university, we find support for this argument. Our study is novel in that it identifies a shift of attention as the mechanism underpinning the beneficial spillover effects from founding a venture on the production of public science. A key implication of our study is that commercial work by academics can drive fundamental advances in science.
There is both widespread interest in encouraging entrepreneurship and universal recognition that the vast majority of these founders will fail, which raises an important unanswered question: What happens to ex-founders when they apply for jobs? Whereas existing research has identified many factors that facilitate movement out of an established organization and into entrepreneurship, far less attention has been devoted to understanding what transpires during the return journey—most notably, how employers evaluate entrepreneurial experience at the point of hire. We propose that employers penalize job candidates with a history of founding a new venture because they believe them to be worse fits and less committed employees than comparable candidates without founding experience. We further predict that the discount for having been an entrepreneur will diminish when other stereotypes about the candidate, particularly those based on gender, will contradict the negative beliefs about ex-founders. We test our proposition using a résumé-based audit and an experimental survey. The audit reveals that founding significantly reduces the likelihood that an employer interviews a male candidate, but there is no comparable penalty for female ex-founders. The experimental survey confirms the gendered nature of the founding penalty and provides evidence that it results from employers’ concerns that founders are less committed and worse organizational fits than nonfounders. Critically, the survey also indicates that these concerns are mitigated for women, helping to explain why they suffer no equivalent founding penalty.
We explore a key tension between certification and entrepreneurial entry. On the one hand, more stringent certifications may provide greater legitimacy. On the other hand, market entry may be facilitated by easing such standards. To reconcile this tension, we examine discontinued certifications. We draw on research into how new ventures use certifications to gain legitimacy, along with quantitative data from new venture credit records. We show that after a certification is discontinued, new ventures in emerging industries continue to conform to these discontinued certified standards. Our study shows that those whose attributes do not provide other sources of legitimacy (e.g., unconventional founders in emerging industries) are more likely than other new ventures to comply with discontinued certification. However, those with other legitimating attributes (e.g., elite institution alumni founders) can overcome such legitimacy deficits and take advantage of new rules easing entry. Overall, our findings show that discontinued certifications can become certification “relics” whose standards continue to linger and influence entry, even after they are no longer formally in effect. Our study and its findings enhance our understanding of institutional support for new ventures, as well as the repertoire of strategic actions available to new ventures to gain legitimacy and acceptance in the face of institutional change.
Research has long recognized the importance of collaboration for innovation, but relatively little is known about the strategic drivers of collaborative innovation in firms. We posit that robust collaboration within firms can increase the interfirm mobility of inventors and increase spillovers of innovative knowledge to competitors by mobile inventors. Therefore, by mitigating these value capture hazards associated with collaboration, barriers to employee mobility may induce firms to increase collaborativeness in innovation. Additionally, consistent with the mechanism underlying this proposition, we hypothesize that firms whose innovation entails more complex knowledge, which is known to impede interfirm knowledge spillovers, will increase collaboration less when employee mobility increases. We test these hypotheses by leveraging quasi-exogenous changes in two legal mobility barriers for inventors across U.S. states and find that higher-mobility barriers are associated with greater inventor collaboration (as observed in patented innovation), and this effect is weaker for firms possessing more complex knowledge. These findings deepen our understanding of the strategic tradeoffs between value creation and value capture entailed in collaborative innovation within firms and of human capital strategies that help to manage these tradeoffs.
This research investigates the relationship between couples’ work-orientation incongruence—the degree to which romantic partners view the meaning of their own work differently—and their ability to succeed in making job transitions and experiencing satisfaction with the jobs they hold. We use a social information-processing approach to develop arguments that romantic partners serve as powerful social referents in the domain of work. By cueing social information regarding the salience and value of different aspects of work, partners with incongruent work orientations can complicate each other’s evaluation of their own jobs and the jobs they seek. In a longitudinal study of couples in which one partner is searching for work, we find that greater incongruence in couples’ calling orientations toward work relates to lower reemployment probability, a relationship that is mediated by an increased feeling of uncertainty about the future experienced by job seekers in such couples. Calling-orientation incongruence also relates to lower job satisfaction for employed partners over time. We contribute to the burgeoning literature on the role romantic partners play in shaping work outcomes by examining the effect of romantic partners’ perception of the meaning of work, offering empirical evidence of the ways in which romantic partners influence key work and organizational outcomes. Our research also contributes to the meaning of work literature by demonstrating how work-orientation incongruence at the dyadic level matters for individual work attitudes and success in making job transitions.
Self-selection–based division of labor has gained visibility through its role in varied organizational contexts such as nonhierarchical firms, agile teams, and project-based organizations. Yet, we know relatively little about the precise conditions under which it can outperform the traditional allocation of work to workers by managers. We develop a computational agent-based model that conceives of division of labor as a matching process between workers’ skills and tasks. This allows us to examine in detail when and why different approaches to division of labor may enjoy a relative advantage. We find a specific confluence of conditions under which self-selection has an advantage over traditional staffing practices arising from matching: when employees are very skilled but at only a narrow range of tasks, the task structure is decomposable, and employee availability is unforeseeable. Absent these conditions, self-selection must rely on the benefits of enhanced motivation or better matching based on worker’s private information about skills, to dominate more traditional allocation processes. These boundary conditions are noteworthy both for those who study as well as for those who wish to implement forms of organizing based on self-selection.
Research shows that reference group selection underpins critical organizational processes, but less is known about publicly disclosed choices of reference groups, such as those for the evaluation of firm performance. Because audiences, such as investors and analysts, prefer reference groups created by independent entities they can trust, they disapprove of choices of custom peer groups created by reporting firms. Nevertheless, firms frequently choose reference groups that do not conform to audiences’ expectations. We seek to explain why firms deviate from these externally held standards even when incurring penalties by developing theory and formulating hypotheses about the influence of chief executive officer (CEO) power. Using data from 10-K filings, we find that firms led by high-power CEOs are more likely to use nonconforming, custom peer groups despite incurring penalties. However, the relationship between CEO power and the use of custom peer groups is weaker when CEOs face greater scrutiny from shareholders and analysts. We also find that low firm performance increases the use of custom peer groups among high-power CEOs. Contrary to our expectations, high CEO compensation attenuates the effect of CEO power on the choice of custom peer groups, arguably because high levels of CEO pay increase scrutiny. Although firms incur costs for using nonconforming reference groups, supplemental analyses reveal that CEOs benefit by receiving higher compensation, especially when performance is low. We conclude by discussing implications for research on publicly disclosed reference groups, the consequences of power, and information disclosure.
Prevailing theory argues that more certifications increase performance. However, emerging empirical evidence implies that obtaining more certifications may actually decrease performance. How do we reconcile this tension? Practically speaking, why would ventures seek additional certifications in light of these recently identified risks? To address this gap between existing theory and recent empirics, we look more closely at ventures’ activities and performance outcomes after they receive their first certification. We posit that different patterns of certification reflect different forms of experimentation. In particular, ventures may be willing to experiment in ways that incur an inappropriateness penalty for the chance to gain a subsequent desirability premium if their experiments succeed. Inappropriateness means that certifications signal divergence from accepted market norms and standards. Desirability means that certifications signal activities that are in the perceived self-interest of the potential audience. We hypothesize that certifications reflecting broad experimentation incur initial inappropriateness penalties, yet when successful, they are more likely to lead to breakthroughs that generate desirability premia. We find support for this idea through an empirical analysis drawing from a sample of 7,440 U.S. ventures that receive one or more Small Business Innovation Research (SBIR) or Small Business Technology Transfer (STTR) grants to commercialize new technologies. This study advances institutional theory of certification to better account not only for its benefits but also for its costs.
Past research often relegates the management of the ideal worker’s overworking body to the nonwork environment. Reflecting a segmentation approach to managing the boundary between work and nonwork, the nonwork setting is treated as a context for recuperation. Yet, segmentation may, ironically, support the ideal worker image and reinforce the persistence of overwork. Drawing on two-year-long ethnographic studies of yoga teacher training, this paper considers how individuals shift how they manage the boundaries around their bodies. In doing so, we challenge the notion that segmentation of nonwork from work is an ideal boundary management strategy for addressing the negative impacts of overwork. Rather, we suggest that an integration strategy developed in a nonwork community may be productive for breaking the cycle of overwork and recuperation promoted by the ideal worker image and creating a virtuous cycle of activation and release. We bring forward the bodily basis to overwork and conceptualize somatic engagement as a form of engagement through which actors come to connect reflexively with their bodily experience across domains. Relatedly, in revealing how individuals come to connect reflexively with their bodily experience, we elaborate our understanding of the relational phenomena that enhance individuals’ somatic experiences across boundaries.
To provide insight into women’s approaches to managing the work-family interface, I introduce the concepts of focal and peripheral role senders and illuminate the importance of their interplay in the enactment of women’s domestic roles. At the core of my theoretical model is the process by which focal and peripheral role senders embrace or reject an ideal enactment of domestic roles and the women’s strategies women use to either acquiesce to ideal roles or acquire idiosyncratic roles. This paper examines the husband as the focal role sender, consistent with the literature’s focus and the pervasiveness of husbands in my data, and considers peripheral role senders, such as parents and in-laws, who also influence women’s role enactment, either by amplifying or muting the husband’s preferred role enactment. This research contributes to existing theory by introducing the importance of focal and peripheral role senders, illuminating how these multiple senders and their interaction influence women’s strategies to deal with role conflict, and documenting how women’s strategies subsequently influence their career trajectories.
What should the managers of a multibusiness firm do when their company’s resources are not used profitably? Research on redeployment proposes that managers should withdraw those resources from the business where they are underutilized and switch them to a business where they can be used more profitably, whereas the literature on divestiture advocates that managers should divest the business containing those resources. In this study, we investigate the factors that lead managers to choose resource redeployment over divestiture as a mode of exit and vice versa. Using a formal model, we establish that the two exit modes act as intertemporal substitutes, whereby redeployment dominates for earlier exits but divestiture dominates for later exits. Although both redeployment and divestiture are inversely related to their implementation costs, redeployment costs amplify the effect of divestiture costs on the likelihood of exit, and divestiture costs amplify the effect of redeployment costs on the likelihood of exit. Finally, we derive a series of results that show that disregarding one of these two exit options as a strategic alternative to the other may lead to misspecifications of empirical models that seek to predict the likelihood of redeployment, divestiture, or exit. Overall, our work contributes to the corporate strategy literature by uniting two streams of research that have largely remained disparate, yet whose insights have significant implications for each other.
Distinguishing between status spillovers and status ripples, we argue that sudden positive status shifts create status ripples when the social actors experiencing the status shifts are more constrained from exploiting their higher status than the social actors to whom they are affiliated. Specifically, we examine the status ripple paradox that the status effects experienced by the affiliated actors sometimes are as strong, or even stronger, as the direct status effects experienced by the ascending actors themselves. Focusing empirically on prestigious CEO awards from U.S. news magazines, we examine the consequences of positive status shifts for the awarded CEOs and the CEOs who are on the boards of directors of the awarded CEOs’ firms. We find evidence of status ripples in CEO compensation by showing that awarded CEOs have relatively greater immediate but smaller subsequent increases in compensation, which results in lower overall compensation effects for the awarded CEOs. Moreover, we provide empirical evidence of the theoretical mechanisms behind the status ripple paradox by showing that external constraints in the form of increased media and analyst attention and increased expectations affect the status ripple effect.
We study the evolution of the African mobile telecommunications industry from its effective beginning and explore the sources of ownership advantages among indigenous firms, by assembling historical qualitative and quantitative firm-level data. Our historical qualitative findings suggest that a few start-ups gained industry-specific knowledge through their pre-entry experience, directed their postentry development of capabilities toward adaptations to challenging market and operational conditions, and leveraged their adaptive capabilities to enter and compete in other African countries. Using our quantitative panel data, we show that these firms successfully internationalized across the continent. In particular, compared with other start-ups, they had higher rates of foreign entry in African countries that had relatively weaker rule of law, and greater market reach in African countries that had relatively larger low-income consumer segments. These patterns corroborate that their capabilities for overcoming the industry’s challenging market and operational conditions were their key ownership advantages. Through our triangulated analysis, we show that inherited industry knowledge provides a foundation for postentry capability development, and entrepreneurial leadership guides this process to create ownership advantages for regional internationalization.
We investigate how CEOs’ implicit motives can shape firms’ competitive intensity in response to external threats. We examine this phenomenon in the context of short seller activism, which occurs when an activist short seller publicly denounces a firm to drive down its stock price. Implicit motives are motivational dispositions that operate outside of an individual’s conscious awareness. We find that CEOs’ implicit needs for achievement and power are associated with a decrease in competitive intensity following short seller activism, implying that implicit motives can lead CEOs to avoid behaviors that they fear may result in failure or the exposure of weakness in the wake of an external threat. This study contributes to research on external threats and corporate governance by highlighting the role of CEOs’ implicit motives in shaping firms’ responses to activists. We emphasize the importance of integrating implicit motives into upper echelons research.
Despite the dynamic nature of knowledge-related activities and the availability of a variety of communication technologies, many global teams habitually use technology in the same way across activities. However, as teams move through cycles of accumulating, integrating, and implementing knowledge, the purposes for communication technologies change. Current theorizing and empirical work on team knowledge management has yet to develop a dynamic theory that incorporates these changes. By conducting a multiwave, mixed method analysis of 48 global teams, we develop a theory of how global teams sustain effectiveness through technology affordance processes. We found that effective teams are those that recognize cues indicating change is necessary and coevolve a symbiosis between new activities, new purposes for interaction, and new uses of communication technologies. This coevolution of purpose with technology use forms new affordances, which enable the team to move on to new knowledge management activities and sustain effectiveness. Our theory more realistically models the dynamics of staying connected while sharing, combining, and implementing knowledge across the globe.
Entering a new product market requires assembling a bundle of resources. Because missing a single resource can foil the entire entry effort, we argue that bottleneck resources—those most difficult to obtain or sell externally—anchor the direction of firm growth. We characterize market resources as bottlenecks to product market entry, because they are (on average) more challenging to obtain and sell than technological resources, and we articulate why the importance of market resources varies with the strength of external markets for technology. Using cross-industry data linking firms’ product portfolios with patents, we find resource dynamics whereby market resources drive the strategic decision to enter, and firms fill technological gaps using both internal research and development and external acquisitions (joint ventures and alliances). Our study underscores the importance of resources for firm growth dynamics and specifically highlights market resources as the bottleneck that constrains and directs the direction of product market entry.
We examine how catastrophic innovation failure affects organizational and industry legitimacy in nascent sectors by analyzing the interactions between Virgin Galactic and stakeholders in the space community in the aftermath of the firm’s 2014 test flight crash. Following catastrophic innovation failure, we find that industry participants use their interpretations of the failure to either uphold or challenge the legitimacy of the firm while maintaining the legitimacy of the industry. These dynamics yield two interesting effects. First, we show that, in upholding the legitimacy of the industry, different industry participants rhetorically redraw the boundaries of the industry to selectively include players they consider legitimate and exclude those they view as illegitimate: detracting stakeholders constrain the boundaries of the industry by excluding the firm or excluding the firm and its segment, whereas the firm and supporting stakeholders amplify the boundaries of the industry by including firms in adjacent high-legitimacy sectors. Second, we show that, in assessing organizational legitimacy, the firm and its stakeholders differ in the way they approach distinctiveness between the identities of the industry and the firm. Detracting stakeholders differentiate the firm from the rest of the industry and isolate it, whereas the firm and supporting stakeholders reidentify the firm with the industry, embedding the firm within it. Overall, our findings illuminate the effects that catastrophic innovation failure has over high-order dynamics that affect the evolution of nascent industries.
Organizational decision making that leverages the collective wisdom and knowledge of multiple individuals is ubiquitous in management practice, occurring in settings such as top management teams, corporate boards, and the teams and groups that pervade modern organizations. Decision-making structures employed by organizations shape the effectiveness of knowledge aggregation. We argue that decision-making structures play a second crucial role in that they shape the learning of individuals that participate in organizational decision making. In organizational decision making, individuals do not engage in learning by doing but, rather, in what we call learning by participating, which is distinct in that individuals learn by receiving feedback not on their own choices but, rather, on the choice made by the organization. We examine how learning by participating influences the efficacy of aggregation and learning across alternative decision-making structures and group sizes. Our central insight is that learning by participating leads to an aggregation–learning trade-off in which structures that are effective in aggregating information can be ineffective in fostering individual learning. We discuss implications for research on organizations in the areas of learning, microfoundations, teams, and crowds.
One of entrepreneurs’ key tasks is mobilizing resources from external resource holders. Although we know how entrepreneurial ventures gain initial access to resources, we do not yet fully understand how they maintain their resource mobilization, particularly in the face of potential threats. During our 11-month study of prosocial ventures that emerged to alleviate the suffering of refugees in Germany, four attacks on the European public occurred that were allegedly committed by refugees. These attacks disrupted the German welcoming culture for refugees, potentially threatening the legitimacy of the prosocial ventures’ core activities. Thus, the attacks provide a starting point for examining how new prosocial ventures are able to maintain access to resources in the face of the potential withdrawal of resource holders. Theorizing from our data, we identify three distinct approaches to explain how prosocial ventures responded to the potential threat undermining the legitimacy of their activities to maintain access to resources. These approaches differ in their initial resource mobilization (i.e., based on the venture’s goal for alleviating suffering), threat appraisals, and responses to maintain resource mobilization in the face of the potential delegitimization of their core activities. Our model provides novel insights into resource mobilization and prosocial venturing.
The benefits of political skill at work have been extensively documented and highlighted. Existing research also reports unexpectedly equivocal and even positive relationships between employee political skill and coworker social undermining, suggesting that politically skilled employees can become targets of coworker social undermining. However, there is a lack of research explaining why and when employee political skill can lead to coworker social undermining. This research, drawing from social rank theory and the theory of rivalry, offers a novel lens to answer these questions. Specifically, I theorize that employee political skill evokes coworker social undermining by affecting coworker perception of status threat, particularly when the coworker views the focal politically skilled employee as a personal rival. Findings from four studies—one correlational two-wave study and three experiments—provide support for these predictions. Further, nonrival competition and interpersonal disliking as alternative potential explanations to the hypothesized moderating effects of rivalry were ruled out. This study provides a theory-driven explanation of an unintended consequence of employee political skill and offers a more balanced understanding of the effects of political skill at work. Theoretical and future research implications are discussed.
We examine how social activism—in the form of public protests against contentious business practices—can spill over into the regulatory domain, extending beyond activists’ articulated goals to affect firms’ regulatory outcomes in areas that are not directly targeted. We argue that firms are likely to experience broader regulatory repercussions after activist protests because public contention invites greater scrutiny of firm behavior by industry regulators, increasing the likelihood that instances of organizational noncompliance will be discovered. Protests can also cause regulators to evaluate targeted firms more negatively in regulatory assessments, especially firms with less favorable preexisting reputations or stakeholder relations, and to tighten regulations on nontargeted issues that signal their commitment to safeguarding the public interest. We further contend that the political context within which regulatory agencies operate shapes the extent of protest spillovers: When political institutions are aligned with activist goals, and when regulators are ideologically sympathetic too, protests have a more pronounced negative impact on firms’ regulatory outcomes in nontargeted domains. We find robust support for our predictions in a statistical analysis of the impact of antinuclear protests—which sought to block nuclear power plant development by electric utilities—on utilities’ subsequent regulated financial rates of return on their assets. Our analysis contributes new insights to research on the indirect consequences for targeted organizations of social activism.
Interorganizational trust plays an important role in facilitating business relationships, especially for the organizational adoption of new services. Prior research suggests that interorganizational trust develops when the trustor has adequate confidence in the reliability of the trustee’s services. Nevertheless, reliability breakdowns are also an inevitable part of service provisioning. Such breakdowns are especially prominent and visible in the context of platform-based services. Yet platform-based services continue to be adopted and used by organizational customers. This increased adoption and use of such services despite their inconsistent reliability pose the following question. How is trust produced in platform-mediated interorganizational relationships? To examine this question, I conducted a 20-month field study of a cloud computing platform provider and its customers, focusing on the practices of trust production in the wake of reliability breakdowns. Here, I describe customer concerns about the platform’s inconsistent reliability that hampered the development of interorganizational trust. I then identify four practices of trust work enacted by the platform provider to address some of these concerns and to co-opt the occupational gatekeepers in customer organizations who are responsible for technology adoption decisions. Following this, I describe how and why these occupational gatekeepers performed justification work to rationalize the continued use of the platform despite its inconsistent reliability. Together, trust work and justification work facilitate the coproduction of interorganizational trust through normalizing reliability breakdowns as “business-as-usual” events. Synthesizing these findings, I developed a normalization model of trust production, and discuss the implications of normalized trust for platform-mediated interorganizational relationships in the digital economy.
Drawing on the Coase theorem, we consider a firm’s decision to transfer patent ownership to another firm in the markets for innovation. We deem that the proximity of a patent’s technology structure to that of a firm’s patent portfolio will generally result in greater marginal productivity of the patent, leading to enhanced prospects for the firm’s economic return. We thus predict that firms are more likely to trade patents when the technology structure of a patent is closer to the technology stock of a potential buyer compared with that of its original assignee. However, such a relationship will be weaker when a potential buyer and the original assignee have greater product-market overlap or when the assignee has superior technological capability. We test these predictions by employing a dyad-level analysis of transactional decisions during the 1987–2016 period on 40,110 U.S. patents assigned to 57 major biopharmaceutical firms. Our study provides novel insights on factors that facilitate or inhibit patent trade in the markets for innovation.
Knowledge transfer within organizations has important implications for organizational performance and competitive advantage. In this virtual special issue, we review articles on this topic published in Organization Science between 2014 and 2020 and identify 53 articles for their theoretical and empirical contributions. These articles examine knowledge transfer through five transfer mechanisms: social networks, routines, personnel mobility, organizational design, and search. We consider the intersection of each transfer mechanism with important components of knowledge transfer (characteristics of sources/recipients, characteristics of knowledge, and characteristics of contexts). We present 15 exemplar articles, each of which reflects the intersection of a mechanism and a component of knowledge transfer. We also present an overview of the methodological approaches and empirical contexts that are utilized. We conclude our article with a discussion of future research opportunities. The articles published in Organization Science have advanced understanding of both the mechanisms through which knowledge transfer occurs and the conditions under which it is most likely.
Born globals, recently established firms that obtain a substantial share of their revenue from foreign markets, can help strengthen countries’ economic vitality and increase innovation levels. The extent of born global formation varies considerably across countries, yet it is unclear why this is the case. Drawing on the neoconfigurational institutional perspective, we develop a typology of institutional contexts associated with high born global formation rates. We posit that high rates of born global formation occur where institutional features favorable to border-spanning activities complement institutional features conducive to entrepreneurial activity, thus forming an institutional configuration that enables, equips, and motivates more societal members to launch born globals. Accordingly, we hypothesize a primary institutional configuration where international transaction facilitators, entrepreneurial educational capital, and entrepreneurial norms combine to propel born global formation. Further, we draw on the internationalization literature to propose two alternative types of institutional configurations conducive to born global formation. These two types provide functional substitutes for the primary type and are distinctly propelled by (1) escapism from low-quality public governance institutions or (2) immigrant entrepreneurship. Fuzzy-set qualitative comparative analysis on data from 66 countries supports our typology and illustrates why born global activity may thrive even in contexts with institutional weaknesses. Our study develops a neoconfigurational model to advance a holistic understanding of the born global phenomenon’s theoretical drivers, contributing to research on comparative capitalism and international entrepreneurship.
Employee misconduct is costly to organizations and has the potential to be even more common in gig and remote work contexts, in which workers are physically distant from their employers. There is, thus, a need for scholars to better understand what employers can do to mitigate misconduct in these nontraditional work environments, particularly as the prevalence of such work environments is increasing. We combine an agency perspective with a behavioral relationship-based perspective to consider two avenues through which gig employers can potentially mitigate misconduct: (1) through the communication of organizational values and (2) through the credible threat of monitoring. We implement a real effort experiment in a gig work context that enables us to cleanly observe misconduct. Consistent with our theory, we present causal evidence that communication of organizational values, both externally facing in the form of social/environmental responsibility and internally facing in the form of an employee ethics code, decreases misconduct. This effect, however, is largely negated when workers are informed that they are being monitored. We provide suggestive evidence that this crowding out is due to a decrease in perceived trust that results from the threat of monitoring. Our results have important theoretical implications for research on employee misconduct and shed light on the trade-offs associated with various potential policy solutions.
We examine how experience-induced adaptations that affect the breadth of an ongoing activity affect performance. The research on organizational learning suggests that accumulating experience, both from repetition and adaptation at the activity level, improves outcomes. Yet, findings on the effects of increasing breadth—the number of different processes making up an activity—are mixed. Greater breadth exposes organizations to diverse activities. It also generates an additional need for coordination that may undermine performance. We examine the joint effect of experience and breadth on waste reduction for U.S. manufacturing facilities managing their toxic waste from 1991 to 2014. These facilities manage toxic waste on a chemical by chemical basis. We find a detrimental effect of breadth on performance that is highest for facilities with low experience; however, this effect is moderated by experience with the waste management activity. Because most facilities manage toxic waste from several chemicals, we also see spillovers—in terms of both learning benefits and the costs of increasing breadth. When a facility expands waste management breadth anywhere, performance decreases for the focal chemical. Yet, this spillover effect of breadth decreases for activities where the facility has accrued more experience. Our research clarifies when facilities should consider adding breadth to a routine activity and why performance in the proximate period may falter as the organization learns and improves in the longer term.
An important problem for many firms is sustaining their rate of innovation by launching new products on an ongoing basis. Accordingly, firms need to replenish their innovation pipelines with new inventions as existing inventions are weeded out or reach fruition. The replenishment can be done through internally generated inventions or through externally sourced inventions via licensing, alliance, or acquisition modes. Drawing on incentives- and knowledge-based views of the firm, we consider the difference in managerial decision making between centralized and decentralized research and development (R&D) organization designs and how it impacts firms’ propensities to draw on externally sourced inventions. As compared with centralized designs, decentralized designs are associated with greater incentives for managers to replenish their firms’ pipelines but are limited in terms of intraorganizational knowledge flows that can facilitate the creation of inventions. We explore these mechanisms using a novel data set of firms’ sourcing decisions within the pharmaceutical industry between 1996 and 2015. We find that firms with decentralized R&D designs replenish their pipelines with a higher proportion of externally sourced inventions than do firms with centralized designs. This difference is found to be mainly attributed to external sourcing via licensing and for inventions of moderate novelty. This study offers an important contribution to the question of how firms organize for innovation, highlighting the relationship between internal R&D organization design and the external sourcing of inventions. In so doing, it illustrates that the choice of organization design in terms of centralization or decentralization can shape a firm’s locus of innovation.
Previous research on the genesis of industrial clusters has focused on macrolevel (e.g., agglomeration economies and institutions) or mesolevel explanatory factors (e.g., serial entrepreneurship, spin-offs). Less studied are the microfoundations of cluster genesis, intended as the individual- and group-level processes underlying such macrolevel outcomes. Yet, microfoundations are key to understanding the “primordial soup” of cluster genesis—that is, the processes unfolding in the early moments of cluster formation, before the first emergence of commercial activity. Through a historical case study of the British Motorsport Valley (1911–1970s), we trace back the primordial origins of this cluster to the casual leisure activities of groups of amateur motorsport enthusiasts who then prompted the professionalization of motorsport racing and its transformation into the business at the core of the industrial cluster. We theorize that clusters emerge through the layering of different domains (casual leisure, serious leisure, and business), each made of three elements (actors, activities, and artifacts), which interact via two microlevel mechanisms: (1) localizing passion, a shared emotional energy by which people become affectively attached to the spaces where they carry out activities that they enjoy; (2) domain repurposing, the shift of a configuration of actors, activities, and artifacts toward a new purpose, originating a new domain. Whereas domain repurposing induces the transformation of activities from leisure to business (thus originating the industry at the core of a cluster), localizing passion anchors the activities to the same geographical area (clustering the industry). Our key contribution is to explore the emotional microfoundations of cluster genesis.
We examine how institutional factors may affect microlevel career decisions by individuals to create new firms by impacting their ability to exercise entrepreneurial preferences, their accumulation of human capital, and the opportunity costs associated with new venture formation. We focus on an important institutional factor—immigration-related work constraints—given that technologically intensive firms in the United States not only draw upon immigrants as knowledge workers but also because such firms are disproportionately founded by immigrants. We examine the implications of these constraints using the National Science Foundation’s Scientists and Engineers Statistical Data System, which tracks the careers of science and engineering graduates from U.S. universities. Relative to natives, we theorize and show that immigration-related work constraints in the United States suppress entrepreneurship as an early career choice of immigrants by restricting labor market options to paid employment jobs in organizational contexts tightly matched with the immigrant’s educational training (job-education match). Work experience in paid employment job-education match is associated with the accumulation of specialized human capital and increased opportunity costs associated with new venture formation. Consistent with immigration-related work constraints inhibiting individuals with entrepreneurial preferences from engaging in entrepreneurship, we show that when the immigration-related work constraints are released, immigrants in job-education match are more likely than comparable natives to found incorporated employer firms. Incorporated employer firms can both leverage specialized human capital and provide the expected returns needed to justify the increased opportunity costs associated with entrepreneurial entry. We discuss our study’s contributions to theory and practice.
This study extends prior research seeking to understand the reproduction and persistence of excessive busyness in professional settings by addressing the relationship between organizational controls and temporal experiences. Drawing on 146 interviews and more than 300 weekly diaries in two professional service firms, we develop a framework centered on the emerging concept of optimal busyness, an attractive, short-lived temporal experience that people try to reproduce/prolong because it makes them feel energized and productive as well as in control of their time. Our findings show that individuals continuously navigate between different temporal experiences separated by a fine line, quiet time, optimal busyness, and excessive busyness, and that optimal busyness that they strive for is a fragile and fleeting state difficult to achieve and maintain. We show that these temporal experiences are the effect of the temporality of controls—that is, the ability of controls to shape professionals’ temporal experience through structuring, rarefying, and synchronizing temporality. Moreover, we find that professionals who regularly face high temporal pressures seek to cope with these by attempting to construct/prolong optimal busyness through manipulating the pace, focus, and length of their temporal experiences, a process we call control of temporality. Our study contributes to a better understanding of the reproduction of busyness by explaining why professionals in their attempts to feel in control of their time routinely end up overworking.
The investigation of the appealing indication that a modular product architecture is best associated to a loosely coupled organizational structure—that is, the mirroring hypothesis—has produced contradictory evidence, especially in the dynamic and ambiguous context of new product development. By integrating modularity theory and product-representation theories, we investigate how individual agency affects coordination in teams developing modular products. We conducted a field study of Flower-Net, a globally distributed team in a major IT company, engaged with the development of a modular software using agile practices. Our grounded model shows that, whereas top managers defined the product as modular and coordinated work accordingly, individuals developed different representations of the product’s architecture and conflicting individual coordination practices. We traced the individual development of product architecture representations back to the individual interpretations of organizational roles as more or less “segmented.” Conflicting individual practices, associated to different role-based product representations, were not addressed by the team—that developed a state of illusory concordance—and impaired the functioning of the team. This study contributes to the literature on modularity and the mirroring hypothesis by proposing individual role-based representations as an underexplored level of analysis for the matching between product and organizational modularity (Mirroring Hypothesis II). It also contributes to the debate on how representations affect team coordination, by detailing how role-based product representations can influence team members’ divergence and sustain illusory concordance.
The current paper revisits and builds upon task demonstrability, which defines the criteria necessary for groups to choose a correct response if any member prefers that response. We identify boundary conditions of the current conceptualization of task demonstrability with respect to its use in understanding modern organizational teams. Specifically, we argue that, in its current form, task demonstrability is not optimally suited to studying ongoing teams in which member expertise varies and teams work to complete complex multifaceted tasks. To address this issue, we provide a revisited perspective on demonstrability. We specify the nomological network of revisited demonstrability and recast each of its criteria in a form that preserves the original intent of the construct, but has broader applicability, particularly to organizational contexts. We then discuss theoretical implications and managerial applications of the construct. Finally, noting that there is no standard assessment tool for demonstrability (original or revisited), we develop and validate a measure to facilitate future research.
To survive, nascent grassroots organizations—and their respective causes—must earn the trust of various audiences that can impact credibility advancement. However, it can be quite difficult for grassroots organizations to access suitable settings, times, and collocated audiences. One context that can yield this type of access is an event as it constitutes a rare opportunity for organizations to engage in practices that impact credibility advancement. We investigate how a volunteer-based grassroots organization orchestrates a high-profile event at the United Nations to promote African diaspora entrepreneurs as a valuable force in the mitigation of development challenges in their home countries. We employ qualitative data collected from ethnographic observations, interviews, and secondary sources and apply grounded theory approaches to demonstrate how organizational credibility can be advanced through performative strategizing within event settings. Drawing from heuristics used in theatrical performances, we found that the grassroots organization mobilized specific audience groups in participative role-playing across two acts, thereby producing and consecrating a temporary simulacrum of a cause-related community it claimed to represent. Our findings demonstrate how an unproven organization can strategically use audience mobilization to convert event settings into performative spaces for simulacrum creation and credibility advancement.
This paper studies philanthropy by multinational enterprises (MNEs) during institutional disruptions—the sudden and unexpected, temporary, and systemic breakdowns in market-oriented institutions. The central argument is that, under institutional disruptions, MNEs aim to restore factors that are essential for the market to function, such as infrastructure and labor markets, and the strength of the market restoration motive is positively associated with the economic importance of the affected country to the MNE. Analyses of donations from 2,000 MNEs headquartered in 63 countries in the aftermath of 265 major epidemics, natural disasters, and terrorist attacks affecting 129 countries suggest that the economic importance of the country to the firm strongly explains donations. Country market concentration, public aid, and the country’s regulatory quality moderate this effect. These associations are robust to a matching method; a vector of firm-, country-, and event-specific time-varying and -constant variables; and alternative motives, such as reputation, altruism, media salience, market standing, and poverty-gap avoidance. They offer evidence that company philanthropy in the aftermath of institutional disruptions may deviate from predicted behavior under stable conditions. Particularly, the findings contest the expectation that philanthropy rises in market competition. Monopolistic firms are comparatively large donors and may act as an economic stop-loss mechanism during large disruptions.
Despite the generally positive consequences associated with justice, recent research suggests that supervisors cannot always enact justice, and responses to justice may not be universally positive. Thus, justice is likely to vary in both how much it is received and the employee reactions it engenders. In order to understand the range of justice responses, we develop a dynamic theory of justice by using person-environment fit to take both the value that an individual places in justice and the justice they received into account. Using this framework, we clarify the consequences of congruence versus incongruence in daily justice received and valued, which have implications for treatment discrepancies and subsequent work behavior. We also identify the differences between excess and deficient justice on cognitive and affective responses to justice. Our findings reveal that employees’ experience of justice is more complicated than simply whether the justice they received was high or low on a particular day. Using experience sampling and polynomial regression methods, we observe that not all instances in which employees receive high levels of justice are equivalent. In fact, we find that, depending on justice valued, receiving high levels of justice can be just as detrimental as receiving low levels. Additionally, we find that although both forms of justice misfit (excess and deficiency) cause-negative work outcomes, they affect these outcomes through differential responses to justice — with excess causing increased rumination and deficiency causing decreased positive affect. We conclude by discussing the implications of these findings for extant justice theory and for supervisor-employee work interactions.
In an era of globalization, it is commonly assumed that multicultural experiences foster leadership effectiveness. However, little research has systematically tested this assumption. We develop a theoretical perspective that articulates how and when multicultural experiences increase leadership effectiveness. We hypothesize that broad multicultural experiences increase individuals’ leadership effectiveness by developing their communication competence. Because communication competence is particularly important for leading teams that are more multinational, we further hypothesize that individuals with broader multicultural experiences are particularly effective when leading more versus less multinational teams. Four studies test our theory using mixed methods (field survey, archival panel, field experiments) and diverse populations (corporate managers, soccer managers, hackathon leaders) in different countries (Australia, Britain, China, America). In Study 1, corporate managers with broader multicultural experiences were rated as more effective leaders, an effect mediated by communication competence. Analyzing a 25-year archival panel of English Premier League soccer managers, Study 2 replicates the positive effect of broad multicultural experiences using a team performance measure of leadership effectiveness. Importantly, this effect was moderated by team national diversity: soccer managers with broader multicultural experiences were particularly effective when leading teams with greater national diversity. Study 3 (digital health hackathon) and Study 4 (COVID-19 policy hackathon) replicate these effects in two field experiments, in which individuals with varying levels of multicultural experiences were randomly assigned to lead hackathon teams that naturally varied in national diversity. Overall, our research suggests that broad multicultural experiences help leaders communicate more competently and lead more effectively, especially when leading multinational teams.
Although scholars have highlighted the benefits of psychological safety, relatively few studies have examined how leaders establish it. Whereas existing research points to the importance of seeking feedback, we draw on theories of self-disclosure, trust, and implicit voice to propose that leaders can also promote psychological safety by sharing feedback—openly discussing criticisms and suggestions they have already received about their own performance. In Study 1, naturally-occurring feedback-seeking and feedback-sharing by CEOs independently predicted board member ratings of top management team psychological safety. In Study 2, a longitudinal field experiment, randomly assigning leaders to share feedback had a positive effect on team psychological safety one year later, whereas assigning leaders to seek feedback did not. In Study 3, to explore the processes through which feedback-sharing had an enduring effect but feedback-seeking did not, we conducted qualitative interviews with participating leaders and employees two years later. We found that leaders initiated vulnerability through seeking feedback, but it dissolved due to defensiveness and inaction. In contrast, sharing feedback normalized and crystallized vulnerability as leaders made a public commitment to keep sharing and employees reciprocated, which opened the door for more actionable feedback, greater accountability, and ongoing practices that allowed psychological safety to endure. Our research suggests that to achieve enduring improvements in psychological safety, it may be particularly effective for leaders to share criticism they have received—and that doing so does not jeopardize their reputations as effective and competent.
Going beyond prior research that has focused on dyadic, party-specific trust, this study investigates the importance of generalized trust, which is not specific to a counterparty and originates from a broader context. We analyze how generalized trust at the regional level affects the extent to which a firm relies on external suppliers and the performance effects of doing so. Furthermore, we assess how these relationships are impacted by an economic downturn. We exploit differences in generalized trust across 145 regions in 12 European countries and use data on more than a million small- and medium-sized enterprises (SMEs) before and during the Great Financial Crisis (from 2008 to 2010). Control variables are selected via a double-selection procedure based on machine learning. We find that firms in high generalized trust regions, compared with those in low generalized trust regions, source more externally (but do not reduce external sourcing less in an economic downturn) and benefit more from external sourcing during an economic downturn.
Various strands of work have explored how spatial proximity helps (metaphorically) bridge barriers to resource mobilization and foster knowledge transfer. However, much of that work takes spatial connectedness as a given. We argue that spatial connectedness is a distinct construct that affects the extent to which spaces are not just proximate but are actually able to link people, ideas, resources, and knowledge together. We explore one such source of connectedness—physical (not metaphorical) bridges. We find that the opening of newly built bridges enhances startup founding in the local geographic community. Beyond their impact on startup founding, newly built bridges also influence the organizing process for such ventures. This includes a positive impact on the entry of prospective founders into entrepreneurship and an increase in the number of early-stage investors. The subsequently founded ventures are also more likely to engage in recombination and to cross industry boundaries. We explore scope conditions around industry and connective heterogeneity. We also test for robustness to various modeling approaches. The discussion highlights contributions of these findings to the study of entrepreneurship, as well as of organizations and the institutional fields in which they operate.
Our aim is to explore whether the benefits to firms of using community-based innovation extend to nascent markets: uncertain, high-velocity settings with novel, often complex products. Grounded in a rare empirical comparison, we closely track the two ventures (one using community-based innovation and the other firm-based) that pioneered the nascent civilian drone market. We unpack how each addressed the three major innovations that shaped this setting. Our primary insight is that the firm organizing form for innovation performs best relative to communities in nascent markets. Firms have a coordination advantage that enables quickly and accurately targeting experimentation and problem-solving processes to reduce the many specific uncertainties that characterize these markets. Although communities can help, their task self-selection advantage works best in stable settings such as established markets with simple products (e.g., modular software) and in ambiguous settings in which low-cost randomness pays off. Broadly, we contribute a theoretical framework that identifies how organizing form and problem type jointly shape innovation performance. Most important, uncertainty forms a boundary condition for when firms should rely on firm-based (versus community-based) organizing for innovation.
The institutional context, which includes the normative, regulative, and cognitive dimensions of social life within the various constitutive spheres of society, has a strong influence on entrepreneurial processes and outcomes. Institutions shape who becomes an entrepreneur, opportunity creation, identification, and evaluation, as well as how entrepreneurs attempt to start new firms. We introduce a novel framework that unifies the two dominant perspectives in sociological neoinstitutionalism, the institutional logics and the institutional pillars typologies, and apply this unified framework to examine the existing research at the nexus of entrepreneurship and institutional theory while outlining a set of entrepreneurial phenomena to which the framework can be applied. We analyzed the citation pattern of all 77 articles published since 1999 in top management journals (Academy of Management Journal, Academy of Management Review, Administrative Science Quarterly, Organization Science, and Strategic Management Journal) that used institutional theory to examine entrepreneurial phenomena, and we demonstrate how the unified framework effectively organizes past research while also pointing to new and important areas for development.
With the journal’s rejection rate of more than 90%, getting published in Organization Science is indeed a salutary achievement. Over the years, however, the volume of submissions in Organization Science has grown significantly, creating a significant reviewing load for the reviewers and diminishing the likelihood of acceptance for authors. To assist submitting authors in better assessing their likelihood of acceptance and to improve it, this paper tries to demystify the journal by identifying what Organization Science looks for in a paper, its key policies and practices, and some key lessons for authors attempting to target this journal.
Organizational learning theory has long examined how organizations learn to perform better as they accumulate experience. Although experience accumulation is inherently related to the timing of the repeated activities carried out by an organization, the direct relationship between activity timing and organizational learning has not been examined explicitly in the literature and remains an open question. Organizational learning theory contains two competing perspectives on how timing should impact learning—one suggesting that iterating faster is better for learning and one suggesting that taking more time between iterations is more helpful. Here, we reconcile these perspectives and develop a theory about the boundary conditions between them, arguing that, in general, iterating more rapidly enhances learning but that iterations of novel or complex activities, or ones following recent failure, benefit from a slower pace. We conduct tests of this theoretical perspective using data from the entire history of the orbital satellite launch industry from 1957–2017, and we find broad support for our theory and hypotheses.
An ongoing discussion in organizational studies has focused on the path-dependent nature of organizational reputation. To date, however, there has been little explanation about when and why some constituents’ reputation judgments remain stable, whereas others are more prone to change. We contribute to this research by developing a relational theory of reputational stability and change. Our fundamental argument is that differences in constituent-organization relationships, as well as in the reputational communities that surround these relationships, affect the stability and change of reputation judgments. First, we highlight three relationship characteristics—favorability, history, and directness—and theorize that the reputation judgments of constituents with more unfavorable, longer, and more direct relationships with an organization are more stable, whereas the reputation judgments of constituents with more favorable, shorter, and more indirect relationships with the organization are less stable. We then develop the concept of reputational communities as a key source of indirect information about organizations. We highlight that the immediacy, size, and level of agreement within reputational communities affect how influential they are in changing individual constituents’ reputation judgments. Specifically, we propose that more immediate and larger reputational communities with a higher level of agreement are most likely to change individual constituents’ reputation judgments, whereas more distant and smaller reputational communities with a lower level of agreement are least likely to do so. Overall, we position constituents’ relationships with an organization and the communities that surround these relationships as central elements for understanding reputational stability and change.
Prior research on collaboration and creativity often assumes that individuals choose to collaborate to improve the quality of their output. Given the growing role of collaboration and autonomous teams in creative work, the validity of this assumption has important implications for organizations. We argue that in the presence of a collaboration credit premium—when the sum of fractional credit allocated to each collaborator exceeds 100%—individuals may choose to work together even when the project output is of low quality or when its prospects are diminished by collaborating. We test our argument on a sample of economists in academia using the norm of alphabetical ordering of authors’ surnames on academic articles as an instrument for selection into collaboration. This norm means that economists whose family name begins with a letter from the beginning of the alphabet receive systematically more credit for collaborative work than economists whose family name begins with a letter from the end of the alphabet. We show that, in the presence of a credit premium, individuals may choose to collaborate, even if this choice decreases output quality. Thus, collaboration can create a misalignment between the incentives of creative workers and the prospects of the project.
I contribute to the literature on institutions, gender, and entrepreneurship by showing that macrolevel institutional policies that do not explicitly target women nonetheless discourage them from leveraging prior professional experience—their own and that of others—in founding new ventures. Most ventures fail, but chances of success are greater if founders can bring to bear their professional expertise. However, employee non-compete agreements enjoin workers from leaving their employer to found a rival business in the same industry. Thus, non-competes add legal risk to business risk. To the extent that women exhibit greater risk aversion, the threat of litigation from their ex-employer may act as a sharper brake on startup activity than for men. Examining all workers who were employed exclusively within 25 states and the District of Columbia from 1990 to 2014, I find that women subject to tighter non-compete policies were less likely to leave their employers and start rival businesses. Non-competes increase the risk of entrepreneurship by making it harder to hire talent with relevant experience, shifting women away from higher potential ventures. A review of thousands of filed lawsuits suggests that firms do not target women in non-compete cases. Rather, it appears that non-competes disproportionately discourage women from leveraging their professional networks in hiring the sort of talent necessary for high-growth startups to succeed.
Temporal focus on past, present, and future of contributions to work is critical to understanding how employees and their line managers navigate career disruptions and minimize their potential for negative impact. This paper reframes temporal focus using a dyadic, relational perspective to explore how temporal focus (in)congruence shapes resocialization experiences for returners and their line managers following maternity leave disruption. Our qualitative study draws on 54 interviews across 27 organizations and demonstrates that a congruent, broader temporal focus—that embraces the past, present, and future—is associated with more positive relational and career outcomes than an incongruent focus, where one dyadic partner holds a narrow temporal focus. Our findings explicate how the adoption of a broad versus narrow temporal focus creates a perception of maternity leave as either a brief interlude or a major disruption. A congruent, broader temporal focus allows returners and their line managers to reduce their reliance on typical motherhood biases and instead, consider the woman’s past, present, and potential future contributions over the course of her career. We highlight the importance of temporal focus congruence at a dyadic level and the value of adopting a broader temporal focus on careers while offering new insights regarding the temporal dynamics inherent to maternity leave transitions for both returners and their managers.
Social status is highly consequential in organizations but remains elusive for many professional women. Status characteristics theory argues that women are particularly status disadvantaged in masculine organizational cultures. These types of cultures valorize traits and abilities stereotypically associated with men, making it difficult for women in these settings to be seen as skilled and gain status. In the present study, we build and test novel theory explaining when and why masculine organizational cultures create the conditions for some women—those willing and able to skillfully navigate the espoused norms—to disproportionately gain status. We introduce and define the construct of a sexist culture of joviality, a type of masculine organizational culture representing the intersection of sexism and joviality that emerged inductively from our initial qualitative data. A sexist culture of joviality is characterized by norms promoting frequent sexist joking and teasing, along with underlying values and assumptions that support these sexist jovial behaviors. In a longitudinal mixed-methods field study, we demonstrate that participation in a sexist culture of joviality via engagement in sexist jovial norms is positively related to status for women but negatively related to status for men. In a follow-up experiment, we replicate this effect and demonstrate that differential perceptions of social skill mediate this interaction. Our findings illuminate the subtle ways sexism is perpetuated in organizations despite changing societal norms, underscoring the importance of disrupting these dynamics and revealing insights into how to do so.
Long-term collaborations are crucial in many creative domains. Although there is ample research on why people collaborate, our knowledge about what drives some collaborations to persist and others to decay is still emerging. In this paper, we extend theory on third-party effects and collaborative persistence to study this question. We specifically consider the role that a third party’s helpful behavior plays in shaping tie durability. We propose that when third parties facilitate helpfulness among their group, the collaboration is stronger, and it persists even in the third’s absence. In contrast, collaborations with third parties that are nonhelpful are unstable and dissolve in their absence. We use a unique data set comprising scientific collaborations among pairs of research immunologists who lost a third coauthor to unexpected death. Using this quasi-random loss as a source of exogenous variation, we separately identify the effect of third parties’ traditional role as an active agent of collaborative stability and the enduring effect of their helpful behavior—as measured by acknowledgments—on the persistence of the remaining authors’ collaboration. We find support for our hypotheses and find evidence that one mechanism driving our effect is that helpful thirds make their coauthors more helpful.
Previous research on knowledge work has started to explore how organizational actors deal with pragmatic boundaries that arise from their different interests, priorities, and viewpoints. Material objects, such as visual artifacts, can be used to shape and manipulate pragmatic boundaries, but our understanding of these dynamics is only partial. In this paper, we maintain that focusing on the uses of visual artifacts offers an opportunity to deepen our understanding of the political aspects of knowledge work. To this end, we conducted a practice-based study of an architectural project in which the building design became contested. Our empirical analysis reveals four practices in which visual artifacts are used to deal with pragmatic boundaries: surfacing, bridging, preventing, and minimizing. Through these practices, organizational actors can make boundaries more or less visible with important implications on their power relations and the project at hand. The main contribution of our study is to advance understanding of the political dynamics in knowledge work by revealing how visual artifacts can be used to manipulate pragmatic boundaries. By so doing, our analysis also helps to move the conversation on visual artifacts beyond their role as epistemic objects that sustain (or hinder) knowledge work.
How do managers’ moves across jobs affect the subordinates they leave behind? Manager mobility disrupts established manager-subordinate relationships, as subordinates must now learn to work with a replacement. We explore how this relational disruption affects subordinates’ objective career success—specifically, their financial rewards and subsequent promotion chances. We argue that manager mobility may have both positive and negative implications for subordinate outcomes. The loss of an established relationship may reduce subordinates’ performance and managers’ propensity to reward them; on the other hand, relational disruption may make subordinates more willing and able to seek out valuable opportunities elsewhere in the organization. We also argue that these effects are likely to be greatest for those subordinates who had worked with the previous manager for longer. Using eight years of personnel data from the U.S. offices of a Fortune 500 healthcare company, we show how managers’ mobility is associated with a decrease in subordinates’ financial rewards but an increase in their promotion prospects.
The Behavioral Theory of the Firm suggests that performance below aspirations triggers problemistic search that can lead to risk taking. This prediction has received empirical support from most studies on the topic. However, this literature has typically focused on the internal determinants of firm search and risk-taking behavior and given little attention to the influences of social networks in which firms are embedded. To this end, we incorporate the network embeddedness perspective regarding firms’ network positions and their roles in firm decision making. We suggest that a firm’s search behavior is jointly directed by its performance feedback and network positions. Specifically, network brokerage and centrality play important yet distinct roles in guiding firm search behavior by differentially shaping the direction of problemistic search: high brokerage directs problemistic search to high-risk solutions, whereas high centrality directs problemistic search to low-risk solutions. Our theoretical predictions receive general empirical support based on analyses using longitudinal data from the Chinese venture capital industry. Our approach incorporates the crucial role of network structures into the problemistic search model and works toward building a problemistic search theory of the embedded firm.
This study builds theory on how people construct moral careers. Analyzing interviews with 102 journalists, we show how people build moral careers by seeking jobs that allow them to fulfill both the institution’s moral obligations and their own material aims. We theorize a process model that traces three common moral claiming strategies that people use over time: conventional, supplemental, and reoriented. Using these strategies, people accept or alter purity and pollution rules, identify appropriate jobs, and orient themselves to specific audiences for validation of their moral claims. People’s careers are punctuated by reckonings that cause them to reconsider how their strategies fulfill their moral and material aims. Experiences of gender and racial discrimination, access to alternate occupational identities, and timing of entry into the occupation also shape people’s movement between strategies. Over time, people combine these moral claiming strategies in different ways such that varying moral careers emerge within the same occupation. Overall, our study shows how people can build moral careers by actively revising purity and pollution rules while holding fast to institutional moral obligations. By theorizing careers as an ongoing series of moral claiming strategies, this research contributes novel ideas about how morals weave through and organize relationships between people, careers, and institutions.
In this paper, we examine the evolution of jobs in the midst of the hiring process: how jobs change between the decision to bring in someone to do a body of work and hiring someone. We analyze data from interviews, observations, and documents about start-up hiring and find that, during hiring, tasks are added and removed from jobs; jobs are abandoned, replaced, and moved; and hiring processes are relaunched. We describe two pathways that this evolution takes: the pathway of anticipated evolution, shaped by the unknown nature of the jobs being filled, and the pathway of accidental evolution, shaped by unanticipated factors surrounding jobs. Although the pathways lead to many of the same immediate consequences, there are differences in the longer-term consequences. Across the pathways, many jobs continue to evolve. On the pathway of anticipated evolution, many job incumbents leave within a year and are not replaced. On the pathway of accidental evolution, the longer-term consequences for job incumbents, structures, and organizations range from stability in structures and incumbents to ongoing conflict and incumbent departure. Not surprisingly, most evolving jobs are new to their organizations, but contrary to common conceptions, job evolution is not the product of managers who lack experience or use lax hiring practices. Our observations provide evidence of the emergent nature of jobs, hiring, and organizations.
Building on an in-depth study of a manufacturing company’s shift from a product to a product-service business model, we explore how single-focus companies transition to a dual orientation. Although companies generally use highly sophisticated practices to manage a dual orientation, those that transition to one successfully start with less sophisticated practices. Early on, the use of simple tradeoff practices, which maintain the product and service logics, helps single-focus companies explore the emergent tensions that their transition to a dual orientation causes. Conversely, adopting more sophisticated practices at this early stage overwhelms them. At a later stage, these companies’ growing understanding of the tensions allows them to experiment with more comprehensive paradox practices that transcend the product and service logics. Conversely, maintaining simple practices at this stage prevents them from gaining the solution experience required to complete the transition. The evolutionary process culminates in sophisticated routinized practices that institutionalize recurrent tensions’ solution, while allowing for further experimentation to deal with new tensions. The different practices’ appropriate sequence and pacing during the evolutionary process facilitate companies’ transition to a dual orientation.
We examine how incumbent organizations respond to complementary-asset discontinuities — technological changes that introduce new manufacturing, distribution, and sales assets but leave the incumbents’ core knowledge preserved. To examine this increasingly common but relatively overlooked phenomenon, we conducted an inductive study of how six newspapers adapted to Internet distribution from 1995 to 2019. Our contribution is a framework that highlights three levels of adaptation (resources, demand, and ecosystem) with related mechanisms and necessary outcomes. At the resource level, incumbents adopt the new complementary assets according to the perception of synergies with their existing core knowledge. At the demand level, the extent to which incumbents update their beliefs about value creation depends on how much they experiment with customers. At the ecosystem level, higher experimentation in the ecosystem helps incumbents to update their beliefs about value capture. The research offers important implications for the technological change, strategic management, and business model innovation literature.
Learning-by-connecting, the formation of connections between lessons, is a fairly common phenomenon, but how does it evolve? We argue that learning-by-connecting unfolds as the relevance of lessons to other lessons is gradually discovered over time. The process of “relevance discovery” unfolds through a dynamic interplay between lessons and their context that provides opportunities to discover the relevance of lessons to other lessons. We develop a theoretical model in which the availability of these opportunities and their sorting in time drive the formation of connections. We explore and test our model in the context of organizational rules that we conceptualize, following rule-based learning theories, as repositories of lessons learned. Our empirical context is the formation of citation ties between clinical practice guidelines (CPGs), a type of organizational rules in healthcare, in a Canadian regional healthcare organization. We find that citation tie formation intensifies when opportunities to discover relevance become available. We also find that learning-by-connecting creates rule networks in which the formation of new ties slows down due to the sorting of opportunities in time. Our findings support our assumption that learning-by-connecting is shaped by relevance discovery. Our study extends models of rule-based learning and contributes to discussions on the formation of connections in contexts of dispersed learning and knowledge.
The governance of front-line professionals is a persistent organizational problem. Regulations designed to make professional work more legible and responsive to both organizational and public expectations depend on these professionals’ willing implementation. This paper examines the important question of how professional control shapes regulatory compliance. Drawing on a seventeen-month ethnographic study of a bioscience laboratory, we show how professionals deploy their discretionary judgment to assemble environmental, health, and safety regulations with their own expert practices, explaining frequently observed differential rates of regulatory compliance. We find that professional scientists selectively implement and blend formal regulations with expert practice to respond to risks the law acknowledges (to workers’ bodies and the environment) and to risks the law does not acknowledge but professionals recognize as critical (to work tasks and collegiality). Some regulations are followed absolutely, others are adapted on a case-by-case basis; in other instances, new practices are produced to control threats not addressed by regulations. Such selective compliance, adaptation and invention enact professional expertise: interpretations of hazard and risk. The discretionary enactment of regulations, at a distance from formal agents, becomes part of the technical, practical, and tacit assemblage of situated practices. Thus, paradoxically, professional expert control is maintained and sometimes enhanced as professionals blend externally imposed regulations with expert practices. In essence, regulation is co-opted in the service of professional control. This research contributes to studies of professional expertise, the legal governance of professionals in organizations, regulatory compliance, and safety cultures.
Although studies underscore the importance of creating a coherent collective identity in order to legitimate a new market category, strategy and entrepreneurship research is divided on whether and to what degree an entrepreneur will engage in collective action to promote the identity. To reconcile the inconsistency, we introduce the concept of entrepreneurial shared fate—the belief of a focal venture that it and its competitors are bound together by a sense of belongingness and equally experience similar consequences—and explore how external threats can influence the degree of shared fate. We conceptually distinguish between communal and individual threats and propose that communal threats will increase, whereas individual threats will decrease, shared fate. We also explore boundary conditions that strengthen and weaken the main effects of perceived communal and individual threats on collective identity promotion. Empirically, we examine venture identity framing in response to forest-conservation activism in the U.S. wood pellet market. Implications for research on new market categories, collective identity, optimal distinctiveness, and forest management are discussed.
Scholars have long wrestled with whether hierarchical differentiation is functional or dysfunctional for teams. Building on emerging research that emphasizes the distinction between power (i.e., control over resources) and status (i.e., respect from others), we aim to help reconcile the functional and dysfunctional accounts of hierarchy by examining the effects of power differentiation on team performance, contingent on status differentiation. We theorize that power differentiation is dysfunctional for teams with high status differentiation by increasing knowledge hiding, which undermines team performance. In contrast, we predict that power differentiation is functional for teams with low status differentiation by decreasing knowledge hiding, which improves team performance. In a field study, we found that power differentiation harmed team performance via knowledge hiding in teams with high status differentiation, but power differentiation had no effect on knowledge hiding or performance in teams with low status differentiation. In an experiment, we again found that power differentiation harmed team performance by increasing knowledge hiding in teams with high status differentiation. However, power differentiation improved team performance by decreasing knowledge hiding in teams with status equality. Finally, in a third study, we confirm the role of status differentiation in making team climates more competitive and examine the effect of power-status alignment within teams, finding that misalignment exacerbates the dysfunctional effects of power differentiation in teams with high status differentiation. By examining how power and status hierarchies operate in tandem, this work underscores the need to take a more nuanced approach to studying hierarchy in teams.Funding: This research is partially supported by the National Natural Science Foundation of China [Grants 71572079 and 71872086].Supplemental Material: The online appendix is available at https://doi.org/10.1287/orsc.2021.1540.
Institutional theory research on institutional intermediation typically focuses on how institutional intermediaries address voids in market-based institutions that inhibit entrepreneurship. In doing so, the research rarely studies what types of institutional intermediaries entrepreneurs prefer to use. We address this gap with a microinstitutional inquiry of how entrepreneurs in a rudimentary market-based economy differ in the relevance they place on different types of institutional intermediaries. Using a sample from the Indrachok market in Kathmandu, Nepal, and using a three-stage qualitative and quantitative abductive investigation of a cascading set of increasingly refined research questions, we identify two key preferences for institutional intermediaries. First, we find a key institutional intermediation tripod consisting of three locally focused institutional intermediaries: family, suppliers, and peer entrepreneurs. The tripod is supplemented by institutional intermediaries with more moderate preference in this context: four other locally focused institutional intermediaries (local politicians, police, religious figures, and political gangs) and three broad-based institutional intermediaries (government, microlenders, and nongovernmental organizations). Second, the importance of suppliers and peers as institutional intermediaries reflects entrepreneurs’ registration status (registered versus unregistered) and microgeographic location (dispersed versus clustered businesses). The research reconceptualizes institutional intermediation in rudimentary market-based economies from the entrepreneurs’ perspective, identifying mechanisms that shape entrepreneurs’ preferences and providing proposition for future testing.Funding: This research was partially supported by the National Natural Science Foundation of China [Grant 71620107001].Supplemental Material: The online appendix is available at https://doi.org/10.1287/orsc.2021.1531.
Are socially irresponsible employment practices, such as abusive discipline and wage theft, systematically tied to manufacturing outcomes in emerging-market countries? Drawing on a stream of stakeholder theory that emphasizes economic interdependencies and insights from the fields of industrial relations and human resource management, we argue that working conditions within a firm are facets of a systemic approach to value creation and value appropriation. Some manufacturers operate “low road” systems that rest on harmful practices. Others operate “high road” systems in which the need to develop employees’ human capital deters socially irresponsible employment practices. To test the theory, we conduct a large-scale study of labor violations and manufacturing outcomes by analyzing data on over four thousand export-oriented small manufacturers in 48 emerging-market countries. The analysis demonstrates that socially irresponsible employment practices are associated with inferior firm-level manufacturing outcomes even after controlling for the effects of firm size, industry, product mix, production processes, host country, destination markets, and buyer mix. The theory and results suggest an opportunity for multinational corporations to improve corporate social performance in global value chains by encouraging their suppliers to transition to systems of value creation that rely on the development of worker human capital.Funding: A. McGahan received funding from Social Sciences and Humanities Research Council [Grant 435-2016-0075].
The forces that threaten to break apart private regulatory institutions are well known, but the forces that sustain them are not. Through a longitudinal inductive study of the Towards Sustainable Mining (TSM) program in the Canadian mining industry, we demonstrate how private regulatory institutions are sustained by strategically manipulating different aspects of an institution’s stringency. Our findings show how shifts in external conditions decreased benefits of participation for firms, triggering institutional destabilization. We demonstrate how the interdependent mechanisms of hollowing—actions that ratchet down aspects of stringency associated with high compliance costs—and fortifying—actions that ratchet up aspects of stringency associated with low compliance costs—worked together to stabilize the institution by rebalancing the competing pressures that underpin it. However, these same mechanisms can hinder the ability of these institutions to substantively address the targeted issues, even as they become more stringent in some areas. Our study advances research on private regulation by showing how different aspects of stringency can be simultaneously ratcheted up and ratcheted down to sustain private regulatory institutions. Further, in positioning institutional stability as an ongoing negotiation, we elucidate the key custodial role of governing organizations like trade associations in institutional maintenance.Funding: Financial support fromthe Social Sciences and Humanities Research Council of Canada [Grant 752-2012-2294] is gratefully acknowledged.
Research has indicated limited effects of formal governance measures on securities fraud prevention in emerging markets due to the weak rule of law. We propose that hierarchical inconsistency, misaligned rank ordering in formal organizational and informal social hierarchies of the corporate elite, can provide a novel monitoring mechanism to reduce securities fraud. Leaders at the top of the two inconsistent hierarchies can feel distressed and motivated to engage in contestation and challenge each other’s authority, thus providing checks and balances and preventing groupthink. This monitoring effect is likely to be stronger when either of the two heads has dominant and unequivocal superiority in their respective hierarchy, making them particularly distressed by the hierarchical inconsistency and prone to contest. We test our argument in the context of publicly listed family-controlled firms in China, where business and family hierarchies may confer superiority to different individuals. Our study contributes to the corporate securities fraud literature by understanding how formal organizational structures and informal social relationships interact and jointly influence governance effectiveness in emerging markets.Funding: This work was supported by the Strategic Management Society [2015 SRF Dissertation Research Grant], Rudolf and Valeria Maag research funds, and INSEAD alumni funds.
Drawing on organizational theory, agency theory, and research in strategic human resource management, this study explores how chain affiliation influences human resource (HR) investments using data from a nationally random survey of restaurant establishments. We propose that chain-affiliated units will make different investments in those areas of the HR system where chains establish superior organizational routines compared with nonaffiliated units. By contrast, we argue that in the absence of chain routines, ownership incentives will drive differences in human resource investments. Specifically, we find that franchisee-owned units focus more on cost reduction by underinvesting in human resource practices compared with company-owned units and independently owned units when organizational routines are not provided by the chain. We provide further support for our theoretical arguments using additional data on multiunit ownership and franchisor influence. Finally, we conduct supplemental analyses to explore the relationship between different human resource investments and two important organizational outcomes: employee turnover and customer satisfaction ratings from Yelp. Our results highlight the types of human resource practices that are important for service work and suggest that the provision of organizational routines can have important implications for the long-run success of chains and their units.Funding: Financial support from the Center for Advanced Human Resource Studies, Cornell University; and the Rockefeller Foundation is gratefully acknowledged.Supplemental Material: The online appendix is available at https://doi.org/10.1287/orsc.2021.1539.
A central theoretical premise is that firms internalize transactions that are not suited for formal contracting. Yet, there is growing evidence that firms rely on formal contracts to govern some of their transactions within the firm. This paper discusses why firms use formal contracts between units within the firm and develops propositions for when formal contracts arise. Internalization does not eliminate transactional problems, and informal agreements for transactions between units often suffer from problems in understanding what the other unit will do and whether it will do what it promises. We argue that many of the features that make formal contracts valuable tools for market exchange are beneficial within firms, even if court enforcement of the contract is not possible. We suggest that formal contracts between units serve as communication and commitment devices that address coordination and incentive problems within the firm by providing clarity and credibility on the rights allocated to the units in the transaction.Funding: Support for this research comes from the London Business School.Supplemental Material: The online appendix is available at https://doi.org/10.1287/orsc.2021.1536.
How does an employee’s centrality in intrafirm research and development activities affect the employee’s propensity for outward mobility? Does this proclivity vary by the type of employment the employee seeks: moving to other firms versus founding a new venture? We maintain that, to answer these questions, we must distinguish between an employee’s centrality in the intrafirm collaboration network and the employee’s centrality in the intrafirm technological recombination network. We utilize the curricula vitae and patent data of corporate inventors at a leading semiconductor company between 1993 and 2012 to test our hypotheses. Contrary to prevailing views, our competing risk model indicates that corporate inventors who are central in the intrafirm collaboration and technological network and, thus, have the most opportunities are less likely to leave the current employer. However, when considering external employment opportunities, their preferences vary. Collaboration-central individuals are more likely to start a new venture than to move to another employer. Their skill in developing interpersonal relationships enables them to attract the tangible and intangible resources needed in a new firm. In contrast, inventors whose technological expertise is central to the firm’s technology recombination network are more likely to move to another employer than to start a new venture. In an established firm, they can leverage their technological know-how using the resources that a new venture would lack. Our theory highlights the trade-offs in employees’ attempts to take advantage of their internal and external value based on their position within the firm’s collaboration and technological networks.Funding: The authors thank LeBow College of Business, SKEMA Business School, and Bocconi University for their financial support.Supplemental Material: The online appendix is available at https://doi.org/10.1287/orsc.2021.1535.
A growing body of scholarship studies the emergence of moral markets—sectors offering market-based solutions to social and environmental issues. To date, researchers have largely focused on the drivers of firm entry into these values-laden sectors. However, we know comparatively little about postentry dynamics or the determinants of firm survival in moral markets. This study examines how regional institutional logics—spatially bound, socially constructed meaning systems that legitimize specific practices and goals within a community—shape firm survival in emerging moral markets. Using a unique panel of firms entering the first eight years of the U.S. green building supply industry, we find that (1) a regional market logic amplifies the impacts of market forces by increasing the positive impact of market adoption and the negative impact of localized competition on firm survival, (2) a regional proenvironmental logic dampens the impacts of adoption and competition on firm survival, and (3) institutional complexity—the co-occurrence of both market and proenvironmental logics in a region—negates the traditional advantages of de alio (diversifying incumbent) firms, creating an opportunity for de novo (entrepreneurial entrant) firms to compete more effectively. Our study integrates research on industry emergence, institutional logics, and firm survival to address important gaps in our knowledge regarding the evolution and growth of environmental entrepreneurship in moral markets.Funding: J. G. York thankfully acknowledges support from the Michael and Sherri Miske Faculty Research Award given by the Leeds School of Business, University of Colorado, Boulder.
Recent scholarship has established several ways in which external hiring—versus filling a role with a comparable internal candidate—is detrimental to firms. Yet, organizational learning theory suggests that external hires benefit firms: by importing knowledge that is unavailable or obscured to insiders and applying it toward experimentation and risky recombination. Accordingly and consistent with studies of learning by hiring and innovation, we predict that external hires are at greater risk of intrapreneurship than internal hires. We test this prediction via a study of product managers in large technology companies. We use machine learning to operationalize intrapreneurship by comparing product manager job descriptions with the founding statements of venture-backed technology entrepreneurs. Our research design employs coarsened exact matching to balance pretreatment covariates between product managers who arrived at their roles internally versus externally. The results of our analysis indicate that externally hired product managers are substantially more intrapreneurial than observably equivalent internal hires. However, we also find that intrapreneurial product managers have a higher turnover rate, an effect that is primarily driven by external hires. This suggests that hiring for intrapreneurship may be a difficult strategy to sustain.Funding: The authors acknowledge financial support from London Business School, the National University of Singapore [Grant WBS R-313-000-128-133], and a dissertation grant from the Ewing Kauffman Foundation, Kansas City, Missouri.Supplemental Material: The online appendix is available at https://doi.org/10.1287/orsc.2021.1530.
This research considers how frontline managers’ construal affects their conceptualization of organizational problems, which in turn influences how they incentivize employees to search out appropriate solutions. Depending on whether they conceptualize problems in more abstract or more concrete ways, frontline managers will vary in organizational control mechanisms they use to incentivize their employees to engage in exploration and exploitation. Based on these relationships, we expect the solutions achieved by employees to vary in terms of efficiency and effectiveness. Using a database of 267 projects in a single firm, we find that, after holding project attributes constant, concrete-oriented managers tend to utilize more process controls that lead employees to solve organizational problems more efficiently, whereas abstract-oriented managers tend toward use of more outcome controls that lead to more effective problem solving. When employees engage in ambidextrous learning, both effectiveness and efficiency of outcomes are enhanced. This research sheds light on important microfoundational influences on organizational outcomes.
Hierarchies emerge as collectives attempt to organize themselves toward successful performance. Consequently, research has focused on how team hierarchies affect performance. We extend existing models of the hierarchy-performance relationship by adopting an alternative: Performance is not only an output of hierarchy but also a critical input, as teams’ hierarchical differentiation may vary based on whether they are succeeding. Integrating research on exploitation and exploration with work on group attributions, we argue that teams engage in exploitation by committing to what they attribute as the cause of their performance success. Specifically, collectives tend to attribute their success to individuals who wielded greater influence within the team; these individuals are consequently granted relatively higher levels of influence, leading to a higher degree of hierarchy. We additionally suggest that the tendency to attribute, and therefore grant more influence, to members believed to be the cause of success is stronger for teams previously higher (versus lower) in hierarchy, as a higher degree of hierarchical differentiation provides clarity as to which members had a greater impact on the team outcome. We test our hypotheses experimentally with teams engaging in an online judgement task and observationally with teams from the National Basketball Association. Our work makes two primary contributions: (a) altering existing hierarchy-performance models by highlighting performance as both an input and output to hierarchy and (b) extending research on the dynamics of hierarchy beyond individual rank changes toward examining what factors increase or decrease hierarchical differentiation of the team as a whole.Supplemental Material: The online appendix is available athttps://doi.org/10.1287/orsc.2021.1528.
Universities and colleges often engage in initiatives aimed at enrolling students from diverse demographic groups. Although substantial research has explored the impact of such diversity initiatives, less understood is the extent to which certain application strategies may continue to favor historically privileged groups, especially white men, as they seek admission to selective programs. With this study, I begin to address this gap by investigating the gender and racial implications of application endorsements—a common, often informal, network practice of signaling support for certain applicants that is shown to significantly boost an applicant’s chances of admission. Using unique data on the applicants and matriculants to a full-time MBA program at one elite U.S. business school, I first assess whether the endorsement advantage differs across demographic groups. Building on the social networks, selection, and inequality literatures, I then identify and test three key theoretical mechanisms by which the endorsement process may potentially benefit white men more than women and racial minorities. Although I do not find evidence in the studied program that the application endorsement is valued differently by key admissions officers or that it provides a different quality signal depending on the applicant’s gender or race, I do find that white men are significantly more likely than women and minorities to receive application endorsements. I conclude by discussing the implications of this study for understanding how gender and racial differences in accessing advantageous (often informal) network processes may undermine organizational efforts to achieve demographic equality and diversity.Funding: Financial support from the James S. Hardigg (1945) Work and Employment Fund and the MIT Sloan School of Management is gratefully acknowledged.
Although prior research on shareholder activism has highlighted how such activism can economically benefit the shareholders of targeted firms, recent studies also suggest that shareholder activism can economically disadvantage nonshareholder stakeholders, notably employees. Our study extends this research by exploring whether shareholder activism by institutional investors (i.e., institutional investor activism) can adversely affect employee health and safety through increased workplace injury and illness. Furthermore, deviating from the assumption that financially motivated institutional investor activists are homogeneous in their goals and preferences, we investigate whether the influence of institutional investor activism on employee health and safety hinges on the political ideology of the shareholder activist and of the board of the targeted firm. Using establishment-level data, we find that institutional investor activism adversely influences workplace injury and illness at targeted firms and that this influence is stronger for nonliberal shareholder activists and for firms with a nonliberal board. Our study contributes to shareholder activism research by highlighting how the political ideology of shareholder activists and boards affects the impact of shareholder activism on stakeholders and how shareholder activism can adversely affect the health and safety of employees. Furthermore, our paper also contributes to research on workplace safety and the management of employee relations and human capital resources by highlighting the detrimental effect of a firm’s ownership by investor activists on its employees and how the board’s political ideology may enable a firm to reduce this risk.Supplemental Material: The online appendices are available at https://doi.org/10.1287/orsc.2021.1542.
Research and practice suggest that cofounded ventures outperform solo-founded ventures. Yet, little work has explored the conditions under which solo founding might be preferable to cofounding. Combining an inductive case-oriented analysis with a Qualitative Comparative Analysis of 70 new entrepreneurial ventures, we examine why and how solo founders can be as successful as their peers in cofounded ventures. We find that successful solo founders strategically use a set of cocreators rather than cofounders to overcome liabilities, retain control, and mobilize resources in unique and unexpected ways. A primary contribution of this paper is an emergent configurational theory of entrepreneurial organizing. Overall, we reveal the broader significance and theoretical importance of adopting a configurational lens for both practitioners and scholars of entrepreneurship.Funding: Financial support from the Strategy Research Foundation [Grant SRF-2018-DP-9156] is gratefully acknowledged.Supplemental Material: The online appendix is available at https://doi.org/10.1287/orsc.2021.1548.
Evidence suggests that possessing more qualifications than is necessary for a job (i.e., overqualification) negatively impacts job candidates’ outcomes. However, unfair discounting of women’s qualifications and negative assumptions about women’s career commitment imply that female candidates must be overqualified to achieve the same outcomes as male candidates. Across two studies, experimental and qualitative data provide converging evidence in support of this assertion, showing that gender differences in how overqualification impacts hiring outcomes are due to the type of commitment—firm or career—that is most salient during evaluations. Overqualified men are perceived to be less committed to the prospective firm, and less likely to be hired as a result, than sufficiently qualified men. But overqualified women are perceived to be more committed to their careers than qualified women because overqualification helps overcome negative assumptions that are made about women’s career commitment. Overqualification also does not decrease perceptions of women’s firm commitment like it does for men: supplemental qualitative and experimental evidence reveals that hiring managers rationalize women’s overqualification in a way they cannot for men by relying on gender stereotypes about communality and assumptions about candidates’ experiences with gender discrimination at prior firms. These findings suggest that female candidates must demonstrate their commitment along two dimensions (firm and career), but male candidates need only demonstrate their commitment along one dimension (firm). Taken together, differences in how overqualification impacts male versus female candidates’ outcomes are evidence of gender inequality in hiring processes, operating through gendered assumptions about commitment.Funding: This research was funded by internal faculty research funds provided by Tepper School of Business, Carnegie Mellon University.Supplemental Material: The online appendices are available at https://doi.org/10.1287/orsc.2021.1550.
According to self-regulation theories, affect plays a crucial role in driving goal-directed behaviors throughout employees’ work lives. Yet past work presents inconsistent results regarding the effects of positive and negative affect with theory heavily relying on understanding the separate, unique effects of each affective experience. In the current research, we integrate tenets of emotional ambivalence with self-regulation theories to examine how the conjoint experience of positive and negative affect yields benefits for behavioral regulation. We test these ideas within a self-regulatory context that has frequently studied the benefits of affect and has implications for all employees at one point in their careers: the job search. Adopting a person-centered (i.e., profile-based) perspective across two within-person investigations, we explore how emotional ambivalence relates to job search success (i.e., interview invitations, job offers) via job search self-regulatory processes (i.e., metacognitive strategies, effort). Results illustrate that the subsequent week (i.e., at time t + 1; Study 1) and month (Study 2) after job seekers experience emotional ambivalence (i.e., positive and negative affect experienced jointly at similar levels at time t), they receive more job offers via increased job search effort and interview invitations. Theoretical and practical implications for studying emotional ambivalence in organizational scholarship are discussed.Supplemental Material: The online appendix is available at https://doi.org/10.1287/orsc.2021.1553.
We clarify conditions under which two seemingly contradictory yet widely observed tendencies occur in cultural markets where amateur connoisseurs evaluate products—reinforcement of previous consensus and contradiction of that same consensus. We start from prior work’s insight that achieving “distinction” requires that evaluators display tastes demonstrating higher skills of discernment and standards that are acknowledged as legitimate by others. Based on this, we argue that evaluators reinforce prior evaluations of products to demonstrate that they share the same quality standards as their peers, but they selectively contradict prior evaluations by downgrading widely acclaimed products, because doing the latter makes the evaluator appear to have even more sophisticated tastes than their peers. We test this account using 1.66 million reviews from an online platform where amateur connoisseurs publicly evaluate beers. Our analyses support an endogenous model explaining why and when evaluators may contradict existing evaluations even though a group plausibly sharing the same quality standards may have established such evaluations in the first place.Supplemental Material: The online appendix is available at https://doi.org/10.1287/orsc.2021.1556.
We argue that the actions for which actors receive recognition vary as they move up the hierarchy. When actors first enter a community, the community rewards them for their easier-to-evaluate contributions to the community. Eventually, however, as these actors rise in status, further increases in stature come increasingly from engaging in actions that are more difficult to evaluate or even impossible to judge. These dynamics produce a positive feedback loop, in which those who have already been accorded some stature garner even greater status through quality-ambiguous actions. We present evidence from Stack Overflow, an online community, and from two online experiments consistent with these expected patterns.Funding: All authors would like to acknowledge funding from the Austrian Science Fund [Grant P 25768-G16].Supplemental Material: The online appendix is available at https://doi.org/10.1287/orsc.2021.1559.
We consider the problem of optimizing over time hundreds or thousands of discrete entities that may be characterized by relatively complex attributes, in the presence of different forms of uncertainty. Such problems arise in a range of operational settings such as transportation and logistics, where the entities may be aircraft, locomotives, containers, or people. These problems can be formulated using dynamic programming but encounter the widely cited “curse of dimensionality.” Even deterministic formulations of these problems can produce math programs with millions of rows, far beyond anything being solved today. This paper shows how we can combine concepts from artificial intelligence and operations research to produce practical solution methods that scale to industrial-strength problems. Throughout, we emphasize concepts, techniques, and notation from artificial intelligence and operations research to show how these fields can be brought together for complex stochastic, dynamic problems.
We examine symbolic tools associated with two modeling systems for mathematical programming, which can be used to automatically detect the presence or absence of convexity and concavity in the objective and constraint functions, as well as convexity of the feasible set in some cases. The coconut solver system [Schichl, H. 2004a. COCONUT: COntinuous CONstraints—Updating the technology] focuses on nonlinear global continuous optimization and possesses its own modeling language and data structures. The Dr. Ampl meta-solver [Fourer, R., D. Orban. 2007. Dr. Ampl—A meta solver for optimization. Technical Report G-2007-10, GERAD, Montréal] aims to analyze nonlinear differentiable optimization models and hooks into the ampl Solver Library [Gay, D. M. 2002. Hooking your solver to AMPL]. Our symbolic convexity analysis may be supplemented, when it returns inconclusive results, with a numerical phase that may detect nonconvexity. We report numerical results using these tools on sets of test problems for both global and local optimization.
We propose a new robust optimization method for problems with objective functions that may be computed via numerical simulations and incorporate constraints that need to be feasible under perturbations. The proposed method iteratively moves along descent directions for the robust problem with nonconvex constraints and terminates at a robust local minimum. We generalize the algorithm further to model parameter uncertainties. We demonstrate the practicability of the method in a test application on a nonconvex problem with a polynomial cost function as well as in a real-world application to the optimization problem of intensity-modulated radiation therapy for cancer treatment. The method significantly improves the robustness for both designs.
We propose a new heuristic for nonlinear global optimization combining a variable neighborhood search framework with a modified trust-region algorithm as local search. The proposed method presents the capability to prematurely interrupt the local search if the iterates are converging to a local minimum that has already been visited or if they are reaching an area where no significant improvement can be expected. The neighborhoods, as well as the neighbors selection procedure, are exploiting the curvature of the objective function. Numerical tests are performed on a set of unconstrained nonlinear problems from the literature. Results illustrate that the new method significantly outperforms existing heuristics from the literature in terms of success rate, CPU time, and number of function evaluations.
Statistical selection procedures are used to select the best of a finite set of alternatives, where “best” is defined in terms of each alternative's unknown expected value, and the expected values are inferred through statistical sampling. One effective approach, which is based on a Bayesian probability model for the unknown mean performance of each alternative, allocates samples based on maximizing an approximation to the expected value of information (EVI) from those samples. The approximations include asymptotic and probabilistic approximations. This paper derives sampling allocations that avoid most of those approximations to the EVI but entails sequential myopic sampling from a single alternative per stage of sampling. We demonstrate empirically that the benefits of reducing the number of approximations in the previous algorithms are typically outweighed by the deleterious effects of a sequential one-step myopic allocation when more than a few dozen samples are allocated. Theory clarifies the derivation of selection procedures that are based on the EVI.
We present the “trivariate reduction extension” (TREx)—an exact algorithm for the fast generation of bivariate Poisson random vectors. Like the normal-to-anything (NORTA) procedure, TREx has two phases: a preprocessing phase when the required algorithm parameters are identified, and a generation phase when the parameters identified during the preprocessing phase are used to generate the desired Poisson vector. We prove that the proposed algorithm covers the entire range of theoretically feasible correlations, and we provide efficient-computation directives and rigorous bounds for truncation error control. We demonstrate through extensive numerical tests that TREx, being a specialized algorithm for Poisson vectors, has a preprocessing phase that is uniformly a hundred to a thousand times faster than a fast implementation of NORTA. The generation phases of TREx and NORTA are comparable in speed, with that of TREx being marginally faster. All code is publicly available.
A very important ingredient for solving hard general integer programs are heuristics that try to quickly find good feasible solutions. One of these heuristics is Wedelin's algorithm, which works for the limited class of 0-1 integer programs. A big advantage of Wedelin's approach is that it does not depend on a solution of the linear programming (LP) relaxation as many other heuristics do. This makes it extremely fast in practice and makes it easy to use the parallelism of the upcoming multicore CPUs, as in an integer programming (IP) solver it could be applied in parallel to the traditional branch-and-bound algorithm.In this paper, we present several extensions and generalizations to Wedelin's algorithm (most can be handled in an implicit manner without much performance cost) and investigate different ways of improving it. We give all necessary details and parameters. We strive for an algorithm that is faster than other heuristics but achieves comparable solution quality.We evaluate the performance of the algorithm on a large set of more than 100 instances from different sources. The results indicate that our heuristic often finds solutions comparable to or even better than those found using current state-of-the-art heuristics while typically needing only a fraction of their running time.Additionally, we report positive findings on the application of the heuristic on feasibility instances from discrete tomography. Our algorithm always finds the IP optimum in less time than the simplex/barrier algorithms and often in less time than it takes the volume algorithm to find just the LP optimum.
The concept of dominance among nodes of a branch-and-bound tree, although known for a long time, is typically not exploited by general-purpose mixed-integer linear programming (MILP) codes. The starting point of our work was the general-purpose dominance procedure proposed in the 1980s by Fischetti and Toth, where the dominance test at a given node of the branch-and-bound tree consists of the (possibly heuristic) solution of a restricted MILP only involving the fixed variables. Both theoretical and practical issues concerning this procedure are analyzed, and important improvements are proposed. In particular, we use the dominance test not only to fathom the current node of the tree, but also to derive variable configurations called “nogoods” and, more generally, “improving moves.” These latter configurations, which we rename “pruning moves” so as to stress their use in a node-fathoming context, are used during the enumeration to fathom large sets of dominated solutions in a computationally effective way. Computational results on a testbed of MILP instances whose structure is amenable to dominance are reported, showing that the proposed method can lead to a considerable speedup when embedded in a commercial MILP solver.
The Second International Timetabling Competition (TTC2007) opened in August 2007. Building on the success of the first competition in 2002, this sequel aimed to further develop research activity in the area of educational timetabling. The broad aim of the competition was to create better understanding between researchers and practitioners by allowing emerging techniques to be developed and tested on real-world models of timetabling problems. To support this, a primary goal was to provide researchers with models of problems faced by practitioners through incorporating a significant number of real-world constraints. Another objective of the competition was to stimulate debate within the widening timetabling research community. The competition was divided into three tracks to reflect the important variations that exist in educational timetabling within higher education. Because these formulations incorporate an increased number of “real-world” issues, it is anticipated that the competition will now set the research agenda within the field. After finishing in January 2008, final results were made available in May 2008. Along with background to the competition, the competition tracks are described here along with a brief overview of the techniques used by the competition winners.
We show that the linear programming relaxation of the cutting-stock problem can be solved efficiently by the recently proposed inexact bundle method. This method saves work by allowing inaccurate solutions to knapsack subproblems. With suitable rounding heuristics, our method solves almost all the cutting-stock instances from the literature.
We present a constraint analysis methodology for linear matrix inequality constraints. If the constraint set is found to be feasible, we search for a minimal representation; otherwise, we search for an irreducible infeasible system. The work is based on the solution of a set-covering problem where each row corresponds to a sample point and is determined by constraint satisfaction at the sampled point. Thus, an implementation requires a method to collect points in the ambient space and a constraint oracle. Much of this paper will be devoted to the development of a hit-and-run sampling methodology. Test results confirm that our approach not only provides information required for constraint analysis but will also, if the feasible region has a nonvoid interior, with probability one, find a feasible point.
The widely used support vector machine (SVM) method has shown to yield very good results in supervised classification problems. Other methods such as classification trees have become more popular among practitioners than SVM thanks to their interpretability, which is an important issue in data mining.In this work, we propose an SVM-based method that automatically detects the most important predictor variables and the role they play in the classifier. In particular, the proposed method is able to detect those values and intervals that are critical for the classification. The method involves the optimization of a linear programming problem in the spirit of the Lasso method with a large number of decision variables. The numerical experience reported shows that a rather direct use of the standard column generation strategy leads to a classification method that, in terms of classification ability, is competitive against the standard linear SVM and classification trees. Moreover, the proposed method is robust; i.e., it is stable in the presence of outliers and invariant to change of scale or measurement units of the predictor variables.When the complexity of the classifier is an important issue, a wrapper feature selection method is applied, yielding simpler but still competitive classifiers.
The quadratic linear ordering problem naturally generalizes various optimization problems such as bipartite crossing minimization or the betweenness problem, which includes linear arrangement. These problems have important applications, e.g., in automatic graph drawing and computational biology. We present a new polyhedral approach to the quadratic linear ordering problem that is based on a linearization of the quadratic objective function.Our main result is a reformulation of the 3-dicycle inequalities using quadratic terms. After linearization, the resulting constraints are shown to be face-inducing for the polytope corresponding to the unconstrained quadratic problem. We use this result both within a branch-and-cut algorithm and within a branch-and-bound algorithm based on semidefinite programming. Experimental results for bipartite crossing minimization show that this approach clearly outperforms other methods.
With improved tools for collecting genetic data from natural and experimental populations, new opportunities arise to study fundamental biological processes, including behavior, mating systems, adaptive trait evolution, and dispersal patterns. Full use of the newly available genetic data often depends upon reconstructing genealogical relationships of individual organisms, such as sibling reconstruction. This paper presents a new optimization framework for sibling reconstruction from single generation microsatellite genetic data. Our framework is based on assumptions of parsimony and combinatorial concepts of Mendel's inheritance rules. Here, we develop a novel optimization model for sibling reconstruction as a large-scale mixed-integer program (MIP), shown to be a generalization of the set covering problem. We propose a new heuristic approach to efficiently solve this large-scale optimization problem. We test our approach on real biological data as presented in other studies as well as simulated data, and compare our results with other state-of-the-art sibling reconstruction methods. The empirical results show that our approaches are very efficient and outperform other methods while providing the most accurate solutions for two benchmark data sets. The results suggest that our framework can be used as an analytical and computational tool for biologists to better study ecological and evolutionary processes involving knowledge of familial relationships in a wide variety of biological systems.
Haplotyping estimation from aligned single nucleotide polymorphism fragments has attracted increasing attention in recent years because of its importance in the analysis of fine-scale genetic data. Its application fields range from mapping of complex disease genes to inferring population histories, passing through designing drugs, functional genomics, and pharmacogenetics. The literature proposes several criteria for haplotyping populations, each of them characterized by biological motivations. One of the most important haplotyping criteria is parsimony, which consists of finding the minimum number of haplotypes necessary to explain a given set of genotypes. Parsimonious haplotype estimation is an 𝒩𝒫-hard problem for which the literature has proposed several integer programming (IP) models. Here, we describe a new polynomial-sized IP model based on the concept of class representatives, already used for the coloring problem. We propose valid inequalities to strengthen our model and show, through computational experiments, that our model outperforms the best IP models currently known in literature.
The Gene Ontology (GO) project provides a structured vocabulary of biological terms used by biological researchers as a tool for standardization of references to biological entities. Genes may be annotated with GO terms to indicate their roles or localizations in the cell. GO has been used in conjunction with high-throughput experimental methods, such as microarrays. In this setting, the interest is to determine whether sets of genes identified by the high-throughput experiment are enriched for GO terms: Do certain terms annotate more genes in the identified set than one might expect? Enriched terms are taken as a potential summary of the cellular function for the identified set of genes and may provide clues leading to new directions for investigation. Current methods for determining whether sets of genes are GO-enriched have certain well-known shortcomings. Many methods do not take the hierarchical structure of the ontology into account in determining enrichment. We address this drawback by introducing a new statistical test (TreeHugger) based on a novel per-gene scoring scheme for GO terms. Given a set of genes and a specified subset of those genes, our method determines enrichment of GO terms in the subset, taking into account the structure of the ontology and ascribing a lower weight to those terms that do not themselves directly annotate the given genes. Tests on simulated and real data indicate that our method is a conservative test for enrichment. Testing TreeHugger on a biological example reveals that it also reduces the redundancy caused by giving high scores to indirect annotations as provided by standard enrichment tests.
This paper presents a tree search algorithm for the three-dimensional container loading problem (3D-CLP). The 3D-CLP is the problem of loading a subset of a given set of rectangular boxes into a rectangular container so that the packing volume is maximized. The method includes two variants: the full-support variant guarantees full support from below for all packed boxes, although this constraint is not taken into account by the nonsupport variant. The guillotine cut constraint is considered by both variants. The method is mainly based on two concepts. On the one hand, the block building approach is generalized. Not only are blocks of identical boxes in the same spatial orientation applied but also blocks of different boxes with small inner gaps. On the other hand, the tree search is carried out in a special fashion called a partition-controlled tree search. This makes the search both efficient and diverse, enabling a sufficient search width as well as a suitable degree of foresight. The approach achieves excellent results for the well-known 3D-CLP instances suggested by Bischoff and Ratcliff [Bischoff, E. E., M. S. W. Ratcliff. 1995. Issues in the development of approaches to container loading. Omega23(4) 377–390] with reasonable computing time.
Two-step mixed integer rounding (MIR) inequalities are valid inequalities derived from a facet of a simple mixed integer set with three variables and one constraint. In this paper we investigate how to effectively use these inequalities as cutting planes for general mixed integer problems. We study the separation problem for single-constraint sets and show that it can be solved in polynomial time when the resulting inequality is required to be sufficiently different from the associated MIR inequalities. We discuss computational issues and present numerical results based on a number of data sets.
We study the multidimensional knapsack problem, present some theoretical and empirical results about its structure, and evaluate different integer linear programming (ILP)-based, metaheuristic, and collaborative approaches for it. We start by considering the distances between optimal solutions to the LP relaxation and the original problem and then introduce a new core concept for the multidimensional knapsack problem (MKP), which we study extensively. The empirical analysis is then used to develop new concepts for solving the MKP using ILP-based and memetic algorithms. Different collaborative combinations of the presented methods are discussed and evaluated. Further computational experiments with longer run times are also performed to compare the solutions of our approaches to the best-known solutions of another so-far leading approach for common MKP benchmark instances. The extensive computational experiments show the effectiveness of the proposed methods, which yield highly competitive results in significantly shorter run times than do previously described approaches.
We present an approximate dynamic programming approach for making ambulance redeployment decisions in an emergency medical service system. The primary decision is where we should redeploy idle ambulances so as to maximize the number of calls reached within a delay threshold. We begin by formulating this problem as a dynamic program. To deal with the high-dimensional and uncountable state space in the dynamic program, we construct approximations to the value function that are parameterized by a small number of parameters. We tune the parameters using simulated cost trajectories of the system. Computational experiments demonstrate the performance of the approach on emergency medical service systems in two metropolitan areas. We report practically significant improvements in performance relative to benchmark static policies.
Hub-and-spoke networks are widely applied in a variety of industries such as transportation, postal delivery, and telecommunications. Although they are designed to exploit economies of scale, hub-and-spoke networks are known to favour congestion, jeopardizing the performance of the entire system. This paper looks at incorporating congestion and capacity decisions in the design stage of such networks. The problem is formulated as a nonlinear mixed-integer program (NMIP) that explicitly minimizes congestion, capacity acquisition, and transportation costs. Congestion at hubs is modeled as the ratio of total flow to surplus capacity by viewing the hub-and-spoke system as a network of M/M/1 queues. To solve the NMIP, we propose a Lagrangean heuristic where the problem is decomposed into an easy subproblem and a more difficult nonlinear subproblem. The nonlinear subproblem is first linearized using piecewise functions and then solved to optimality using a cutting plane method. The Lagrangean lower bound is found using subgradient optimization. The solution from the subproblems is used to find a heuristic solution. Computational results indicate the efficiency of the methodology in providing a sharp bound and in generating high-quality feasible solutions in most cases.
In many branch-and-price algorithms, the column generation pricing problem consists of computing feasible paths in a network. In this paper, we show how, in this context, path-reduced costs can be used to remove some arcs from the underlying network without compromising optimality, and we introduce a bidirectional search technique to compute these reduced costs. This arc elimination method can lead to a substantial speedup of the pricing process and the overall branch-and-price algorithm. Special attention is given to variants of shortest-path problems with resource constraints. Computational results obtained for the vehicle routing problem with time windows show the efficiency of the proposed method.
We develop a solution approach for the fixed-charge network flow (FCNF) problem that produces provably high-quality solutions quickly. The solution approach combines mathematical programming algorithms with heuristic search techniques. To obtain high-quality solutions, it relies on neighborhood search with neighborhoods that involve solving carefully chosen integer programs derived from the arc-based formulation of FCNF. To obtain lower bounds, the linear programming relaxation of the path-based formulation of FCNF is used and strengthened with cuts discovered during the neighborhood search. The solution approach incorporates randomization to diversify the search and learning to intensify the search. Computational experiments demonstrate the efficacy of the proposed approach.
The nodes and arcs of a network configuration replicated over time is a common structure found in many applications, particularly in the area of logistics. A common cost structure for flows in arcs for such problems involves both a fixed and variable cost. Combining the two concepts results in the uncapacitated time-space fixed-charge network flow problem. These problems can be modeled as mixed binary linear programs and can be solved with commercial software. To create these models for uncapacitated arcs requires determining artificial arc capacities that are sufficiently large so that the solution space has not been altered but are small enough that the linear programming relaxations are tight. In this investigation, we present a strategy for determining these artificial arc capacities for any time-space fixed-charge network flow problem. In extensive empirical tests, we provide statistical evidence that the strategy is superior to the usual techniques applied to this class of problem. Many of the most difficult problems were solved in only 5% of the computational time required by standard techniques.
A combinatorial auction is an auction where multiple items are for sale simultaneously to a set of buyers. Furthermore, buyers are allowed to place bids on subsets of the available items. This paper focuses on a combinatorial auction where a bidder can express his preferences by means of a so-called ordered matrix bid. Ordered matrix bids are a bidding language that allows a compact representation of a bidder's preferences and was developed by Day [Day, R. W. 2004. Expressing preferences with price-vector agents in combinatorial auctions. Ph.D. thesis, University of Maryland, College Park]. We give an overview of how a combinatorial auction with matrix bids works. We discuss the relevance of recognizing whether a given matrix bid has properties related to elements of economic theory such as free disposal, subadditivity, submodularity, and the gross substitutes property. We show that verifying whether a matrix bid has these properties can be done in polynomial time by solving one or more shortest-path problems. Finally, we investigate to what extent randomly generated matrix bids satisfy these properties.
Electronic auction markets are economic information systems that facilitate transactions between buyers and sellers. Whereas auction design has traditionally been an analytic process that relies on theory-driven assumptions such as bidders' rationality, bidders often exhibit unknown and variable behaviors. In this paper we present a data-driven adaptive auction mechanism that capitalizes on key properties of electronic auction markets, such as the large transaction volume, access to information, and the ability to dynamically alter the mechanism's design to acquire information about the benefits from different designs and adapt the auction mechanism online in response to actual bidders' behaviors. Our auction mechanism does not require an explicit representation of bidder behavior to infer about design profitability—a key limitation of prior approaches when they address complex auction settings. Our adaptive mechanism can also incorporate prior general knowledge of bidder behavior to enhance the search for effective designs. The data-driven adaptation and the capacity to use prior knowledge render our mechanisms particularly useful when there is uncertainty regarding bidders' behaviors or when bidders' behaviors change over time. Extensive empirical evaluations demonstrate that the adaptive mechanism outperforms any single fixed mechanism design under a variety of settings, including when bidders' strategies evolve in response to the seller's adaptation; our mechanism's performance is also more robust than that of alternatives when prior general information about bidders' behaviors differs from the encountered behaviors.
In this paper, we present two versions of an algorithm for the computation of all nondominated extreme points in the outcome set of a multiobjective integer programme. We define adjacency of these points based on weight space decomposition. Thus, our algorithms generalise the well-known dichotomic scheme to compute the set of nondominated extreme points in the outcome set of a biobjective programme. Both algorithms are illustrated with and numerically tested on instances of the assignment and knapsack problems with three objectives.
In this paper we consider the order-preserving submatrix (OPSM) problem. This problem is known to be NP-hard. Although in recent years some heuristic methods have been presented to find OPSMs, they lack the guarantee of optimality. We present exact solution approaches based on linear mixed 0–1 programming formulations and develop algorithmic enhancements to aid in solvability. Encouraging computational results are reported both for synthetic and real biological data. In addition, we discuss theoretical computational complexity issues related to finding fixed patterns in matrices.
We consider a particular bin packing problem in which some pairs of items may be in conflict and cannot be assigned to the same bin. The problem, denoted as the bin packing problem with conflicts, is of practical and theoretical interest because of its many real-world applications and because it generalizes both the bin packing problem and the vertex coloring problem. We present new lower bounds, upper bounds, and an exact approach, based on a set covering formulation solved through a branch-and-price algorithm. We investigate the behavior of the proposed procedures by means of extensive computational results on benchmark instances from the literature.
Inhomogeneous continuous-time Markov chains play an important role in different application areas. In contrast to homogeneous continuous-time Markov chains, where a large number of numerical analysis techniques are available and have been compared, few results about the performance of numerical techniques in the inhomogeneous case are known. This paper presents a new variant of the uniformization technique, the most efficient approach for homogeneous Markov chains. The new uniformization technique allows for the stable computation of strict bounds for the transient distribution of inhomogeneous continuous-time Markov chains, which is not possible with other numerical techniques that provide only an approximation of the distribution and asymptotic bounds. Furthermore, another variant of uniformization is presented that computes an approximation of the transient distribution and is shown to outperform standard differential equation solvers if transition rates change slowly.
To control possible spills in liquid or gas transporting pipe systems, the systems are usually equipped with shutoff valves. In case of an accidental leak, these valves separate the system into a number of pieces, limiting the spill effect. In this paper, we consider the problem, for a given edge-weighted network representing a pipe system and for a given number of valves, of placing the valves in the network in such a way that the maximum possible spill, i.e., the maximum total weight of a piece, is minimized. We show that the problem is NP-hard even if restricted to any of the following settings: (i) series-parallel graphs, and hence graphs of treewidth two; and (ii) all edge weights equal one. If the network is a simple path, a cycle, or a tree, the problem can be solved in polynomial time. We also give a pseudopolynomial-time algorithm and a fully polynomial-time approximation scheme for networks of bounded treewidth.
In this paper, we develop a revenue management model to jointly make the capacity allocation and overbooking decisions over an airline network. Our approach begins with the dynamic programming formulation of the capacity allocation and overbooking problem and uses an approximation strategy to decompose the dynamic programming formulation by the flight legs. This decomposition idea opens up the possibility of obtaining approximate solutions by concentrating on one flight leg at a time, but the capacity allocation and overbooking problem that takes place over a single flight leg still turns out to be intractable. We use a state aggregation approach to obtain high-quality solutions to the single-leg problem. Overall, our model constructs separable approximations to the value functions, which can be used to make the capacity allocation and overbooking decisions for the whole airline network. Computational experiments indicate that our model performs significantly better than a variety of benchmark strategies from the literature.
Replication of their DNA genomes is a central step in the reproduction of many viruses. Procedures to find replication origins, which are initiation sites of the DNA replication process, are therefore of great importance for controlling the growth and spread of such viruses. Existing computational methods for viral replication origin prediction have mostly been tested within the family of herpesviruses. This paper proposes a new approach by least-squares support vector machines (LS-SVMs) and tests its performance not only on the herpes family but also on a collection of caudoviruses coming from three viral families under the order of caudovirales. The LS-SVM approach provides sensitivities and positive predictive values superior or comparable to those given by the previous methods. When suitably combined with previous methods, the LS-SVM approach further improves the prediction accuracy for the herpesvirus replication origins. Furthermore, by recursive feature elimination, the LS-SVM has also helped find the most significant features of the data sets. The results suggest that the LS-SVMs will be a highly useful addition to the set of computational tools for viral replication origin prediction and illustrate the value of optimization-based computing techniques in biomedical applications.
The massive amount of sensitive survey data about individuals that agencies collect and share through the Internet is causing a great deal of privacy concerns. These concerns may discourage individuals from revealing their sensitive information. Existing data collection techniques have serious downsides in terms of both efficiency and the levels of protection they offer against various realizations of threats. Moreover, they do not provide any flexibility to the users to be able to specify acceptable levels of privacy protection before deciding whether to participate in the surveys. In this paper, we propose a two-pronged privacy protection model corresponding to these two privacy concerns: these are a new efficient anonymity preserving data collection technique and a method to incorporate heterogeneous privacy constraints. Together, they help preserve the privacy of respondents both during and after data collection.
Controlled sequential bifurcation (CSB) is a factor-screening method for discrete-event simulations. It combines a multistage hypothesis testing procedure with the original sequential bifurcation procedure to control both the power for detecting important effects at each bifurcation step and the Type I error for each unimportant factor under heterogeneous variance conditions when a main-effects model applies. This paper improves the CSB procedure in two aspects. First, a new fully sequential hypothesis-testing procedure is introduced that greatly improves the efficiency of CSB. Moreover, this paper proposes CSB-X, a more general CSB procedure that has the same error control for screening main effects that CSB does, even when two-factor interactions are present. The performance of the new method is proven and compared with the original CSB procedure.
This paper studies two mixed-integer linear programming (MILP) formulations for piecewise linear functions considered in Li et al. [Li, H.-L., H.-C. Lu, C.-H. Huang, N.-Z. Hu. 2009. A superior representation method for piecewise linear functions. INFORMS J. Comput.21(2) 314–321]. Although the ideas used to construct one of these formulations are theoretically interesting and could eventually provide a computational advantage, we show that their use in modeling piecewise linear functions yields a poor MILP formulation. We specifically show that neither of the formulations in this paper has a favorable strength property shared by all standard MILP formulations for piecewise linear functions. We also show that both formulations in Li et al. (2009) are significantly outperformed computationally by standard MILP formulations.
The main airline operations consist of schedule planning, fleet assignment, aircraft routing, and crew scheduling. To improve profitability, we present in this paper an integrated fleet assignment model with schedule planning by simultaneously considering optional flight legs to select along with the assignment of aircraft types to all scheduled legs. In addition, we consider itinerary-based demands for multiple fare classes. A polyhedral analysis is conducted of the proposed mixed-integer programming model to tighten its representation via several classes of valid inequalities. Solution approaches are developed by applying Benders' decomposition method to the resulting lifted model, and computational results are presented using real data obtained from a major U.S. airline to demonstrate the efficacy of the proposed procedures.
Unit two-variable-per-inequality (UTVPI) constraints form one of the largest class of integer constraints that are polynomial time solvable (unless P = NP). There is considerable interest in their use for constraint solving, abstract interpretation, spatial database algorithms, and theorem proving. In this paper we develop new incremental algorithms for UTVPI constraint satisfaction and implication checking that require ℴ(m + n log n + p) time and ℴ(n + m + p) space to incrementally check satisfiability of m UTVPI constraints on n variables, and we check the implication of p UTVPI constraints. The algorithms can be straightforwardly extended to create nonincremental implication checking and generation of all (nonredundant) implied constraints, as well as generate minimal unsatisfiable subsets and minimal implicants.
In this paper, we present two bicriteria uncapacitated, multiple allocation p-hub location problems. In the first problem, our first objective is to minimize the total transportation costs of the uncapacitated multiple allocation p-hub median problem with a positive interhub transfer cost. Our second objective is to minimize the total traveling costs between hub points and origin–destination points. In this case, the interhub transfer cost is negligible, and the problem turns into a well-known facility location problem, i.e., the p-median problem. In the second problem, we address the delays occurring with the congestion during service at the hubs. We consider the trade-off between our first objective and a new objective function, which is to minimize the maximum delay at each hub. We propose bicriteria evolutionary algorithms to approximate the efficient frontiers of these problems. We test the performance of our algorithm on Turkish Postal System, Australian Post, and U.S. Civil Aeronautics Board data sets.
We describe computational procedures for solving a wide-ranging class of stochastic programs with chance constraints where the random components of the problem are discretely distributed. Our procedures are based on a combination of Lagrangian relaxation and scenario decomposition, which we solve using a novel variant of Rockafellar and Wets' progressive hedging algorithm [Rockafellar, R. T., R. J.-B. Wets. 1991. Scenarios and policy aggregation in optimization under uncertainty. Math. Oper. Res.16(1) 119–147]. Experiments demonstrate the ability of the proposed algorithm to quickly find near-optimal solutions—where verifiable—to both difficult and very large chance-constrained stochastic programs, both with and without integer decision variables. The algorithm exhibits strong scalability in terms of both run time required and final solution quality on large-scale instances.There is a Video Overview associated with this paper. Click here to view the Video Overview. To save the file, right click and choose “Save Link As” from the menu.
We describe a new solver for convex mixed-integer nonlinear programs (MINLPs) that implements a linearization-based algorithm. The solver is based on an algorithm of Quesada and Grossmann [Quesada, I., I. E. Grossmann. 1992. An LP/NLP based branch-and-bound algorithm for convex MINLP optimization problems. Comput. Chemical Engrg.16(10–11) 937–947] that avoids the complete re-solution of a master mixed-integer linear program (MILP) by adding new linearizations at open nodes of the branch-and-bound tree whenever an integer solution is found. The new solver, FilMINT, combines the MINTO branch-and-cut framework for MILP with filterSQP to solve the nonlinear programs that arise as subproblems in the algorithm. The MINTO framework allows us to easily employ cutting planes, primal heuristics, and other well-known MILP enhancements for MINLPs. We present detailed computational experiments that show the benefit of such advanced MILP techniques. We offer new suggestions for generating and managing linearizations that are shown to be efficient on a wide range of MINLPs. By carefully incorporating and tuning all these enhancements, an effective solver for convex MINLPs is constructed.
We design and implement an intensity-modulated radiation therapy plan generation technology that effectively and efficiently optimizes beam geometry as well as beam intensities. Our approach is based on an existing linear programming-based fluence map optimization model that approximates dose-volume requirements using conditional value-at-risk (C-VaR) constraints. We show how the parameters of the C-VaR constraints can be used to control various metrics of treatment plan quality. Next, we develop an automated search strategy for parameter tuning. Finally, beam angle selection is integrated with fluence map optimization. The beam angle selection scheme employs a bicriteria scoring of beam angle geometries and a selection mechanism to choose from among the set of nondominated geometries. The overall technology is automated and generates several high-quality treatment plans satisfying dose prescription requirements in a single invocation and without human guidance. The technology has been tested on various real-patient cases with uniform success.
The connected facility location (ConFL) problem arises in a number of applications that relate to the design of telecommunication networks as well as data distribution and management problems on networks. It combines features of the uncapacitated facility location problem with the Steiner tree problem and is known to be NP-complete. In this setting, we wish to install a set of facilities on a communication network and assign customers to the installed facilities. In addition, the set of selected facilities needs to be connected by a Steiner tree. In this paper, we propose a dual-based local search heuristic that combines dual ascent and local search, which together yield strong lower and upper bounds to the optimal solution. Our procedure is applied to a slightly more general version of the ConFL problem that embraces a family of four different problems—the Steiner tree-star problem, the general Steiner tree-star problem, the ConFL problem, and the rent-or-buy problem—that combine facility location decisions with connectivity requirements. Consequently, our solution methodology successfully applies to all of them. We discuss a wide range of computational experiments that indicate that our heuristic is a very effective procedure that finds high-quality solutions very rapidly.
We propose a new generic framework for solving combinatorial optimization problems that can be modeled as a set covering problem. The proposed algorithmic framework combines metaheuristics with exact algorithms through a guiding mechanism based on diversification and intensification decisions. After presenting this generic framework, we extensively demonstrate its application to the vehicle routing problem with time windows. We then conduct a thorough computational study on a set of well-known test problems, where we show that the proposed approach not only finds solutions that are very close to the best-known solutions reported in the literature, but also improves them. We finally set up an experimental design to analyze the effects of different parameters used in the proposed algorithm.
In this paper, we model and solve the strategic problem of minimizing the expected loss inflicted by a hostile terrorist organization. An appropriate allocation of certain capability-related, intent-related, vulnerability-related, and consequence-related resources is used to reduce the probabilities of success in the respective attack-related actions and to ameliorate losses in case of a successful attack. We adopt a nested event tree optimization framework and formulate the problem as a specially structured nonconvex factorable program. We develop two branch-and-bound schemes based, respectively, on utilizing a convex nonlinear relaxation and a linear outer approximation, both of which are proven to converge to a global optimal solution. We also design an alternative direct mixed-integer programming model representation for this case, and we investigate a fundamental special-case variant for this scheme that provides a relaxation and affords an optimality gap measure. Several range reduction, partitioning, and branching strategies are proposed, and extensive computational results are presented to study the efficacy of different compositions of these algorithmic ingredients, including comparisons with the commercial software BARON. A sensitivity analysis is also conducted to explore the effect of certain key model parameters.
Methicillin-resistant Staphylococcus aureus (MRSA) is a significant ongoing problem in health care, posing a substantial threat to hospitals and communities as well. Its spread among patients causes many downstream effects, such as a longer length of stay for patients, higher costs for hospitals and insurance companies, and fatalities. An agent-based simulation model is developed to investigate the dynamics of MRSA transmission within a hospital. The simulation model is used to examine the effectiveness of various infection control procedures and explore more specific questions relevant to hospital administrators and policy makers. Simulation experiments are performed to examine the effects of hand-hygiene compliance and efficacy, patient screening, decolonization, patient isolation, and health-care worker-to-patient ratios on the incidence of MRSA transmission and other relevant metrics. Experiments are conducted to investigate the dynamic between the number of colonizations directly attributable to nurses and physicians, including rogue health-care workers who practice poor hygiene. We begin to explore the most likely threats to trigger an outbreak in hospitals that practice high hand-hygiene compliance and additional preventive measures.
On behalf of the Editorial Board, I would like to thank the following people, who acted as Referees during the past year.
Since their introduction, local search algorithms have consistently represented the state of the art in solution techniques for the classical job-shop scheduling problem. This dominance is despite the availability of powerful search and inference techniques for scheduling problems developed by the constraint programming community. In this paper, we introduce a simple hybrid algorithm for job-shop scheduling that leverages both the fast, broad search capabilities of modern tabu search algorithms and the scheduling-specific inference capabilities of constraint programming. The hybrid algorithm significantly improves the performance of a state-of-the-art tabu search algorithm for the job-shop problem and represents the first instance in which a constraint programming algorithm obtains performance competitive with the best local search algorithms. Furthermore, the variability in solution quality obtained by the hybrid is significantly lower than that of pure local search algorithms. Beyond performance demonstration, we perform a series of experiments that provide insights into the roles of the two component algorithms in the overall performance of the hybrid.
This paper presents a parallel branch-and-bound method to address the two-dimensional rectangular guillotine strip cutting problem. Our paper focuses on a parallel branching schema. We present a series of computational experiments to evaluate the strength of the approach. Optimal solutions have been found for some benchmark instances that had unknown solutions until now. For many other instances, we demonstrate that the proposed approach is time effective. The efficiency of the parallel version of the algorithm is compared and the speedup, when increasing the number of processors, is clearly demonstrated with an upper bound calculated by a specialised heuristic procedure.
Minimum linear arrangement is a classical basic combinatorial optimization problem from the 1960s that turns out to be extremely challenging in practice. In particular, for most of its benchmark instances, even the order of magnitude of the optimal solution value is unknown, as testified by the surveys on the problem that contain tables in which the best-known solution value often has one more digit than the best-known lower bound value. In this paper, we propose a linear programming-based approach to compute lower bounds on the optimum. This allows us, for the first time, to show that the best-known solutions are indeed not far from optimal for most of the benchmark instances.
This paper presents a branch-and-price algorithm for the capacitated hub location problem with single assignment, in which Lagrangean relaxation is used to obtain tight lower bounds of the restricted master problem. A lower bound that is valid at any stage of the column generation algorithm is proposed. The process to obtain this valid lower bound is combined with a constrained stabilization method that results in a considerable improvement on the overall efficiency of the solution algorithm. Numerical results on a battery of benchmark instances of up to 200 nodes are reported. These seem to be the largest instances that have been solved to optimality for this problem.
The second generation of optical networks with wavelength division multiplexing (WDM) is based on the notion of two layer networks, where the first layer represents a logical topology defined over the physical topology of optical fibers and the second layer represents multiple traffic requests combined (multiplexed) over the paths established in the logical topology. Because the design of both of these layers is challenging by itself, researchers have mainly focused on solving these problems either independently or in a sequential fashion. In this paper, we look at the WDM optical network design problem with nonbifurcated traffic flows and propose an exact branch-and-price procedure that simultaneously solves logical topology design and traffic routing over the established logical topology. The unique feature of the proposed algorithm is that it works with a row-incomplete mathematical formulation and two types of variables that exponentially grow in number with the problem size. We discuss computational issues related to the use of this procedure and propose two approximate branch-and-price procedures that can be used to obtain lower and upper bounds for this problem. Finally, we present the results of our computational experiments for two design objectives and alternative optical network settings.
We consider the network loading problem (NLP) under a polyhedral uncertainty description of traffic demands. After giving a compact multicommodity flow formulation of the problem, we state a decomposition property obtained from projecting out the flow variables. This property considerably simplifies the resulting polyhedral analysis and computations by doing away with metric inequalities. Then we focus on a specific choice of the uncertainty description, called the “hose model,” which specifies aggregate traffic upper bounds for selected endpoints of the network. We study the polyhedral aspects of the NLP under hose demand uncertainty and use the results as the basis of an efficient branch-and-cut algorithm. The results of extensive computational experiments on well-known network design instances are reported.
Peer-to-peer (P2P) computing already accounts for a large part of the traffic on the Internet, and it is likely to become as ubiquitous as current client/server architectures in next generation information systems. This paper addresses a central problem of P2P systems: the design of an optimal overlay communication network for a set of processes on the Internet. Such a network defines membership to the P2P group and allows for members to disseminate information within the group. The problem, named the membership overlay problem (MOP), can be formulated as a dynamic optimization problem where classical combinatorial optimization techniques must face the further challenge of time-varying input data. This paper proposes an innovative, fully distributed, and asynchronous subgradient optimization algorithm for the Lagrangean relaxation of the MOP, which can run online in fully decentralized P2P systems, and integrates it with a distributed heuristic that can achieve sound hot-start states for fast response to varying network structures.
We present a relaxation-based dynamic programming algorithm for solving resource-constrained shortest-path problems (RCSPPs) in the context of column generation for the dial-a-flight problem. The resulting network formulation and pricing problem require solving RCSPPs on extremely large time-expanded networks having a huge number of local resource constraints, i.e., constraints that apply to small subnetworks. The relaxation-based dynamic programming algorithm alternates between a forward and a backward search. Each search employs bounds derived in the previous search to prune the search space. Between consecutive searches, the relaxation is tightened using a set of critical resources and a set of critical arcs over which these resources are consumed. As a result, a relatively small state space is maintained, and many paths can be pruned while guaranteeing that an optimal path is ultimately found.
We give a dynamic programming solution to the problem of scheduling scenes to minimize the cost of the talent. Starting from a basic dynamic program, we show a number of ways to improve the dynamic programming solution by preprocessing and restricting the search. We show how by considering a bounded version of the problem, and determining lower and upper bounds, we can improve the search. We then show how ordering the scenes from both ends can drastically reduce the search space. The final dynamic programming solution is orders of magnitude faster than competing approaches and finds optimal solutions to larger problems than were considered previously.
A large number of collaborative filtering algorithms have been proposed in the literature as the foundation of automated recommender systems. However, the underlying justification for these algorithms is lacking, and their relative performances are typically domain and data dependent. In this paper, we aim to develop initial understanding of the recommendation model/algorithm validation and selection issues based on the graph topological modeling methodology. By representing the input data in the form of consumer–product interactions as a bipartite graph, the consumer–product graph, we develop bipartite graph topological measures to capture patterns that exist in the input data relevant to the transaction-based recommendation task. We observe the deviations of these topological measures of real-world consumer–product graphs from the expected values for simulated random bipartite graphs. These deviations help explain why certain collaborative filtering algorithms work for particular recommendation data sets. They can also serve as the basis for a comprehensive model selection framework that “recommends” appropriate collaborative filtering algorithms given characteristics of the data set under study. We validate our approach using three real-world recommendation data sets and demonstrate the effectiveness of the proposed bipartite graph topological measures in selection and validation of commonly used heuristic-based recommendation algorithms, the user-based, item-based, and graph-based algorithms.
This paper develops algorithms for finding Coxian generators to phase-type (PH)-majorize a PH-generator T with only real eigenvalues. In the first part of this paper, we investigate matrices S and P satisfying TP = PS and Pe = e. Conditions on T are identified for S to be an ordered Coxian generator and for P to be nonnegative, which consequently implies that SPH-majorizes T. It is shown that every PH-generator with only real eigenvalues is PH-majorized by some Coxian generator. In the second part of this paper, the results on S and P and the conditions on T are used to develop efficient algorithms for Coxianization of PH-generators. Numerical examples are presented for a comparison between the developed algorithms.
In applying matrix-analytic methods to M/G/1-type and tree-like quasi-birth-death (QBD) Markov chains, it is crucial to determine the solution to a (set of) nonlinear matrix equation(s). This is usually done via iterative methods. We consider the highly structured subclass of triangular M/G/1-type and tree-like QBD Markov chains that allows for an efficient direct solution of the matrix equation.
Through recent technical advances, multiple resources can be connected to provide a computing grid for processing computationally intensive applications. We build on an approach, termed sequential grid computing, that takes advantage of idle processing power by routing jobs that require lengthy processing through a sequence of processors. We present two models that solve the static and dynamic versions of the sequential grid scheduling problem for a single job. In the static and dynamic versions, the model maximizes a reward function tied to the probability of completion within service-level agreement parameters. In the dynamic version, the static model is modified to accommodate real-time deviations from the plan. We then extend the static model to accommodate multiple jobs. Extensive computational experiments highlight situations (a) where the models provide improvements over scheduling the job on a single processor and (b) where certain factors affect the quality of solutions obtained.
Large-scale maintenance in industrial plants requires the entire shutdown of production units for disassembly, comprehensive inspection, and renewal. We derive models and algorithms for this so-called turnaround scheduling that include different features such as time-cost trade-off, precedence constraints, external resource units, resource leveling, different working shifts, and risk analysis. We propose a framework for decision support that consists of two phases. The first phase supports the manager in finding a good makespan for the turnaround. It computes an approximate project time-cost trade-off curve together with a stochastic evaluation. Our risk measures are the expected tardiness at time t and the probability of completing the turnaround within time t. In the second phase, we solve the actual scheduling optimization problem for the makespan chosen in the first phase heuristically and compute a detailed schedule that respects all side constraints. Again, we complement this by computing upper bounds for the same two risk measures.Our experimental results show that our methods solve large real-world instances from chemical manufacturing plants quickly and yield an excellent resource utilization. A comparison with solutions of a mixed-integer program on smaller instances proves the high quality of the schedules that our algorithms produce within a few minutes.
We propose a message-passing paradigm for resource allocation problems. This serves to connect ideas from the message-passing literature, which has primarily grown out of the communications, statistical physics, and artificial intelligence communities, with a problem central to operations research. This also provides a new framework for decentralized management that generalizes price-based systems by allowing incentives to vary across activities and consumption levels. We demonstrate that message-based incentives, which are characterized by a new equilibrium concept, lead to system-optimal behavior for convex resource allocation problems yet yield allocations superior to those from price-based incentives for nonconvex problems. We describe a distributed and asynchronous message-passing algorithm for computing equilibrium messages and allocations, and we demonstrate its merits in the context of a network resource allocation problem.
Operating room (OR) scheduling is an important operational problem for most hospitals. In this study, we present a novel two-stage stochastic mixed-integer programming model to minimize total expected operating cost given that scheduling decisions are made before the resolution of uncertainty in surgery durations. We use this model to quantify the benefit of pooling ORs as a shared resource and to illustrate the impact of parallel surgery processing on surgery schedules. Decisions in our model include the number of ORs to open each day, the allocation of surgeries to ORs, the sequence of surgeries within each OR, and the start time for each surgeon. Realistic-sized instances of our model are difficult or impossible to solve with standard stochastic programming techniques. Therefore, we exploit several structural properties of the model to achieve computational advantages. Furthermore, we describe a novel set of widely applicable valid inequalities that make it possible to solve practical instances. Based on our results for different resource usage schemes, we conclude that the impact of parallel surgery processing and the benefit of OR pooling are significant. The latter may lead to total cost reductions between 21% and 59% on average.
The flexibility of time and location as well as the availability of an abundance of both old and new products makes online auctions an important part of people's daily shopping experience. Whereas many bidders rely on variants of the well-documented early or last-minute bidding strategies, neither strategy takes into account the aspect of auction competition: at any point in time, there are hundreds, even thousands, of the same or similar items up for sale, competing for the same bidder. In this paper, we propose a novel automated and data-driven bidding strategy. Our strategy consists of two main components. First, we develop a dynamic, forward-looking model for price in competing auctions. By incorporating dynamic features of the auction process and its competitive environment, our model is capable of accurately predicting an auction's price and outperforming model alternatives such as the generalized additive model, classification and regression trees, or Neural Networks. Then, using the idea of maximizing a bidder's surplus, we build a bidding framework around this model that selects the best auction to bid on and determines the best bid amount. The best auction is given by the one that yields the highest predicted surplus; the best bid amount is given by its predicted auction price. Our approach maximizes expected surplus and balances the probability of winning an auction with its average surplus. In simulations, we compare our automated strategy with early and last-minute bidding and find that our approach extracts 97% and 15% more expected surplus, respectively.
This paper considers the efficient exact computation of the counterpart of the Gittins index for a finite-horizon discrete-state bandit, which measures for each initial state the average productivity, given by the maximum ratio of expected total discounted reward earned to expected total discounted time expended that can be achieved through a number of successive plays stopping by the given horizon. Besides characterizing optimal policies for the finite-horizon one-armed bandit problem, such an index provides a suboptimal heuristic index rule for the intractable finite-horizon multiarmed bandit problem, which represents the natural extension of the Gittins index rule (optimal in the infinite-horizon case). Although such a finite-horizon index was introduced in classic work in the 1950s, investigation of its efficient exact computation has received scant attention. This paper introduces a recursive adaptive-greedy algorithm using only arithmetic operations that computes the index in (pseudo-)polynomial time in the problem parameters (number of project states and time horizon length). In the special case of a project with limited transitions per state, the complexity is either reduced or depends only on the length of the time horizon. The proposed algorithm is benchmarked in a computational study against the conventional calibration method.
This paper is concerned with the computation of the interval availability (proportion of time in a time interval in which the system is up) distribution of a fault-tolerant system modeled by a finite (homogeneous) continuous-time Markov chain (CTMC). General-purpose methods for performing that computation tend to be very expensive when the CTMC and the time interval are large. Based on a previously available method (regenerative transformation) for computing the interval availability complementary distribution, we develop a method called bounding regenerative transformation for the computation of bounds for that measure. Similar to regenerative transformation, bounding regenerative transformation requires the selection of a regenerative state. The method is targeted at a certain class of models, including both exact and bounding failure/repair models of fault-tolerant systems with increasing structure function, with exponential failure and repair time distributions and repair in every state with failed components having failure rates much smaller than repair rates (F/R models), with a “natural” selection for the regenerative state. The method is numerically stable and computes the bounds with well-controlled error. For models in the targeted class and the natural selection for the regenerative state, computational cost should be traded off with bounds tightness through a control parameter. For large models in the class, the version of the method that should have the smallest computational cost should have small computational cost relative to the model size if the value above which the interval availability has to be guaranteed to be is close to 1. In addition, under additional conditions satisfied by F/R models, the bounds obtained with the natural selection for the regenerative state by the version that should have the smallest computational cost seem to be tight for all time intervals or not small time intervals, depending on whether the initial probability distribution of the CTMC is concentrated in the regenerative state or not.CORRECTED VERSION OF RECORD, SEE LAST PAGE OF ARTICLE
The standard approach to process a data envelopment analysis (DEA) data set, and the one in widespread use, consists of solving as many linear programs (LPs) as there are entities. The dimensions of these LPs are determined by the size of the data sets, and they keep their dimensions as each decision-making unit is scored. This approach can be computationally demanding, especially with large data sets. We present an algorithm for DEA based on a two-phase procedure. The first phase identifies the extreme efficient entities, the frame, of the production possibility set. The frame is then used in a second phase to score the rest of the entities. The new procedure applies to any of the four standard DEA returns to scale. It also imparts flexibility to a DEA study because it postpones the decision about orientation, benchmarking measurements, etc., until after the frame has been identified. Extensive computational testing on large data sets verifies and validates the procedure and demonstrates that it is computationally fast.
An analysis is given for an extensive experimental performance evaluation of Skart, an automated sequential batch means procedure for constructing an asymptotically valid confidence interval (CI) on the steady-state mean of a simulation output process. Skart is designed to deliver a CI satisfying user-specified requirements on absolute or relative precision as well as coverage probability. Skart exploits separate adjustments to the half-length of the classical batch means CI so as to account for the effects on the distribution of the underlying Student's t-statistic that arise from skewness (nonnormality) and autocorrelation of the batch means. Skart also delivers a point estimator for the steady-state mean that is approximately free of initialization bias. In an experimental performance evaluation involving a wide range of test processes, Skart compared favorably with other steady-state simulation analysis methods—namely, its predecessors ASAP3, WASSP, and SBatch, as well as ABATCH, LBATCH, the Heidelberger–Welch procedure, and the Law–Carson procedure. Specifically, Skart exhibited competitive sampling efficiency and closer conformance to the given CI coverage probabilities than the other procedures, especially in the most difficult test processes.
The vehicle routing problem (VRP) is a difficult and well-studied combinatorial optimization problem. We develop a parallel algorithm for the VRP that combines a heuristic local search improvement procedure with integer programming. We run our parallel algorithm with as many as 129 processors and are able to quickly find high-quality solutions to standard benchmark problems. We assess the impact of parallelism by analyzing our procedure's performance under a number of different scenarios.
Drug discovery is the process of designing compounds that have desirable properties, such as activity and nontoxicity. Molecule classification techniques are used along with this process to predict the properties of the compounds to expedite their testing. Ideally, the classification rules found should be accurate and reveal novel chemical properties, but current molecule representation techniques lead to less-than-adequate accuracy and knowledge discovery. This work extends the propositionalization approach recently proposed for multirelational data mining in two ways: it generates expressive attributes exhaustively, and it uses randomization to sample a limited set of complex (“deep”) attributes. Our experimental tests show that the procedure is able to generate meaningful and interpretable attributes from molecular structural data, and that these features are effective for classification purposes.
We present a new technique for adaptively choosing the sequence of molecular compounds to test in drug discovery. Beginning with a base compound, we consider the problem of searching for a chemical derivative of the molecule that best treats a given disease. The problem of choosing molecules to test to maximize the expected quality of the best compound discovered may be formulated mathematically as a ranking-and-selection problem in which each molecule is an alternative. We apply a recently developed algorithm, known as the knowledge-gradient algorithm, that uses correlations in our Bayesian prior distribution between the performance of different alternatives (molecules) to dramatically reduce the number of molecular tests required, but it has heavy computational requirements that limit the number of possible alternatives to a few thousand. We develop computational improvements that allow the knowledge-gradient method to consider much larger sets of alternatives, and we demonstrate the method on a problem with 87,120 alternatives.
Many applications, such as telecommunication and commercial video broadcasting systems, computer and networks, and Web mining, require modeling data streams that exhibit context dependency. Context dependency refers to the fact that the statistical distribution of a new sample is heavily conditioned by a set of the most recent samples that precedes it. However, statistical models such as context trees (CTs) that capture context dependency tend to be poorly scalable. This paper proposes a solution to the scalability problem of these models by transforming a data stream into high-level aggregates of clusters instead of modeling the original data stream. Using an information-theoretical approach, we leverage existing clustering techniques for static categorical data sets to capture dynamic data streams based on the CT models. Because the proposed approach can be applied repeatedly on different levels of a clustering hierarchy, it is suitable for predicting trends and detecting anomalies at any aggregate (or detail) level required. Experimental results, including video stream modeling, network intrusion detection, and Monte Carlo simulations, show that the proposed method is efficient in capturing high-level aggregates of large-scale dynamic systems and very effective for trend prediction and anomaly detection.
We present a simple first-order approximation algorithm for the support vector classification problem. Given a pair of linearly separable data sets and ϵ ∈ (0,1), the proposed algorithm computes a separating hyperplane whose margin is within a factor of (1−ϵ) of that of the maximum-margin separating hyperplane. We discuss how our algorithm can be extended to nonlinearly separable and inseparable data sets. The running time of our algorithm is linear in the number of data points and in 1/ϵ. In particular, the number of support vectors computed by the algorithm is bounded above by O(ζ/ϵ) for all sufficiently small ϵ > 0, where ζ is the square of the ratio of the distances between the farthest and closest pairs of points in the two data sets. Furthermore, we establish that our algorithm exhibits linear convergence. Our computational experiments, presented in the online supplement, reveal that the proposed algorithm performs quite well on standard data sets in comparison with other first-order algorithms. We adopt the real number model of computation in our analysis.
The most widely used progress measure for branch-and-bound (B&B) algorithms when solving mixed-integer programs (MIPs) is the MIP gap. We introduce a new progress measure that is often much smoother than the MIP gap. We propose a double exponential smoothing technique to predict the solution time of B&B algorithms and evaluate the prediction method using three MIP solvers. Our computational experiments show that accurate predictions of the solution time are possible, even in the early stages of B&B algorithms.
We provide a branch-and-price algorithm for the bin packing problem with conflicts, a variant of the classical bin packing problem that has major applications in scheduling and resource allocation. The proposed algorithm benefits from a number of special features that greatly contribute to its efficiency. First, we use a branching rule that matches the conflicting constraints, preserving the structure of the subproblems after branching. Second, maximal clique valid inequalities are generated based on the conflicting constraints and are added to the subproblems. The algorithm is tested on a standard set of problems and is compared to a recently proposed approach. Numerical results indicate its efficiency and stability.
We describe families of inequalities for 0–1 mixed-integer programming problems that are obtained by lifting cover and packing inequalities. We show that these inequalities can be separated from single rows of the simplex tableaux of their linear programming relaxations. We present the results of a computational study comparing their performance with that of Gomory mixed-integer cuts on a collection of MIPLIB and randomly generated 0–1 mixed-integer programs. The computational study shows that these cuts yield better results than Gomory mixed-integer cuts.
It is known that a knapsack inequality can be reduced to one having the same solutions but with “minimal” integer coefficients. Although this procedure is not practical because an exponential amount of work may be required to find such minimal equivalent knapsacks, knowledge of minimal equivalent knapsacks can reduce hard knapsacks to trivial ones, as we show for both Todd and Avis knapsacks. In this paper, we show that even with an oracle able to supply minimal equivalent knapsacks at no computational cost, their practical value may not materialize because there are minimal knapsack inequalities with exponential values.
This paper presents a biobjective genetic algorithm (GA) to design reliable two-node connected telecommunication networks. Because the exact calculation of the reliability of a network is NP-hard, network designers have been reluctant to use network reliability as a design criterion; however, it is clearly an important aspect. Herein, three methods of reliability assessment are developed: an exact reliability calculation method using factoring, an efficient Monte Carlo estimation procedure using the sequential construction technique and network reductions, and an upper bound for the all-terminal reliability of networks with arbitrary arc reliabilities. These three methods of reliability assessment are used collectively in a biobjective GA with specialized mutation operators that perturb solutions without disturbing two-node connectivity. Computational experiments show that the proposed approach is tractable and significantly improves upon the results found by single-objective heuristics.
Given a weighted directed network G, we consider the problem of computing kbalanced paths from given source nodes to given destination nodes of G, i.e., k paths such that the difference in cost between the longest path and the shortest path is minimized.Although not yet investigated by the OR scientific community, except for some preliminary theoretical results concerning the special case of acyclic networks, balanced path problems arise in several interesting applications, such as in transportation and in telecommunication settings.In this work, the focus is on the computation of node-disjoint balanced paths in the general case, where the input graph G could have any structure. Starting from some algorithmic ideas proposed for acyclic networks, a general framework based on the color-coding method for computing simple paths is first described. Then the general framework is specialized, and a pool of algorithms is designed that includes both an exact approach as well as alternative heuristics. The algorithms have been tested on a large suite of instances generated from some benchmark telecommunication instances. An additional set of instances, generated from some benchmark crew scheduling instances, has been used to get an idea of the behavior of the algorithms in the context of transportation applications.The obtained computational results are very interesting. For the telecommunication instances, in some cases the exact algorithm produced the optimal solution very rapidly; in the remaining cases, some of the proposed heuristics were able to generate high-quality solutions in a very quick time. As for the crew scheduling instances, which are larger and sometimes appear more difficult than the telecommunication ones, a suitable combination of the proposed color-coding issues allowed us to compute the optimal solutions in very short times.
We present an algorithm for preprocessing a class of stochastic shortest-path problems on networks that have no negative cost cycles, almost surely. Our method adds utility to existing frameworks by significantly reducing input problem sizes and thereby increasing computational tractability. Given random costs with finite lower and upper bounds on each edge, our algorithm removes edges that cannot be in any optimal solution to the deterministic shortest-path problem, for any realization of the random costs. Although this problem is NP-complete, our algorithm efficiently preprocesses nearly all edges in a given network. We provide computational results both on sparse networks from PSPLIB—a well-known project evaluation and review technique library [Kolisch, R., A. Sprecher. 1996. PSPLIB—A project scheduling problem library. Eur. J. Oper. Res.96(1) 205–216]—and dense synthetic ones: on average, less than 0.1% of the edges in the PSPLIB instances and 0.5% of the edges in the dense instances remain unclassified after preprocessing.
We study a reliable facility location problem wherein some facilities are subject to failure from time to time. If a facility fails, customers originally assigned to it have to be reassigned to other (operational) facilities. We formulate this problem as a two-stage stochastic program and then as a nonlinear integer program. Several heuristics that can produce near-optimal solutions are proposed for this NP-hard problem. For the special case where the probability that a facility fails is a constant (independent of the facility), we provide an approximation algorithm with a worst-case bound of 4. The effectiveness of our heuristics is tested by extensive computational studies, which also lead to some managerial insights.
Aposynomial geometric programming problem is composed of a posynomial being minimized in the objective function subject to posynomial constraints. This study proposes an efficient method to solve a posynomial geometric program with separable functions. Power transformations and exponential transformations are utilized to convexify and underestimate posynomial terms. The inverse transformation functions of decision variables generated in the convexification process are approximated by superior piecewise linear functions. The original program therefore can be converted into a convex mixed-integer nonlinear program solvable to obtain a global optimum. Several numerical experiments are presented to investigate the impact of different convexification strategies on the obtained approximate solution and to demonstrate the advantages of the proposed method in terms of both computational efficiency and solution quality.
In many fields, we come across problems where we want to optimize several conflicting objectives simultaneously. To find a good solution for such multiobjective optimization problems, an approximation of the Pareto set is often generated. In this paper, we consider the approximation of higher-dimensional convex Pareto sets using sandwich algorithms.We extend higher-dimensional sandwich algorithms in three different ways. First, we introduce the new concept of adding dummy points to the inner approximation of a Pareto set. By using these dummy points, we can determine accurate inner and outer approximations more efficiently, i.e., using less time-consuming optimizations. Second, we introduce a new method for the calculation of an error measure that is easy to interpret. Third, we show how transforming certain objective functions can improve the results of sandwich algorithms and extend their applicability to certain nonconvex problems.To show the effect of these enhancements, we make a numerical comparison using four test cases, including a four-dimensional case from the field of intensity-modulated radiation therapy. The results of the different cases show that we can achieve an accurate approximation using significantly fewer optimizations by using the enhancements.
Digital microfluidic biochips (DMFBs) are rectangular arrays of electrodes, or cells, that enable precise manipulation of nanoliter-sized droplets of biological fluids and chemical reagents. Because of the safety-critical nature of their applications, biochips must be tested frequently, both off-line (e.g., postmanufacturing) and concurrent with assay execution. Under both scenarios, testing is accomplished by routing one or more test droplets across the chip and recording their arrival at the destination. In this paper, we formalize the DMFB-testing problem under the common objective of completion time minimization, including previously ignored constraints of droplet noninterference. Our contributions include a proof that the general version of the problem is NP-hard, tight lower bounds for both off-line and concurrent testing, optimal and approximation algorithms for off-line testing of commonly used rectangular shaped biochips, as well as a concurrent testing heuristic producing solutions within 23%–34% of the lower bound in experiments conducted on data sets simulating varying percentages of biochip cells occupied by concurrently running assays.
In this paper, we deal with a column generation-based algorithm for the classical cutting stock problem. This algorithm is known to have convergence issues, which are addressed in this paper. Our methods are based on the fact that there are interesting characterizations of the structure of the dual problem, and that a large number of dual solutions are known. First, we describe methods based on the concept of dual cuts, proposed by Valério de Carvalho [Valério de Carvalho, J. M. 2005. Using extra dual cuts to accelerate column generation. INFORMS J. Comput.17(2) 175–182]. We introduce a general framework for deriving cuts, and we describe a new type of dual cut that excludes solutions that are linear combinations of some other known solutions. We also explore new lower and upper bounds for the dual variables. Then we show how the prior knowledge of a good dual solution helps improve the results. It tightens the bounds around the dual values and makes the search converge faster if a solution is sought in its neighborhood first. A set of computational experiments on very hard instances is reported at the end of the paper; the results confirm the effectiveness of the methods proposed.
By means of a model based on a set covering formulation, it is shown how the p-median problem can be solved with just a column generation approach that is embedded in a branch-and-bound framework based on dynamic reliability branching. This method is more than competitive in terms of computational times and size of the instances that have been optimally solved. In particular, problems of a size larger than the largest ones considered in the literature up to now are solved exactly in this paper.
A centralized multiechelon, multiproduct supply chain network is presented in a multiperiod setting with products that show varying demand against price. An important consideration in such complex supply chains is to maintain system performance at high levels for varying demands that may be sensitive to product price. To examine the price-centric behavior of the customers, the concept of price elasticity of demand is addressed. The proposed approach includes many realistic features of typical supply chain systems such as production planning and scheduling, inventory management, transportation delay, transportation cost, and transportation limits. In addition, the proposed system can be extended to meet unsatisfied demand in future periods by backordering. Effects of the elasticity in price demand in production and inventory decisions are also examined. The supply chain model is formulated as a convex mixed-integer nonlinear programming problem. Reformulations are presented to make the problem tractable. The differential equations are reformulated as difference equations, and unbounded derivatives in the nonlinear objective function are handled with an approximation, with guaranteed bounds on the loss of optimality. The approach is illustrated on a multiechelon, multiproduct supply chain network.
Since its appearance in 1947, the primal simplex algorithm has been one of the most popular algorithms for solving linear programs. It is often very efficient when there is very little degeneracy, but it often struggles in the presence of high degeneracy, executing many pivots without improving the objective function value. In this paper, we propose an improved primal simplex algorithm that deals with this issue. This algorithm is based on new theoretical results that shed light on how to reduce the negative impact of degeneracy. In particular, we show that, from a nonoptimal basic solution with p positive-valued variables, there exists a sequence of at most m - p + 1 simplex pivots that guarantee the improvement of the objective value, where m is the number of constraints in the linear program. These pivots can be identified by solving an auxiliary linear program. Finally, we briefly summarize computational results that show the effectiveness of the proposed algorithm on degenerate linear programs.
There has been a recent interest in cutting planes generated from two or more rows of the optimal simplex tableau. One can construct examples of integer programs for which a single cutting plane generated from two rows dominates the entire split closure. Motivated by these theoretical results, we study the effect of adding a family of cutting planes generated from two rows on a set of instances from the MIPLIB library. The conclusion of whether these cuts are competitive with Gomory mixed-integer cuts is very sensitive to the experimental setup. In particular, we consider the issue of reliability versus aggressiveness of the cut generators, an issue that is usually not addressed in the literature.
In this paper, piecewise-linear upper and lower bounds for univariate convex functions are derived that are only based on function value information. These upper and lower bounds can be used to approximate univariate convex functions. Furthermore, new sandwich algorithms are proposed that iteratively add new input data points in a systematic way until a desired accuracy of the approximation is obtained. We show that our new algorithms that use only function value evaluations converge quadratically under certain conditions on the derivatives. Under other conditions, linear convergence can be shown. Some numerical examples that illustrate the usefulness of the algorithm, including a strategic investment model, are given.
In this paper, we study the problem of technical transient gas network optimization, which can be considered a minimum cost flow problem with a nonlinear objective function and additional nonlinear constraints on the network arcs. Applying an implicit box scheme to the isothermal Euler equation, we derive a mixed-integer nonlinear program. This is solved by means of a combination of (i) a novel mixed-integer linear programming approach based on piecewise linearization and (ii) a classical sequential quadratic program applied for given combinatorial constraints. Numerical experiments show that better approximations to the optimal control problem can be obtained by using solutions of the sequential quadratic programming algorithm to improve the mixed-integer linear program. Moreover, iteratively applying these two techniques improves the results even further.
When the leader's objective function of a nonlinear bilevel programming problem is nondifferentiable and the follower's problem of it is nonconvex, the existing algorithms cannot solve the problem. In this paper, a new effective evolutionary algorithm is proposed for this class of nonlinear bilevel programming problems. First, based on the leader's objective function, a new fitness function is proposed that can be easily used to evaluate the quality of different types of potential solutions. Then, based on Latin squares, an efficient crossover operator is constructed that has the ability of local search. Furthermore, a new mutation operator is designed by using some good search directions so that the offspring can approach a global optimal solution quickly. To solve the follower's problem efficiently, we apply some efficient deterministic optimization algorithms in the MATLAB Toolbox to search for its solutions. The asymptotically global convergence of the algorithm is proved. Numerical experiments on 25 test problems show that the proposed algorithm has a better performance than the compared algorithms on most of the test problems and is effective and efficient.
Although significant technical advances have been made in the commercial deployment of grid computing, the pricing and allocation of distributed computing resources remains understudied. We develop a customized clock auction that is able to allocate grid resources and discover separate prices for the different computing resources under the condition that buyers do not know with certainty how much of these resources they will need. The proposed clock auction facilitates the discovery of unit prices for the resources in each time period in a finite-horizon market. Our mechanism exploits the lopsided nature of the grid market where a small number of large-scale jobs are expected to be completed by a large number of heterogeneous, distributed machines. The traditional stopping rule used for clock auctions is not effective in our setting, and therefore we design several adaptations that can be implemented in real time, geared toward ending the auction process quickly while producing a close-to-efficient allocation. Our extensive computations show that our clock-and-offer auction outperforms the traditional clock auction in terms of computational tractability, social welfare, and expected bidder's utility. For large problems of practical interest, we develop a transportation-based heuristic for the NP-complete bid feasibility problem and demonstrate theoretically and computationally that it quickly produces high-quality solutions to the overall problem.
On behalf of the Editorial Board, I would like to thank the following people, who acted as Reviewers during the past year.
We automate the bivariate change-of-variables technique for bivariate continuous random variables with arbitrary distributions. This extends the algorithm for univariate change-of-variables devised by Glen et al. [Glen, A. G., L. M. Leemis, J. H. Drew. 1997. A generalized univariate change-of-variable transformation technique. INFORMS J. Comput.9(3) 288–295]. Our transformation procedure handles one–to–one, k–to–one, and piecewise k–to–one transformations for both independent and dependent random variables. We also present other procedures that operate on bivariate random variables (e.g., calculating correlation and marginal distributions).
The exact distribution of the nth customer's sojourn time in an M/M/s queue with k customers initially present is derived. Algorithms for computing the covariance between sojourn times for an M/M/1 queue with k customers present at time 0 are also developed. Maple computer code is developed for practical application of transient queue analysis for many system measures of performance without regard to traffic intensity (i.e., the system may be unstable with traffic intensity greater than 1).
We present an efficient, reliable, and easy-to-implement algorithm to compute steady-state probabilities for birth-death processes whose upper-tail probabilities decay geometrically or faster. The algorithm can provide any required accuracy and avoids over- and underflow. In addition to steady-state probabilities, the algorithm can compute any performance measure that can be expressed as the expected value of a function of the population size, for nonnegative functions that are bounded by a constant, linear, or quadratic function of population size. The algorithm works with conditional steady-state probabilities, given that the population is in a range that is extended up and down as the algorithm progresses. These conditional probabilities facilitate the derivation of truncation error bounds. We illustrate the application of the algorithm to the Erlang B, C, and A queueing systems.
In this paper, we propose an algorithm to fit heavy-tailed (HT) distribution functions by generalized hyperexponential (GH) distribution functions. A discussion of the steps, usage, and accuracy of the GH algorithm is given. Several examples in this paper show that the proposed method can be applied to fit HT distributions with a completely monotone probability density function (pdf) very well, like the Pareto distribution and the Weibull distribution with the shape parameter less than one, as well as HT distributions whose pdf is not completely monotone, like the lognormal distribution. In addition, we provide an example that shows that the proposed method can be applied to density estimation of real data presenting a heavy tail.
This paper derives a novel, asymptotic statistical test of the Karush–Kuhn–Tucker first-order necessary optimality conditions in random simulation models with multiple responses. This test combines a simple form of the delta method and a generalized version of Wald's statistic. The test is applied to both a toy problem and an (s, S) inventory-optimization problem with a service-level constraint; its numerical results are encouraging.
In this paper, we address the general multiperiod production/inventory problem with nonstationary stochastic demand and supplier lead time under service-level constraints. A replenishment cycle policy is modeled. We propose two hybrid algorithms that blend constraint programming and local search for computing near-optimal policy parameters. Both algorithms rely on a coordinate descent local search strategy; what differs is the way this strategy interacts with the constraint programming solver. These two heuristics are first, compared for small instances against an existing optimal solution method. Second, they are tested and compared with each other in terms of solution quality and run time on a set of larger instances that are intractable for the exact approach. Our numerical experiments show the effectiveness of our methods.
We consider two approaches for solving the classical minimum vertex coloring problem—that is, the problem of coloring the vertices of a graph so that adjacent vertices have different colors and minimizing the number of used colors—namely, constraint programming and column generation. Constraint programming is able to solve very efficiently many of the benchmarks but suffers from a lack of effective bounding methods. On the contrary, column generation provides tight lower bounds by solving the fractional vertex coloring problem exploited in a branch-and-price algorithm, as already proposed in the literature. The column generation approach is here enhanced by using constraint programming to solve the pricing subproblem and to compute heuristic solutions. Moreover, new techniques are introduced to improve the performance of the column generation approach in solving both the linear relaxation and the integer problem. We report extensive computational results applied to the benchmark instances: we are able to prove optimality of 11 new instances and to improve the best-known lower bounds on 17 other instances. Moreover, we extend the solution approaches to a generalization of the problem known as the minimum vertex graph multicoloring problem, where a given number of colors has to be assigned to each vertex.
We consider an inventory routing problem in discrete time where a supplier has to serve a set of customers over a multiperiod horizon. A capacity constraint for the inventory is given for each customer, and the service cannot cause any stockout situation. Two different replenishment policies are considered: the order-up-to-level and the maximum-level policies. A single vehicle with a given capacity is available. The transportation cost is proportional to the distance traveled, whereas the inventory holding cost is proportional to the level of the inventory at the customers and at the supplier. The objective is the minimization of the sum of the inventory and transportation costs. We present a heuristic that combines a tabu search scheme with ad hoc designed mixed-integer programming models. The effectiveness of the heuristic is proved over a set of benchmark instances for which the optimal solution is known.
Recommender systems rely on the opinions of many users to predict the preferences of potential customers. These systems have been broadly used to make quality recommendations to increase sales. However, recommender systems are vulnerable to even small data inputs of malicious information. Inappropriate products can be offered to users by injecting a few unscrupulous “shilling” profiles into the recommender system. This research proposes to identify a cluster of profiles by focusing on “filler” ratings. We examine a number of properties of such profiles, followed by empirical evidence and detailed analysis of various characteristics of the shilling attacks. We then propose a hybrid two-phase procedure for shilling attack detection. First, a multidimensional scaling approach is adopted to identify distinct behaviors that help to detect and secure the recommendation activities. Clustering-based methods are subsequently proposed to discriminate attack users. Experimental studies are conducted to show the effectiveness of the proposed method.
The traveling salesman problem with time windows (TSPTW) is the problem of finding a minimum-cost path visiting a set of cities exactly once, where each city must be visited within a given time window. We present an extended formulation for the problem based on partitioning the time windows into subwindows that we call buckets. We present cutting planes for this formulation that are computationally more effective than the ones known in the literature because they exploit the division of the time windows into buckets. To obtain a good partition of the time windows, we propose an iterative linear programming (LP)-based procedure that may produce buckets of different sizes. The LP relaxation of this formulation yields strong lower bounds for the TSPTW and provides a good starting point for our branch-and-cut algorithm. We also present encouraging computational results on hard test problems from the literature, namely, asymmetric instances arising from a practical scheduling application, as well as randomly generated symmetric instances. In particular, we solve a number of previously unsolved benchmark instances.
We provide a practical methodology for solving the generalized joint replenishment (GJR) problem, based on a mathematical programming approach to approximate dynamic programming. We show how to automatically generate a value function approximation basis built upon piecewise-linear ridge functions by developing and exploiting a theoretical connection with the problem of finding optimal cyclic schedules. We provide a variant of the algorithm that is effective in practice, and we exploit the special structure of the GJR problem to provide a coherent, implementable framework.
We study an exact separation procedure—SEP-MK—for the knapsack set with a single continuous variable XMK. Then, we address the question of whether SEP-MK can be of practical use in tightening mixed-integer programming (MIP) formulations when using standard (floating-point) MIP solvers. To this purpose, we present a separation procedure for MIP problems—SEP-MIPMK—where we derive knapsack sets of the form XMK by aggregating the continuous variables in the mixed knapsack inequalities of the formulation. Then, we use SEP-MK to generate cutting planes. Before the continuous variables are aggregated, the mixed knapsack inequalities are modified through the use of a bound substitution procedure to take into account fixed and variable bounds on the continuous variables. Bound substitution is made according to some heuristic rules, so even if its basic component SEP-MK is “exact,” the overall separation procedure for MIP problems, SEP-MIPMK, is heuristic. We perform a computational study on a wide set of mixed-integer programming instances from the MIPLIB 2003 [Achterberg, T., T. Koch, A. Martin. 2006. Mixed Integer Problem Library (MIPLIB) 2003. Konrad-Zuse-Zentrum für Informationstechnik Berlin, Berlin. http://miplib.zib.de] and Mittelmann [Mittelmann, H. 2010. MILP testcases. http://plato.asu.edu/ftp/milp] benchmark sets. Computational experiments confirm that lifted cover and mixed-integer rounding (MIR) inequalities are effective from a computational viewpoint. Nevertheless, there are several instances where SEP-MIPMK is able to significantly raise the lower bounds given by lifted cover and MIR inequalities.
The disjunctive decomposition (D2) algorithm has emerged as a powerful tool to solve stochastic integer programs. In this paper, we consider two-stage stochastic integer programs with binary first-stage and mixed-binary second-stage decisions and present several computational enhancements to D2. First, we explore the use of a cut generation problem restricted to a subspace of the variables, which yields significant computational savings. Then, we examine problems with generalized upper bound constraints in the second stage and exploit this structure to generate cuts. We establish convergence of D2 variants. We present computational results on a new stochastic scheduling problem with an uncertain number of jobs motivated by companies in industries such as consulting and defense contracting, where these companies bid on future contracts but may or may not win the bid. The enhancements reduced computation time on average by 45% on a set of test problems.
Route planning in large-scale time-dependent road networks is an important practical application of the shortest-path problem that greatly benefits from speedup techniques. In this paper, we extend a two-level hierarchical approach for point-to-point shortest-path computations to the time-dependent case. This method, also known as core routing in the literature for static graphs, consists of the selection of a small subnetwork where most of the computations can be carried out, thus reducing the search space. We combine this approach with bidirectional goal-directed search to obtain an algorithm capable of finding shortest paths in a matter of milliseconds on continental-sized networks. Moreover, we tackle the dynamic scenario where the piecewise linear functions that we use to model time-dependent arc costs are not fixed but can have their coefficients updated requiring only a small computational effort.
We apply the level-3 reformulation-linearization technique (RLT3) to the quadratic assignment problem (QAP). We then present our experience in calculating lower bounds using an essentially new algorithm based on this RLT3 formulation. Our method is not guaranteed to calculate the RLT3 lower bound exactly, but it approximates this lower bound very closely and reaches it in some instances. For Nugent problem instances up to size 24, our RLT3-based lower bound calculation solves these problem instances exactly or serves to verify the optimal value. Calculating lower bounds for problem sizes larger than size 27 still presents a challenge because of the large amount of memory needed to implement the RLT3 formulation. Our presentation emphasizes the steps taken to significantly conserve memory by using the numerous problem symmetries in the RLT3 formulation of the QAP. We implemented this RLT3-based bound calculation in a branch-and-bound algorithm. Experimental results project significant runtime improvement over all other published QAP branch-and-bound solvers.
Metaheuristic approaches based on the neighborhood search escape local optimality by applying predefined rules and constraints, such as tabu restrictions (in tabu search), acceptance criteria (in simulated annealing), and shaking (in variable neighborhood search). We propose a general approach that attempts to learn (off-line) the guiding constraints that, when applied online, will result in effective escape directions from local optima. Given a class of problems, the learning process is performed off-line, and the results are applied to constrained neighborhood searches to guide the solution process out of local optimality. Computational results on the constrained task allocation problem show that adding these guiding constraints to a simple tabu search improves the quality of the solutions found, making the overall method competitive with state-of-the-art methods for this class of problems. We also present a second set of tests on the matrix bandwidth minimization problem.
This paper presents an optimal constraint programming approach for the open-shop scheduling problem, which integrates recent constraint propagation and branching techniques with new upper bound heuristics. Randomized restart policies combined with nogood recording allow us to search diversification and learning from restarts. This approach is compared with the best-known metaheuristics and exact algorithms, and it shows better results on a wide range of benchmark instances.
The many-to-many stable matching problem (MM), defined in the context of a job market, asks for an assignment of workers to firms satisfying the quota of each agent and being stable, pairwise or setwise, with respect to given preference lists or relations. In this paper, we propose a time-optimal algorithm that identifies all stable worker–firm pairs and all stable assignments under pairwise stability, individual preferences, and the max-min criterion. We revisit the poset graph of rotations to obtain an optimal algorithm for enumerating all solutions to the MM and an improved algorithm finding the minimum-weight one. Furthermore, we establish the applicability of all aforementioned algorithms under more complex preference and stability criteria. In a constraint programming context, we introduce a constraint that models the MM and an encoding of the MM as a constraint satisfaction problem. Finally, we provide a series of computational results, including the case where side constraints are imposed.
We study the problem of (re)designing the regional network by which cadaveric livers are allocated. Whereas prior research focused mainly on maximizing a measure of efficiency of the network that was based on aggregate patient survival, we explicitly account for the trade-off between efficiency and a measure of geographical equity in the allocation process. To this end, we extend earlier optimization models to incorporate both objectives and develop an exact branch-and-price approach to solve this problem, generalizing a solution approach studied for the case where only efficiency is taken into account. In addition, we propose an effective solution algorithm that approximates the (generally nonconcave) frontier of Pareto-efficient solutions with respect to the two objectives by simultaneously generating and successively improving upper and lower bounds on this frontier. We implement and test our approach on observed data and show that solutions significantly dominating the current configuration in both efficiency and equity can be found. Of course, other subjective criteria are needed to choose among the different Pareto-efficient candidate solutions.
Aphylogeny is an unrooted binary tree that represents the evolutionary relationships of a set of n species. Phylogenies find applications in several scientific areas ranging from medical research, to drug discovery, to epidemiology, to systematics, and to population dynamics. In such applications, the available information is usually restricted to the leaves of a phylogeny and is represented by molecular data extracted from the analyzed species, such as DNA, RNA, amino acid, or codon fragments. On the contrary, the information about the phylogeny itself is generally missing and is determined by solving an optimization problem, called the phylogeny estimation problem (PEP), whose versions depend on the criterion used to select a phylogeny from among plausible alternatives. In this paper, we investigate a recent version of the PEP, called the balanced minimum evolution problem (BMEP). We present a mixed-integer linear programming model to exactly solve instances of the BMEP and develop branching rules and families of valid inequalities to further strengthen the model. Our results give perspective on the mathematics of the BMEP and suggest new directions on the development of future efficient exact approaches to solutions of the problem.
We propose C-NORTA, an exact algorithm to generate random variates from the tail of a bivariate NORTA random vector. (A NORTA random vector is specified by a pair of marginals and a rank or product–moment correlation, and it is sampled using the popular NORmal-To-Anything procedure.) We first demonstrate that a rejection-based adaptation of NORTA on such constrained random vector generation problems may often be fundamentally intractable. We then develop the C-NORTA algorithm, relying on strategic conditioning of the NORTA vector, followed by efficient approximation and acceptance/rejection steps. We show that, in a certain precise asymptotic sense, the sampling efficiency of C-NORTA is exponentially larger than what is achievable through a naïve application of NORTA. Furthermore, for at least a certain class of problems, we show that the acceptance probability within C-NORTA decays only linearly with respect to a defined rarity parameter. The corresponding decay rate achievable through a naïve adaptation of NORTA is exponential. We provide directives for efficient implementation.
In recent years, many algorithms have been proposed to extract process models from process execution logs. The process models describe the ordering relationships between tasks in a process in terms of standard constructs like sequence, parallel, choice, and loop. Most algorithms assume that each trace in a log represents a correct execution sequence based on a model. In practice, logs are often noisy, and algorithms designed for correct logs are not able to handle noisy logs. In this paper we share our key insights from a study of noise in process logs both real and synthetic. We found that all process logs can be explained by a block-structured model with two special self-loop and optional structures, making it trivial to build a fully accurate process model for any given log, even one with inaccurate data or noise present in it. However, such a model suffers from low quality. By controlling the use of self-loop and optional structures of tasks and blocks of tasks, we can balance the quality and accuracy trade-off to derive high-quality process models that explain a given percentage of traces in the log. Finally, new quality metrics and a novel quality-based algorithm for model extraction from noisy logs are described. The results of the experiments with the algorithm on real and synthetic data are reported and analyzed at length.
The 𝒩𝒫-hard maximum monomial agreement problem consists of finding a single logical conjunction that is most consistent with or “best fits” a weighted data set of “positive” and “negative” binary vectors. Computing weighted voting classifiers using boosting methods involves a maximum agreement subproblem at each iteration, although such subproblems are typically solved in practice by heuristic methods. Here, we describe an exact branch-and-bound method for maximum agreement over Boolean monomials, improving on the earlier work of Goldberg and Shan [Goldberg, N., C. Shan. 2007. Boosting optimal logical patterns. Proc. 7th SIAM Internat. Conf. Data Mining, SIAM, Philadelphia, 228–236]. Specifically, we develop a tighter upper bounding function and an improved branching procedure that exploits knowledge of the bound and the particular data set, while having a lower branching factor. Experimental results show that the new method is able to solve larger problem instances and runs faster within a linear programming boosting procedure applied to medium-sized data sets from the UCI Machine Learning Repository. The new algorithm also runs much faster than applying a commercial mixed-integer programming solver, which uses linear programming relaxation-based bounds, to an integer linear programming formulation of the problem.
This paper introduces a hybrid algorithm for the dynamic dial-a-ride problem in which service requests arrive in real time. The hybrid algorithm combines an exact constraint programming algorithm and a tabu search heuristic. An important component of the tabu search heuristic consists of three scheduling procedures that are executed sequentially. Experiments show that the constraint programming algorithm is sometimes able to accept or reject incoming requests, and that the hybrid method outperforms each of the two algorithms when they are executed alone.
The traveling salesman problem with time windows (TSPTW) is the problem of finding in a weighted digraph a least-cost tour starting from a selected vertex, visiting each vertex of the graph exactly once according to a given time window, and returning to the starting vertex. This n𝒫-hard problem arises in routing and scheduling applications. This paper introduces a new tour relaxation, called ngL-tour, to compute a valid lower bound on the TSPTW obtained as the cost of a near-optimal dual solution of a problem that seeks a minimum-weight convex combination of nonnecessarily elementary tours. This problem is solved by column generation. The optimal integer TSPTW solution is computed with a dynamic programming algorithm that uses bounding functions based on different tour relaxations and the dual solution obtained. An extensive computational analysis on basically all TSPTW instances (involving up to 233 vertices) from the literature is reported. The results show that the proposed algorithm solves all but one instance and outperforms all exact methods published in the literature so far.
We address the discovery of periodic patterns in sequence data. Building on prior work in this area, we present definitions and new methods for characterizing and identifying four types of periodic patterns. A unifying concept across the different types of periodic patterns we consider is the use of statistical variance to define periodicity. This lends itself to efficient variance-reduction algorithms for identifying periodic patterns. We motivate and test our approach using both extensive simulated sequences and real sequence data from online clickstream data.
We address an optimization problem that requires deciding the location of a set of facilities, the allocation of customers to those facilities under capacity constraints, and the allocation of customers to trucks at those facilities under truck travel-distance constraints. We present a hybrid approach that combines integer and constraint programming using logic-based Benders decomposition. Computational experiments demonstrate that the Benders model is able to find and prove optimal solutions up to three orders-of-magnitude faster than an existing integer programming approach; it also finds better feasible solutions in less time when compared with an existing tabu search algorithm.
The multidimensional knapsack problem (MKP) is a well-known, strongly NP-hard problem and one of the most challenging problems in the class of the knapsack problems. In the last few years, it has been a favorite playground for metaheuristics, but very few contributions have appeared on exact methods. In this paper we introduce an exact approach based on the optimal solution of subproblems limited to a subset of variables. Each subproblem is faced through a recursive variable-fixing process that continues until the number of variables decreases below a given threshold (restricted core problem). The solution space of the restricted core problem is split into subspaces, each containing solutions of a given cardinality. Each subspace is then explored with a branch-and-bound algorithm. Pruning conditions are introduced to improve the efficiency of the branch-and-bound routine. In all the tested instances, the proposed method was shown to be, on average, more efficient than the recent branch-and-bound method proposed by Vimont et al. [Vimont, Y., S. Boussier, M. Vasquez. 2008. Reduced costs propagation in an efficient implicit enumeration for the 0-1 multidimensional knapsack problem. J. Combin. Optim.15(2) 165–178] and CPLEX 10. We were able to improve the best-known solutions for some of the largest and most difficult instances of the OR-LIBRARY data set [Chu, P. C., J. E. Beasley. 1998. A genetic algorithm for the multidimensional knapsack problem. J. Heuristics4(1) 63–86].
Our work addresses internal information breaches that emanate from organizational workflows. Information breaches are particularly piquant in organizational workflows, as the underlying tasks constitute natural points where private information on individuals is accessed to execute the workflows. Our work builds on and extends the widely used role-based access controls by considering processwide security considerations to both optimize the efficiency of workflow staffing and minimize data exposure in complex workflows. We employ a Jackson queueing network modeling framework, which allows both predictable and stochastic variability as well as varied employee skill sets. This framework enables the modeling of internal security threats that emanate from cross-task and cross-personnel assignments and the development of optimal staffing strategies that meet security requirements at minimum operational costs. Our detailed implementation analysis reveals that the model developed is not demanding in terms of required parameters and that the proposed approach is practical and adaptable to evolving business, regulatory, and workforce conditions. Our model is applicable to any digital transformation that involves confidential data sequences that carry security vulnerability, as is often the case in many settings such as health care, online banking, electronic payment systems, and interorganizational data interchange.
We present a new exact algorithm for the assembly line balancing problem. The algorithm finds and verifies the optimal solution for every problem in the combined benchmarks of Hoffmann, Talbot, and Scholl in less than one-half second per problem, on average, including one problem that has remained open for over 10 years. The previous best-known algorithm is able to solve 257 of the 269 benchmarks. The new algorithm is based on a branch-and-bound method that uses memory to eliminate redundant subproblems.
This paper studies the minimum-energy broadcasting problem (MEBP) in wireless sensor networks. The aim of the MEBP is to determine the power assignment of each node in a wireless sensor network such that a specified source node can broadcast messages to each of the other nodes and the total energy consumption is minimized. We first present a new formulation involving an exponential number of constraints for the broadcasting requirement. We then prove that under a mild condition, these constraints for the broadcasting requirement are facet defining. Directly using the proposed formulation, we further present a new branch-and-cut (B&C) solution approach to optimally solve the MEBP. We propose three ways to identify violated cuts at each node in the enumeration tree. Finally, we test the proposed B&C approach on 1,000 randomly generated instances with different properties and compare it with other alternative methods in the literature. Computational results demonstrate the effectiveness and efficiency of our approach using the proposed formulation for instances with up to 100 nodes.
This paper presents an abstract data type (ADT) for producing higher-dimensional rectilinear packings using constructive methods, the Skyline ADT. Furthermore, a novel method and several approaches from the literature are presented in the form of concrete implementations of the presented ADT. Formal definitions of two three-dimensional packing problems are given, the concept of gaps is explained, and the polynomial growth worst-case behaviour of gaps is shown. The complexity of both the best-fit algorithm and implementations of the ADT are presented, and comparative runtime speeds are analysed over a range of data sets from the literature.
Optimization of simulated systems is the goal of many methods, but most methods assume known environments. We, however, develop a “robust” methodology that accounts for uncertain environments. Our methodology uses Taguchi's view of the uncertain world but replaces his statistical techniques by design and analysis of simulation experiments based on Kriging (Gaussian process model); moreover, we use bootstrapping to quantify the variability in the estimated Kriging metamodels. In addition, we combine Kriging with nonlinear programming, and we estimate the Pareto frontier. We illustrate the resulting methodology through economic order quantity (EOQ) inventory models. Our results suggest that robust optimization requires order quantities that differ from the classic EOQ. We also compare our results with results we previously obtained using response surface methodology instead of Kriging.
We consider the problem of deciding whether a given directed graph can be vertex partitioned into two acyclic subgraphs. Applications of this problem include testing rationality of collective consumption behavior, a subject in microeconomics. We prove that the problem is NP-complete even for oriented graphs and argue that the existence of a constant-factor approximation algorithm is unlikely for an optimization version that maximizes the number of vertices that can be colored using two colors while avoiding monochromatic cycles. We present three exact algorithms—namely, an integer-programming algorithm based on cycle identification, a backtracking algorithm, and a branch-and-check algorithm. We compare these three algorithms both on real-life instances and on randomly generated graphs. We find that for the latter set of graphs, every algorithm solves instances of considerable size within a few seconds; however, the CPU time of the integer-programming algorithm increases with the number of vertices in the graph more clearly than the CPU time of the two other procedures. For real-life instances, the integer-programming algorithm solves the largest instance in about a half hour, whereas the branch-and-check algorithm takes approximately 10 minutes and the backtracking algorithm less than 5 minutes. Finally, for every algorithm, we also study empirically the transition from a high to a low probability of a YES answer as a function of the number of arcs divided by the number of vertices.
The purpose of software partitioning is to assign code segments of a given computer program to a range of execution locations such as general-purpose processors or specialist hardware components. These execution locations differ in speed, communication characteristics, and size. In particular, hardware components offering high speed tend to accommodate only few code segments. The goal of software partitioning is to find an assignment of code segments to execution locations that minimizes the overall program run time and respects the size constraints. In this paper we demonstrate that an additional speedup is obtained if we allow code segments to be instantiated on more than one location. We further show that the program run time not only depends on the execution frequency of the code segments but also on their execution order if there are multiply instantiated code segments. Unlike frequency information, however, sequence information is not available at the time when the software partition is selected. This motivates us to formulate the software-partitioning problem as a robust optimization problem with decision-dependent uncertainty. We transform this problem to a mixed-integer linear program of moderate size and report on promising numerical results.
Beam-on time is an important measure of the delivery efficiency in intensity-modulated radiation therapy (IMRT). Traditionally, minimizing beam-on time has been postponed until the leaf sequencing stage, where the treatment plan quality is already determined and fixed. However, there is a trade-off between the beam-on time and the treatment plan quality. The aim of this study is to incorporate the beam-on time into the treatment plan optimization stage using a direct aperture optimization approach that allows for explicitly quantifying the trade-off. The proposed approach can provide clinicians with valuable information for each patient case so that they can design clinically attractive yet efficient treatment plans. Using the special structure of the problem, we propose an exact solution approach that sequentially characterizes segments of the Pareto-efficient frontier. In addition, an approximate solution technique that is applicable to more classes of evaluation criteria is developed. Our approximate technique is tested on clinical cancer cases, and its performance is compared with the general approximation techniques that are available for convex bicriteria optimization problems. The results of our experiments validate that our approach can achieve a more accurate representation of the Pareto-efficient frontier with less computational effort.
Given a graph G = (N, E), the covering salesman problem (CSP) is to identify the minimum length tour “covering” all the nodes. More specifically, it seeks the minimum-length tour visiting a subset of the nodes in N such that each node i not on the tour is within a predetermined distance di of a node on the tour. In this paper, we define and develop a generalized version of the CSP, and we refer to it as the generalized covering salesman problem (GCSP). Here, each node i needs to be covered at least ki times, and there is a cost associated with visiting each node. We seek a minimum-cost tour such that each node i is covered at least ki times by the tour. We define three variants of the GCSP. In the first case, each node can be visited by the tour at most once. In the second case, visiting a node i more than once is possible, but an overnight stay is not allowed (i.e., to revisit a node i, the tour has to visit another node before it can return to i). Finally, in the third case, the tour can visit each node more than once consecutively. In this paper, we develop two local search heuristics to find high-quality solutions to the three GCSP variants. To test the proposed algorithms, we generated data sets based on traveling salesman problem library instances. Because the CSP and the generalized traveling salesman problem are special cases of the GCSP, we tested our heuristics on both of those problems as well. Overall, the results show that our proposed heuristics find high-quality solutions very rapidly.
This paper describes a generic branch-and-cut algorithm applicable to the solution of multiobjective optimization problems for which a lower bound can be defined as a polynomially solvable multiobjective problem. The algorithm closely follows standard branch and cut except for the definition of the lower and upper bounds and some optional speed-up mechanisms. It is applied to a routing problem called the multilabel traveling salesman problem, a variant of the traveling salesman problem in which labels are attributed to the edges. The goal is to find a Hamiltonian cycle that minimizes the tour length and the number of labels in the tour. Implementations of the generic multiobjective branch-and-cut algorithm and speed-up mechanisms are described. Computational experiments are conducted, and the method is compared to the classical ϵ-constraint method.
We consider an uncapacitated stochastic vehicle routing problem in which vehicle depot locations are fixed, and client locations in a service region are unknown but are assumed to be independent and identically distributed samples from a given probability density function. We present an algorithm for partitioning the service region into subregions so as to balance the workloads of all vehicles when the service region is simply connected and point-to-point distances follow some “natural” metric, such as any Lp norm. This algorithm can also be applied to load balancing of other combinatorial structures, such as minimum spanning trees and minimum matchings.
Consider a network 𝒩 =(N, A) and associate with each arc e ∈ A a fixed cost ce for using arc e, an interval [le, ue] (le, ue ∈ ℤ) specifying the range of allowable resource consumption quantities along arc e, and a per-unit cost  for resource consumed along e. Furthermore, for each node n ∈ N, let Un ∈ ℤ be the maximum amount of resource consumption a path can accumulate before visiting node n. Given a source node ns ∈ N and sink node nt ∈ N, the fixed-charge shortest-path problem (FCSPP) seeks to find a minimum-cost-feasible path from ns to nt. When resource consumption is simply additive, the resource-constrained shortest-path problem (RCSPP) is a special case of FCSPP. We develop a new dynamic programming algorithm for FCSPP. The algorithm uses solutions from labeling and dominance techniques for standard RCSPPs on slightly modified problems, and it combines these solutions by exploiting the structure provided by certain classes of knapsack problems to efficiently construct an optimal solution to FCSPP. Computational experiments demonstrate that our algorithm is often several orders of magnitude faster than naive dynamic programming procedures.
This paper presents a simple branch-and-bound method based on Lagrangean relaxation and subgradient optimization for solving large instances of the capacitated facility location problem (CFLP) to optimality. To guess a primal solution to the Lagrangean dual, we average solutions to the Lagrangean subproblem. Branching decisions are then based on this estimated (fractional) primal solution. Extensive numerical results reveal that the method is much faster and more robust than other state-of-the-art methods for solving the CFLP exactly.
The explosive growth in the variety and size of social networks has focused attention on searching these networks for useful structures. Like the Internet or the telephone network, the ability to efficiently search large social networks will play an important role in the extent of their use by individuals and organizations alike. However, unlike these domains, search on social networks is likely to involve measures that require a set of individuals to collectively satisfy some skill requirement or be tightly related to each other via some underlying social property of interest.The aim of this paper is to highlight—and demonstrate via specific examples—the need for algorithmic results for some fundamental set-based notions on which search in social networks is expected to be prevalent. To this end, we argue that the concepts of an influential set and a central set that highlight, respectively, the specific role and the specific location of a set are likely to be useful in practice. We formulate two specific search problems: the elite group problem (EGP) and the portal problem (PP), that represent these two concepts and provide a variety of algorithmic results. We first demonstrate the relevance of EGP and PP across a variety of social networks reported in the literature. For simple networks (e.g., structured trees and bipartite graphs, cycles, paths), we show that an optimal solution to both EGP and PP is easy to obtain. Next, we show that EGP is polynomially solvable on a general graph, whereas PP is strongly NP-hard. Motivated by practical considerations, we also discuss (i) a size-constrained variant of EGP together with its penalty-based relaxation and (ii) the solution of PP on balanced and full d-trees and general trees.
Absolute precision stopping rules are often used to determine the length of sequential experiments to estimate confidence intervals for simulated performance measures. Much is known about the asymptotic behavior of such procedures. In this paper, we introduce coverage contours to quantify the trade-offs in interval coverage, stopping times, and precision for finite-sample experiments using absolute precision rules. We use these contours to evaluate the coverage of a basic absolute precision stopping rule, and we show that this rule will lead to a bias in coverage even if all of the assumptions supporting the procedure are true. We define optimal stopping rules that deliver nominal coverage with the smallest expected number of observations. Contrary to previous asymptotic results that suggest decreasing the precision of the rule to approach nominal coverage in the limit, we find that it is optimal to increase the confidence coefficient used in the stopping rule, thus obtaining nominal coverage in a finite-sample experiment. If the simulation data are independent and identically normally distributed, we can calculate coverage contours analytically and find a stopping rule that is insensitive to the variance of the data while delivering at least nominal coverage for any precision value.
The performance of a maximum-period multiple recursive generator (MRG) depends on the choices of the recurrence order k, the prime modulus p, and the multipliers used. For a maximum-period MRG, a large-order k not only means a large period length (i.e., pk − 1) but, more importantly, also guarantees the equidistribution property in high dimensions (i.e., up to k dimensions), a desirable feature for a good random-number generator. As to generating efficiency, in addition to the multipliers, some special choices of the prime modulus p can significantly speed up the generation of pseudo-random numbers by replacing the expensive modulo operation with efficient logical operations. To construct efficient maximum-period MRGs of a large order, we consider the prime modulus p = 231 − 1 and, via extensive computer search, find two large values of k, 7,499 and 20,897, for which pk − 1 can be completely factorized. The successful search is achieved with the help of some results in number theory as well as some modern factorization methods. A general class of MRGs is introduced, which includes several existing classes of efficient generators. With the factorization results, we are able to identify via computer search within this class many portable and efficient maximum-period MRGs of order 7,499 or 20,897 with prime modulus 231 − 1 and multipliers of powers-of-two decomposition. These MRGs all pass the stringent TestU01 test suite empirically.
This paper addresses the issue of data quality management in information systems within an enterprise. Motivated by legislative mandates such as the Sarbanes–Oxley Act of 2002 on the reliability and integrity of the data and the enterprise systems from which the data are produced, we propose a process-based modeling framework to assess the impact of data errors in the business process information flow and the resulting data quality metrics. This framework is then integrated with a business control framework in which the placement and effectiveness of control procedures alter the propagation of errors and, ultimately, the quality of the data in the business process. This integrated framework enables mathematical formulations of managerial problems that lead to effective data quality control strategies. We develop a two-stage multiple-choice knapsack model as a special case, and we illustrate the model and analysis through a revenue realization process.
We address the problem of modeling energy resource allocation, including dispatch, storage, and the long-term investments in new technologies, capturing different sources of uncertainty such as energy from wind, demands, prices, and rainfall. We also wish to model long-term investment decisions in the presence of uncertainty. Accurately modeling the value of all investments, such as wind turbines and solar panels, requires handling fine-grained temporal variability and uncertainty in wind and solar in the presence of storage. We propose a modeling and algorithmic strategy based on the framework of approximate dynamic programming (ADP) that can model these problems at hourly time increments over an entire year or several decades. We demonstrate the methodology using both spatially aggregate and disaggregate representations of energy supply and demand. This paper describes the initial proof of concept experiments for an ADP-based model called SMART; we describe the modeling and algorithmic strategy and provide comparisons against a deterministic benchmark as well as initial experiments on stochastic data sets.
On behalf of the Editorial Board, I would like to thank the following people, who acted as Reviewers during the past year.
A network simplex algorithm is described for the minimum-cost network flow problem on a generalized network, with the additional constraint that there exist sets of arcs that must carry equal amounts of flow. This problem can be modeled as a linear programming problem and solved using the standard simplex algorithm. However, because of the structure of the problem, more efficient algorithms are possible that solve the problem by operating directly on the network itself. One such algorithm is described that leads to improved asymptotic performance per iteration over the standard simplex algorithm, as long as the number of side constraints is small relative to the size of the network. Computational results are given comparing this algorithm to CPLEX's primal simplex solver on randomly generated graphs.
Given a graph with nonnegative edge weights and node pairs Q, we study the problem of constructing a minimum weight set of edges so that the induced subgraph contains at least K edge-disjoint paths containing at most L edges between each pair in Q. Using the layered representation introduced by Gouveia [Gouveia, L. 1998. Using variable redefinition for computing lower bounds for minimum spanning and Steiner trees with hop constraints. INFORMS J. Comput.10(2) 180–188], we present a formulation for the problem valid for any K, L ≥ 1. We use a Benders decomposition method to efficiently handle the large number of variables and constraints. We show that our Benders cuts contain constraints used in previous studies to formulate the problem for L = 2, 3, 4, as well as new inequalities when L ≥ 5. Whereas some recent works on Benders decomposition study the impact of the normalization constraint in the dual subproblem, we focus here on when to generate the Benders cuts. We present a thorough computational study of various branch-and-cut algorithms on a large set of instances including the real-based instances from SNDlib. Our best branch-and-cut algorithm combined with an efficient heuristic is able to solve the instances significantly faster than CPLEX 12 on the extended formulation.
In this paper, we propose a “multistart-type” algorithm for solving the max-k-cut problem. Central to our algorithm is an auxiliary function we propose. We formulate the max-k-cut problem as an explicit mathematical form, which allows us to use an easy implementable local search. The construction of the auxiliary function requires a local maximizer of the max-k-cut problem. If the best local maximizer obtained is used in the construction of the auxiliary function, then the local maximization of the auxiliary function leads to a better maximizer of the max-k-cut problem. This proves to be a good strategy to escape from the current local optima and to search a broader solution space. Indeed, we have shown, both numerically and theoretically, that the maximization of the auxiliary function by the local search method can escape successfully from previously converged discrete local maximizers by taking increasing values of a parameter. Computational results on many test instances with different sizes and densities show that the proposed algorithm is efficient and stable to find approximate global solutions for the max-k-cut problems. Although we have presented results for k ≥ 2, the robustness of our algorithm is shown for k = 2 by comparisons with a number of recent methods. A number of theoretical results are also presented, which justify the design of our algorithm.
The double traveling salesman problem with multiple stacks is a variant of the pickup and delivery traveling salesman problem in which all pickups must be completed before any delivery. In addition, items can be loaded on multiple stacks in the vehicle, and each stack must obey the last-in-first-out policy. The problem consists of finding the shortest Hamiltonian cycles covering all pickup and delivery locations while ensuring the feasibility of the loading plan. We formulate the problem as two traveling salesman problems linked by infeasible path constraints. We also introduce several strengthenings of these constraints, which are used within a branch-and-cut algorithm. Computational results performed on instances from the literature show that the algorithm outperforms existing exact algorithms. Instances with up to 28 requests (58 nodes) have been solved to optimality.
We propose a novel simulation-based method that exploits a generalized splitting (GS) algorithm to estimate the reliability of a graph (or network), defined here as the probability that a given set of nodes are connected, when each link of the graph fails with a given (small) probability. For large graphs, in general, computing the exact reliability is an intractable problem and estimating it by standard Monte Carlo methods poses serious difficulties, because the unreliability (one minus the reliability) is often a rare-event probability. We show that the proposed GS algorithm can accurately estimate extremely small unreliabilities and we exhibit large examples where it performs much better than existing approaches. It is also flexible enough to dispense with the frequently made assumption of independent edge failures.
We consider the problem of determining whether a given set of rectangular items can be cut from a larger rectangle using so-called guillotine cuts only. We introduce a new class of arc-colored directed graphs called guillotine graphs and show that each guillotine graph can be associated with a specific class of pattern solutions that we call a guillotine-cutting class. The properties of guillotine graphs are examined, and some effective algorithms for dealing with guillotine graphs are proposed. As an application, we then describe a constraint programming method based on guillotine graphs, and we propose effective filtering techniques that use the graph model properties in order to reduce the search space efficiently. Computational experiments are reported on benchmarks from the literature: our algorithm outperforms previous methods when solving the most difficult instances exactly.
A new class of non-Markovian models is introduced that results from the combination of stochastic automata networks and a very general class of stochastic processes, namely, rational arrival processes, which are derived from matrix exponential distributions. It is shown that the modeling formalism allows a compact representation of complex models with large state spaces. The resulting stochastic process is non-Markovian, but it can be analyzed with numerical techniques like a Markov chain, and the results at the level of the automata are stochastic distributions that can be used to compute standard performance and dependability results. The model class includes stochastic automata networks with phase-type distributed and correlated event times and also includes models that have a finite state space but cannot be represented by finite Markov chains. The paper introduces the model class, shows how the descriptor matrix can be represented in compact form, presents some example models, and outlines methods to analyze the new models.
Dynamic pricing for a network of resources over a finite selling horizon has received considerable attention in recent years, yet few papers provide effective computational approaches to solve the problem. We consider a resource decomposition approach to solve the problem and investigate the performance of the approach in a computational study. We compare the performance of the approach to static pricing and choice-based availability control. Our numerical results show that dynamic pricing policies from network resource decomposition can achieve significant revenue lift compared with choice-based availability control and static pricing, even when the latter is frequently resolved. As a by-product of our approach, network decomposition provides an upper bound in revenue, which is provably tighter than the well-known upper bound from a deterministic approximation.
We formulate and solve two new stochastic linear programming formulations of appointment scheduling problems that are motivated by the management of health services. We assume that service durations and the number of customers to be served on a particular day are uncertain. In the first model, customers may fail to show up for their appointments (“no-show”). This model is formulated as a two-stage stochastic linear program. In the second model, customers are scheduled dynamically, one at a time, as they request appointments. This model is formulated as a multistage stochastic linear program with stages defined by customer appointment requests. We analyze the structure of the models and adapt decomposition-based algorithms to solve the problems efficiently. We present numerical results that illustrate the impact of uncertainty on dynamic appointment scheduling, and we identify useful insights that can be applied in practice. We also present a case study based on real data for an outpatient procedure center.
We propose an adaptive hyperbox algorithm (AHA), which is an instance of a locally convergent, random search algorithm for solving discrete optimization via simulation problems. Compared to the COMPASS algorithm, AHA is more efficient in high-dimensional problems. By analyzing models of the behavior of COMPASS and AHA, we show why COMPASS slows down significantly as dimension increases, whereas AHA is less affected. Both AHA and COMPASS can be used as the local search algorithm within the Industrial Strength COMPASS framework, which consists of a global search phase, a local search phase, and a final cleanup phase. We compare the performance of AHA to COMPASS within the framework of Industrial Strength COMPASS and as stand-alone algorithms. Numerical experiments demonstrate that AHA scales up well in high-dimensional problems and has similar performance to COMPASS in low-dimensional problems.
We consider a multiperiod system operation problem with two conflicting objectives, minimizing cost and risk. Risk stems from uncertain disruptions to the system during operation. Whereas a general model would hedge against disruptions in each time period, we study special cases in which only a modest number of disruptions occur. To optimize for risk, we employ a convex approximation based on constraint sampling. We develop a stratified sampling scheme based on distributional information on the time of disruption. We establish that our scheme yields significant savings in sampling costs—up to an order of magnitude in the number of time periods—over naive sampling. Moreover, in the absence of distributional information, we exhibit a sampling strategy that has comparable performance to optimal stratification. We numerically demonstrate that stratification improves cost over naive sampling, improving the solution's proximity to the efficient frontier of the bicriteria problem.
In the online world, customers can easily navigate to different online stores to make purchases. The products purchased on one site are often associated with product purchases on other sites (e.g., a hotel reservation on one site and a car rental on another site). Whereas market basket analysis is often used to discover associations among products for brick-and-mortar stores, it is rarely applied in the online setting where consumers navigate among different online stores to buy products. We define online shopping patterns and develop two novel methods to perform market basket analysis across websites. While this research is motivated by online shopping applications, our contribution is mainly methodological. The two methods we develop in this paper can not only be used to identify various online shopping patterns across sites and products but can also be applied to settings where patterns exist across different dimensions. Experiments on both synthetic data and real online shopping data demonstrate the effectiveness of our methods.
The replicated batch means (RBM) method for steady-state simulation output analysis generalizes both the independent replications (IR) and batch means (BM) methods. We analyze the performance of RBM in situations where the underlying stochastic process possesses an additive initial transient. Our analysis differs from prior work in that the initial transient is stochastic, and hence the sample paths of the transient process may be replication dependent, and possibly also correlated across replications. We provide asymptotic expressions for the mean and variance of the RBM estimators of the steady-state mean and variance parameter of the stochastic process being simulated. We then use our results to study the performance of RBM as a function of the number of replications, initialization method for the replications, and decay rate of the associated initialization bias. Our results provide guidance on when IR, BM, or a combination thereof is the best choice, and also on effective choices of initial states for the replications.
The multitrip vehicle routing problem (MTVRP) is a variant of the capacitated vehicle routing problem where each vehicle can perform a subset of routes, called a vehicle schedule, subject to maximum driving time constraints. Despite its practical importance, the MTVRP has received little attention in the literature. Few heuristics have been proposed, and only an exact algorithm has been presented for a variant of the MTVRP with customer time window constraints and unlimited driving time for each vehicle. We describe two set-partitioning-like formulations of the MTVRP. The first formulation requires the generation of all feasible routes, whereas the second formulation is based on the generation of all feasible schedules. We study valid lower bounds, based on the linear relaxations of both formulations enforced with valid inequalities, that are embedded into an exact solution method. The computational results show that the proposed exact algorithm can solve MTVRP instances taken from the literature, with up to 120 customers.
We develop a new local search algorithm for binary optimization problems, whose complexity and performance are explicitly controlled by a parameter Q, measuring the depth of the local search neighborhood. We show that the algorithm is pseudo-polynomial for general cost vector c, and achieves a w2/(2w-1) approximation guarantee for set packing problems with exactly w ones in each column of the constraint matrix A, when using Q = w2. Most importantly, we find that the method has practical promise on large, randomly generated instances of both set covering and set packing problems, as it delivers performance that is competitive with leading general-purpose optimization software (CPLEX 11.2).
We discuss two possible parallel strategies for randomized restart algorithms. Given a set of available algorithms, one can either choose the best performing algorithm and run its multiple copies in parallel (single algorithm portfolio) or choose some subset of algorithms to run in parallel (mixed algorithm portfolio). It has been previously shown that the latter approach may provide better results computationally. In this paper, we provide theoretical investigation of the extent of such improvement generalizing some of the known results from the literature. In particular, we estimate the computational value of mixing randomized restart algorithms with different properties. Under some mild assumptions, we prove that in the best case the mixed algorithm portfolio may perform approximately up to 1.58 times faster than the best single algorithm portfolio. We also show that the obtained upper bound is sharp. Furthermore, the constructive proof of the main result allows us to characterize algorithms that are likely to form an effective mixed algorithm portfolio.
Response surface methodology (RSM) is a widely used method for simulation optimization. Its strategy is to explore small subregions of the decision space in succession instead of attempting to explore the entire decision space in a single attempt. This method is especially suitable for complex stochastic systems where little knowledge is available. Although RSM is popular in practice, its current applications in simulation optimization treat simulation experiments the same as real experiments. However, the unique properties of simulation experiments make traditional RSM inappropriate in two important aspects: (1) It is not automated; human involvement is required at each step of the search process; (2) RSM is a heuristic procedure without convergence guarantee; the quality of the final solution cannot be quantified. We propose the stochastic trust-region response-surface method (STRONG) for simulation optimization in attempts to solve these problems. STRONG combines RSM with the classic trust-region method developed for deterministic optimization to eliminate the need for human intervention and to achieve the desired convergence properties. The numerical study shows that STRONG can outperform the existing methodologies, especially for problems that have grossly noisy response surfaces, and its computational advantage becomes more obvious when the dimension of the problem increases.
The bin packing problem with conflicts consists of packing items in a minimum number of bins of limited capacity while avoiding joint assignments of items that are in conflict. Our study demonstrates that a generic implementation of a branch-and-price algorithm using specific pricing oracle yields comparatively good performance for this problem. We use our black-box branch-and-price solver BaPCod, relying on its generic branching scheme and primal heuristics. We developed a dynamic programming algorithm for pricing when the conflict graph is an interval graph, and a depth-first-search branch-and-bound approach for pricing when the conflict graph has no special structure. The exact method is tested on instances from the literature where the conflict graph is an interval graph, as well as harder instances that we generated with an arbitrary conflict graph and larger number of items per bin. Our computational experiment report sets new benchmark results for this problem, closing all open instances of the literature in one hour of CPU time.
Given a set of customers, a set of potential facility locations, and some interconnection nodes, the goal of the connected facility location problem (ConFL) is to find the minimum-cost way of assigning each customer to exactly one open facility and connecting the open facilities via a Steiner tree. The sum of costs needed for building the Steiner tree, facility opening costs, and the assignment costs needs to be minimized. If the number of edges between a prespecified node (the so-called root) and each open facility is limited, we speak of the hop constrained facility location problem (HC ConFL). This problem is of importance in the design of data-management and telecommunication networks.In this article we provide the first theoretical and computational study for this new problem that has not been studied in the literature so far. We propose two disaggregation techniques that enable the modeling of HC ConFL: (i) as a directed (asymmetric) ConFL on layered graphs, or (ii) as the Steiner arborescence problem (SA) on layered graphs. This allows for usage of best-known mixed integer programming models for ConFL or SA to solve the corresponding hop constrained problem to optimality. In our polyhedral study, we compare the obtained models with respect to the quality of their linear programming lower bounds. These models are finally computationally compared in an extensive computational study on a set of publicly available benchmark instances. Optimal values are reported for instances with up to 1,300 nodes and 115,000 edges.
Fast computation of valid linear programming (LP) bounds serves as an important subroutine for solving mixed-integer programming problems exactly. We introduce a new method for computing valid LP bounds designed for this application. The algorithm corrects approximate LP dual solutions to be exactly feasible, giving a valid bound. Solutions are repaired by performing a projection and a shift to ensure all constraints are satisfied; bound computations are accelerated by reusing structural information through the branch-and-bound tree. We demonstrate this method to be widely applicable and faster than solving a sequence of exact LPs. Several variations of the algorithm are described and computationally evaluated in an exact branch-and-bound algorithm within the mixed-integer programming framework SCIP (Solving Constraint Integer Programming).
In this paper, we consider time-varying multiserver queues with abandonment and retrials. For their performance analysis, fluid and diffusion limits utilizing strong approximations have been widely used in the literature. Although those limits are asymptotically exact, they may not accurately approximate performance of multiserver queues even if the number of servers is large. To address that concern, this paper focuses on developing a methodology by taking fluid and diffusion limits in a nontraditional fashion. We show that our approximation is significantly more accurate and also asymptotically true. We illustrate the effectiveness of our methodology by performing several numerical experiments.
We develop an exact algorithm for integer programs that uses restrictions of the problem to produce high-quality solutions quickly. Column generation is used both for generating these problem restrictions and for producing bounds on the value of the optimal solution. The performance of the algorithm is greatly enhanced by using structure, such as arises in network flow type applications, to help define the restrictions that are solved. In addition, local search around the current best solution is incorporated to enhance overall performance. The approach is parallelized and computational experiments on a classical problem in network design demonstrate its efficacy.
Some existing simulation optimization algorithms (e.g., adaptive random search) become pure random search methods and thus are ineffective for the zero-one optimization via simulation problem. In this paper, we present highly efficient rapid screening procedures for solving the zero-one optimization via simulation problem. Three approaches adopting different sampling rules and providing different statistical guarantees are described. We also propose efficient neighborhood search methods and a simple algorithm for generation of initial solutions, all of which can be incorporated into our rapid screening procedures. The proposed procedures are more adaptive than ordinary ranking and selection procedures because in each iteration they can eliminate inferior solutions and intelligently sample promising solutions from the neighborhood of the survivors. Empirical studies are performed to compare the efficiency of the proposed procedures with other existing ones.
This paper presents a binary search heuristic algorithm for the rectangular strip-packing problem. The problem is to pack a number of rectangles into a sheet of given width and infinite height so as to minimize the required height. We first transform this optimization problem into a decision problem. A least-waste-first strategy and a minimal-inflexion-first strategy are proposed to solve the related decision problem. Lastly, we develop a binary search heuristic algorithm based on randomized local search to solve the original optimization problem. The computational results on six classes of benchmark problems have shown that the presented algorithm can find better solutions within a reasonable time than the published best heuristic algorithms for most zero-waste instances. In particular, the presented algorithm is proved to be the dominant algorithm for large zero-waste instances.
This paper presents a genetic algorithm (GA) for solving the traveling salesman problem (TSP). To construct a powerful GA, we use edge assembly crossover (EAX) and make substantial enhancements to it: (i) localization of EAX together with its efficient implementation and (ii) the use of a local search procedure in EAX to determine good combinations of building blocks of parent solutions for generating even better offspring solutions from very high-quality parent solutions. In addition, we develop (iii) an innovative selection model for maintaining population diversity at a negligible computational cost. Experimental results on well-studied TSP benchmarks demonstrate that the proposed GA outperforms state-of-the-art heuristic algorithms in finding very high-quality solutions on instances with up to 200,000 cities. In contrast to the state-of-the-art TSP heuristics, which are all based on the Lin–Kernighan (LK) algorithm, our GA achieves top performance without using an LK-based algorithm.
We discuss the evaluation of expected tardiness of an order at the time of arrival in an M/M/c queuing system with N priority classes, considering both nonpreemptive and preemptive service disciplines. Upon arrival, a customer order is quoted a lead time of d, and placed in the queue according to the priority class of the customer. Orders within the same priority class are processed on a first-come, first-served basis. We derive the Laplace transforms of the expected tardiness of the order given the quoted lead time, priority class of the order, and system status. For the special case of single priority class, the Laplace transform can be inverted into a closed-form expression. For the case with multiple priority classes, a closed-form expression cannot be obtained, hence, we develop three customized numerical inverse Laplace transformation algorithms. Two of these algorithms provide upper and lower bounds for the expected tardiness under a simple condition on system parameters. Using this property, we obtain error bounds for our customized algorithms; such bounds are not available for general purpose numerical inversion algorithms in the literature. Next, we develop a novel methodology to compare the precision of general purpose numerical inversion algorithms and analyze the performances of three algorithms from the literature. Finally, we provide a recommendation scheme given computational time and error tolerances of the decision maker. The methods developed in this paper for the accurate estimation of expected tardiness establish an important step toward developing due date quotation policies in a multiclass queue, contributing to the due date quotation literature that has been largely focused on single-class queues.
We consider the problem of approximating Pareto surfaces of convex multicriteria optimization problems by a discrete set of points and their convex combinations. Finding the scalarization parameters that optimally limit the approximation error when generating a single Pareto optimal solution is a nonconvex optimization problem. This problem can be solved by enumerative techniques but at a cost that increases exponentially with the number of objectives. We present an algorithm for solving the Pareto surface approximation problem that is practical with 10 or less conflicting objectives, motivated by an application to radiation therapy optimization. Our enumerative scheme is, in a sense, dual to a family of previous algorithms. The proposed technique retains the quality of the best previous algorithm in this class while solving fewer subproblems. A further improvement is provided by a procedure for discarding subproblems based on reusing information from previous solves. The combined effect of the enhancements is empirically demonstrated to reduce the computational expense of solving the Pareto surface approximation problem by orders of magnitude. For problems where the objectives have positive curvature, an improved bound on the approximation error is demonstrated using transformations of the initial objectives with strictly increasing and concave functions.
Robust dynamic programming (robust DP) mitigates the effects of ambiguity in transition probabilities on the solutions of Markov decision problems. We consider the computation of robust DP solutions for discrete-stage, infinite-horizon, discounted problems with finite state and action spaces. We present robust modified policy iteration (RMPI) and demonstrate its convergence. RMPI encompasses both of the previously known algorithms, robust value iteration and robust policy iteration. In addition to proposing exact RMPI, in which the “inner problem” is solved precisely, we propose inexact RMPI, in which the inner problem is solved to within a specified tolerance. We also introduce new stopping criteria based on the span seminorm. Finally, we demonstrate through some numerical studies that RMPI can significantly reduce computation time.
This paper presents a variable depth search for the nurse rostering problem. The algorithm works by chaining together single neighbourhood swaps into more effective compound moves. It achieves this by using heuristics to decide whether to continue extending a chain and which candidates to examine as the next potential link in the chain. Because end users vary in how long they are willing to wait for solutions, a particular goal of this research was to create an algorithm that accepts a user specified computational time limit and uses it effectively. When compared against previously published approaches the results show that the algorithm is very competitive.
Motivated by addressing probabilistic 0–1 programs we study the conic quadratic knapsack polytope with generalized upper bound (GUB) constraints. In particular, we investigate separating and extending GUB cover inequalities. We show that, unlike in the linear case, determining whether a cover can be extended with a single variable is 𝒩𝒫-hard. We describe and compare a number of exact and heuristic separation and extension algorithms which make use of the structure of the constraints. Computational experiments are performed for comparing the proposed separation and extension algorithms. These experiments show that a judicious application of the extended GUB cover cuts can reduce the solution time of conic quadratic 0–1 programs with GUB constraints substantially.
Continuous nonnegative functions, such as Poisson rate functions, are sometimes approximated as piecewise-constant functions. We consider the problem of automatically smoothing such functions while maintaining the integral of each piece and maintaining nonnegativity everywhere, without specifying a parametric function. We develop logic for SMOOTH (Smoothing via Mean-constrained Optimized-Objective Time Halving), a quadratic-optimization algorithm that yields a smoother nonnegative piecewise-constant rate function having twice as many time intervals, each of half the length. I-SMOOTH (Iterated SMOOTH) iterates the SMOOTH formulation to create a sequence of piecewise-constant rate functions that, in the limit, yields a nonparametric continuous function. We consider two contexts: finite-horizon and cyclic. We develop a sequence of computational simplifications for SMOOTH, moving from numerically minimizing the quadratic objective function, to numerically computing a matrix inverse, to a closed-form matrix inverse obtained as finite sums, to optimal decision-variable values that are linear combinations of the given rates, and to simple approximations.
Given n points in ℝd and a maximum allowed tolerance ϵ > 0, the minimum hyperplanes clustering problem consists in finding a minimum number of hyperplanes such that the Euclidean distance between each point and the nearest hyperplane is at most ϵ. We present a column generation approach for this problem based on a mixed integer nonlinear formulation in which the master is a set covering problem and the pricing subproblem is a mixed integer program with a nonconvex normalization constraint. We propose different ways of generating the initial pool of columns and investigate their impact on the overall algorithm. Since the pricing subproblem is substantially complicated by the ℓ2-norm constraint, we consider approximate pricing subproblems involving different norms. Some strategies for refining the solution and speeding-up the overall method are also discussed. The performance of our column generation algorithm is assessed on realistic randomly generated instances as well as on real-world instances.
We present a branch-and-price algorithm to solve personalized multi-activity shift scheduling problems. The subproblems in the column generation method are formulated using grammars and solved with dynamic programming. The expressiveness of context-free grammars is exploited to easily model restrictions over shifts, allowing the branch-and-price algorithm to solve large-scale problem instances. We present computational experiments on two types of multi-activity shift scheduling problems and compare our approach with existing methods in the literature. These experiments show that our approach can efficiently solve large-scale instances and is flexible enough to model different classes of problems.
Split cuts constitute a class of cutting planes that has been successfully employed by the majority of branch-and-cut solvers for mixed-integer linear programs. Given a basis of the linear programming (LP) relaxation and a split disjunction, the corresponding split cut can be computed with a closed-form expression. In this paper, we use the lift-and-project framework introduced by Balas and Perregaard to provide the basis, and the reduce-and-split algorithm as described by Cornuéjols and Nannicini to compute the split disjunction. We propose a cut generation algorithm that starts from a Gomory mixed-integer cut and alternates between lift-and-project and reduce-and-split in order to strengthen it. This paper has two main contributions. First, we extend the Balas and Perregaard procedure for strengthening cuts arising from split disjunctions involving one variable to split disjunctions on multiple variables. Second, we apply the reduce-and-split algorithm to nonoptimal bases of the LP relaxation. We provide detailed computational testing of the proposed methods.
In the context of multistage stochastic optimization problems, we propose a hybrid strategy for generalizing to nonlinear decision rules, using machine learning, a finite data set of constrained vector-valued recourse decisions optimized using scenario-tree techniques from multistage stochastic programming. The decision rules are based on a statistical model inferred from a given scenario-tree solution and are selected by out-of-sample simulation given the true problem. Because the learned rules depend on the given scenario tree, we repeat the procedure for a large number of randomly generated scenario trees and then select the best solution (policy) found for the true problem. The scheme leads to an ex post selection of the scenario tree itself. Numerical tests evaluate the dependence of the approach on the machine learning aspects and show cases where one can obtain near-optimal solutions, starting with a “weak” scenario-tree generator that randomizes the branching structure of the trees.
Peer-to-peer (P2P) networks are social networks for pooling network and information resources and are considered superior conduits for distributed computing and data management. In this paper, we utilize the theories of social networks and economic incentives to investigate the formation of P2P networks with rational participating agents (active peers). The paper proposes a framework for multilevel formation dynamics, including an individual level (content-sharing decision and group selection) and a group level (membership admission, splitting, and interconnection). It is found that if the network size (the number of peer nodes) is sufficiently large, the stable (self-selected equilibrium) free-riding ratio could be nonzero, contrary to the common belief that everybody should free ride. The efficient (welfare-maximizing) free-riding ratio is not necessarily zero; that is, a certain degree of free riding is beneficial and should be tolerated. The sharing level in a network increases (decreases) with the download (upload) capacities of its peer nodes. In addition, the heterogeneity of content availability and upload capacity discourages sharing activities. Although the sharing level of a stable group is typically lower than that of an efficient group, the self-formed network may have a larger or smaller group size than what is efficient, depending on the structure of the group admission decision process. It is also observed that self-organized interconnections among groups lead to network inefficiency because the network may be over- or underlinked. To recover the efficiency loss during the formation process, we propose internal transfer mechanisms to force stable networks to become efficient.
Model aggregation is the process of constructing several base models that are then combined into a single model for prediction. Ensemble classification has been studied by many researchers and found to provide significant performance improvements over single models. This paper presents a new base model combination algorithm for K-nearest neighbor (KNN) ensemble models based on One-Versus-All (OVA) classification. The proposed algorithm uses two decision functions to determine the best prediction among the many predictions provided by the base models. It is demonstrated in this paper that tied or conflicting predictions can be effectively resolved when a probabilistic function and a distance function are used by a combination algorithm for OVA KNN base model predictions. The resolution of tied predictions leads to improvements in predictive performance.
Consider the context of selecting an optimal system from among a finite set of competing systems, based on a “stochastic” objective function and subject to multiple “stochastic” constraints. In this context, we characterize the asymptotically optimal sample allocation that maximizes the rate at which the probability of false selection tends to zero. Since the optimal allocation is the result of a concave maximization problem, its solution is particularly easy to obtain in contexts where the underlying distributions are known or can be assumed. We provide a consistent estimator for the optimal allocation and a corresponding sequential algorithm fit for implementation. Various numerical examples demonstrate how the proposed allocation differs from competing algorithms.
An adaptive process management system (APMS) allows for flexible, dynamic, and even ad hoc adaptation of processes based on case data, context, and events. These processes may arise in various domains such as business, healthcare, etc. In knowledge-intensive environments, it is important that APMS technology ensures error-free process execution and compliance with semantic constraints. However, most process design tools handle only syntactic constraints. This restricts their value in real-world applications considerably. This paper proposes a novel approach to check the compliance of process models against semantic constraints and the validity of process change operations using mixed-integer programming (MIP). The MIP formulation allows us to describe existential, dependency, ordering, and various other relationships among tasks along with business policies in a standard way. In addition to incorporating the semantic constraint specifications into an MIP formulation, we introduce three novel ideas in this paper: (1) the notion of degree of compliance of processes to constraints based on a penalty function, (2) the concepts of full and partial validity of change operations, and (3) the idea of compliance by compensation. Thus, compensation operations derived from compliance degree can transform a noncompliant process into a compliant one both at design and execution time. We illustrate our approach in the context of a healthcare workflow as a way to reduce medical errors and argue that it is more elegant and superior to a pure logic-based approach. Complex scenarios with multiple concurrent processes (and constraints across them) for a single patient are also considered.
We study a natural generalization of the knapsack problem, in which each item exists only for a given time interval. One has to select a subset of the items (as in the classical case), guaranteeing that for each time instant, the set of existing selected items has total weight no larger than the knapsack capacity. We focus on the exact solution of the problem, noting that prior to our work, the best method was the straightforward application of a general-purpose solver to the natural integer linear programming formulation. Our results indicate that much better results can be obtained by using the same general-purpose solver to tackle a nonstandard Dantzig-Wolfe reformulation in which subproblems are associated with groups of constraints. This is also interesting because the more natural Dantzig-Wolfe reformulation of single constraints performs extremely poorly in practice.
In this paper, we study approximation algorithms for two supply chain network design problems, namely, the warehouse-retailer network design problem (WRND) and the stochastic transportation-inventory network design problem (STIND). These two problems generalize the classical uncapacitated facility location problem by incorporating, respectively, the warehouse-retailer echelon inventory cost and the warehouse cycle inventory together with the safety stock costs. The WRND and the STIND were initially studied, respectively, by Teo and Shu (Teo CP, Shu J (2004) Warehouse-retailer network design problem. Oper. Res. 52(3):396–408) and Shu et al. (Shu J, Teo CP, Shen ZJM (2005) Stochastic transportation-inventory network design problem. Oper. Res. 53(1):48–60), where they are formulated as set-covering problems, and column-generation algorithms were used to solve their linear programming relaxations. Both problems can be regarded as special cases of the so-called facility location with submodular facility costs proposed by Svitkina and Tardos (Svitkina Z, Tardos É (2010) Facility location with hierarchical facility costs. ACM Trans. Algorithms 6(2), Article No. 37), for which only a logarithmic-factor approximation algorithm is known. Our main contribution is to obtain efficient constant-factor approximation algorithms for the WRND and the STIND, which are capable of solving large-scale instances of these problems efficiently.
The bandwidth packing problem (BWP) concerns the selection of calls from a given set and the assignment of one path to each selected call. The ultimate aim of the BWP is to maximize profit while the routings of the selected calls observe the capacity constraints of the links. Here, we additionally consider queueing delays in the network, which may cause a deterioration in the quality of service to users if they exceed the acceptable limits. The integer programming formulation for the BWP with the queueing delay restriction contains a nonlinear constraint that is intrinsic to the model. We apply the Dantzig-Wolfe decomposition to this nonlinear constraint, and since the Dantzig-Wolfe decomposition has exponentially many variables, we propose the branch-and-price procedure to find optimal solutions. We also propose a generalized Dantzig-Wolfe reformulation based on the aggregation of variables, which makes our branch-and-price algorithm more competitive. Computational results on cases of randomly generated networks and some real-life telecommunication networks demonstrate that our algorithm performs well for large networks.
The INFORMS Journal on Computing Book Reviews section covers books on subjects at the interface between operations research and computer science. We welcome books on theory, applications, computer systems, and generally any subject covered by a JOC area, or any combination of these. This includes both printed and electronic books. In addition, we consider comparative reviews, i.e., several books on one relevant topic. Team reviews are also possible, particularly for a large, broad-scope book such as an encyclopedia. Please send your suggestions for books to be reviewed to the JOC Book Reviews Editor.This issue contains two reviews of books that approach the general topic of algorithms from different perspectives. David Bader, professor at Georgia Tech's College of Computing, reviews A Guide to Experimental Algorithmics, by Catherine McGeoch. This book, aimed at software developers, describes the practice of performance analysis and tuning of software. Topics include strategies for test-driven development, experimental designs, and statistical analysis techniques. Lance Fortnow's The Golden Ticket: P, NP, and the Search for the Impossible is a book for general readers about complexity theory. The reviewer is Mike Trick, senior associate dean of Carnegie Mellon's Tepper School of Business, and a widely-read blogger about operations research. The book attempts to introduce the nonspecialist reader to the field of algorithmic complexity and the problem of whether P = NP. To the extent that it is successful, it should help improve the general public's understanding of the work that computational operations researchers do.
In 2007, W. H. Cunningham and J. Geelen describe an algorithm for solving max{cTx: Ax=b,x≥0,x∈ℤn}, where A∈ℤm×n≥0, b∈ℤm, and c∈ℤn, which utilizes a branch-decomposition of the matrix A and techniques from dynamic programming. In this paper, we report on the first implementation of the CG algorithm and compare our results with the commercial integer programming software Gurobi. Using branch-decomposition trees produced by heuristics and optimal trees produced by algorithms developed in our previous studies, we test both a memory-intensive and low-memory version of the CG algorithm on problem instances such as graph 3-coloring, set partition, market split, and knapsack. We isolate a class of set partition instances where the CG algorithm runs twice as fast as Gurobi, and demonstrate that certain infeasible market split and knapsack instances with width ≤6 range from running twice as fast as Gurobi, to running in a matter of minutes versus a matter of hours.
We present a semidefinite programming (SDP) approach for the problem of ordering vertices of a layered graph such that the edges of the graph are drawn as vertical as possible. This multilevel vertical ordering (MLVO) problem is a quadratic ordering problem and conceptually related to the well-studied problem of multilevel crossing minimization (MLCM). In contrast to the latter, it can be formulated such that it does not merely consist of multiple sequentially linked bilevel quadratic ordering problems, but as a genuine multilevel problem with dense cost matrix. This allows us to describe the graphs' structures more compactly and therefore obtain solutions for graphs too large for MLCM in practice.In this paper we give motivation and mathematical models for MLVO. We formulate linear and quadratic programs, including some strengthening constraint classes, and an SDP relaxation for MLVO. We compare all approaches both theoretically and experimentally and show that MLVO's properties render linear and quadratic programming approaches inapplicable, even for small sparse graphs, while the SDP works surprisingly well in practice. This is in stark contrast to other ordering problems like MLCM, where such graphs are typically solved more efficiently with integer linear programs. Finally, we also compare our approach to related MLCM approaches.
Commodities such as cloud resources (storage, computing, bandwidth) are often sold to clients on a pay-as-you-go basis. Thus, resource providers absorb all risk arising from end users' demand volatilities. We focus on the revenue risk management of commodities with highly volatile demand profiles using cloud computing as the application domain and bandwidth as the exemplar commodity. We extend the state of the art in risk hedging by introducing a new concept of dynamic forward contracts where a provider and a client flexibly interact through offers and responses over a set of time periods in a horizon. We develop an optimal pricing mechanism that takes into account the risk propensities of the provider and the client. The overall mechanism is modeled as a pair of nested dynamic programs denoting the offer-response interactions. The mechanism also incorporates two learning components: short-term learning on the client's demand and long-term learning on the client's risk propensity. We characterize two approaches for predicting the client's demand—a recursive demand prediction model and an aggregate demand prediction model. Detailed experimental studies of the proposed mechanism using real Web traffic data on the clients of Amazon Web Services have been carried out. The empirical results clearly demonstrate the superiority of the proposed mechanism over benchmark mechanisms such as the current industry practice of spot markets and static forward pricing mechanisms proposed in the literature in ex ante and ex post settings. The results also highlight key interaction effects among parameters controllable by a provider and the risk propensities of the market players, leading to valuable managerial implications for the practical adoption of the proposed mechanism.
This paper studies the classical task assignment problem (TAP) in which M unbreakable tasks are assigned to N agents with the objective to minimize the communication and process costs subject to each agent's capacity constraint. Because a large-size TAP involves many binary variables, most, if not all, traditional methods experience the difficulty in solving the problem within a reasonable time period. Recent works present a logarithmic approach to reduce the number of binary variables in problems with mixed-integer variables. This study proposes a new logarithmic method that significantly reduces the numbers of binary variables and inequality constraints in solving task assignment problems. Our numerical experiments demonstrate that the proposed method is superior to other known methods of this kind for solving large-size TAPs.
A new method is proposed using a gradient-based zigzag search approach for multiobjective optimization (MOO) or vector optimization problems. The key idea of this method is searching around the Pareto front by applying an efficient local search procedure using the gradients of the objective functions. This local search zigzags along the Pareto surface guided by the gradients and iteratively returns the visited Pareto optima. Many continuous MOO problems have smooth objective functions and the set of the nondominated objective function values forms a regular surface in the image space. This fact motivates developing the zigzag search method for such relatively well-posed MOO problems. A simple implementation of this method, z-algorithm, is presented particularly for continuous bi-objective optimization (BOO) problems with well-connected Pareto optimal solutions. The efficiency of the z-algorithm is studied with a set of BOO problems and the algorithm performances are compared to those of a recently developed MOO algorithm, Pareto front approximation with adaptive weighted sum method.
This paper proposes a new way to construct uncertainty sets for robust optimization. Our approach uses the available historical data for the uncertain parameters and is based on goodness-of-fit statistics. It guarantees that the probability the uncertain constraint holds is at least the prescribed value. Compared to existing safe approximation methods for chance constraints, our approach directly uses the historical data information and leads to tighter uncertainty sets and therefore to better objective values. This improvement is significant, especially when the number of uncertain parameters is low. Other advantages of our approach are that it can handle joint chance constraints easily, it can deal with uncertain parameters that are dependent, and it can be extended to nonlinear inequalities. Several numerical examples illustrate the validity of our approach.
Scheduling elective procedures in an operating suite is a formidable task because of competing performance metrics and uncertain surgery durations. In this paper, we present an optimization framework for batch scheduling within a block booking system that maximizes the expected utilization of operating room resources subject to a set of probabilistic capacity constraints. The algorithm iteratively solves a series of mixed-integer programs that are based on a normal approximation of cumulative surgery durations. This approximation is suitable for high-volume medical specialities but might not be acceptable for the specialties that perform few procedures per block. We test our approach using the data from the ophthalmology department of the Veterans Affairs Pittsburgh Healthcare System. The performance of the schedules obtained by our approach is significantly better than schedules produced by simple heuristic scheduling rules.
We present an exact mixed-integer programming (MIP) solution scheme where a set-covering model is used to find a small set of first-choice branching variables. In a preliminary “sampling” phase, our method quickly collects a number of relevant low-cost fractional solutions that qualify as obstacles for the linear programming (LP) relaxation bound improvement. Then a set covering model is solved to detect a small subset of variables (a “backdoor,” in the artificial intelligence jargon) that “cover the fractionality” of the collected fractional solutions. These backdoor variables are put in a priority branching list, and a black-box MIP solver is eventually run—in its default mode—by taking this list into account, thus avoiding any other interference with its highly optimized internal mechanisms. Computational results on a large set of instances from the literature are presented, showing that some speedup can be achieved even with respect to a state-of-the-art solver such as IBM ILOG CPLEX 12.2.
We propose a partial replication strategy to construct risk-averse enhanced index funds. Our model takes into account the parameter estimation risk by defining the asset returns and the return covariance terms as random variables. The variance of the index fund return is required to be below a low-risk threshold with a large probability, thereby limiting the market risk exposure of the investors. The resulting stochastic integer problem is reformulated through the derivation of a deterministic equivalent for the risk constraint and the use of a block decomposition technique. We develop an exact outer approximation method based on the relaxation of some binary restrictions and the reformulation of the cardinality constraint. The method provides a hierarchical organization of the computations with expanding sets of integer-restricted variables and outperforms the Bonmin and the CPLEX solvers. The method can solve large instances (up to 1,000 securities), converges fast, scales well, and is general enough to be applicable to problems with buy-in-threshold constraints.
We consider reliable facility location models in which facilities are subject to unexpected failures, and customers may be reassigned to facilities other than their regular facilities. The objective is to minimize the total expected costs in normal and failure scenarios. We allow facilities to have different failure rates and do not limit the number of facilities that might be assigned to a customer. Lower bounds for reliable uncapacitated fixed-charge location problem (RUFLP) are derived and used to introduce a class of efficient algorithms for solving the RUFLP problem.
We consider the problem of dividing a geographic region into subregions so as to minimize the maximum workload of a collection of facilities over that region. We assume that the cost of servicing a demand point is a monomial function of the distance to its assigned facility and that demand points follow a continuous probability density. We show that, when our objective is to minimize the maximum workload of all facilities, the optimal partition consists of a collection of circular arcs that are induced by a multiplicatively weighted Voronoi diagram. When we require that all subregions have equal area, the optimal partition consists of a collection of hyperbolic or quartic curves. We show that, for both problems, the dual variables correspond to “prices” for a facility to serve a demand point, and our objective is to determine a set of prices such that the entire region is “purchased” by the facilities, i.e., that the market clears. This allows us to solve the partitioning problem quickly without discretizing the service region.
One of the most widely used techniques to obtain transient measures is the uniformization method. However, although uniformization has many advantages, the computational cost required to calculate transient probabilities is very large for stiff models. We study efficient solutions that can be applied to an approximate method developed for calculating transient state probabilities of Markov models and cumulative expected reward measures over a finite interval. Our work is based on a method that approximates the state probabilities at time t by the state probabilities calculated at a random time with Erlangian distribution. The original method requires an inversion of a matrix obtained from the state transition rate matrix that destroys special structures such as sparseness and banded matrices. This precludes the use of the technique for large models. In our work we propose efficient solutions that can take advantage of special structures. Finally, we present examples that show that the proposed technique is computationally very efficient for stiff models when compared with uniformization.
This paper considers time-dependent Pht/Mt/s/c queueing nodes and small tandem networks of such nodes. We examine characteristics of the departure processes from a multiserver queueing node; in particular, we focus on solving for the first two time-dependent moments of the departure-count process. A finite set of partial moment differential equations is developed to numerically solve for the departure-count moments over specified intervals of time [ti, ti + τi). We also present a distribution fitting algorithm to match these key characteristics with a ˜Pht process serving as the approximate departure process. A distribution fitting algorithm is presented for time-dependent point processes where a two-level balanced mixture of Erlang distribution is used to serve as the approximating process. We then use the ˜Pht approximating departure process as the approximate composite arrival process to downstream node(s) in a network of tandem queues.
We develop a new randomization-based general-purpose method for the computation of the interval availability distribution of systems modeled by continuous-time Markov chains (CTMCs). The basic idea of the new method is the use of a randomization construct with different randomization rates for up and down states. The new method is numerically stable and computes the measure with well-controlled truncation error. In addition, for large CTMC models, when the maximum output rates from up and down states are significantly different, and when the interval availability has to be guaranteed to have a level close to one, the new method is significantly or moderately less costly in terms of CPU time than a previous randomization-based state-of-the-art method, depending on whether the maximum output rate from down states is larger than the maximum output rate from up states, or vice versa. Otherwise, the new method can be more costly, but a relatively inexpensive for large models switch of reasonable quality can be easily developed to choose the fastest method. Along the way, we show the correctness of a generalized randomization construct, in which arbitrarily different randomization rates can be associated with different states, for both finite CTMCs with infinitesimal generator and uniformizable CTMCs with denumerable state space.
The process of constructing several base models that are then combined into a single classification model for prediction is called model aggregation or ensemble classification. Positive-versus-negative (pVn) classification is a new method for the implementation of base models for aggregation. pVn classification involves the decomposition of a k-class prediction task into m (m < k) subproblems. One base model is constructed for each subproblem to predict a subset of the k classes. The base models are then combined into one aggregate model for prediction. This paper reports studies that were conducted to demonstrate the performance of pVn classification when large volumes of data are available for modeling as is commonly the case in data mining. It is demonstrated in this paper that pVn modeling provides the capability to use a large amount of available data (in a large data set) for base model training. It is also demonstrated that pVn models created from large data sets provide a higher level of predictive performance compared to single k-class models.
The split closure has been proved in practice to be a very tight approximation of the integer hull formulation of a generic mixed-integer linear program. However, exact separation procedures for optimizing over the split closure have unacceptable computing times in practice; hence, many different heuristic strategies have been proposed in the last few years. In this paper we present a new overall framework for approximating the split closure that merges different ideas from the previous approaches. Computational results prove the effectiveness of the proposed procedure compared to the state of the art, showing that a good approximation of the split closure bound can be obtained with very reasonable computing times.
On behalf of the Editorial Board, I would like to thank the following people, who acted as Reviewers during the past year.
The motivation behind this study is the essential need for survivability in the telecommunications networks. An optical signal should find its destination even if the network experiences an occasional fiber cut. We consider the design of a two-level survivable telecommunications network. Terminals compiling the access layer communicate through hubs forming the backbone layer. To hedge against single link failures in the network, we require the backbone subgraph to be two-edge connected and the terminal nodes to connect to the backbone layer in a dual-homed fashion, i.e., at two distinct hubs. The underlying design problem partitions a given set of nodes into hubs and terminals, chooses a set of connections between the hubs such that the resulting backbone network is two-edge connected, and for each terminal chooses two hubs to provide the dual-homing backbone access. All of these decisions are jointly made based on some cost considerations. We give alternative formulations using cut inequalities, compare these formulations, provide a polyhedral analysis of the small-sized formulation, describe valid inequalities, study the associated separation problems, and design variable fixing rules. All of these findings are then utilized in devising an efficient branch-and-cut algorithm to solve this network design problem.
We derive a new semidefinite programming relaxation for the general graph partition problem (GPP). Our relaxation is based on matrix lifting with matrix variable having order equal to the number of vertices of the graph. We show that this relaxation is equivalent to the Frieze-Jerrum relaxation for the maximum k-cut problem with an additional constraint that involves the restrictions on the subset sizes. Because the new relaxation does not depend on the number of subsets k into which the graph should be partitioned we are able to compute bounds for large k. We compare theoretically and numerically the new relaxation with other semide-finite programming (SDP) relaxations for the GPP. The results show that our relaxation provides competitive bounds and is solved significantly faster than any other known SDP bound for the general GPP.
A common structure in convex mixed-integer nonlinear programs (MINLPs) is separable nonlinear functions. In the presence of such structures, we propose three improvements to the outer approximation algorithms. The first improvement is a simple extended formulation, the second is a refined outer approximation, and the third is a heuristic inner approximation of the feasible region. As a side result, we exhibit a simple example where a classical implementation of the outer approximation would take an exponential number of iterations, whereas it is easily solved with our modifications. These methods have been implemented in the open source solver Bonmin and are available for download from the Computational Infrastructure for Operations Research project website. We test the effectiveness of the approach on three real-world applications and on a larger set of models from an MINLP benchmark library. Finally, we show how the techniques can be extended to perspective formulations of several problems. The proposed tools lead to an important reduction in computing time on most tested instances.
The detection of illicit nuclear materials is a major tool in preventing and deterring nuclear terrorism. The detection task is extremely difficult because of physical limitations of nuclear radiation detectors, shielding by intervening cargo materials, and the presence of background noise. We aim at enhancing the capabilities of detectors with algorithmic methods specifically tailored for nuclear data. This paper describes a novel graph-theory-based methodology for this task. This research considers for the first time the utilization of supervised normalized cut (SNC) for data mining and classification of measurements obtained from plastic scintillation detectors that are of particularly low resolution. Specifically, the situation considered here is for when both energy spectra and the time dependence of such data are acquired.We present here a computational study, comparing the supervised normalized cut method with alternative classification methods based on support vector machine (SVM), specialized feature-reducing SVMs (i.e., 1-norm SVM, recursive feature elimination SVM, and Newton linear program SVM), and linear discriminant analysis (LDA). The study evaluates the performance of the suggested method in binary and multiple classification problems of nuclear data. The results demonstrate that the new approach is on par or superior in terms of accuracy and much better in computational complexity to SVM (with or without dimension or feature reduction) and LDA with principal components analysis as preprocessing. For binary and multiple classifications, the SNC method is more accurate, more robust, and is computationally more efficient by a factor of 2–80 than the SVM-based and LDA methods.
Motivated by large-scale service systems with network structure, we introduced in a previous paper a time-varying open network of many-server fluid queues with customer abandonment from each queue and time-varying proportional routing among the queues, and showed how performance functions can be determined. The deterministic fluid model serves as an approximation for the corresponding non-Markovian stochastic network of many-server queues with Markovian routing, experiencing periods of overloading at the queues. In this paper we develop a new algorithm for the previous model and generalize the model to include non-exponential service-time distributions. In this paper we report results of implementing the algorithms and studying their computational complexity. We also conduct simulation experiments to confirm that the algorithms are effective in computing the performance functions and that these performance functions provide useful approximations for the corresponding stochastic models.
We consider the problem of deriving confidence intervals for the mean response of a system that is represented by a stochastic simulation whose parametric input models have been estimated from “real-world” data. As opposed to standard simulation confidence intervals, we provide confidence intervals that account for uncertainty about the input model parameters; our method is appropriate when enough simulation effort can be expended to make simulation-estimation error relatively small. To achieve this we introduce metamodel-assisted bootstrapping that propagates input variability through to the simulation response via an equation-based model rather than by simulating. We develop a metamodel strategy and associated experiment design method that avoid the need for low-order approximation to the response and that minimizes the impact of intrinsic (simulation) error on confidence level accuracy. Asymptotic analysis and empirical tests over a wide range of simulation effort show that confidence intervals obtained via metamodel-assisted bootstrapping achieve the desired coverage.
In this paper we present an exact algorithm for the capacitated location-routing problem (CLRP) based on cut-and-column generation. The CLRP is formulated as a set-partitioning problem that also inherits all of the known valid inequalities for the flow formulations of the CLRP. We introduce five new families of inequalities that are shown to dominate some of the cuts from the two-index formulation. The problem is solved by column generation, where the subproblem consists in finding a shortest path of minimum reduced cost under capacity constraints. We first use the two-index formulation for enumerating all of the possible subsets of depot locations that could lead to an optimal solution of cost less than or equal to a given upper bound. For each of these subsets, the corresponding multiple depot vehicle routing problem is then solved by means of column generation. The results show that we can improve the bounds found in the literature, solve to optimality some previously open instances, and improve the upper bounds on some other instances.
The inventory routing problem (IRP) and the production routing problem (PRP) are two difficult problems arising in the planning of integrated supply chains. These problems are solved in an attempt to jointly optimize production, inventory, distribution, and routing decisions. Although several studies have proposed exact algorithms to solve the single-vehicle problems, the multivehicle aspect is often neglected because of its complexity. We introduce multivehicle PRP and IRP formulations, with and without a vehicle index, to solve the problems under both the maximum level (ML) and order-up-to level (OU) inventory replenishment policies. The vehicle index formulations are further improved using symmetry breaking constraints; the nonvehicle index formulations are strengthened by several cuts. A heuristic based on an adaptive large neighborhood search technique is also developed to determine initial solutions, and branch-and-cut algorithms are proposed to solve the different formulations. The results show that the vehicle index formulations are superior in finding optimal solutions, whereas the nonvehicle index formulations are generally better at providing good lower bounds on larger instances. IRP and PRP instances with up to 35 customers, three periods, and three vehicles can be solved to optimality within two hours for the ML policy. By using parallel computing, the algorithms could solve the instances for the same policy with up to 45 and 50 customers, three periods, and three vehicles for the IRP and PRP, respectively. For the more difficult IRP (PRP) under the OU policy, the algorithms could handle instances with up to 30 customers, three (six) periods, and three vehicles on a single core machine, and up to 45 (35) customers, three (six) periods, and three vehicles on a multicore machine.
One method to obtain high-quality bid prices for network revenue management problems involves using the approximate linear programming approach on the dynamic programming formulation of the problem. This approach ends up with a linear program whose number of constraints increases exponentially with the number of flight legs in the airline network. The linear program is solved using constraint generation, where each constraint can be generated by solving a separate integer program. The necessity to solve integer programs and the slow convergence behavior of constraint generation are generally recognized as drawbacks of this approach. In this paper, we show how to effectively eliminate these drawbacks. In particular, we establish that constraint generation can actually be carried out by solving minimum-cost network flow problems with natural integer solutions. Furthermore, using the structure of minimum-cost network flow problems, we a priori reduce the number of constraints in the linear program from exponential to linear in the number of flight legs. It turns out that the reduced linear program can be solved without using separate problems to generate constraints. The reduced linear program also provides a practically appealing interpretation. Computational experiments indicate that our results can speed up the computation time for the approximate linear programming approach by a factor ranging between 13 and 135.
Containers are widely used in the shipping industry mainly because of their capability to facilitate multimodal transportation. How to effectively reposition the nonrevenue empty containers is the key to reduce the cost and improve the service in the liner shipping industry. In this paper, we propose a two-stage robust optimization model that takes into account the laden containers routing as well as the empty container repositioning, and define the robustness for this model with uncertainties in the supply and demand of the empty containers. Based on this definition, we present the robust formulations for the uncertainty sets corresponding to the ℓp-norm, where p = 1, 2, and ∞, and analyze the computational complexities for all of these formulations. The only polynomial-time solvable case corresponds to the ℓ1-norm, which we use to conduct the numerical study. We compare our approach with both the deterministic model and the stochastic model for the same problem in the rolling horizon simulation environment. The computational results establish the potential practical usefulness of the proposed approach.
In this paper, we study a class of stochastic optimization problems, where although the objective functions may not be convex, they satisfy a generalization of convexity called the sequentially convex property. We focus on a setting where the distribution of the underlying uncertainty is unknown and the manager must make a decision in real time based on historical data. Because sequentially convex functions are not necessarily convex, they pose difficulties in applying standard adaptive methods for convex optimization. We propose a nonparametric algorithm based on a gradient descent method and show that the T-season average expected cost differs from the minimum cost by at most . Our analysis is based on a careful quantification of the bias that is inherent in gradient estimation because of the adaptive nature of the problem. We demonstrate the usefulness of the concept of sequential convexity by applying it to three canonical problems in inventory control, capacity allocation, and the lifetime buy decision, under the assumption that the manager does not know the demand distributions and has access only to historical sales (censored demand) data.
Discrete k-median (DKM) clustering problems arise in many real-life applications that involve time-series data sets, in which nondiscrete clustering methods may not represent the problem domain adequately. In this study, we propose mathematical programming formulations and solution methods to efficiently solve the DKM clustering problem. We develop approximation algorithms from a bilinear formulation of the discrete k-median problem using an uncoupled bilinear program algorithm. This approximation algorithm, which we refer to as DKM-L, is composed of two alternating linear programs, where one can be solved in linear time and the other is a minimum cost assignment problem. We then modify this algorithm by replacing the assignment problem with an efficient sequential algorithm for a faster approximation, which we call DKM-S. We also propose a compact exact integer formulation, DKM-I, and a more efficient network design-based exact mixed-integer formulation, DKM-M. All of our methods use arbitrary pairwise distance matrices as input. We apply our methods to simulated single-variate and multivariate random walk time-series data. We report comparative clustering performances using normalized mutual information (NMI) and solution speeds among the DKM methods we propose. We also compare our methods to other clustering algorithms that can operate with distance matrices, such as hierarchical cluster trees (HCT) and partition around medoids (PAM). We present NMI scores and classification accuracies of our DKM algorithms compared to HCT and PAM using five different distance measures on simluated data, as well as public benchmark and real-life neural time-series data sets. We show that DKM-S is much faster than HCT, PAM, and all other DKM methods and produces consistently good clustering results on all data sets.
It is well known that the standard (linear) knapsack problem can be solved exactly by dynamic programming in 𝒪(nc) time, where n is the number of items and c is the capacity of the knapsack. The quadratic knapsack problem, on the other hand, is NP-hard in the strong sense, which makes it unlikely that it can be solved in pseudo-polynomial time. We show, however, that the dynamic programming approach to the linear knapsack problem can be modified to yield a highly effective constructive heuristic for the quadratic version. In our experiments, the lower bounds obtained by our heuristic were consistently within a fraction of a percent of optimal. Moreover, the addition of a simple local search step enabled us to obtain the optimal solution of all instances considered.
The United States pediatric vaccine manufacturing market is analyzed using a static Bertrand oligopoly pricing model that characterizes oligopolistic interactions between asymmetric firms in a homogeneous multiple product market. Firms satisfy demand by appropriately pricing and selling its given set of bundles, where each bundle contains one or more products. In analyzing the pediatric vaccine market, a bundle is a vaccine, where each vaccine contains one or more immunogenic antigens. Consumers seek to purchase at least one of each antigen at an overall minimum cost. Demand is captured by defining a weighted set covering optimization problem, with the weights (prices) controlled by firms engaged in Bertrand competition. A repeated game version of the model enables multiple interactions between firms, allowing examination of tacit collusion. An iterative improvement algorithm is defined that constructs a pure strategy Nash equilibrium (some in the limiting sense) for the static game. Sufficient conditions for the existence of pure strategy Nash equilibria are provided, indicating that this class of games always yields at least one pure strategy equilibrium. Practical results of the pediatric vaccine market analysis follow from the difference in the repeated game equilibrium prices between two combination vaccines, Pediarix® and Pentacel®. Assuming the manufacturers of these vaccines agree to share the market equally with respect to volume, the equilibrium prices from the repeated game indicate a price difference of $0.86, whereas the difference in price between Pediarix® and Pentacel® for contract prices ending March 31, 2010 was $2.74. Interestingly, the subsequent public sector vaccine price list (contract prices ending March 31, 2011) shows a price difference of $0.95, with the price of Pentacel® actually reduced from the previous year—an unusual occurrence. The results presented in this paper suggest that a smaller price difference between these two important combination vaccines is appropriate, which is what occurred. In general, such results could serve to inform both manufacturers and purchasers on the appropriate pricing of combination vaccines, given the existence of a reasonable set of collusive agreements.
Proportional symbol maps are a cartographic tool to assist in the visualization and analysis of quantitative data associated with specific locations, such as earthquake magnitudes, oil well production, and temperature at weather stations. As the name suggests, symbol sizes are proportional to the magnitude of the physical quantities that they represent. We present two novel integer linear programming (ILP) models to solve this computational geometry problem: how to draw opaque disks on a map so as to maximize the total visible border of all disks. We focus on drawings obtained by layering symbols on top of each other, also known as stacking drawings. We introduce decomposition techniques as well as several families of facet-defining inequalities, which are used to strengthen the ILP models that are supplied to a commercial solver. We demonstrate the effectiveness of our approach through a series of computational experiments using hundreds of instances generated from real demographic and geophysical data sets. To the best of our knowledge, we are the first to use ILP to tackle this problem, and the first to provide provably optimal symbol maps for those data sets.
Understanding differences between groups in a data set is one of the fundamental tasks in data analysis. As relevant applications accumulate, data-mining methods have been developed to specifically address the problem of group difference detection. Contrast set mining discovers group differences in the form of conjunction of feature-value pairs or items. In this paper, we incorporate absolute difference, relative difference, and statistical significance in our definition of a group difference, and develop a novel method named DIFF that uses the prefix-tree structure to compress the search space, follows a tree traversal procedure to discover the complete set of significant group differences, and employs efficient pruning strategies to expedite the search process. We conducted comprehensive experiments to compare our method with existing methods on completeness of results, pruning efficiency, and computational efficiency. The experiments demonstrate that our method guarantees completeness of results and achieves higher pruning efficiency and computational efficiency compared to STUCCO. In addition, our definition of group difference is more general than STUCCO. Our method is more effective than traditional approaches, such as classification trees, in discovering the complete set of significant group differences.
Following the flurry of recent theoretical work on cutting planes from two-row mixed integer group relaxations of a linear programming tableau, we report on computational tests to evaluate the strength of two-row cuts based on lattice-free triangles having more than one integer point on one side. A heuristic procedure to generate such triangles (referred to in the literature as “type 2” triangles) is presented, and then the coefficients of the integer variables are tightened by lifting. To test the effectiveness of triangle cuts, we compare the gap closed using Gomory mixed integer cuts for one round, the gap closed in one round using all the triangle cuts generated by our heuristic, and the gap closed by a small number of two-row split cuts. Our tests are carried out on randomly generated instances designed to represent different problem features by varying the number of integer nonbasic variables, bounds, nonnegativity constraints, and density, as well as on the classical MIPLIB instances. The outcome of this computational analysis is some insight into key characteristics of MIP instances whose presence makes two-row triangle cuts computationally effective. In particular, it appears to be necessary that the tableau row pairs are dense, and more subjectively that the nonbasic continuous variables are “important.” Unfortunately these characteristics seem to be rarely present among real-life instances, and more specifically the tableau rows of the MIPLIB instances are far from dense.
Completion time requirements are often imposed on a classification task. In practice, the desired completion time for classifying a subject may depend on its label (target) value. For example, a timely diagnosis is important for an illness that requires immediate medical attention. It is common in medical diagnoses, therefore, to set completion times based on the severity level of the illness. In this study, we use “label-dependent” completion time requirements to formulate a new classification problem for cost-sensitive decision tree induction by adding “late constraints” to control the rate of tardy classifications for each label value. Adding the late constraints generalizes and enriches the decision tree induction problem, but also poses a challenge to developing an efficient solution algorithm because the conventional approach based on the “divide-and-conquer” strategy cannot be used. We develop a novel algorithm that relaxes the late constraints and iteratively solves a series of cost-sensitive decision tree problems under systematically-generated late penalties. The results of an extensive numerical experiment show that the proposed algorithm is effective in finding the optimal or a near-optimal solution.
We explore the idea of obtaining bounds on the value of an optimization problem from a discrete relaxation based on binary decision diagrams (BDDs). We show how to construct a BDD that represents a relaxation of a 0-1 optimization problem, and how to obtain a bound for a separable objective function by solving a shortest (or longest) path problem in the BDD. As a test case we apply the method to the maximum independent set problem on a graph. We find that for most problem instances, it delivers tighter bounds in less computation time, than state-of-the-art integer programming software obtains by solving a continuous relaxation augmented with cutting planes.
A random vector X with given univariate marginals can be obtained by first applying the normal distribution function to each coordinate of a vector Z of correlated standard normals to produce a vector U of correlated uniforms over (0,1) and then transforming each coordinate of U by the relevant inverse marginal. One approach to fitting requires, separately for each pair of coordinates of X, the rank correlation, r(ρ), or the product-moment correlation, rL(ρ), where ρ is the correlation of the corresponding coordinates of Z, to equal some target r*. We prove the existence and uniqueness of a solution for any feasible target, without imposing restrictions on the marginals. For the case where r(ρ) cannot be computed exactly because of an infinite discrete support, the relevant infinite sums are approximated by truncation, and lower and upper bounds on the truncation errors are developed. With a function ˜r(ρ) defined by the truncated sums, a bound on the error r(ρ*) − r* is given, where ρ* is a solution to ˜r(ρ∗)=r∗. Based on this bound, an algorithm is proposed that determines truncation points so that the solution has any specified accuracy. The new truncation method has potential for significant work reduction relative to truncating heuristically, largely because as required accuracy decreases, so does the number of terms in the truncated sums. This is quantified with examples. The gain appears to increase with the heaviness of tails.
We give a fast and simple factor 2.74 approximation algorithm for the problem of choosing the k medians of the continuum of demand points defined by a convex polygon C. Our algorithm first surrounds the input region with a bounding box, then subdivides the bounding box into subregions with equal area. Simulation results on the convex hulls of the 50 states in the United States show that the practical performance of our algorithm is within 10% of the optimal solution in the vast majority of cases.
This paper considers a very general class of customer-to-resource assignment problems relevant to a variety of manufacturing contexts. This problem class addresses settings in which subsets of customer types share resource capacities as well as a fixed amount of capacity consumption, independent of production volume. More broadly, our model addresses cross-facility production limits and shared capacity consumption within each customer type. To solve these large-scale optimization problems, we apply a branch-and-price solution approach. This approach relies on an effective solution method for a novel class of nonlinear knapsack pricing problems. As our computational results demonstrate, despite the fact that the resulting master problem is not a simple set-partitioning problem, the problem's relaxation is sufficiently tight to produce an algorithm that significantly outperforms CPLEX for a wide range of problem parameter settings.
Replication is a widely-used technique in information retrieval and database systems for providing fault tolerance and reducing parallelization and processing costs. Combinatorial models based on hypergraph partitioning are proposed for various problems arising in information retrieval and database systems. We consider the possibility of using vertex replication to improve the quality of hypergraph partitioning. In this study, we focus on the constrained min-cut replication (CMCR) problem, where we are initially given a maximum replication capacity and a K-way hypergraph partition with an initial imbalance ratio. The objective in the CMCR problem is finding the optimal vertex replication sets for each part of the given partition such that the initial cut size of the partition is minimized, where the initial imbalance is either preserved or reduced under the given replication capacity constraint. In this study, we present a complexity analysis of the CMCR problem and propose a model based on a unique blend of coarsening and integer linear programming (ILP) schemes. This coarsening algorithm is derived from a novel utilization of the Dulmage-Mendelsohn decomposition. Experiments show that the ILP formulation coupled with the Dulmage-Mendelsohn decomposition-based coarsening provides high quality results in practical execution times for reducing the cut size of a given K-way hypergraph partition.
We consider a generalization of the classical quadratic assignment problem, where material flows between facilities are uncertain, and only upper and lower bounds are known for each flow. The objective is to find a minmax regret solution. We present an exact Benders decomposition algorithm based on two developed mathematical programming formulations and on the developed linearizations of master problems, and a heuristic based on using tabu search in the context of a Benders decomposition framework. Then, we develop a hybrid Benders decomposition approach that allows us to combine the speed of heuristics with the rigor and precision of the exact Benders method. We discuss the results of extensive computational experiments.
We discuss the method recently proposed by S. Chubanov [Chubanov S (2012a) A strongly polynomial algorithm for linear systems having a binary solution. Math. Programming 134(3):533–570] for the linear feasibility problem. We present new, concise proofs and geometric interpretations of some of his results. From our ideas we derive the first strongly polynomial time algorithm based on relaxation method techniques for special classes of linear feasibility problems. Under certain conditions, these results provide new proofs of classical results obtained by Tardos for combinatorial linear programs. The paper ends with some experimental investigations.
Recommender systems are being used to help users find relevant items from a large set of alternatives in many online applications. Most existing recommendation techniques have focused on improving recommendation accuracy; however, diversity of recommendations has also been increasingly recognized in research literature as an important aspect of recommendation quality. This paper proposes several optimization-based approaches for improving aggregate diversity of top-N recommendations, including a greedy maximization heuristic, a graph-theoretic approach based on maximum flow or maximum bipartite matching computations, and an integer programming approach. The proposed approaches are evaluated using real-world movie rating data sets and demonstrate substantial improvements in both diversity and accuracy as compared to the recommendation reranking approaches, which have been introduced in prior literature for the purposes of diversity improvement and were used for baseline comparisons in our study. The paper also discusses the computational complexity and the scalability of the proposed approaches, as well as the potential directions for future work.
We consider the stochastic obstacle scene problem wherein an agent needs to traverse a spatial arrangement of possible obstacles, and the status of the obstacles may be disambiguated en route at a cost. The goal is to find an algorithm that decides what and where to disambiguate en route so that the expected length of the traversal is minimized. We present a polynomial-time method for a graph-theoretical version of the problem when the associated graph is restricted to parallel avenues with fixed policies within the avenues. We show how previously proposed algorithms for the continuous space version can be adapted to a discrete setting. We propose a generalized framework encompassing these algorithms that uses penalty functions to guide the navigation in real time. Within this framework, we introduce a new algorithm that provides near-optimal results within very short execution times. Our algorithms are illustrated via computational experiments involving synthetic data as well as an actual naval minefield data set.Data, as supplemental material, are available at http://dx.doi.org/10.1287/ijoc.2013.0571.
We study optimization problems with value-at-risk (VaR) constraints. Because it lacks subadditivity, VaR is not a coherent risk measure and does not necessarily preserve the convexity. Thus, the problems we consider are typically not provably convex. As such, the conditional value-at-risk (CVaR) approximation is often used to handle such problems. Even though the CVaR approximation is known as the best convex conservative approximation, it sometimes leads to solutions with poor performance. In this paper, we investigate the CVaR approximation from a different perspective and demonstrate what is lost in this approximation. We then show that the lost part of this approximation can be remedied using a sequential convex approximation approach, in which each iteration only requires solving a CVaR-like approximation via certain Monte Carlo techniques. We show that the solution found by this approach generally makes the VaR constraints binding and is guaranteed to be better than the solution found by the CVaR approximation and moreover is empirically often globally optimal for the target problem. The numerical experiments show the effectiveness of our approach.
Models incorporating more realistic models of customer behavior, as customers choosing from an offer set, have recently become popular in assortment optimization and revenue management. The dynamic program for these models is intractable and approximated by a deterministic linear program called the choice deterministic linear program (CDLP), which has an exponential number of columns. Column generation has been proposed but finding an entering column is NP-hard when segment consideration sets overlap. In this paper we propose a new approach called segment-based deterministic concave program (SDCP) based on segments and their consideration sets. SDCP is a relaxation of CDLP and hence forms a looser upper bound on the dynamic program, but coincides with CDLP for the case of nonoverlapping segments. If the number of elements in a consideration set for a segment is not very large, SDCP can be applied to any discrete-choice model of consumer behavior. We tighten the SDCP bound by (i) simulations, called the randomized concave programming method, and (ii) by adding cuts to a recent compact formulation (SBLP) of the problem for a latent multinomial-choice model (MNL) of demand. This latter approach turns out to be very effective, essentially obtaining CDLP value, even for overlapping segments. By formulating the problem as a separation problem, we give insight into why CDLP is easy for the MNL with nonoverlapping consideration sets and why generalizations of MNL pose difficulties. Numerical conclusions that we derive from the present paper are the following: (a) The randomized linear programming approach that obtains significant tightening of the linear program upper bound under an older independent-class model seems to have relatively little effect for the choice case; (b) for the MNL choice model, the SBLP+ formulation we give here for overlapping segments is very fast and is potentially scalable to industrial-size problems.
We address a variant of the Euclidean traveling salesman problem known as the close-enough traveling salesman problem (CETSP), where the traveler visits a node if it enters a compact neighborhood set of that node. We formulate a mixed-integer programming model based on a discretization scheme for the problem. Both lower and upper bounds on the optimal CETSP tour length can be derived from the solution of this model, and the quality of the bounds obtained depends on the granularity of the discretization scheme. Our approach first develops valid inequalities that enhance the bound and solvability of this formulation. We then provide two alternative formulations, one that yields an improved lower bound on the optimal CETSP tour length, and one that greatly improves the solvability of the original formulation by recasting it as a two-stage problem amenable to decomposition. Computational results demonstrate the effectiveness of the proposed methods.
In Web-based environments, a site has the ability to recommend multiple items to a customer in each interaction. Traditionally, rules used to make recommendations either have single items in their consequents or have conjunctions of items in their consequents. Such rules may be of limited use when the site wishes to maximize the likelihood of the customer being interested in at least one of the items recommended in each interaction (with a session comprising multiple interactions). Rules with disjunctions of items in their consequents and conjunctions of items in their antecedents are more appropriate for such environments. We refer to such rules as disjunctive consequent rules. We have developed a novel mining algorithm to obtain such rules. We identify several properties of disjunctive consequent rules that can be used to prune the search space when mining such rules. We demonstrate that the pruning techniques drastically reduce the proportion of disjunctive rules explored, with the pruning effectiveness increasing rapidly with an increase in the number of items to be recommended. We conduct experiments to compare the use of disjunctive rules with that of traditional (conjunctive) association rules on several real-world data sets and show that the accuracies of recommendations made using disjunctive consequent rules are significantly higher than those made using traditional association rules. We also compare the disjunctive consequent rules approach with two other state-of-the-art recommendation approaches—collaborative filtering and matrix factorization. Its performance is generally superior to both these techniques on two transactional data sets. The relative performance on a very sparse click-stream data set is mixed. Its performance is inferior to that of collaborative filtering and superior to that of matrix factorization for that data set.
We examine some properties of the points produced by certain classes of long-period linear multiple recursive random number generators proposed by L.-Y. Deng and his co-authors in several papers. These generators have their parameters selected in special ways to make the implementation faster. We show that as a result, the points produced by these generators have a poor lattice structure, and a poor initialization of the state can have long-lasting impact, because of the limited diffusion capacity of the recurrence.
Classical supervised machine learning techniques have been explored for semantically annotating unstructured textual data such as consumers' comments archived at social media websites to extract business intelligence. However, these techniques often require a large number of manually labeled training examples to produce accurate annotations. Several active learning approaches that are designed based on probabilistic sequence models have been explored to minimize the number of labeled training examples for semantic annotation tasks. Recent research has shown that large-margin classifiers are viable alternatives to automated semantic annotation, given their strong generalization capabilities and the ability to process high-dimensional data. However, the existing active learning methods that are designed for probabilistic sequence models cannot be easily adapted and applied to large-margin classifiers. The main contribution of this paper is the development of novel active learning methods for large-margin classifiers to fill the aforementioned research gap. In particular, we propose an innovative perspective of taking active learning as a search of optimal parameters for large-margin classifiers. A rigorous evaluation involving two benchmark tests and an empirical test based on real-world data extracted from Amazon.com reveals that the proposed active learning methods can train effective classifiers with significantly fewer training examples while achieving similar annotation performance, compared to a typical state-of-the-art classifier that only uses several labeled training examples. More specifically, one of our proposed active learning methods can reduce the number of training examples by 19.74% at the 68% level of F1 when compared to the best baseline method, as evaluated based on the Amazon data set. Our research opens the door to the application of intelligent semantic annotation techniques to support real-world applications such as automatically analyzing consumer comments for customer relationship management.
Traditional regression assumes that the only data available are measurements of the value of the dependent variable for each combination of values for the independent variable. However, in many settings in stochastic (Monte Carlo) simulation, directly estimated derivative information is also available via techniques such as perturbation analysis or the likelihood ratio method. In this paper, we investigate potential modeling improvements that can be achieved by exploiting this additional gradient information in the regression setting. Using least squares and maximum likelihood estimation, we propose various direct gradient augmented regression (DiGAR) models that incorporate direct gradient estimators, starting with a one-dimensional independent variable and then extending to multidimensional input. For some special settings, we are able to characterize the variance of the estimated parameters in DiGAR and compare them analytically with the standard regression model. For a more typical stochastic simulation setting, we investigate the potential effectiveness of the augmented model by comparing it with standard regression in fitting a functional relationship for a simple queueing model, including both one-dimensional and four-dimensional examples. The preliminary empirical results are quite encouraging, as they indicate how DiGAR can capture trends that the standard model would miss. Even in queueing examples where there is a high correlation between the output and the gradient estimators, the basic DiGAR model that does not explicitly account for these correlations performs significantly better than the standard regression model.
We develop a population-based algorithm for the optimization of multiple, nonconvex, nondifferentiable, and possibly discontinuous objective functions. The algorithm employs Markov kernels, Hit-and-Run, and Pattern Hit-and-Run for exploration of the solution space and Pareto ordering rules for the selection of the population and to update the approximate Pareto optimal list. Our multiobjective interacting particle algorithm asymptotically converges to the stationary distribution associated with the Pareto ordering rules. We present numerical benchmark results on test problems.
This paper addresses the static aircraft sequencing problem over a mixed-mode single runway (or closely interacting parallel runways), which commonly constitutes a critical bottleneck at airports. In contrast with disjunctive formulations, our modeling approach takes advantage of the underlying structure of an asymmetric traveling salesman problem with time-windows. This enables the development of efficient preprocessing and probing procedures, and motivates the derivation of several classes of valid inequalities along with partial convex hull representations to enhance problem solvability via tighter reformulations. The lifted model is further embedded within the framework of two proposed heuristics that are compared against the traditional first-come first-served (FCFS) heuristic with landing priority: an optimized FCFS policy (OFCFS) and a threshold-based suboptimized heuristic (TSH) with an a priori fixing of the relative order of aircraft that are sufficiently time-separated. Computational results using real data based on Doha International Airport (DOH) as well as simulated instances are reported to demonstrate the efficacy of the proposed exact and heuristic solution methods. In particular, for the DOH instances, heuristics OFCFS and TSH achieved an attractive runway utilization (4.3% and 5.0% makespan reduction, respectively, over the base FCFS policy with landing priority), while exhibiting limited aircraft position deviations (0.45 and 0.49 deviations on average, respectively, from the base FCFS positions with landing priority, with similar results being obtained for the simulated instances). The superiority of the proposed optimization models over previous disjunctive formulations is also demonstrated for challenging problem instances, resulting in over 50% CPU savings for the larger instances in our test-bed.
We consider a class of linear programs involving a set of covering constraints of which at most k are allowed to be violated. We show that this covering linear program with violation is strongly 𝒩𝒫-hard. To improve the performance of mixed-integer programming-based schemes for these problems, we introduce and analyze a coefficient strengthening scheme, adapt and analyze an existing cutting plane technique, and present a branching technique. Through computational experiments, we empirically verify that these techniques are significantly effective in improving solution times over the CPLEX mixed-integer programming solver. In particular, we observe that the proposed schemes can cut down solution times from as much as six days to under four hours.
Matching is widely used in the estimation of treatment effects in observational studies. However, the matching paradigm may be too restrictive in many cases because exact matches often do not exist in the available data. One mechanism for overcoming this issue is to relax the requirement of exact matching on some or all of the covariates (attributes that may affect the response to treatment) to a requirement of balance on the covariate distributions for the treatment and control groups. The balance optimization subset selection (BOSS) model can be used to identify a control group featuring optimal covariate balance. This paper explores the relationship between the matching and BOSS models and shows how BOSS subsumes matching. Complexity and approximation results are presented for the resulting models. Computational results demonstrate some of the important trade-offs between matching and BOSS.Data, as supplemental material, are available at http://dx.doi.org/10.1287/ijoc.2013.0583.There is a video associated with this paper. Click here to view the Video Overview. To save the file, right click and choose “Save Link As” from the menu.
Solution methods are presented for a mixed-integer program (MIP) associated with a method for constrained discrimination. In constrained discrimination, one wishes to maximize the probability of correct classification subject to intergroup misclassification limits. The misclassification limits are satisfied by allowing the placement of observations in a reserved judgment group. The approach investigated here involves modifying a standard classification rule by solving an optimization problem. A polynomial-time algorithm for solving the problem is given for two-group discrimination. The decision problem upon which the optimization problem is based is shown to be NP complete for a general number of groups. For three or more groups, an MIP is used to solve the problem. Solution methods incorporating cutting planes from conflict graphs are presented for solving instances in a branch-and-bound framework. These methods are used to enhance industry-standard software, and are shown to provide as much as a 20-fold reduction in computational time over the software alone. Computational experiments illustrate the tradeoff between misclassification rates and reserved judgment rates. Some base classifiers are not well suited to be modified to a constrained discrimination rule. The method for constrained discrimination studied here performs particularly well in the presence of class imbalance. For certain other data sets, however, the method is outperformed by a simple centroid method.
Sentiment classification is one of the most extensively studied problems in sentiment analysis, and supervised learning methods, which require labeled data for training, have been proven quite effective. However, supervised methods assume that the training domain and the testing domain share the same distribution; otherwise, accuracy drops dramatically. Although this does not pose problems when training data are readily available, in some circumstances, labeled data is quite expensive to acquire. For instance, if we want to detect sentiment from Tweets or Facebook comments, the only way to acquire is to manually label it, and this is prohibitively burdensome and time-consuming. In this paper, we propose a hybrid approach that integrates the sentiment information from source-domain labeled data and a set of preselected sentiment words to solve this problem. The experimental results suggest that our method statistically outperforms the state of the art and even, in some cases, surpasses the in-domain gold standard.
Implantable cardioverter defibrillators (ICDs) include small, battery-powered generators, the longevity of which depends on a patient's rate of consumption. Generator replacement, however, involves risks, including death. Hence, a trade-off exists between prematurely exposing the patient to these risks and allowing for the possibility that the device is unable to deliver therapy when needed. Currently, replacements are performed using a one-size-fits-all approach. Here, we develop a Markov decision process model to determine patient-specific optimal replacement policies as a function of patient age and the remaining battery capacity. We analytically establish that the optimal policy is of threshold-type in the remaining capacity, but not necessarily in patient age. Based on clinical data, we conduct a large computational study that suggests that under the optimal policy, patients undergoing initial implantation at age 30–40, 41–60, and 61–80 see an approximate decrease in the total expected number of replacements of 8%–14%, 8%–15% and 8%–19%, respectively, while achieving the same or greater expected lifetime.
We consider a least squares estimator for estimating a convex function f*: [0, 1]d → ℝ with bounded subgradients. A rate at which the sum of squared differences between the estimator and the true function f* converges to zero is computed. This work sheds light on computing the convergence rate of the multidimensional convex regression estimator.
The last decade has witnessed an explosion in the modeling of complex systems. Predominantly, graphs are used to represent these systems. The problem of detecting overlapping clusters in graphs is of utmost importance. We present a novel definition of overlapping clusters. A noncooperative game is proposed such that the equilibrium conditions of the game correspond to the clusters in the graph. Several properties of the game are analyzed and exploited to show the existence of a pure Nash equilibrium (NE) and compute it effectively. We present two algorithms to compute NE and prove their convergence. Empirically, the running times of both algorithms are nearly linear in the number of edges. Also, one of the algorithms can be readily parallelized, making it scalable. Finally, our approach is compared with existing overlapping cluster detection algorithms and validated on several artificial and real data sets.
We present exact algorithms for solving the minimum connected dominating set problem in an undirected graph. The algorithms are based on two approaches: a Benders decomposition algorithm and a branch-and-cut method. We also develop a hybrid algorithm that combines these two approaches. Two variants of each of the three resulting algorithms are considered: a stand-alone version and an iterative probing variant. The latter variant is based on a simple property of the problem, which states that if no connected dominating set of a given cardinality exists, then there are no connected dominating sets of lower cardinality. We present computational results on a large set of instances from the literature.
We consider the short-term production scheduling problem for a network of multiple open-pit mines and ports. Ore produced at each mine is transported by rail to a set of ports and blended into signature products for shipping. Consistency in the grade and quality of production over time is critical for customer satisfaction, whereas the maximal production of blended products is required to maximise profit. In practice, short-term schedules are formed independently at each mine, tasked with achieving the grade and quality targets outlined in a medium-term plan. However, because of uncertainty in the data available to a medium-term planner and the dynamics of the mining environment, such targets may not be feasible in the short term. We present a decomposition-based heuristic for this short-term scheduling problem in which the grade and quality goals assigned to each mine are collaboratively adapted—ensuring the satisfaction of blending constraints at each port and exploiting opportunities to maximise production in the network that would otherwise be missed.
In many fault-detection problems, we want to identify defective items from a set of n items using the minimum number of tests. Group testing is a scenario in which each test is on a subset of items and determines whether the subset contains at least one defective item. In practice, the number d of defective items is often unknown in advance. In this paper, we present a new algorithm for the above group testing problem and prove that it has very good performance guarantee. More specifically, the number of tests used by the new algorithm is bounded from above by d log(n/d) + 3d + O(log2d). The new algorithm is designed based on a zig-zag approach that has not been studied before and is intuitive and easy to implement. When 0 < d < ρ0n where ρ0 = 1 − 4/e2 = 0.45…, which holds for most practical applications, our new algorithm has better performance guarantee than any previous best result. Computational results show that the new algorithm has very good practical performances.
We consider in this paper quadratic programming problems with cardinality and minimum threshold constraints that arise naturally in various real-world applications such as portfolio selection and subset selection in regression. This class of problems can be formulated as mixed-integer 0-1 quadratic programs. We propose a new semidefinite program (SDP) approach for computing the “best” diagonal decomposition that gives the tightest continuous relaxation of the perspective reformulation of the problem. We also give an alternative way of deriving the perspective reformulation by applying a special Lagrangian decomposition scheme to the diagonal decomposition of the problem. This derivation can be viewed as a “dual” method to the convexification method employing the perspective function on semicontinuous variables. Computational results show that the proposed SDP approach can be advantageous for improving the performance of mixed-integer quadratic programming solvers when applied to the perspective reformulations of the problem.
Branch-and-price algorithms for the graph coloring problem use an exponentially sized independent set-based integer programming formulation to produce usually tight lower bounds to enable more aggressive pruning in the branch-and-bound tree. One major problem inherent to any branch-and-price scheme for graph coloring is that to avoid destroying the pricing problem structure during column generation, difficult-to-implement branching rules that modify the underlying graph must be used. This paper proposes an alternative branching strategy that does not change the graph to solve the pricing problem but rather modifies the search tree to require fewer calls to difficult instances of the pricing problem. This approach, called wide branching, generates many subproblems at each node in the branch-and-price tree; this significantly reduces the length of any path through the search tree. In contrast, traditional deep branching only creates two subproblems per node, assigning a variable to either 0 or 1. A delayed branching procedure is introduced that prevents the branching factor at any particular node from growing too large in this scheme. Finally, computational results are presented that show the wide branching strategy to be competitive with state-of-the-art graph coloring solvers.
Linear active-power-only power flow approximations are pervasive in the planning and control of power systems. However, AC power systems are governed by a system of nonlinear nonconvex power flow equations. Existing linear approximations fail to capture key power flow variables, including reactive power and voltage magnitudes, both of which are necessary in many applications that require voltage management and AC power flow feasibility. This paper proposes novel linear-programming models (the LPAC models) that incorporate reactive power and voltage magnitudes in a linear power flow approximation. The LPAC models are built on a polyhedral relaxation of the cosine terms in the AC equations as well as Taylor approximations of the remaining nonlinear terms. Experimental comparisons with AC solutions on a variety of standard IEEE and Matpower benchmarks show that the LPAC models produce accurate values for active and reactive power, phase angles, and voltage magnitudes. The potential benefits of the LPAC models are illustrated on two “proof-of-concept” studies in power restoration and capacitor placement.
We consider a class of packing problems with uncertain data, which we refer to as the chance-constrained binary packing problem. In this problem, a subset of items is selected that maximizes the total profit so that a generic packing constraint is satisfied with high probability. Interesting special cases of our problem include chance-constrained knapsack and set packing problems with random coefficients. We propose a problem formulation in its original space based on the so-called probabilistic covers. We focus our solution approaches on the special case in which the uncertainty is represented by a finite number of scenarios. In this case, the problem can be formulated as an integer program by introducing a binary decision variable to represent feasibility of each scenario. We derive a computationally efficient coefficient strengthening procedure for this formulation, and demonstrate how the scenario variables can be efficiently projected out of the linear programming relaxation. We also study how methods for lifting deterministic cover inequalities can be leveraged to perform approximate lifting of probabilistic cover inequalities. We conduct an extensive computational study to illustrate the potential benefits of our proposed techniques on various problem classes.
Determination of a protein's structure can facilitate an understanding of how the structure changes when that protein combines with other proteins or smaller molecules. In this paper we study a semidefinite programming (SDP) relaxation of the (NP-hard) side chain positioning problem presented in Chazelle et al [Chazelle B, Kingsford C, Singh M (2004) A semidefinite programming approach to side chain positioning with new rounding strategies. INFORMS J. Comput. 16:380-392]. We show that the Slater constraint qualification (strict feasibility) fails for the SDP relaxation. We then show the advantages of using facial reduction to regularize the SDP. In fact, after applying facial reduction, we have a smaller problem that is more stable both in theory and in practice. We include cutting planes to improve the rounded SDP approximate solutions.
We study the lot-sizing problem with piecewise concave production costs and concave holding costs. This problem is a generalization of the lot-sizing problem with quantity discounts, minimum order quantities, capacities, overloading, subcontracting or a combination of these. We develop a dynamic programming algorithm to solve this problem and answer an open question in the literature: we show that the problem is polynomially solvable when the breakpoints of the production cost function are time invariant and the number of breakpoints is fixed. For the special cases with capacities and subcontracting, the time complexity of our algorithm is as good as the complexity of algorithms available in the literature. We report the results of a computational experiment where the dynamic programming is able to solve instances that are hard for a mixed-integer programming solver. We enhance the mixed-integer programming formulation with valid inequalities based on mixing sets and use a cut-and-branch algorithm to compute better bounds. We propose a state space reduction–based heuristic algorithm for large instances and show that the solutions are of good quality by comparing them with the bounds obtained from the cut-and-branch.
In this paper, we study whether cuts obtained from two simplex tableau rows at a time can strengthen the bounds obtained by Gomory mixed-integer (GMI) cuts based on single tableau rows. We also study whether cross and crooked cross cuts, which generalize split cuts, can be separated in an effective manner for practical mixed-integer programs (MIPs) and can yield a nontrivial improvement over the bounds obtained by split cuts. We give positive answers to both these questions for MIPLIB 3.0 problems. Cross cuts are a special case of the t-branch split cuts studied by Li and Richard [Li Y, Richard J-PP (2008) Cook, Kannan and Schrijvers's example revisited. Discrete Optim. 5:724–734]. Split cuts are 1-branch split cuts, and cross cuts are 2-branch split cuts. Crooked cross cuts were introduced by Dash, Günlük, and Lodi [Dash S, Günlük O, Lodi A (2010) MIR closures of polyhedral sets. Math Programming 121:33–60] and were shown to dominate cross cuts by Dash, Günlük, and Molinaro [Dash S, Günlük O, Molinaro M (2012b) On the relative strength of different generalizations of split cuts. IBM Technical Report RC25326, IBM, Yorktown Heights, NY].
The facility layout problem is the problem of assigning facilities to locations. We study the case of limited machine capacity and hence multiple copies of each machine type. We take into account stochastic demand, described by several types of jobs (i.e., sequences of machine types), each with an uncertain demand level. We develop a heuristic framework allowing us to find good solutions to the stochastic case whenever it is possible to solve the corresponding deterministic quadratic assignment problem (QAP), exactly or heuristically. Athough the QAP is a very hard problem in its own right, our approach allows randomness (and hence relevance) to be added at only a marginal increase in computational costs.Data, as supplemental material, are available at http://dx.doi.org/10.1287/ijoc.2014.0599.
This paper proposes a heuristic for districting problems arising in an arc routing context. The aim is to design districts by amalgamating edges of a graph as opposed to cells. Solutions must satisfy two hard criteria (complete and exclusive assignment as well as connectedness) and several soft criteria (balance, small deadheading, local compactness, and global compactness). The latter criteria are amalgamated into a weighted objective. The proposed heuristic applies a construction procedure followed by a tabu search improvement phase in which several subroutines are defined and selected according to a roulette wheel mechanism, as in adaptive large neighborhood search. Extensive tests conducted on instances derived from real-world street data confirm the efficiency of the proposed methodology.
We present two decomposition algorithms for single product deep-sea maritime inventory routing problems (MIRPs) that possess a core substructure common in many real-world applications. The problem involves routing vessels, each belonging to a particular vessel class, between loading and discharging ports, each belonging to a particular region. Our algorithms iteratively solve a MIRP by zooming out and then zooming in on the problem. Specifically, in the “zoomed out” phase, we solve a first-stage master problem in which aggregate information about regions and vessel classes is used to route vessels between regions, while only implicitly considering inventory and capacity requirements, berth limits, and other side constraints. In the “zoomed in” phase, we solve a series of second-stage subproblems, one for each region, in which individual vessels are routed through each region and load and discharge quantities are determined. Computational experience shows that an integrated approach that combines these two algorithms is vastly superior to solving the problem directly with a commercial mixed-integer programming solver.
Estimating the sensitivities of portfolio credit risk with respect to the underlying model parameters is an important problem for credit risk management. In this paper, we consider performance measures that may be expressed as an expectation of a performance function of the portfolio credit loss and derive closed-form expressions of its sensitivities to the underlying parameters. Our results are applicable to both idiosyncratic and macroeconomic parameters and to performance functions that may or may not be continuous. Based on the closed-form expressions, we first develop an estimator for sensitivities, in a general framework, that relies on the kernel method for estimation. The unified estimator allows us to further derive two general forms of the estimators by using conditioning techniques on either idiosyncratic or macroeconomic factors. We then specialize our results to develop faster estimators for three popular classes of models used for portfolio credit risk: latent variable models, Bernoulli mixture models, and doubly stochastic models.
Access control mechanisms in software systems administer user privileges by granting users permission to perform certain operations while denying unauthorized access to others. Such mechanisms are essential to ensure that important business functions in an organization are conducted securely and smoothly. Currently, the dominant access control approach in most major software systems is role-based access control. In this approach, permissions are first assigned to roles, and users acquire permissions by becoming members of certain roles. However, given the dynamic nature of organizations, a fixed set of roles usually cannot meet the demands that users (existing or new) have to conduct business.The typical response to this problem is to myopically create new roles to meet immediate demand that cannot be satisfied by an existing set of roles. This ad hoc creation of roles invariably leads to a proliferation in the number of roles with the accompanying administrative overhead. Based on discussions with practitioners, we propose a role refinement scheme that reconstructs a system of roles to reduce the cost of role management. We first show that the role-refinement problem is strongly NP-hard and then provide two polynomial-time approximation algorithms (a greedy algorithm and a randomized rounding algorithm) and establish their performance guarantees. Finally, numerical experiments—based on a real data set from a firm's enterprise resource planning system—are conducted to demonstrate the applicability and performance of our refinement scheme.
The distribution of the number of renewals for bulk arrivals in continuous time is calculated using an algorithm employed through MAPLE software. These numerical results are acquired by considering rational as well as nonrational Laplace transforms and Padé-approximated Laplace transforms for the distributions of interrenewal times. Further, through the use of Laplace transforms an elegant solution to determine the asymptotic results for the first and second moments of the number of bulk renewals is presented. These derivations help validate the numerical results and are an extension of previous work by the authors regarding single-arrival renewal theory in discrete time.
In this paper, we extend our former investigation on conceiving reliable fixed point-to-point wireless networks under outage probability constraints. We consider the problem of determining the minimum cost bandwidth assignment of a network, while guaranteeing a reliability level of the solution. If the optimal bandwidth assignment and routing of traffic demands are accomplished, the reliability criterion requires that network flows remain feasible with high probability, regarding that the performance of microwave links is prone to variations due to external factors, e.g., weather. We introduce a chance-constrained programming approach to tackle this problem and we present reformulations to standard integer linear programming models, including a budget-constrained formulation. To improve the solving performance, we propose new valid inequalities and a primal heuristic. Computational results present a performance analysis of the valid inequalities and the heuristic. Further, the outperformance of the novel model compared to more traditional approaches is documented.
On behalf of the Editorial Board, I would like to thank the following people, who acted as Reviewers during the past year.
We consider a network design application that is modeled as the two-level network design problem under uncertainty. In this problem, one of the two available technologies can be installed on each edge and all customers of the network need to be served by at least the lower level (secondary) technology. The decision maker is confronted with uncertainty regarding the set of primary customers, i.e., the set of nodes that need to be served by the higher level (primary) technology. A set of discrete scenarios associated with the possible realizations of primary customers is available. The network is built in two stages. In the first stage the network topology must be determined. One may decide to install the primary technology on some of the edges in the first stage, or one can wait to see which scenario will be realized, in which case, edges with the installed secondary technology may be upgraded, if necessary to primary technology, but at higher recovery cost. The overall goal then is to build a “recoverable robust” spanning tree in the first stage that serves all customers by at least the lower level technology, and that minimizes the first-stage installation cost plus the worst-case cost needed to upgrade the edges of the selected tree, so that the primary customers of each scenario can be served using the primary technology. We discuss the complexity of the problem, provide mixed-integer programming models, and develop a branch-and-cut algorithm to solve it. Our extensive computational experiments demonstrate the efficacy of our approach.
Statistical database auditing is the process of checking aggregate queries that are submitted in a continuous manner, to prevent inference disclosure. Compared to other data protection mechanisms, auditing has the features of flexibility and maximum information. Auditing is typically accomplished by examining responses to past queries to determine whether a new query can be answered. It has been recognized that query denials release information and can cause data disclosure. This paper proposes an auditing mechanism that is free of query denial threat and applicable to mixed types of aggregate queries, including sum, max, min, deviation, etc. The core ideas are (i) deriving the complete information leakage from each query denial and (ii) carrying the complete leaked information derived from past answered and denied queries to audit each new query. The information leakage deriving problem can be formulated as a set of parametric optimization programs, and the whole auditing process can be modeled as a series of convex optimization problems.
Establishing family relationships, such as parentage and sibling relationships, is fundamental in biological research, especially in wild species, as they are often important to understanding evolutionary, ecological, and behavioral processes. Because it is commonly impossible to determine familial relationships from field observations alone, the reconstruction of sibling relationships often depends on informative genetic markers coupled with accurate sibling reconstruction algorithms. Most studies in the literature reconstruct sibling relationships using methods that are based on either statistical analyses (i.e., likelihood estimation) or combinatorial concepts (i.e., Mendelian inheritance laws) of genetic data. We present a novel computational framework that integrates both combinatorial concepts and statistical analyses into one sibling reconstruction optimization model. To solve this integrated model, we propose a column-generation approach with a branch-and-price method. Under the assumption of parsimonious reconstruction, the master problem is to find the minimum set of sibling groups to cover the tested population. Pricing subproblems, which include both statistical similarity and combinatorial concepts of genetic data, are iteratively solved to generate high-quality sibling group candidates. Tested on real biological data sets, our approach efficiently provides reconstruction results that are more accurate than those provided by other state-of-the-art reconstruction algorithms.
In this paper, a cutting-plane neighborhood structure is proposed for the fixed-charge capacitated multicommodity network design (CMND) problem. In the proposed structure, different strategies are used to select an open arc in the incumbent solution to be closed. Then a linear programming (LP) model is generated on the basis of the modified incumbent solution by relaxing binary variables and adding new constraints. The generated LP solution is improved using different cutting-plane inequalities. Subsequently, a new sub-mixed integer programming (MIP) model is created by fixing a number of variables in the generated LP solution. Then the local branching algorithm is used to solve the sub-MIP model and its solution is considered as a neighboring solution. A tabu search algorithm is used to evaluate the proposed neighborhood structure. To tune the parameters of the tabu search algorithm, we have used the design of experiments method. Standard problems with different sizes are employed to evaluate the proposed tabu search algorithm. Results show the efficiency and effectiveness of the tabu search algorithm compared to the best methods found in the literature.
In this work, we propose an algorithmic approach to improve mixed-integer models that are originally formulated as convex generalized disjunctive programs (GDPs). The algorithm seeks to obtain an improved continuous relaxation of the mixed-integer linear and mixed-integer nonlinear programming (MILP/MINLP) model reformulation of the GDP while limiting the growth in the problem size. There are three main stages that form the basis of the algorithm. The first one is a presolve, consequence of the logic nature of GDP, which allows us to reduce the problem size, find good relaxation bounds, and identify properties that help us determine where to apply a basic step. The second stage is the iterative application of basic steps, selecting where to apply them and monitoring the improvement of the formulation. Finally, we use a hybrid reformulation of GDP that seeks to exploit both of the advantages attributed to the two common GDP-to-MILP/MINLP transformations, the Big-M, and the Hull reformulation. We illustrate the application of this algorithm with several examples. The results show the improvement in the problem formulations by generating models with improved relaxed solutions and relatively small growth of continuous variables and constraints. The algorithm generally leads to reduction in the solution times.
We derive a new lower bound for the bandwidth of a graph that is based on a new lower bound for the min-cut problem. Our new semidefinite programming relaxation of the min-cut problem is obtained by strengthening the known semidefinite programming relaxation for the quadratic assignment problem (or for the graph partition problem) by fixing two vertices in the graph; one on each side of the cut. Fixing results in several smaller subproblems that need to be solved to obtain the new bound. To efficiently solve these subproblems we exploit symmetry in the data; that is, both symmetry in the min-cut problem and symmetry in the graphs. To obtain upper bounds for the bandwidth of graphs with symmetry, we develop a heuristic approach based on the well-known reverse Cuthill–McKee algorithm, and that improves significantly its performance on the tested graphs. Our approaches result in the best known lower and upper bounds for the bandwidth of all graphs under consideration, i.e., Hamming graphs, 3-dimensional generalized Hamming graphs, Johnson graphs, and Kneser graphs, with up to 216 vertices.
This paper presents a solution scheme for a class of multistage stochastic programs (possibly mixed-integer at all stages) in which a hierarchy of decisions emerges. A special structure, common to many strategic problems affected by uncertainty, allows decomposing the problem into a master problem and many independent linear programming subproblems, facilitating the isolation and reduction of the complicating mixed-integer component of the problem. Specialized (possibly heuristic) procedures can be used for solving the master problem while subproblems can be efficiently solved to optimality. We adapt and test the decomposition scheme for a case of the maritime fleet renewal problem, whose real life instances cannot be solved by means of commercial off-the-shelf solvers.
We consider the role of security in the maintenance of an automated system, controlled by a network of sensors and simple computing devices. Such systems are widely used in transportation, utilities, healthcare, and manufacturing. Devices in the network are subject to traditional failures that can lead to a larger system failure if not repaired. However, the devices are also subject to security breaches that can also lead to catastrophic system failure. These security breaches could result from either cyber attacks (such as viruses, hackers, or terrorists) or physical tampering. We formulate a stochastic model of the system to examine the repair policies for both real and suspected failures. We develop a linear programming-based model for optimizing repair priorities. We show that, given the state of the system, the optimal repair policy follows a unique threshold indicator (either work on the real failures or the suspected ones). We examine the behavior of the optimal policy under different failure rates and threat levels. Finally, we examine the robustness of our model to violations in the underlying assumptions and find the model remains useful over a range of operating assumptions.
We introduce the bi-objective prize-collecting Steiner tree problem, whose goal is to find a subtree considering the conflicting objectives of minimizing the edge costs for building that tree, and maximizing the collected node revenues. We consider five iterative mixed-integer programming (MIP) frameworks that identify the complete Pareto front, i.e., one efficient solution for every point on the Pareto front. More precisely, the following methods are studied: an ε-constraint method, a two-phase method, a binary search in the objective space, a weighted Chebyshev norm method, and a method of Sylva and Crema. We also investigate how to exploit and recycle information gained during these iterative MIP procedures to accelerate the solution process. We consider (i) additional strengthening valid inequalities, (ii) procedures for initializing feasible solutions (using a solution pool), (iii) procedures for recycling violated cuts (using a cut pool), and (iv) guiding the branching process by previously detected Pareto optimal solutions. This work is a first study on exact approaches for solving the bi-objective prize-collecting Steiner tree problem. Standard benchmark instances from the literature are used to assess the efficacy of the proposed methods.
Research on due date-oriented objectives in the parallel machine environment is at best scarce compared to objectives such as minimizing the makespan or the completion time-related performance measures. Moreover, almost all existing work in this area is focused on the identical parallel machine environment. In this study, we leverage on our previous work on the single machine total weighted tardiness (TWT) and total weighted earliness/tardiness (TWET) problems and develop a new preemptive relaxation for both problems on a bank of unrelated parallel machines. The key contribution of this paper is devising a computationally effective Benders decomposition algorithm to solve the preemptive relaxation formulated as a mixed-integer linear program. The optimal solution of the preemptive relaxation provides a tight lower bound. Moreover, it offers a near-optimal partition of the jobs to the machines. We then exploit recent advances in solving the nonpreemptive single-machine TWT and TWET problems for constructing nonpreemptive solutions of high quality to the original problem. We demonstrate the effectiveness of our approach with instances of up to five machines and 200 jobs.
Given a bound, the bounded-diameter minimum spanning tree (BDMST) problem seeks a spanning tree of minimum total weight with a diameter not exceeding the given diameter bound. Depending on the tightness of the bound, optimal solutions have different structures. For loose bounds, optimal solutions are similar to the much easier minimum spanning tree problem, and greedy heuristics perform best. In contrast, these approaches fail for tight diameter bounds. This paper investigates how the structure of good solutions, and in particular their backbones, change depending on the diameter bound. Two new heuristics are then designed to overcome the shortcomings of existing approaches; required parameters are investigated; and the paper presents performance results for Euclidean BDMST instances.
This paper explores techniques for solving the maximum clique and vertex coloring problems on very large-scale real-life networks. Because of the size of such networks and the intractability of the considered problems, previously developed exact algorithms may not be directly applicable. The proposed approaches aim to reduce the network instances to a size that is tractable for existing solvers, while preserving optimality. Two clique relaxation structures are exploited for this purpose. In addition to the known k-core structure, a newly introduced clique relaxation, k-community, is used to further reduce the instance size. Experimental results on real-life graphs (collaboration networks, P2P networks, social networks, etc.) show the proposed procedures to be effective by finding, for the first time, exact solutions for instances with over 18 million vertices.
This paper considers the minimum k-connected d-dominating set problem, which is a fault-tolerant generalization of the minimum connected dominating set (MCDS) problem. Three integer programming formulations based on vertex cuts are proposed (depending on whether d < k, d = k, or d > k) and their integer hulls are studied. The separation problem for the vertex-cut inequalities is a weighted vertex-connectivity problem and is polytime solvable, meaning that the LP relaxation can be solved in polytime despite having exponentially many constraints. A new class of valid inequalities—r-robust vertex-cut inequalities—is introduced and is shown to induce exponentially many facets. Finally, a lazy-constraint approach is shown to compare favorably with existing approaches for the MCDS problem (the case k = d = 1), and is in fact the fastest in literature for standard test instances. A key subroutine is an algorithm for finding an inclusion-wise minimal vertex cut in linear time. Computational results for (k, d) = (2,1), (2,2), (3,3), (4,4) are provided as well.
The INFORMS Journal on Computing Book Review section covers books on subjects at the interface between operations research and computer science. We welcome books on theory, applications,  computer systems, and generally any subject covered by an IJOC Area, or any combination of these. This includes both printed and electronic books. In addition, we consider comparative  reviews, i.e. several books on one relevant topic. Team reviews are also possible, particularly for a large, broad-scope book such as an encyclopedia. Please send your suggestions for books to be reviewed to  the IJOC book reviews editor.This issue contains two reviews of books in the area of computational algebra and geometry, which is of increasing importance in discrete optimization.  Modern Computer Algebra, Third Edition, by Joachim von zur Gathen and Jürgen Gerhard is a general introduction to the field, suitable for use as a text or research-level  introduction. It covers algorithms for problems involving integers and polynomials. Reviewer Michael Burr calls the book, “practically useful while theoretically complete, providing  intuition for the newcomer while also pointing the expert to more history and cutting edge results.” While Gathen and Gerhard’s book serves as an introduction to the field,  Algebraic and Geometric Ideas in the Theory of Discrete Optimization, by Jesus A. De Loera, Raymond Hemmecke, and Matthias Köppe (reviewed by Jon Lee), connects those ideas to  optimization. Lee reports that the book is oriented toward researchers who know discrete optimization as operations researchers and mathematical programmers who want to learn about advances made possible by applying methods from other areas of discrete mathematics.
Mixtures of normal distributions provide a useful modeling extension of the normal distribution—both univariate and multivariate. Unlike the normal distribution, mixtures of normals can capture the kurtosis (fat tails) and nonzero skewness often necessary for accurately modeling a variety of real-world variables. An efficient analytical Monte Carlo method is proposed for considering multivariate mixtures of normal distributions having arbitrary covariance matrices. The method consists of a linear transformation of a multivariate normal having a computed covariance matrix into the desired multivariate mixture of normal distributions. The computed covariance matrix is derived analytically. Among the properties of the multivariate mixture of normals that we demonstrate is that any linear combination of mixtures of normal distributions is also a mixture of normal distributions. Methods of fitting mixtures of normal distributions are briefly discussed. A motivating example carried throughout this paper is the use of multivariate mixtures of normals for modeling daily changes in market variables.
In an optical network a signal can only travel a maximum distance dmax before its quality deteriorates to the point that it must be regenerated by installing regenerators at nodes of the network. As the cost of a regenerator is high, we wish to deploy as few regenerators as possible in the network, while ensuring all nodes can communicate with each other. In this paper we introduce the generalized regenerator location problem (GRLP) in which we are given a set S of nodes that corresponds to candidate locations for regenerators, and a set T of nodes that must communicate with each other. If S = T = N, we obtain the regenerator location problem (RLP), which we have studied previously and shown to be NP-complete. Our solution procedure to the RLP is based on its equivalence to the maximum leaf spanning tree problem (MLSTP). Unfortunately, this equivalence does not apply to the GRLP, nor do the procedures developed previously for the RLP. To solve the GRLP, we propose reduction procedures, two construction heuristics, and a local search procedure that we collectively refer to as a heuristic framework. We also establish a correspondence between the (node-weighted) directed Steiner forest problem and the GRLP. Using this fact, we provide several ways to derive natural and extended integer programming (IP) and mixed-integer programming (MIP) models for the GRLP and compare the strength of these models. Using the strongest model derived on the natural node selection variables we develop a branch-and-cut approach to solve the problem to optimality. The results indicate that the exact approach can easily solve instances with up to 200 nodes to optimality, whereas the heuristic framework is a high-quality approach for solving large-scale instances.
We present a highly effective dynamic programming driven memetic algorithm for the Steiner tree problem with revenues, budget, and hop constraints (STPRBH), which aims at determining a subtree of an undirected graph, so as to maximize the collected revenue, subject to both budget and hop constraints. The main features of the proposed algorithm include a probabilistic constructive procedure to generate initial solutions, a neighborhood search procedure using dynamic programming to significantly speed up neighborhood exploration, a backbone-based crossover operator to generate offspring solutions, as well as a quality-and-distance updating strategy to manage the population. Computational results based on four groups of 384 well-known benchmarks demonstrate the value of the proposed algorithm, compared to the state of the art approaches. In particular, for the 56 most challenging instances with unknown optima, our algorithm succeeds in providing 45 improved best known solutions within a short computing time. We additionally provide results for a group of 30 challenging instances that are introduced in the paper. We provide a complexity analysis of the proposed algorithm and study the impact of some ingredients on the performance of the algorithm.
The state of numerical computing is currently characterized by a divide between highly efficient yet typically cumbersome low-level languages such as C, C++, and Fortran and highly expressive yet typically slow high-level languages such as Python and MATLAB. This paper explores how Julia, a modern programming language for numerical computing that claims to bridge this divide by incorporating recent advances in language and compiler design (such as just-in-time compilation), can be used for implementing software and algorithms fundamental to the field of operations research, with a focus on mathematical optimization. In particular, we demonstrate algebraic modeling for linear and nonlinear optimization and a partial implementation of a practical simplex code. Extensive cross-language benchmarks suggest that Julia is capable of obtaining state-of-the-art performance.Data, as supplemental material, are available at http://dx.doi.org/10.1287/ijoc.2014.0623.
The link prediction problem is an emerging real-life social network problem in which data mining techniques have played a critical role. It arises in many practical applications such as recommender systems, information retrieval, and marketing analysis of social networks. We propose a new mathematical programming approach for predicting a future network using estimated node degree distribution identified from historical data. The link prediction problem is formulated as an integer programming problem that maximizes the sum of link scores (probabilities) with respect to the estimated node degree distribution. The performance of the proposed framework is tested on real-life social networks, and the computational results show that the proposed approach can improve the performance of previously published link prediction methods.
We introduce a simple technique for disjunctive machine scheduling problems and show that this method can match or even outperform state-of-the-art algorithms on a number of problem types. Our approach combines a number of generic search techniques such as restarts, adaptive heuristics, and solution-guided branching on a simple model based on a decomposition of disjunctive constraints and on the reification of these disjuncts.This paper describes the method and its application to variants of the job shop scheduling problem (JSP). We show that our method can easily be adapted to handle additional side constraints and different objective functions, often outperforming the state-of-the-art and closing a number of open problems. Moreover, we perform in-depth analysis of the various factors that make this approach efficient. We show that, while most of the factors give moderate benefits, the variable and value ordering components are key.
One critical operational decision facing online advertisers when they engage in sponsored search advertising is concerned with the allocation of a limited advertising budget. In particular, dealing with multi-keyword search markets over multiple decision periods poses significant decision-making challenges. In this paper, we develop a novel budget allocation optimization model with multiple search advertising markets and a finite time horizon. One key element of our modeling work is developing a customized advertising response function when considering distinctive features of sponsored search, including the quality score and the dynamic advertising effort. We derive a feasible solution to our budget model and study its properties. Computational experiments are conducted on real-world data to evaluate our budget model and perform parameter sensitivity analysis. Experimental results indicate that our budget allocation strategy significantly outperforms several baseline strategies. In addition, the identified properties derived from the solution process illuminate critical managerial insights for advertisers in sponsored search.
We consider a balance-constrained stochastic bottleneck spanning tree problem (BCSBSTP) where edge weights are independently distributed but may follow arbitrary continuous distributions. The goal is to minimize a threshold variable that may be exceeded by the maximum edge weight at certain risk, subject to the minimum edge weight being no less than a fixed threshold with a probability guarantee. We characterize these two requirements as chance constraints, which are typically used for bounding the risk of undesirable random outcomes. Given independently distributed edge weights, we reformulate BCSBSTP as a mixed-integer nonlinear program, approximated by two mixed-integer linear programs based on special ordered set of type one (SOS1) and special ordered set of type two (SOS2) variables. By relaxing the probabilistic guarantee on the minimum edge weight in BCSBSTP, we also consider a stochastic bottleneck spanning tree problem (SBSTP), of which optimal tree solutions are approximated via a bisection algorithm in pseudopolynomial time. We demonstrate computational results of our models and algorithms by testing randomly generated instances with edge weights following a diverse set of independent distributions.
Selecting the solution with the largest or smallest mean of a primary performance measure from a finite set of solutions while requiring secondary performance measures to satisfy certain constraints is called constrained selection of the best (CSB) in the simulation ranking and selection literature. In this paper, we consider CSB problems with secondary performance measures that must satisfy probabilistic constraints, and we call such problems chance constrained selection of the best (CCSB). We design procedures that first check the feasibility of all solutions and then select the best among all the sample feasible solutions. We prove the statistical validity of these procedures for variations of the CCSB problem under the indifference-zone formulation. Numerical results show that the proposed procedures can efficiently handle CCSB problems with up to 100 solutions, each with five chance constraints.
We consider two variants of a pricing problem under the nested logit model. In the first variant, the set of products offered to customers is fixed, and we want to determine the prices of the products. In the second variant, we jointly determine the set of offered products and their corresponding prices. In both variants, the price of each product has to be chosen within given upper and lower bounds specific to the product, each customer chooses among the offered products according to the nested logit model, and the objective is to maximize the expected revenue from each customer. We give approximation methods for both variants. For any ρ > 0, our approximation methods obtain a solution with an expected revenue deviating from the optimal expected revenue by no more than a factor of 1 + ρ. To obtain such a solution, our approximation methods solve a linear program whose size grows at rate 1/ρ. In addition to our approximation methods, we develop a linear program that we can use to obtain an upper bound on the optimal expected revenue. In our computational experiments, we compare the expected revenues from the solutions obtained by our approximation methods with the upper bounds on the optimal expected revenues and show that we can obtain high-quality solutions quite fast.
Stochastic programming models are large-scale optimization problems that are used to facilitate decision making under uncertainty. Optimization algorithms for such problems need to evaluate the expected future costs of current decisions, often referred to as the recourse function. In practice, this calculation is computationally difficult as it requires the evaluation of a multidimensional integral whose integrand is an optimization problem. In turn, the recourse function has to be estimated using techniques such as scenario trees or Monte Carlo methods, both of which require numerous functional evaluations to produce accurate results for large-scale problems with multiple periods and high-dimensional uncertainty. In this work, we introduce an importance sampling framework for stochastic programming that can produce accurate estimates of the recourse function using a small number of samples. Our framework combines Markov chain Monte Carlo methods with kernel density estimation algorithms to build a nonparametric importance sampling distribution, which can then be used to produce a lower-variance estimate of the recourse function. We demonstrate the increased accuracy and efficiency of our approach using variants of well-known multistage stochastic programming problems. Our numerical results show that our framework produces more accurate estimates of the optimal value of stochastic programming models, especially for problems with moderate variance, multimodal, or rare-event distributions.
Recent progress in solving quadratic assignment problems (QAPs) from the QAPLIB (Quadratic Assignment Problem Library) test set has come from mixed-integer linear or quadratic programming models that are solved in a branch-and-bound framework. Semidefinite programming (SDP) bounds for QAPs have also been studied in some detail, but their computational impact has been limited so far, mostly because of the restrictive size of the early relaxations. Some recent progress has been made by studying smaller SDP relaxations and by exploiting group symmetry in the QAP data. In this work, we introduce a new SDP relaxation, where the matrix variables are only of the order of the QAP dimension, and we show how one may exploit group symmetry in the problem data for this relaxation. We also provide a detailed numerical comparison with related bounds from the literature. In particular, we compute the best-known lower bounds for two QAPLIB instances.
We consider a generalization of the 0–1 knapsack problem in which the profit of each item can take any value in a range characterized by a minimum and a maximum possible profit. A set of specific profits is called a scenario. Each feasible solution associated with a scenario has a regret, given by the difference between the optimal solution value for such scenario and the value of the considered solution. The interval min–max regret knapsack problem (MRKP) is then to find a feasible solution such that the maximum regret over all scenarios is minimized. The problem is extremely challenging both from a theoretical and a practical point of view. Its decision version is complete for the second level of the polynomial hierarchy hence it is most probably not in 𝒩𝒫. In addition, even computing the regret of a solution with respect to a scenario requires the solution of an 𝒩𝒫-hard problem. We examine the behavior of classical combinatorial optimization approaches when adapted to the solution of the MRKP. We introduce an iterated local search approach and a Lagrangian-based branch-and-cut algorithm and evaluate their performance through extensive computational experiments.
Model counting is the #P problem of counting the number of satisfying solutions of a given propositional formula. Here we focus on a restricted variant of this problem, where the input formula is monotone (i.e., there are no negations). A monotone conjunctive normal form (CNF) formula is sufficient for modeling various graph problems, e.g., the vertex covers of a graph. Even for this restricted case, there is no known efficient approximation scheme. We show that the classical Spectra technique that is widely used in network reliability can be adapted for counting monotone CNF formulas. We prove that the proposed algorithm is logarithmically efficient for random monotone 2-CNF instances. Although we do not prove the efficiency of Spectra for k-CNF where k > 2, our experiments show that it is effective in practice for such formulas.
We present an efficient scenario decomposition algorithm for solving large-scale convex stochastic programming problems that involve a particular class of downside risk measures. The considered risk functionals encompass coherent and convex measures of risk that can be represented as an infimal convolution of a convex certainty equivalent, and include well-known measures, such as conditional value-at-risk, as special cases. The resulting structure of the feasible set is then exploited via iterative solving of relaxed problems, and it is shown that the number of iterations is bounded by a parameter that depends on the problem size. The computational performance of the developed scenario decomposition method is illustrated on portfolio optimization problems involving two families of nonlinear measures of risk, the higher-moment coherent risk measures, and log-exponential convex risk measures. It is demonstrated that for large-scale nonlinear problems the proposed approach can provide up to an order-of-magnitude improvement in computational time in comparison to state-of-the-art solvers, such as CPLEX, Gurobi, and MOSEK.
We study the multi-item capacitated lot sizing problem with setup times. Based on two strong reformulations of the problem, we present a transformed reformulation and valid inequalities that speed up column generation and Lagrange relaxation. We demonstrate computationally how both ideas enhance the performance of our algorithm and show theoretically how they are related to dual space reduction techniques. We compare several solution methods and propose a new efficient hybrid scheme that combines column generation and Lagrange relaxation in a novel way. Computational experiments show that the proposed solution method for finding lower bounds is competitive with textbook approaches and state-of-the-art approaches found in the literature. Finally, we design a branch-and-price-based heuristic and report computational results. The heuristic scheme compares favorably or outperforms other approaches.
Automatic layout of tables is useful in word processing applications and is required in online applications because of the need to tailor the layout to viewport width, choice of font, and dynamic content. However, if the table contains text, minimizing the height of the table for a given maximum width is a difficult combinatorial optimization problem because of the need to find the right choice of height/width configuration for each cell in the table. We investigate the modelling decisions involved in formulating this problem for use with standard combinatorial optimization techniques that are guaranteed to find the minimal-height table. To the best of our knowledge, we are the first to do so. We provide a detailed empirical evaluation of the resulting models using mixed integer programming and constraint programming with lazy clause generation.
Effective patch management is critical to ensure the security of information systems that modern organizations count on today. Facing numerous patch releases from vendors, an information technology (IT) manager must weigh the costs of frequent patching against the security risks that can arise from delays in patch application. To this end, we develop a rigorous quantitative framework to analyze and compare several patching policies that are of practical interest. Our analyses of pure policies—policies that rely on a single metric such as elapsed time or patch severity level—show that certain policies are never optimal and no single policy may fit all information systems uniformly well. Depending on the context parameters, particularly the setup and business disruption costs for patching, either a time-based approach or an approach based on the cumulative severity level may be effective. To develop a more complete guideline for policy selection, we decipher hybrid policies that combine multiple metrics. Finally, we conduct extensive numerical experiments to verify the robustness of our analytical results. Overall, our paper establishes a comprehensive framework for analyzing various patching policies and furnishes useful insights for IT managers.
In the online checkpointing problem, the task is to continuously maintain a set of k checkpoints that allow rewinding an ongoing computation faster than by a full restart. The only operation allowed is to replace an old checkpoint by the current state. Our aim is checkpoint placement strategies that minimize rewinding cost, i.e., such that at all times T when requested to rewind to some time t ≤ T the number of computation steps that need to be redone to get to t from a checkpoint before t is as few as possible. In particular, we want the closest checkpoint earlier than t to be no farther away from t than qk times the ideal distance T/(k + 1), where qk is a small constant.Improving earlier work showing 1 + 1/k ≤ qk ≤ 2, we show that qk can be chosen asymptotically less than 2. We present algorithms with asymptotic discrepancy qk ≤ 1.59 + o(1) valid for all k and qk ≤ ln(4) + o(1) ≤ 1.39 + o(1) valid for k being a power of two. Experiments indicate the uniform bound pk ≤ 1.7 for all k. For small k, we show how to use a linear programming approach to compute good checkpointing algorithms. This gives discrepancies of less than 1.55 for all k < 60.We prove the first lower bound that is asymptotically more than 1, namely qk ≥ 1.30 − o(1). We also show that optimal algorithms (yielding the infimum discrepancy) exist for all k.
We consider an automatic overload control for two large service systems modeled as multiserver queues such as call centers. We assume that the two systems are designed to operate independently, but want to help each other respond to unexpected overloads. The proposed overload control automatically activates sharing (sending some customers from one system to the other) once a ratio of the queue lengths in the two systems crosses an activation threshold (with ratio and activation threshold parameters for each direction). In this paper, we are primarily concerned with ensuring that the system recovers rapidly after the overload is over, either because (i) the two systems return to normal loading or (ii) the direction of the overload suddenly shifts in the opposite direction. To achieve rapid recovery, we introduce lower thresholds for the queue ratios, below which one-way sharing is released. As a basis for studying the complex dynamics, we develop a new six-dimensional fluid approximation for a system with time-varying arrival rates, extending a previous fluid approximation involving a stochastic averaging principle. We conduct simulations to confirm that the new algorithm is effective for predicting the system performance and choosing effective control parameters. The simulation and the algorithm show that the system can experience an inefficient nearly periodic behavior, corresponding to an oscillating equilibrium (congestion collapse) if the sharing is strongly inefficient and the control parameters are set inappropriately.
The expansion of a telecommunications network faces two sources of uncertainty, which are the demand for traffic that will transit through the expanded network and the outsourcing cost that the network operator will have to pay to handle the traffic that exceeds the capacity of her network. The latter is determined by the future cost of telecommunications services, whose negative correlation with the total demand is empirically measured in the literature through the price elasticity of demand.
There is growing interest in the use of grid-level storage to smooth variations in supply that are likely to arise with an increased use of wind and solar energy. Energy arbitrage, the process of buying, storing, and selling electricity to exploit variations in electricity spot prices, is becoming an important way of paying for expensive investments into grid-level storage. Independent system operators such as the New York Independent System Operator (NYISO) require that battery storage operators place bids into an hour-ahead market (although settlements may occur in increments as small as five minutes, which is considered near “real-time”). The operator has to place these bids without knowing the energy level in the battery at the beginning of the hour and simultaneously accounting for the value of leftover energy at the end of the hour. The problem is formulated as a dynamic program. We describe and employ a convergent approximate dynamic programming (ADP) algorithm that exploits monotonicity of the value function to find a revenue-generating bidding policy; using optimal benchmarks, we empirically show the computational benefits of the algorithm. Furthermore, we propose a distribution-free variant of the ADP algorithm that does not require any knowledge of the distribution of the price process (and makes no assumptions regarding a specific real-time price model). We demonstrate that a policy trained on historical real-time price data from the NYISO using this distribution-free approach is indeed effective.
Using a novel stochastic programming (SP) formulation, we develop an algorithm for inventory control in industrial-size assemble-to-order (ATO) systems that has unparalleled efficiency and scalability. Applying our algorithm to several numerical examples, we generate new insights regarding the control and optimization of these systems.We consider a continuous time model, seeking base-stock levels for components that minimize the sum of holding costs and product-specific backorder costs. Our initial focus is on first-come, first-served (FCFS) allocation of components to products; for this setting our algorithm quickly computes solutions for which the asymptotic optimality gap with the optimal FCFS base-stock policy is less than 1%. We then turn to two related questions: How do common heuristics used in practice compare to our performance, and how costly is the FCFS assumption?For the first question, we investigate the effectiveness of ignoring simultaneous stock-outs (ISS), a heuristic that has been used by companies such as IBM and Dell. Our experiments indicate that ISS performance, when compared to the optimal FCFS base-stock policy, improves as the average newsvendor (NV) fractiles increase but suffers under lead-time demand correlations.For the second question, we develop an efficiently computable upper bound on the benefit of optimal allocation over FCFS. We find that for many large ATO systems, FCFS performs surprisingly well and that its performance improves with decreasing NV fractile asymmetry among products and, again, with increasing average NV fractiles. We also investigate simple no-holdback allocation policies and find that they tend to outperform the best FCFS policies.
Ontology is the backbone of the Semantic Web, helping users search for relevant resources from the Web of linked data. The existing context-free mapping approach between tags and concepts fails to address the problems of social synonymy and social polysemy when ontologies are induced from folksonomies. The novel contributions of this paper are threefold. First, grounded in the cognitively motivated category utility measure, a novel basic-level concept mining algorithm is developed to construct semantically rich concept vectors to alleviate the problem of social synonymy. Second, contextual aspects of ontology learning are exploited via probabilistic topic modeling to address the problem of social polysemy. Third, a novel context-sensitive domain ontology learning algorithm that combines link- and content-based semantic analysis is developed to identify both taxonomic and associative relations among concepts. To the best of our knowledge, this is the first successful research that exploits a cognitively motivated method to learn context-sensitive domain ontologies from folksonomies. By using the Open Directory Project ontology as a benchmark, we examined the effectiveness of the proposed algorithms based on social annotations crawled from three different folksonomy sites. Our experimental results show that the proposed ontology learning system significantly outperforms the best baseline system by 13.83% in terms of taxonomic F-measure. The practical implication of our research is that high-quality ontologies are constructed with minimal human intervention to facilitate concept-driven retrieval of linked data and the knowledge-based interoperability among enterprises.
We present the first criterion space search algorithm, the triangle splitting method, for finding all nondominated points of a biobjective mixed integer program. The algorithm is relatively easy to implement and converges quickly to the complete set of nondominated points. The algorithm maintains, at any point in time, a diverse set of nondominated points, and is thus ideally suited for fast approximation of the nondominated frontier. An extensive computational study demonstrates the efficacy of the triangle splitting method.Data, as supplemental material, are available at http://dx.doi.org/10.1287/ijoc.2015.0646.
Product assortment and availability are important determinants of sales success for firms of industrial commodity products. Well-known pricing and promotion strategies for differentiated products do not translate well to such products where price is closely tied to the cost of the products. Consequently, firms with multiple stores of commodity products are faced with the problem of product assortment that incorporates varying geographic and demographic conditions of locations they serve. The paper presents a model for assortment planning and optimization for multiple stores of a company. The novelty of our approach is twofold: first, it deploys data mining techniques to identify sales pattern information across multiple stores through existing sales data across segments and across stores; second, it identifies the optimal product assortment for each store and permits analyses of assortment efficiency evaluation among all existing stores. Our model first finds frequent itemsets based on association rule analysis and prunes them using a novel conflict resolution method. It then incorporates the identified product combinations into the development of the optimization formulation. Our methodology offers solutions that have important implications on product assortment, including complements versus substitutes and product bundling, and sheds lights on product planning and assortment strategies in general. A data set from an industry leading plastics manufacturer and retailer in the United States is used to demonstrate our model.
We study a natural extension of the classical traveling salesman problem (TSP) in the situation where multiple salesmen are dispatched from a number of different depots. As with the TSP, this problem is motivated by a large range of applications in vehicle routing. Although it is known to have a 2-approximation algorithm, whether the problem has a 3/2-approximation algorithm, as is the case with the well-known Christofides heuristic for the TSP, remains an open question. We answer this question positively by providing a 3/2-approximation algorithm for the problem with a fixed number of depots. The algorithm uses an edge exchange strategy, and its analysis hinges on a newly discovered exchange property of matroids. In addition, the algorithm is applied to multidepot extensions of other TSP variants, and we show for the first time, to our knowledge, that for these multidepot extensions the same best constant approximation ratios can be achieved as for their respective single-depot cases.
Recent work by Han and Van Roy [Han J, Van Roy B (2011) Control of diffusions via linear programming. Infanger G, ed. Stochastic Programming: The State of the Art, in Honor of George B. Dantzig (Springer, New York), 329–354] introduced a linear programming technique to compute good suboptimal solutions to high-dimensional control problems in a diffusion-based setting. Their problem formulation worked with finite horizon problems where the horizon, T, is an exponentially distributed random variable. We extend their approach to finite horizon problems with a fixed horizon T. We also apply these techniques to dynamic portfolio optimization problems and then simulate the resulting policies to obtain lower bounds on the optimal value functions. We also use these policies in conjunction with convex duality methods designed for portfolio optimization problems to construct upper bounds on the optimal value functions. In our numerical experiments we find that the primal and dual bounds are very close, and so we conclude, for these problems at least, that the linear programming approach performs very well.
In this article we introduce the quadratic capacitated vehicle routing problem (QCVRP) motivated by two applications in engineering and logistics: the capacitated vehicle routing problem with angle penalties (angle-CVRP) and the capacitated vehicle routing problem with reload costs (CVRP-RC). We introduce a three-index vehicle-flow formulation of the problem, which is strengthened with valid inequalities, and we derive a branch-and-cut algorithm capable of providing tight lower bounds and solving small- to medium-size instances in short to moderate computing times. Furthermore, we present a hybrid metaheuristic capable of providing high quality solutions in short computing times. The two algorithms are tested on several instances from the CVRP literature modified to mimic the two problems that motivate our study.
LU and Cholesky factorizations are computational tools for efficiently solving linear systems that play a central role in solving linear programs and several other classes of mathematical programs. In many documented cases, however, the roundoff errors accrued during the construction and implementation of these factorizations lead to the misclassification of feasible problems as infeasible and vice versa. Hence, reducing these roundoff errors or eliminating them altogether is imperative to guarantee the correctness of the solutions provided by optimization solvers. To achieve this goal without having to use rational arithmetic, we introduce two roundoff-error-free factorizations that require storing the same number of individual elements and performing a similar number of operations as the traditional LU and Cholesky factorizations. Additionally, we present supplementary roundoff-error-free forward and backward substitution algorithms, thereby providing a complete tool set for solving systems of linear equations exactly and efficiently. An important property shared by the featured factorizations and substitution algorithms is that their individual coefficients’ maximum word length—i.e., the maximum number of digits required for expression—is bounded polynomially. Unlike the rational arithmetic methods used in practice to solve linear systems exactly, however, the algorithms herein presented do not require any gcd calculations to bound the entries’ word length. We also derive various other related theoretical results, including the total computational complexity of all the roundoff-error-free processes herein presented.
Many combinatorial problems require that their solutions achieve a certain balance of given features. For this important aspect of modeling, the spread and deviation constraints have been proposed in Constraint Programming to express balance among a set of variables by constraining their mean and overall deviation from the mean. To our knowledge, the only practical filtering algorithms known for these constraints achieve bounds consistency. In this paper we improve that filtering by presenting an efficient domain consistency algorithm that applies to both constraints. We also extend it to count solutions so that it can be used in counting-based search, a generic and effective family of branching heuristics that free the user from having to write problem-specific search heuristics. We provide a time complexity analysis of our contributions and empirically evaluate them on benchmark problems.
We apply virtual machine abstractions to networked vehicles, enabling what we call cloud computing in space to create performance isolation between customers. In analogy to conventional system virtualization and cloud computing, there are customer-operated virtual vehicles that essentially perform like real vehicles, although they are in reality hosted by fewer, shared, provider-operated real vehicles. The motion of the virtual vehicles and real vehicles creates migration gain. As a result, cloud computing in space can do better than conventional cloud computing in the sense of realizing high performance isolation (e.g., 98%) while requiring significantly fewer real vehicles (e.g., approximately one for five).There is a video associated with this paper. Click here to view the Video Overview. To save the file, right click and choose “Save Link As” from the menu.
Recent research has highlighted the need for upstream planning in healthcare service delivery systems, patient scheduling, and resource allocation in the hospital inpatient setting. This study examines the value of upstream planning within hospital-wide resource allocation decisions based on machine learning (ML) and mixed-integer programming (MIP), focusing on prediction of diagnosis-related groups (DRGs) and the use of these predictions for allocating scarce hospital resources. DRGs are a payment scheme employed at patients’ discharge, where the DRG and length of stay determine the revenue that the hospital obtains. We show that early and accurate DRG classification using ML methods, incorporated into an MIP-based resource allocation model, can increase the hospital’s contribution margin, the number of admitted patients, and the utilization of resources such as operating rooms and beds. We test these methods on hospital data containing more than 16,000 inpatient records and demonstrate improved DRG classification accuracy as compared to the hospital’s current approach. The largest improvements were observed at and before admission, when information such as procedures and diagnoses is typically incomplete, but performance was improved even after a substantial portion of the patient’s length of stay, and under multiple scenarios making different assumptions about the available information. Using the improved DRG predictions within our resource allocation model improves contribution margin by 2.9% and the utilization of scarce resources such as operating rooms and beds from 66.3% to 67.3% and from 70.7% to 71.7%, respectively. This enables 9.0% more nonurgent elective patients to be admitted as compared to the baseline.
We present a new criterion space search algorithm, the balanced box method, for finding all nondominated points of a biobjective integer program. The method extends the box algorithm, is easy to implement, and converges quickly to the complete set of nondominated points. Because the method maintains, at any point in time, a diverse set of nondominated points, it is ideally suited for fast approximation of the efficient frontier. In addition, we present several enhancements of the well-known ε-constraint, augmented weighted Tchebycheff, and perpendicular search methods. An extensive computational study, using instances from different classes of combinatorial optimization problems, demonstrates the efficacy of the balanced box method.
We propose a multistage stochastic mixed-integer programming formulation for the assignment of surgeries to operating rooms over a finite planning horizon. We consider the demand for and the duration of surgery to be random variables. The objective is to minimize three competing criteria: expected cost of surgery cancellations, patient waiting time, and operating room overtime. We discuss properties of the model and an implementation of the progressive hedging algorithm to find near-optimal surgery schedules. We conduct numerical experiments using data from a large hospital to identify managerial insights related to surgery planning and the avoidance of surgery cancellations. We compare the progressive hedging algorithm to an easy-to-implement heuristic for practical problem instances to estimate the value of the stochastic solution. Finally, we discuss an implementation of the progressive hedging algorithm within a rolling horizon framework for extended planning periods.
In this article, we propose a general framework for an algorithm derived from the primal simplex that guarantees a strict improvement in the objective after each iteration. Our approach relies on the identification of compatible variables that ensure a nondegenerate iteration if pivoted into the basis. The problem of finding a strict improvement in the objective function is proved to be equivalent to two smaller problems, respectively, focusing on compatible and incompatible variables. We then show that the improved primal simplex (IPS) is a particular implementation of this generic theoretical framework. The resulting new description of IPS naturally emphasizes what should be considered as necessary adaptations of the framework versus specific implementation choices. This provides original insight into IPS that allows for the identification of weaknesses and potential alternative choices that would extend the efficiency of the method to a wider set of problems. We perform experimental tests on an extended collection of data sets including instances of Mittelmann’s benchmark for linear programming. The results confirm the excellent potential of IPS and highlight some of its limits while showing a path toward an improved implementation of the generic algorithm.
We analyze the effect of tumor repopulation on optimal dose delivery in radiation therapy. We are primarily motivated by accelerated tumor repopulation toward the end of radiation treatment, which is believed to play a role in treatment failure for some tumor sites. A dynamic programming framework is developed to determine an optimal fractionation scheme based on a model of cell kill from radiation and tumor growth in between treatment days. We find that faster tumor growth suggests shorter overall treatment duration. In addition, the presence of accelerated repopulation suggests larger dose fractions later in the treatment to compensate for the increased tumor proliferation. We prove that the optimal dose fractions are increasing over time. Numerical simulations indicate a potential for improvement in treatment effectiveness.
On behalf of the Editorial Board, I would like to thank the following people, who acted as Reviewers during the past year.David L. WoodruffEditor-in-Chief
Recent research in hybrid optimization shows that a combination of technologies that exploits their complementary strengths can significantly speed up computation. The use of high-level metaconstraints in the problem formulation can achieve a substantial share of these computational gains by better communicating problem structure to the solver. During the solution process, however, metaconstraints give rise to reformulations or relaxations that introduce auxiliary variables, and some of the variables in one metaconstraint’s reformulation may be functionally the same as or related to variables in another metaconstraint’s reformulation. These relationships must be recognized to obtain a tight overall relaxation. We propose a modeling scheme based on semantic typing that systematically addresses this problem while providing simpler, self-documenting models. It organizes the model around predicates and declares variables by associating each with a predicate through a keyword that is analogous to a database query. We present a series of examples to illustrate this idea over a wide variety of applications.
A new mixed-integer linear programming (MILP) formulation for nonpreemptive single machine scheduling problems is presented. The model is a generalisation of the classical time indexed (TI) model to one in which at most two jobs can be processing in each time period. Like the TI model, the new model, called the bucket indexed (BI) model, partitions the planning horizon into periods of equal length, or buckets. Unlike the TI model, the length of a period is a parameter of the BI model and can be chosen to be as long as the processing time of the shortest job. The two models are equivalent if a period is of unit length, but when longer periods are used in the BI model, it can have significantly fewer variables and nonzeros than the corresponding TI model. A computational study using weighted tardiness instances, and weighted completion time instances with release dates, reveals that the BI model significantly outperforms the TI model on instances where the minimum processing time of the jobs is large. Furthermore, the performance of the BI model is less vulnerable to increases in average processing time when the ratio of the largest processing time to the smallest is held constant.
Floating-point computations are quickly finding their way in the design of safety- and mission-critical systems, despite the fact that designing floating-point algorithms is significantly more difficult than designing integer algorithms. For this reason, verification and validation of floating-point computations are hot research topics. An important verification technique, especially in some industrial sectors, is testing. However, generating test data for floating-point intensive programs proved to be a challenging problem. Existing approaches usually resort to random or search-based test data generation, but without symbolic reasoning it is almost impossible to generate test inputs that execute complex paths controlled by floating-point computations. Moreover, because constraint solvers over the reals or the rationals do not natively support the handling of rounding errors, the need arises for efficient constraint solvers over floating-point domains. In this paper, we present and fully justify improved algorithms for the propagation of arithmetic IEEE 754 binary floating-point constraints. The key point of these algorithms is a generalization of an idea by B. Marre and C. Michel that exploits a property of the representation of floating-point numbers.
We propose a general branch-and-bound algorithm for discrete optimization in which binary decision diagrams (BDDs) play the role of the traditional linear programming relaxation. In particular, relaxed BDD representations of the problem provide bounds and guidance for branching, and restricted BDDs supply a primal heuristic. Each problem is given a dynamic programming model that allows one to exploit recursive structure, even though the problem is not solved by dynamic programming. A novel search scheme branches within relaxed BDDs rather than on values of variables. Preliminary testing shows that a rudimentary BDD-based solver is competitive with or superior to a leading commercial integer programming solver for the maximum stable set problem, the maximum cut problem on a graph, and the maximum 2-satisfiability problem. Specific to the maximum cut problem, we tested the BDD-based solver on a classical benchmark set and identified tighter relaxation bounds than have ever been found by any technique, nearly closing the entire optimality gap on four large-scale instances.
Branch-and-price algorithms combine a branch-and-bound search with an exponentially sized LP formulation that must be solved via column generation. Unfortunately, the standard branching rules used in branch and bound for integer programming interfere with the structure of the column generation routine; therefore, most such algorithms employ alternate branching rules to circumvent this difficulty. This paper shows how a zero-suppressed binary decision diagram can be used to solve the pricing problem in a branch-and-price algorithm for the graph coloring problem, even in the presence of constraints imposed by branching decisions. This approach facilitates a much more direct solution method and can improve convergence of the column generation subroutine.
We study the unrelated parallel machine scheduling problem with sequence and machine-dependent setup times and the objective of makespan minimization. Two exact decomposition-based methods are proposed based on logic-based Benders decomposition and branch and check. These approaches are hybrid models that make use of a mixed-integer programming (MIP) master problem and a specialized solver for travelling salesman subproblems. The master problem is used to assign jobs to machines, whereas the subproblems find optimal schedules on each machine given the master problem assignments. Computational results show that the decomposition models are able to find optimal solutions up to four orders of magnitude faster than the existing state of the art as well as solve problems six times larger than an existing MIP model. We further investigate the solution quality versus runtime trade-off for large problem instances for which the optimal solutions cannot be found and proved in a reasonable time. We demonstrate that the branch-and-check hybrid algorithm is able to produce better schedules in less time than the state-of-the-art metaheuristic, while also providing an optimality gap.
The Canadian traveler problem (CTP) is a simple, yet challenging, stochastic optimization problem wherein an agent is given a graph where some edges are blocked with certain probabilities and the status of these edges can be disambiguated dynamically upon reaching an incident vertex. The goal is to devise a traversal policy that results in the shortest expected walk length between a given starting vertex and a termination vertex. CTP has been shown to be intractable in many broad settings. In this paper, we introduce an optimal algorithm for the problem based on a Markov decision process formulation, which is a new improvement on AO* search that takes advantage of the special problem structure in CTP. We call our algorithm CAO*, which stands for AO* with caching. CAO* uses a caching mechanism to avoid re-expansion of previously visited states and makes use of admissible upper bounds at a node level for dynamic state-space pruning. CAO* is not polynomial time, but it can dramatically shorten the execution time needed to find an exact solution for moderately sized instances. We present computational experiments on a realistic variant of the problem involving an actual maritime minefield data set.
Coherent risk measures have become a popular tool for incorporating risk aversion into stochastic optimization models. For dynamic models in which uncertainty is resolved at more than one stage, however, using coherent risk measures within a standard single-level optimization framework becomes problematic. To avoid severe time-consistency difficulties, the current state of the art is to employ risk measures of a specific nested form, which unfortunately have some undesirable and somewhat counterintuitive modeling properties. This paper summarizes the potential drawbacks of nested-form risk measure issues and then presents an alternative multilevel optimization modeling approach that enforces a form of time consistency through constraints rather than by restricting the modeler’s choice of objective function. This technique leads to models that are time consistent even while using time-inconsistent risk measures and can easily be formulated to be law invariant with respect to the final wealth if so desired. We argue that this approach should be the starting point for all multistage optimization modeling. When used with time-consistent objective functions, we show its multilevel optimization constraints become redundant, and the associated models thus simplify to a more familiar single-objective form. Unfortunately, we also show that our proposed approach leads to 𝒩𝒫-hard models, even in the simplest imaginable setting in which it would be needed: three-stage linear problems on a finite probability space, using the standard average value-at-risk and first-order mean-semideviation risk measures. Finally, we show that for a simple but reasonably realistic test application, the kind of models we propose, although drawn from an 𝒩𝒫-hard family and certainly more time consuming to solve than those obtained from the nested-objective approach, are readily solvable to global optimality using a standard commercial mixed-integer linear programming solver. Therefore, there seems some promise of our proposed modeling approach being useful despite its computational complexity properties.
Recommendation stability measures the extent to which a recommendation algorithm provides predictions that are consistent with each other. Several approaches have been proposed in prior work to defining, measuring, and improving the stability of recommendation algorithms. Previous studies have focused primarily on understanding and evaluating recommendation stability in prediction-oriented settings, i.e., recommendation settings where it is crucial to provide the precise prediction of a user’s preference rating for an item. However, the research literature has been largely silent on the topic of recommendation stability in other important types of settings, such as classification-oriented (i.e., where it is important to accurately classify the item as relevant versus irrelevant, without having to quantify the user’s preference precisely), ranking-oriented (i.e., where it is important to provide accurate relative ranking of items to users), or top-K oriented (i.e., where it is important to suggest K items that are most appealing to the user). Therefore, this paper builds on prior work by generalizing the notion of stability to a broader set of recommendation settings and developing corresponding stability metrics. The paper also provides a comprehensive empirical analysis of classification, ranking, and top-K stability performance of popular recommender algorithms on real-world rating data sets under a variety of settings.
In this paper, we study a scheduling problem on a single machine, provided that the jobs have individual release dates and deadlines, and the processing times are controllable. The objective is to find a feasible schedule that minimizes the total cost of reducing the processing times. We reformulate the problem in terms of maximizing a linear function over a submodular polyhedron intersected with a box. For the latter problem of submodular optimization, we develop a recursive decomposition algorithm and apply it to solving the single machine scheduling problem to achieve the best possible running time.
The critical node selection problem (CNP) has important applications in telecommunication, supply chain design, and disease propagation prevention. In practice, the weights on the connections are often uncertain or hard to estimate. For this reason, robust optimization approaches have been considered recently for CNP. In this article, we address very general uncertainty sets, only requiring a linear optimization oracle for the set of potential scenarios. In particular, we can deal with discrete scenario based uncertainty, gamma uncertainty, and ellipsoidal uncertainty. For this general class of robust critical node selection problems, we propose an exact solution method based on Benders decomposition. The Benders subproblem, which in our approach is a robust optimization problem, is efficiently solved by applying the Floyd-Warshall algorithm. The presented approach is tested on 384 instances based on Forest-Fire, Barabási-Albert, Erdős-Rényi, and Watts-Strogatz graphs with different number of nodes and edges, where running times are compared to CPLEX being directly applied to the robust problem formulation. The computational results show the advantage of the proposed approach in handling the uncertainty thus outperforming CPLEX most notably for the ellipsoidal uncertainty cases.
Column generation (CG) models have several advantages over compact formulations: they provide better linear program bounds, may eliminate symmetry, and can hide nonlinearities in their subproblems. However, users also encounter drawbacks in the form of slow convergence, also known as the tailing-off effect, and the oscillation of the dual variables. Among different alternatives for stabilizing the CG process, Ben Amor et al. [Ben Amor H, Desrosiers J, Valério de Carvalho JM (2006) Dual-optimal inequalities for stabilized column generation. Oper. Res. 54(3):454–463] suggest the use of dual-optimal inequalities (DOIs) in the context of cutting stock and bin packing problems. We generalize their results, provide new classes of (deep) DOIs, and show the applicability to other problems (vector packing, vertex coloring, bin packing with conflicts). We also suggest the dynamic addition of violated dual inequalities in a cutting-plane fashion and the use of dual inequalities that are not necessarily (deep) DOIs. In the latter case, a recovery procedure is needed to restore primal feasibility. Computational results proving the usefulness of the methods are presented.
We formulate the statistical selection problem in a general dynamic framework comprising fully sequential sampling allocation and optimal design selection. Because the traditional probability of correct selection measure is not sufficient to capture both aspects in this more general framework, we introduce the integrated probability of correct selection to better characterize the objective. As a result, the usual selection policy of choosing the design with the largest sample mean as the estimate of the best is no longer necessarily optimal. Rather, the optimal selection policy is to choose the design that maximizes the posterior integrated probability of correct selection, which is a function of the posterior mean and the correlation structure induced by the posterior variance. Because determining the optimal selection policy is generally intractable, we also devise an approximation scheme to efficiently approximate the optimal selection policy. For the allocation policy, we study an asymptotic policy called general Bayesian budget allocation, which is comprised of a sampling statistic and a sequential rule. The optimal computing budget allocation algorithm can be interpreted as a special case of the asymptotical sampling statistics. Numerical examples are provided to illustrate the potential performance improvements, especially in small sample behavior.
In this work, we propose a cutting plane algorithm to improve optimization models that are originally formulated as convex generalized disjunctive programs. Generalized disjunctive programs are traditionally reformulated as mixed-integer nonlinear programming (MINLP) problems using either the big M (BM) or the hull reformulation (HR). The former yields a smaller MILP/MINLP problem, whereas the latter yields a tighter one. The HR can be further strengthened by using the concept of basic step from disjunctive programming. The proposed algorithm uses the strengthened formulation to derive cuts for the big-M formulation, generating a stronger formulation with small growth in problem size. We test the algorithm with several instances. The results show that the algorithm improves generalized disjunctive programming convex models, in the sense of providing formulations with stronger continuous relaxations than the BM formulation, with few additional constraints. In general, the algorithm also leads to a reduction in the solution time of the problems.
The Greeks are derivatives of option price with respect to market parameters and play an important role in financial risk management. Among various simulation methods for estimating the Greeks, the pathwise method typically has a low variance. However, when the option payoff is discontinuous, the pathwise method is not applicable and the Greek involves a conditional expectation taken over a hypersurface that is a probability-zero set. In this paper, we propose an importance sampling (IS) method to estimate this conditional expectation. More specifically, IS is applied in a way that all simulated observations fall into a set constructed by thickening the hypersurface. Allowing the thickness of the set to go to zero then leads to a new representation of the Greek as an ordinary expectation, thus leading to an unbiased estimator. The resulting estimator makes use of the pathwise derivatives and can be viewed as an extension of the pathwise method to cases with discontinuous payoffs. Numerical results show that the proposed IS method works well.
The consistency between review summaries and review ranking lists is important for consumers so they can utilize online reviews effectively and efficiently in their purchase decisions. This paper examines this consistency issue and formulates it as an optimization problem. Based on consumers’ reading behaviors, all possible sets of reviews that consumers would read from ranking lists are considered; the objective is to maximize the expected consistency. Because of the NP-hardness of the problem, exact methods that search for the optimal ranking lists are generally not acceptable in practice. Hence, a heuristic approach (the enhanced stepwise optimization procedure) is proposed. This approach is an effective and efficient approximation that selects reviews iteratively to add to the ranking lists in light of expected consistency value, superiority, and execution time. Intensive experiments on both synthetic and real data are conducted, with various environments and settings, along with a relevant user study, revealing that the proposed approach outperforms other related methods.
We study a single server queuing model with multiple classes and impatient customers. The goal is to determine a service policy to maximize the long-run reward rate earned from serving customers net of holding costs and penalties respectively due to customers waiting for and leaving before receiving service. We first show that it is without loss of generality to study a pure-reward model. Since standard methods can usually only compute the optimal policy for problems with up to three customer classes, our focus is to develop a suite of heuristic approaches, with a preference for operationally simple policies with good reward characteristics. One such heuristic is the Rμθ rule—a priority policy that ranks all customer classes based on the product of reward R, service rate μ, and abandonment rate θ. We show that the Rμθ rule is asymptotically optimal as customer abandonment rates approach zero and often performs well in cases where the simpler Rμ rule performs poorly. The paper also develops an approximate policy improvement method that uses simulation and interpolation to estimate the bias function for use in a dynamic programming recursion. For systems with two or three customer classes, our numerical study indicates that the best of our simple priority policies is near optimal in most cases; when it is not, the approximate policy improvement method invariably tightens up the gap substantially. For systems with five customer classes, our heuristics typically achieve within 4% of an upper bound for the optimal value, which is computed via a linear program that relies on a relaxation of the original system. The computational requirement of the approximate policy improvement method grows rapidly when the number of customer classes or the traffic intensity increases.
Cloud computing is a new emerging paradigm that aims to streamline the on-demand provisioning of resources as services, providing end users with flexible and scalable services accessible through the Internet on a pay-per-use basis. Because modern cloud systems operate in an open and dynamic world characterized by continuous changes, the development of efficient resource provisioning policies for cloud-based services becomes increasingly challenging.This paper aims to study the hourly basis service provisioning problem through a generalized Nash game model. We take the perspective of Software as a Service (SaaS) providers that want to minimize the costs associated with the virtual machine instances allocated in a multiple Infrastructures as a Service (IaaS) scenario while avoiding incurring penalties for execution failures and providing quality of service guarantees. SaaS providers compete and bid for the use of infrastructural resources, whereas the IaaSs want to maximize their revenues obtained providing virtualized resources.We propose a solution algorithm based on the best-reply dynamics, which is suitable for a distributed implementation. We demonstrate the effectiveness of our approach by performing numerical tests, considering multiple workloads and system configurations. Results show that our algorithm is scalable and provides significant cost savings with respect to alternative methods (5% on average but up to 260% for individual SaaS providers). Furthermore, varying the number of IaaS providers means an 8%–15% cost savings can be achieved from the workload distribution on multiple IaaSs.
Market surveillance systems (MSSs) are increasingly used to monitor trading activities in financial markets to maintain market integrity. Existing MSSs primarily focus on statistical analysis of market activity data and largely ignore textual market information, including, but not limited to, news reports and various social media. As suggested by both theoretical explorations in finance and prevailing market surveillance practice, unstructured market information holds major yet underexplored opportunities for surveillance. In this paper, we propose a news analysis approach with the help of commonsense knowledge to assess the risk of suspicious transactions identified in market activity analysis. Our approach explicitly models semantic relations between transactions and news articles and provides semantic references to words in news articles. We conducted experiments using data collected from a real-world market and found that our proposed approach significantly outperforms the existing methods, which are based on transaction characteristics or traditional textual analysis methods. Experiments also show that the performance advantage of the proposed approach mainly comes from the modeling of news-transaction relationships. The research contributes to the market surveillance literature and has significant practical implications.
This article introduces the minimum spanning k-core problem that seeks to find a spanning subgraph with a minimum degree of at least k (also known as a k-core) that minimizes the total cost of the edges in the subgraph. The concept of k-cores was introduced in social network analysis to identify denser portions of a social network. We exploit the graph-theoretic properties of this model to introduce a new approach to survivable interhub network design via spanning k-cores; the model preserves connectivity and diameter under limited edge failures. The deterministic version of the problem is polynomial-time solvable due to its equivalence to generalized graph matching. We propose two conditional value-at-risk (CVaR) constrained optimization models to obtain risk-averse solutions for the minimum spanning k-core problem under probabilistic edge failures. We present polyhedral reformulations of the convex piecewise linear loss functions used in these models that enable Benders-like decomposition approaches. A decomposition and branch-and-cut approach is then developed to solve the scenario-based approximation of the CVaR-constrained minimum spanning k-core problem for the aforementioned loss functions. The computational performance of the algorithm is investigated via numerical experiments.
We study the approximability of the classical quadratic knapsack problem (QKP) on special graph classes. In this case the quadratic terms of the objective function are not given for each pair of knapsack items. Instead, an edge weighted graph, whose vertices represent the knapsack items, induces a quadratic profit for every pair of items, which is adjacent in the graph. We show that the problem permits an FPTAS on graphs of bounded treewidth and a PTAS on planar graphs and more generally on H-minor free graphs. We also show strong 𝒩𝒫-hardness of QKP on graphs that are 3-book embeddable, a natural graph class that is related to planar graphs. In addition, we will argue that the problem is likely to have bad approximability behaviour on all graph classes that include the complete graph or contain large cliques. These hardness of approximation results under certain complexity assumptions carry over from the densest k-subgraph problem.
We consider a bilevel integer programming model that extends the classic 0–1 knapsack problem in a very natural way. The model describes a Stackelberg game where the leader’s decision interdicts a subset of the knapsack items for the follower. As this interdiction of items substantially increases the difficulty of the problem, it prevents the application of the classical methods for bilevel programming and of the specialized approaches that are tailored to other bilevel knapsack variants. Motivated by the simple description of the model, by its complexity, by its economic applications, and by the lack of algorithms to solve it, we design a novel viable way for computing optimal solutions. Finally, we present extensive computational results that show the effectiveness of the new algorithm on instances from the literature and on randomly generated instances.
This paper presents a branch-and-price approach to solve personalized tour-scheduling problems in a multiactivity context. Two formulations are considered. In the first, columns correspond to daily shifts that are modeled with context-free grammars, and tours are assembled in the master problem by means of extra constraints. In the second formulation, columns correspond to tours that are built in a two-phase procedure. The first phase involves the composition of daily shifts; the second assembles those shifts to generate tours using a shortest path problem with resource constraints. Both formulations are flexible enough to allow different start times, lengths, and days-off patterns, as well as multiple breaks and continuity and discontinuity in labor requirements. We present computational experiments on problems dealing with up to five work activities and a one-week planning horizon. The results show that the second formulation is stronger in terms of its lower bound and that it is able to find high-quality solutions for all instances with an optimality gap lower than 1%.
In rare situations, stochastic programs can be solved analytically. Otherwise, approximation is necessary to solve stochastic programs with a large or infinite number of scenarios to a desired level of accuracy. This involves statistical sampling or deterministic selection of a finite set of scenarios to obtain a tractable deterministic equivalent problem. Some of these approaches rely on bounds for primal and dual decision variables of the second stage. We develop new algorithms to improve these bounds and reduce the deterministic approximation error. Experiments were conducted to compare a sequential approximation approach with and without these new algorithms. Each algorithm is applied to a set of test instances for a problem of managing semiconductor inventory with downward substitutions, where random variables only appear in the right-hand side of the second stage. Experiments were also conducted using a sample average approximation (SAA) algorithm. The sequential approximation and SAA algorithm generate a feasible solution upon termination. We directly compare the quality of these solutions using a paired student t-test.
We study the ambulance relocation problem in which one tries to respond to possible future incidents quickly. For this purpose, we consider compliance table policies: a relocation strategy commonly used in practice. Each compliance table level indicates the desired waiting site locations for the available ambulances. To compute efficient compliance tables, we introduce the minimum expected penalty relocation problem (MEXPREP), which we formulate as an integer linear program. In this problem, one has the ability to control the number of waiting site relocations. Moreover, different performance measures related to response times, such as survival probabilities, can be incorporated. We show by simulation that the MEXPREP compliance tables outperform both the static policy and compliance tables obtained by the maximal expected coverage relocation problem (MECRP), which both serve as benchmarks. Besides, we perform a study on different relocation thresholds and on two different methods to assign available ambulances to desired waiting sites.
We study a risk-averse approach to multistage stochastic linear programming, where the conditional value-at-risk is incorporated into the objective function as the risk measure. We consider five decompositions of the resulting risk-averse model to solve it via the nested L-shaped method. We introduce separate approximations of the mean and the risk measure and also investigate the effectiveness of multiple cuts. As an application, we formulate a water allocation problem by risk-averse multistage programming, which has the advantage of controlling high-risk severe water shortage events. We apply the proposed formulation to the southeastern portion of Tucson, AZ to best use the limited water resources available to that region. In numerical experiments we (1) present a comparative computational study of the risk-averse nested L-shaped variants and (2) analyze the risk-averse approach to the water allocation problem.
We examine the optimal location of Integrated Air Defense System (IADS) missile batteries to protect a country’s assets, formulated as a Defender-Attacker-Defender three-stage sequential, perfect information, zero-sum game between two opponents. We formulate a trilevel nonlinear integer program for this Defender-Attacker-Defender model and seek a subgame perfect Nash equilibrium (i.e., a set of attacker and defender strategies from which neither player has an incentive to deviate). Such a trilevel formulation is not solvable via conventional optimization software, and an exhaustive enumeration of the game tree based on the discrete set of strategies is only tractable for small instances. We develop and test a customized heuristic over a set of small instances having deliberate parametric variations in a designed experiment, comparing its performance to an exhaustive enumeration algorithm. Testing results indicate the enumeration approach to be severely limited for realistically sized instances, so we demonstrate the heuristic on a larger instance from the literature for which it maintains computational efficiency.
One ultimate goal of wireless sensor networks is to collect the sensed data from a set of sensors and transmit them to some sink node via a data gathering tree. In this work, we are interested in data aggregation, where the sink node wants to know the value for a certain function of all sensed data, such as minimum, maximum, average, and summation. Given a data aggregation tree, sensors receive messages from children periodically, merge them with its own packet, and send the new packet to its parent. The problem of finding an aggregation tree with the maximum lifetime has been proved to be NP-hard and can be generalized to finding a spanning tree with the minimum maximum vertex load, where the load of a vertex is a nondecreasing function of its degree in the tree. Although there is a rich body of research in those problems, they either fail to meet a theoretical bound or need high running time. In this paper, we develop a novel algorithm with provable performance bounds for the generalized problem. We show that the running time of our algorithm is in the order of O(mnα(m, n)), where m is the number of edges, n is the number of sensors, and α is the inverse Ackerman function. Though our work is motivated by applications in sensor networks, the proposed algorithm is general enough to handle a wide range of degree-oriented spanning tree problems, including bounded degree spanning tree problem and minimum degree spanning tree problem. When applied to these problems, it incurs a lower computational cost in comparison to existing methods. Simulation results validate our theoretical analysis.
This paper presents an efficient algorithm for an integrated operating room planning and scheduling problem. It combines the assignment of surgeries to operating rooms and scheduling over a short-term planning horizon. This integration results in more stable planning through consideration of the operational details at the scheduling level, and this increases the chance of successful implementation. We take into account the maximum daily working hours of surgeons, prevent the overlapping of surgeries performed by the same surgeon, allow time for the obligatory cleaning when switching from infectious to noninfectious cases, and respect the surgery deadlines. We formulate the problem using a mathematical programming model and develop a branch-and-price-and-cut algorithm based on a constraint programming model for the subproblem. We also develop dominance rules and a fast infeasibility-detection algorithm based on a multidimensional knapsack problem to improve the efficiency of the constraint programming model. The computational results show that our method has an average optimality gap of 2.81% and significantly outperforms a compact mathematical formulation in the literature.
We describe an iterative refinement procedure for computing extended-precision or exact solutions to linear programming (LP) problems. Arbitrarily precise solutions can be computed by solving a sequence of closely related LPs with limited-precision arithmetic. The LPs solved share the same constraint matrix as the original problem instance and are transformed only by modification of the objective function, right-hand side, and variable bounds. Exact computation is used to compute and store the exact representation of the transformed problems, and numeric computation is used for solving LPs. At all steps of the algorithm the LP bases encountered in the transformed problems correspond directly to LP bases in the original problem description. We show that this algorithm is effective in practice for computing extended-precision solutions and that it leads to direct improvement of the best known methods for solving LPs exactly over the rational numbers. Our implementation is publically available as an extension of the academic LP solver SoPlex.
We introduce horizon decomposition in the context of Dantzig-Wolfe decomposition, and apply it to the capacitated lot-sizing problem with setup times. We partition the problem horizon in contiguous overlapping intervals and create subproblems identical to the original problem, but of smaller size. The user has the flexibility to regulate the size of the master problem and the subproblem via two scalar parameters. We investigate empirically which parameter configurations are efficient, and assess their robustness at different problem classes. Our branch-and-price algorithm outperforms state-of-the-art branch-and-cut solvers when tested to a new data set of challenging instances that we generated. Our methodology can be generalized to mathematical programs with a generic constraint structure.
We consider the integer L-shaped method for two-stage stochastic integer programs. To improve the performance of the algorithm, we present and combine two strategies. First, to avoid time-consuming exact evaluations of the second-stage cost function, we propose a simple modification that alternates between linear and mixed-integer subproblems. Next, to better approximate the shape of the second-stage cost function, we present a general framework to generate optimality cuts via a cut-generating linear program that considers information from all solutions found up to any given stage of the method. To address the impact of the proposed approaches, we report computational results on two classes of stochastic integer problems.
In this paper we derive and exploit duality in general two-stage adaptive linear optimization models. The equivalent dualized formulation we derive is again a two-stage adaptive linear optimization model. Therefore, all existing solution approaches for two-stage adaptive models can be used to solve or approximate the dual formulation. The new dualized model differs from the primal formulation in its dimension and uses a different description of the uncertainty set. We show that the optimal primal affine policy can be directly obtained from the optimal affine policy in the dual formulation. We provide empirical evidence that the dualized model in the context of two-stage lot-sizing on a network and two-stage facility location problems solves an order of magnitude faster than the primal formulation with affine policies. We also provide an explanation and associated empirical evidence that offer insight on which characteristics of the dualized formulation make computations faster. Furthermore, the affine policy of the dual formulations can be used to provide stronger lower bounds on the optimality of affine policies.
Oil and gas companies are drilling and developing fields in the Arctic Ocean, which is an environment with ice floes. These companies must protect their platforms from ice floe collisions. One proposal is to use a system that consists of autonomous underwater vehicles (AUVs) and docking stations. The AUVs measure the under-water topography of the ice floes, while the docking stations launch the AUVs and recharge their batteries. Given resource constraints, we optimize locations and quantities for the docking stations and the AUVs, as well as the AUV scheduling policies, to maximize security of the platform. We model the system using a multistage stochastic facility location problem to optimize the docking station locations, the AUV allocations, and the scheduling policies of the AUVs. A two-stage stochastic facility location problem and two efficient online scheduling heuristics provide lower bounds and upper bounds for the multistage model. Even though the model is motivated by an oil industry project, most of the modeling and optimization methods apply more broadly to two-dimensional radial detection.
We consider a Stackelberg game in a network where a leader minimizes the cost of interdicting arcs and a follower seeks the shortest distance between given origin and destination nodes under uncertain arc traveling cost. In particular, we consider a risk-averse leader, who aims to keep high probability that the follower’s traveling distance is longer than a given threshold, interpreted by a chance constraint. Under the assumption of a wait-and-see follower—i.e., the follower selects a shortest path after seeing realizations of the random arc cost—we propose a branch-and-cut algorithm and apply lifting techniques to exploit the combinatorial structure of the risk-averse leader’s interdiction problem. We demonstrate the computational efficacy of our approaches, risk-averse interdiction solution patterns, and result sensitivity, by testing instances of randomly generated grid networks and real-world transportation networks.
In recent years, GPU computing has become an increasingly important tool to develop efficient applications in several areas, including optimization. One of the optimization approaches that seems to take most advantage from GPU computing is dynamic programming. In this paper, we investigate the application of GPU computing to the two-dimensional guillotine cutting problem, solved by dynamic programming. We show a possible implementation and we discuss a number of technical issues. Computational results on test instances available in the literature and on new larger instances show the effectiveness of the dynamic programming approach based on GPU computing for this problem.Data, as supplemental material, are available at http://dx.doi.org/10.1287/ijoc.2016.0693.
In this paper we propose a methodology for constructing decision rules for integer and continuous decision variables in multiperiod robust linear optimization problems. This type of problem finds application in, for example, inventory management, lot sizing, and manpower management. We show that by iteratively splitting the uncertainty set into subsets, one can differentiate the later-period decisions based on the revealed uncertain parameters. At the same time, the problem’s computational complexity stays at the same level, as for the static robust problem. This also holds in the nonfixed recourse situation. In the fixed recourse situation our approach can be combined with linear decision rules for the continuous decision variables. We provide theoretical results on splitting the uncertainty set by identifying sets of uncertain parameter scenarios to be divided for an improvement in the worst-case objective value. Based on this theory, we propose several splitting heuristics. Numerical examples entailing a capital budgeting and a lot sizing problem illustrate the advantages of the proposed approach.
This paper introduces a novel class of generalized assignment problems with location/allocation considerations that arises in several applications including retail shelf space allocation. We consider a set of items where each item may represent a family of products and a set of variable-sized knapsacks that may represent shelves, which comprise contiguous segments having distinct attractiveness. The decision-maker seeks to assign the set of items to these knapsacks, specify their segment assignments within knapsacks, and determine their total allocated space within predetermined lower/upper bounds in a fashion that maximizes a reward-based objective function. We develop an effective optimization-based very large-scale neighborhood search (VLNS), which greatly outperforms the best solution identified by CPLEX within one CPU hour, whereas general-purpose solver heuristics failed to provide feasible solutions to most of the larger instances within a time limit comparable to the VLNS algorithm run times. Our computational study was carried out on randomly generated computationally challenging instances with up to 210 items and 42 knapsacks and on a case study motivated by a shelf space allocation problem. Our results demonstrate that the proposed approach consistently produces high-quality solutions.
We investigate modeling approaches and exact solution methods for a generalized assignment problem with location/allocation (GAPLA) considerations. In contrast with classical generalized assignment problems, each knapsack in GAPLA is discretized into consecutive segments having different levels of attractiveness. To maximize a total reward function, the decision maker decides not only about item knapsack assignments, but also the specific location of items within their assigned knapsacks and their total space allocation within prespecified lower and upper bounds. Mathematical programming formulations are developed for single and multiple knapsack variants of this problem along with valid inequalities, preprocessing routines, and model enhancements. Further, a branch-and-price algorithm is devised for a set partitioning reformulation of GAPLA, and is demonstrated to yield substantial computational savings over solving the original formulation using branch-and-bound/cut solvers such as CPLEX over challenging problem instances.
Most inventory models in the literature assume that demands are independent among different time periods. However, a number of recent studies suggest that demands are often correlated over different time periods, which motivates our work here. In this paper, we study a class of periodic review (s, S) inventory systems in which demands are correlated and modeled as a Markov-modulated process. Using a Maclaurin series analysis and a Pade approximation, as well as an infinite system of linear equations, we develop algorithms to calculate the moments of the inventory level based on which various performance measures of the system can also be evaluated. Numerical experiments show that our approach is quite efficient and provides accurate estimates for the moments of the inventory level and other related performance measures.
In this paper, we propose and analyze a distributed negotiation strategy for a multi-agent, multi-attribute negotiation in which the agents have no information about the utility functions of other agents. We analytically prove that, if the zone of agreement is nonempty and the agents concede up to their reservation utilities, agents generating offers using our offer-generation strategy, namely the sequential projection strategy, will converge to an agreement acceptable to all the agents; the convergence property does not depend on the specific concession strategy. In considering agents’ incentive to concede during the negotiation, we propose and analyze a reactive concession strategy. Through computational experiments, we demonstrate that our distributed negotiation strategy yields performance sufficiently close to the Nash bargaining solution and that our algorithms are robust to potential deviation strategies. Methodologically, our paper advances the state of the art of alternating projection algorithms, in that we establish the convergence for the case of multiple, moving sets (as opposed to two static sets in the current literature). Our paper introduces a new analytical foundation for a broad class of computational group decision and negotiation problems.
In this paper, stochastic kriging (SK) is considered as a metamodeling tool to capture risk-related properties of complex stochastic system outputs. To better assess the tail behavior of the underlying distribution of a system output, we specifically focus on global prediction of value-at-risk and conditional value-at-risk. Going beyond the standard SK framework intended for metamodeling of mean response surfaces, we rethink the original formulation to allow for the flexibility of utilizing different estimation methods for metamodel construction. The resulting impact on the predictive performance of SK is examined in detail. In parallel with the study by Chen et al. [Chen X, Ankenman BE, Nelson BL (2013) Enhancing stochastic kriging metamodels with gradient estimators. Oper. Res. 61(2):512–528], we further consider the situation in which noisy gradient information can be incorporated into SK metamodel construction and prediction. The theoretical results are illustrated by two numerical examples.
Expansion of natural gas networks is a critical process involving substantial capital expenditures with complex decision-support requirements. Given the nonconvex nature of gas transmission constraints, global optimality and infeasibility guarantees can only be offered by global optimisation approaches. Unfortunately, state-of-the-art global optimisation solvers are unable to scale up to real-world size instances. In this study, we present a convex mixed-integer second-order cone relaxation for the gas expansion planning problem under steady-state conditions. The underlying model offers tight lower bounds with high computational efficiency. In addition, the optimal solution of the relaxation can often be used to derive high-quality solutions to the original problem, leading to provably tight optimality gaps and, in some cases, global optimal solutions. The convex relaxation is based on a few key ideas, including the introduction of flux direction variables, exact McCormick relaxations, on/off constraints, and integer cuts. Numerical experiments are conducted on the traditional Belgian gas network, as well as other real larger networks. The results demonstrate both the accuracy and computational speed of the relaxation and its ability to produce high-quality solutions.
Many patients face difficulties when accessing medical facilities, particularly in rural areas. To alleviate these concerns, medical centers may offer transportation to eligible patients. However, the operation of such services is typically not tightly coordinated with the scheduling of medical appointments. Motivated by our collaborations with the U.S. Veterans Health Administration, we propose an integrated approach that simultaneously considers patient routing and operating room scheduling decisions. We model this problem as a mixed-integer program. Unfortunately, realistically sized instances of this problem are intractable, so we focus on a special case of the problem that captures the needs of low-volume (e.g., rural) hospitals. We establish structural properties that are exploited to develop a branch-and-price algorithm, which greatly outperforms a commercial solver on the original formulation. We discuss several algorithmic strategies to improve the overall solution efficiency. We evaluate the performance of the proposed approach through an extensive computational study calibrated with clinical data. Our results demonstrate that there exist opportunities for healthcare providers to significantly improve the quality of their services by integrating scheduling and routing decisions.
In the Hamiltonian p-median problem (HpMP), the target is to find p cycles that partition a given undirected graph with the objective of minimizing the total sum of the costs of these p cycles. Even though this problem has several applications, the current state-of-the-art algorithms are only able to solve instances with up to 100 nodes. In this paper, we devise a branch-and-price algorithm that is able to solve instances with up to 318 nodes. To achieve this, we modified the set partitioning formulation of HpMP—a minor modification yet with significant algorithmic and computational advantages. Furthermore, our computational results demonstrate that the practical complexity of HpMP and the performance of the algorithms to solve it strongly depend on the value of p. In addition, to solve the pricing problem, we make contributions on a couple of problems that are important on their own right: (1) we develop a new efficient algorithm to find the least-cost cycle in undirected graphs with arbitrary edge costs and no negative cycles; and (2) we develop an algorithm to find the most negative cycle in undirected graphs with arbitrary edge costs. Finally, we prove that for every value of p, HpMP is NP-hard even when restricted to Euclidean graphs.
For a cost-sharing cooperative game with an empty core, we study the problem of calculating a near-optimal cost allocation that satisfies coalitional stability constraints and maximizes the total cost allocated to all players. One application of such a problem is finding the minimum level of subsidy required to stabilize the grand coalition. To obtain solutions, we propose a new generic framework based on Lagrangian relaxation, which has several advantages over existing work that exclusively relies on linear programming (LP) relaxation techniques. Our approach can generate better cost allocations than LP-based algorithms, and is also applicable to a broader range of problems. To illustrate the efficiency and performance of the Lagrangian relaxation framework, we investigate two different facility location games. The results demonstrate that our new approach can find better cost allocations than the LP-based algorithm, or provide alternative optimal cost allocations for cases that the LP-based algorithm can also solve to optimality.
We propose a new exact algorithm for the two-dimensional stage-unrestricted guillotine cutting/packing decision problem, which asks if a set of rectangular items can be cut from a single stock rectangle using guillotine cuts only, with fixed item orientation or with 90-degree item rotation. Our algorithm constructs patterns of items by means of horizontal and vertical builds. To speed up the algorithm and reduce its memory requirement, patterns are constructed in the order of nondecreasing waste, the patterns that use the same subset of items are grouped together, and dominated patterns in each group are discarded. Moreover, a heuristic capable of completing a partial pattern is repeatedly used during the algorithm to quickly determine a feasible solution. Furthermore, the algorithm tries to prove infeasibility with a subset of items before considering all items. We test our algorithm on benchmark instances for the two-dimensional guillotine strip-cutting problem, which we solve by varying the strip height and testing with our algorithm whether a feasible solution exists. We show that our approach outperforms all previously proposed algorithms for the problem with fixed item orientation. Computational experiments for the problem with item rotation are also reported.
We present a new framework for sequential information collection in applications where regression is used to learn about a set of unknown parameters and alternates with optimization to design new data points. Such problems can be handled using the framework of ranking and selection (R&S), but traditional R&S procedures will experience high computational costs when the decision space grows combinatorially. This challenge arises in many applications of business analytics; in particular, we are motivated by the problem of efficiently learning effective strategies for nonprofit fundraising. We present a value of information procedure for simultaneously learning unknown regression parameters and unknown sampling noise. We then develop approximate versions of the procedure, based on optimal quantization, that retain good performance and scale better to large problems.
We propose a framework to model general guillotine restrictions in two-dimensional cutting problems formulated as mixed-integer linear programs (MIPs). The modeling framework requires a pseudopolynomial number of variables and constraints, which can be effectively enumerated for medium-size instances. Our modeling of general guillotine cuts is the first one that, once it is implemented within a state-of-the-art MIP solver, can tackle instances of challenging size. We mainly concentrate our analysis on the guillotine two-dimensional knapsack problem (G2KP), for which a model, and an exact procedure able to significantly improve the computational performance, are given. We also show how the modeling of general guillotine cuts can be extended to other relevant problems such as the guillotine two-dimensional cutting stock problem and the guillotine strip packing problem (GSPP). Finally, we conclude the paper discussing an extensive set of computational experiments on G2KP and GSPP benchmark instances from the literature.
This paper addresses the close-enough traveling salesman problem. In this problem, rather than visiting the vertex (customer) itself, the salesman must visit a specific region containing such vertex. To solve this problem, we propose a simple yet effective exact algorithm, based on branch-and-bound and second order cone programming. The proposed algorithm was tested in 824 instances suggested in the literature. Optimal solutions are obtained for open problems with up to a thousand vertices. We consider instances both in two- and three-dimensional space.
Despite the significant attention they have drawn, big-bucket lot-sizing problems remain notoriously difficult to solve. Previous literature contained results (computational and theoretical) indicating that what makes these problems difficult are the embedded single-machine, single-level, multiperiod submodels. We therefore consider the simplest such submodel, a multi-item, two-period capacitated relaxation. We propose a methodology that can approximate the convex hulls of all such possible relaxations by generating violated valid inequalities. To generate such inequalities, we separate two-period projections of fractional linear programming solutions from the convex hulls of the two-period closure we study. The convex hull representation of the two-period closure is generated dynamically using column generation. Contrary to regular column generation, our method is an outer approximation and can therefore be used efficiently in a regular branch-and-bound procedure. We present computational results that illustrate how these two-period models could be effective in solving complicated problems.
This paper presents a new methodology for university exam timetabling problems, which draws upon earlier work on the Great Deluge metaheuristic. The new method introduces a “flexible” acceptance condition. Even a simple variant of this technique (with fixed flexibility) outperforms the original Great Deluge algorithm. Moreover, it enables a run-time adaptation of an acceptance condition for each particular move. We investigate the adaptive mechanism where the algorithm accepts the movement of exams in a way that is dependent upon the difficulty of assigning that exam. The overall motivation is to encourage the exploration of a wider region of the search space. We present an analysis of the results of our tests of this technique on two international collections of benchmark exam timetabling problems. We show that 9 of 16 solutions in the first collection and 11 of 12 solutions in the second collection produced by our technique have a higher level of quality than previously published methodologies.
On behalf of the Editorial Board, I would like to thank the following people, who acted as Reviewers during the past year.David L. WoodruffEditor-in-Chief
Nonstationary queueing networks are often difficult to approximate. Recent novel methods for approximating the moments of nonstationary queues use the functional version of the Kolmogorov forward equations in conjunction with orthogonal polynomial expansions. However, these methods require closed form expressions for the expectations that appear in the functional Kolmogorov forward equations. When closed form expressions cannot be easily derived, these methods cannot be used. In this paper, we present a new sampling algorithm to overcome this difficulty; our sampling algorithm accurately estimates the expectations using simulation. We apply our algorithm to priority queues, which are useful for modeling hospital triage systems. We show that our sampling algorithm accurately estimates the mean and variance of the priority queue without spending significantly more computational time than integrating ordinary differential equations. Last, we compare our sampling algorithm to the closed form analytical approximations for the Erlang-A queueing model and find that our method is comparable in time and accuracy.
Dynamic pricing for network revenue management has received considerable attention in research and practice. Based on data obtained from a major hotel, we use a large-scale numerical study to compare the performance of several heuristic approaches proposed in the literature. The heuristic approaches we consider include deterministic linear programming with resolving and three variants of dynamic programming decomposition. Dynamic programming decomposition is considered one of the strongest heuristics and is the method chosen in some recent commercial implementations, and remains a topic of research in the recent academic literature. In addition to a plain-vanilla implementation of dynamic programming decomposition, we consider two variants proposed in recent literature. For the base scenario generated from the real data, we show that the method based on Zhang (2011) [An improved dynamic programming decomposition approach for network revenue management. Manufacturing Service Oper. Management 13(1):35–52.] leads to a small but significant lift in revenue compared with all other approaches. We generate many alternative problem scenarios by varying capacity-demand ratio and network structure and show that the performance of the different heuristics can be strongly influenced by both. Overall, our paper shows the promise of some recent proposals in the academic literature but also offers a cautionary tale on the choice of heuristic methods for practical network pricing problems.
We consider pricing problems when customers choose among the products according to the nested logit model and there is a quality consistency constraint on the prices charged for the products. We consider two types of quality consistency constraint. In the first, there is an inherent ordering between the qualities of the products in a particular nest and the price for a product of a higher quality should be larger. In the second type of constraint, different nests correspond to different quality levels; the price for any product in a nest corresponding to a higher quality level should be larger than the price for any product in a nest corresponding to a lower quality level. Prices for the products are chosen from a finite set of possible prices. We develop algorithms to find the prices to charge for the products to maximize the expected revenue obtained from a customer, while adhering to a quality consistency constraint. Our algorithms are based on solving linear programs whose sizes scale polynomially with the number of nests, number of products, and number of possible prices for the products. We also give extensions to the cases beyond the two types of quality consistency constraints. Numerical experiments indicate that our algorithms can effectively compute the optimal prices even when there is a large number of products in consideration.
We present a homogeneous algorithm equipped with a modified potential function for the monotone complementarity problem. We show that this potential function is reduced by at least a constant amount if a scaled Lipschitz condition (SLC) is satisfied. A practical algorithm based on this potential function is implemented in a software package named iOptimize. The implementation in iOptimize maintains global linear and polynomial time convergence properties, while achieving practical performance. It either successfully solves the problem, or concludes that the SLC is not satisfied. When compared with the mature software package MOSEK (barrier solver version 6.0.0.106), iOptimize solves convex quadratic programming problems, convex quadratically constrained quadratic programming problems, and general convex programming problems in fewer iterations. Moreover, several problems for which MOSEK fails are solved to optimality. We also find that iOptimize detects infeasibility more reliably than the general nonlinear solvers Ipopt (version 3.9.2) and Knitro (version 8.0).
With stochastic integer programming as the motivating application, we investigate techniques to use integrality constraints to obtain improved cuts within a Benders decomposition algorithm. We compare the effect of using cuts in two ways: (i) cut-and-project, where integrality constraints are used to derive cuts in the extended variable space, and Benders cuts are then used to project the resulting improved relaxation, and (ii) project-and-cut, where integrality constraints are used to derive cuts directly in the Benders reformulation. For the case of split cuts, we demonstrate that although these approaches yield equivalent relaxations when considering a single split disjunction, cut-and-project yields stronger relaxations in general when using multiple split disjunctions. Computational results illustrate that the difference can be very large, and demonstrate that using split cuts within the cut-and-project framework can significantly improve the performance of Benders decomposition.
The short period right after the release of a song is typically associated with extensive publicity and the highest sales. However, these songs are often found well before the official release date on peer-to-peer (P2P) networks. Not surprisingly, the industry considers this so-called “prerelease” piracy to be extremely damaging to the potential sales of a song. In this paper, we analyze the effect of antipiracy measures on the propagation of unauthorized content in P2P networks. For this purpose, we develop a model that describes the dynamics of demand and supply of piracy from the early stages. Using a unique data set, we quantify the impact of antipiracy measures; specifically, we analyze two antipiracy measures: the reduction of file supply and the reduction of file demand. We find that the impact of a demand reduction on demand is greater than the impact of a supply reduction on supply. From a policy point of view, our results suggest that taking antipiracy measures early on is important. In addition, and potentially more controversial, our results provide support for the notion that to reduce piracy, punishing end users is as effective as controlling the supply of unauthorized music files.
Polynomial discrete programming problems are commonly faced but hard to solve. Treating the nonconvex cross-product terms is the key. State-of-the-art methods usually convert such a problem into a 0-1 mixed-integer linear programming problem and then adopt the branch-and-bound scheme to find an optimal solution. Much effort has been spent on reducing the required numbers of variables and linear constraints as well as on avoiding unbalanced branch-and-bound trees. This study presents a set of equations that linearize the discrete cross-product terms in an extremely effective manner. It is shown that embedding the proposed “equations for linearizing discrete products” into those state-of-the-art methods in the literature not only significantly reduces the required number of linear constraints from O(h3n3) to O(hn) for a cubic polynomial discrete program with n variables in h possible values but also tighten these methods with much more balanced branch-and-bound trees. Numerical experiments confirm a two-order (102-times) reduction in computational time for some randomly generated cubic polynomial discrete programming problems.There is a Video Overview associated with this article, available as supplemental material.
This paper examines a class of three-stage sequential defender-attacker-defender problems. In these problems the defender first selects a subset of assets to protect, the attacker next damages a subset of unprotected assets in the “interdiction” stage, after which the defender optimizes a “recourse” problem over the surviving assets. These problems are notoriously difficult to optimize, and almost always require the recourse problem to be a convex optimization problem. Our contribution is a new approach to solving defender-attacker-defender problems. We require all variables in the first two stages to be binary-valued, but allow the recourse problem to take any form. The proposed framework focuses on solving the interdiction problem by restricting the defender to select a recourse decision from a sample of feasible vectors. The algorithm then iteratively refines the sample to force finite convergence to an optimal solution. We demonstrate that our algorithm not only solves interdiction problems involving NP-hard recourse problems within reasonable computational limits, but it also solves shortest path fortification and interdiction problems more efficiently than state-of-the-art algorithms tailored for that problem, finding optimal solutions to real-road networks having up to 300,000 nodes and over 1,000,000 arcs.
We consider a finite-buffer single-server queue with batch Markovian arrival process. In the case of finite-buffer batch arrival queue, there are different customer rejection/acceptance strategies such as partial batch rejection, total batch rejection, and total batch acceptance policy. We consider partial batch rejection strategy throughout our paper. We obtain queue length distributions at various epochs such as pre-arrival, arbitrary, and post-departure as well as some important performance measures, like probability of loss for the first, an arbitrary, and the last customer of a batch, mean queue lengths, and mean waiting times. The corresponding queueing model under single and multiple vacation policy has also been investigated. Some numerical results have been presented in the form of tables by considering phase-type and Pareto service time distributions. The proposed analysis is based on the successive substitutions in the Markov chain equations of the queue-length distribution at an embedded post-departure epoch of a customer. We also establish relationships among the queue-length distributions at post-departure, arbitrary, and pre-arrival epochs using the classical argument based on Markov renewal theory and semi-Markov processes. Such queueing systems find applications in the performance evaluation of teletraffic part in 4G A11-IP networks.
We study the planar maximum coverage location problem (MCLP) with rectilinear distance and rectangular demand zones in the case where “partial coverage” is allowed in its true sense, i.e., when covering part of a demand zone is allowed and the coverage accrued as a result of this is proportional to the demand of the covered part only. We pose the problem in a slightly more general form by allowing service zones to be rectangular instead of squares, thereby addressing applications in camera view-frame selection as well. More specifically, our problem, referred to as PMCLP-PCR (planar MCLP with partial coverage and rectangular demand and service zones), is to position a given number of rectangular service zones (SZs) on the two-dimensional plane to (partially) cover a set of existing (possibly overlapping) rectangular demand zones (DZs) such that the total covered demand is maximized. Previous studies on (planar) MCLP have assumed binary coverage, even when nonpoint objects such as lines or polygons have been used to represent demand. Under the binary coverage assumption, the problem can be readily formulated and solved as a binary linear program; whereas, partial coverage, although much more realistic, cannot be efficiently handled by binary linear programming, making PMCLP-PCR much more challenging to solve. In this paper, we first prove that PMCLP-PCR is NP-hard if the number of SZs is part of the input. We then present an improved algorithm for the single-SZ PMCLP-PCR, which is at least two times faster than the existing exact plateau vertex traversal algorithm. Next, we study multi-SZ PMCLP-PCR for the first time and prove theoretical properties that significantly reduce the search space for solving this problem, and we present a customized branch-and-bound exact algorithm to solve it. Our computational experiments show that this algorithm can solve relatively large instances of multi-SZ PMCLP-PCR in a short time. We also propose a fast polynomial time heuristic algorithm. Having optimal solutions from our exact algorithm, we benchmark the quality of solutions obtained from our heuristic algorithm. Our results show that for all the random instances solved to optimality by our exact algorithm, our heuristic algorithm finds a solution in a fraction of a second, where its objective value is at least 91% of the optimal objective in 90% of the instances (and at least 69% of the optimal objective in all the instances).
We propose an aggregation method to reduce the size of column generation (CG) models for covering problems in which the feasible subsets depend on a resource constraint. The aggregation relies on a correlation between the resource consumption of the elements and the corresponding optimal dual values. The resulting aggregated dual model is a restriction of the original one, and it can be rapidly optimized to obtain a feasible dual solution. A primal bound can also be obtained by restricting the set of columns to those saturated by the dual feasible solution obtained by aggregation. The convergence is realized by iterative disaggregation until the gap is closed by the bounds. Computational results show the usefulness of our method for different cutting-stock problems. An important advantage is the fact that it can produce high-quality dual bounds much faster than the traditional Lagrangian bound used in stabilized column generation.
We present in this paper a new generic approach to variable branching in branch and bound for mixed-integer linear problems. Our approach consists in imitating the decisions taken by a good branching strategy, namely strong branching, with a fast approximation. This approximated function is created by a machine learning technique from a set of observed branching decisions taken by strong branching. The philosophy of the approach is similar to reliability branching. However, our approach can catch more complex aspects of observed previous branchings to take a branching decision. The experiments performed on randomly generated and MIPLIB problems show promising results.
We introduce a framework for finding and analyzing all optimal solutions to a linear-programming-based model for cellular metabolism. The implementation of a pivoting-based method for generating alternate optimal reaction fluxes is described. We present a novel strongly polynomial algorithm to decompose a matrix of alternate optimal solutions of an optimization problem into independent subsets of variables and their respective alternate solutions. The matrix can be reconstructed as a Cartesian product of these subsets of alternate solutions. We demonstrate that our strategy for enumeration is more efficient than other methods, and that our Cartesian product matrix decomposition can quickly recover independent substructures. The framework is applied to analyze the metabolic reconstruction of a disease-causing organism, revealing metabolic pathways that are independently regulated.Data and the online supplement are available at https://doi.org/10.1287/ijoc.2016.0724.
We consider two types of convex approximations of two-stage totally unimodular integer recourse models. Although worst-case error bounds are available for these approximations, their actual performance has not yet been investigated, mainly because this requires solving the original recourse model. In this paper we assess the quality of the approximating solutions using Monte Carlo sampling, or more specifically, using the so-called multiple replications procedure. Based on numerical experiments for an integer newsvendor problem, a fleet allocation and routing problem, and a stochastic activity network investment problem, we conclude that the error bounds are reasonably sharp if the variability of the random parameters in the model is either small or large; otherwise, the actual error of using the convex approximations is much smaller than the error bounds suggest. Moreover, we conclude that the solutions obtained using the convex approximations are good only if the variability of the random parameters is medium to large. In case this variability is small, however, typically sampling methods perform best, even with modest sample sizes. In this sense, the convex approximations and sampling methods can be considered as complementary solution methods. Moreover, as required for our applications, we extend our approach to derive new error bounds dealing with deterministic second-stage side constraints and relatively complete recourse, and perfect dependencies in the right-hand side vector.
We consider several integrated production, inventory, and delivery problems that arise in a number of practical settings where customer orders have pre-specified delivery time windows. These orders are first processed in a plant and then delivered to the customers by transporters (such as trains and air flights) which have fixed delivery departure times. If an order is completed but not immediately delivered by a transporter, the order is kept temporarily in inventory, which incurs an inventory cost. There is a delivery cost for delivering an order, which varies with the departure time. Given a set of orders, the objective is to find an integrated schedule for processing the orders, keeping finished orders in inventory if necessary, and delivering them to the customers such that the total inventory and delivery cost is minimum. We consider two classes of problems: where order delivery is splittable and where order delivery is nonsplittable. For each of the problems considered, we study its computational complexity by either showing that the problem is NP-hard or proposing an algorithm that can find an optimal solution. For the two most general problems, we show that any polynomial time algorithm has an arbitrarily bad worst-case performance bound, and propose combined column generation and tabu search heuristic algorithms that can find near optimal solutions for them in a reasonable computational time.The online appendix is available at https://doi.org/10.1287/ijoc.2016.0726.
Queueing networks featuring fork/join stations are natural models for a variety of computer and manufacturing systems. Unfortunately, an exact solution for a Markovian fork/join network can only be obtained by analyzing the underlying Markov chain using numerical methods, and these methods are computationally feasible only for networks with small population sizes and numbers of service stations. In this paper we present a new, simple, and accurate analytical approximation method to estimate the throughput (and other performance metrics) of a closed queueing network that features a single fork/join station receiving inputs from general subnetworks. An extensive numerical study illustrates the high accuracy of our proposed technique, especially for networks with large populations and numbers of stations. It also shows that the accuracy of our approximation method improves with increasing population size, deteriorating network balance, and increasing number of stations when the added stations weaken the network balance. Furthermore, our method has significant computational advantages compared to simulation and existing approximation techniques, the latter of which are in general less accurate than ours and in many cases even fail to provide a solution in our numerical study. We also bound analytically the relative error of our method for a broad class of networks, which provides theoretical support for some of our numerical observations.The online appendix is available at https://doi.org/10.1287/ijoc.2016.0727.
Input distortion is a common problem faced by expert systems, particularly those deployed with a Web interface. In this study, we develop novel methods to distinguish liars from truth-tellers, and redesign rule-based expert systems to address such a problem. The four proposed methods are termed split tree (ST), consolidated tree (CT), value-based split tree (VST), and value-based consolidated tree (VCT), respectively. Among them, ST and CT aim to increase an expert system’s accuracy of recommendations, and VST and VCT attempt to reduce the misclassification cost resulting from incorrect recommendations. We observe that ST and VST are less efficient than CT and VCT in that ST and VST always require selected attribute values to be verified, whereas CT and VCT do not require value verification under certain input scenarios. We conduct experiments to compare the performances of the four proposed methods and two existing methods, i.e., the traditional true tree (TT) method that ignores input distortion and the knowledge modification (KM) method proposed in prior research. The results show that CT and ST consistently rank first and second, respectively, in maximizing the recommendation accuracy, and VCT and VST always lead to the lowest and second lowest misclassification cost. Therefore, CT and VCT should be the methods of choice in dealing with users’ lying behaviors. Furthermore, we find that KM is outperformed by not only the four proposed methods, but sometimes even by the TT method. This result further confirms the advantage necessity of differentiating liars from truth-tellers when both types of users exist in the population.A supplemental video can be found at https://doi.org/10.1287/ijoc.2016.0728.
In this paper, we study a multisourcing supply network design problem, in which each retailer faces uncertain demand and can source products from more than one distribution center (DC). The decisions to be simultaneously optimized include DC locations and inventory levels, which set of DCs serves each retailer, and the amount of shipments from DCs to retailers. We propose a nonlinear mixed integer programming model with a joint chance constraint describing a certain service level. Two approaches—set-wise approximation and linear decision rule-based approximation—are constructed to robustly approximate the service level chance constraint with incomplete demand information. Both approaches yield sparse multisourcing distribution networks that effectively match uncertain demand using on-hand inventory, and hence successfully reach a high service level. We show through extensive numerical experiments that our approaches outperform other commonly adopted approximations of the chance constraint.
Clusterwise linear regression (CLR), a clustering problem intertwined with regression, finds clusters of entities such that the overall sum of squared errors from regressions performed over these clusters is minimized, where each cluster may have different variances. We generalize the CLR problem by allowing each entity to have more than one observation and refer to this as generalized CLR. We propose an exact mathematical programming-based approach relying on column generation, a column generation–based heuristic algorithm that clusters predefined groups of entities, a metaheuristic genetic algorithm with adapted Lloyd’s algorithm for K-means clustering, a two-stage approach, and a modified algorithm of Späth [Späth (1979) Algorithm 39 clusterwise linear regression. Comput. 22(4):367–373] for solving generalized CLR. We examine the performance of our algorithms on a stock-keeping unit (SKU)-clustering problem employed in forecasting halo and cannibalization effects in promotions using real-world retail data from a large supermarket chain. In the SKU clustering problem, the retailer needs to cluster SKUs based on their seasonal effects in response to promotions. The seasonal effects result from regressions with predictors being promotion mechanisms and seasonal dummies performed over clusters generated. We compare the performance of all proposed algorithms for the SKU problem with real-world and synthetic data.The online supplement is available at https://doi.org/10.1287/ijoc.2016.0729.
We present algorithm MIQCR-CB that is an advancement of MIQCR. MIQCR is a method for solving mixed-integer quadratic programs and works in two phases: the first phase determines an equivalent quadratic formulation with a convex objective function by solving a semidefinite problem (SDP); in the second phase, the equivalent formulation is solved by a standard solver. Because the reformulation relies on the solution of a large-scale semidefinite program, it is not tractable by existing semidefinite solvers even for medium-sized problems. To surmount this difficulty, we present in MIQCR-CB a subgradient algorithm within a Lagrangian duality framework for solving (SDP) that substantially speeds up the first phase. Moreover, this algorithm leads to a reformulated problem of smaller size than the one obtained by the original MIQCR method, which results in a shorter time for solving the second phase. We present extensive computational results to show the efficiency of our algorithm. First, we apply MIQCR-CB to the k-cluster problem that can be formulated by a binary quadratic program. As an illustration of the efficiency of our new algorithm, for instances of size 80 and of density 25%, MIQCR-CB is on average 78 times faster for phase 1 and 24 times faster for phase 2 than the original MIQCR. We also compare MIQCR-CB with QCR and with BiqCrunch, two methods devoted to binary quadratic programming. We show that MIQCR-CB is able to solve most of the 225 considered instances within three hours of CPU time. We also present experiments on two classes of general integer instances where we compare MIQCR-CB with MIQCR, Couenne, and Cplex12.6. We demonstrate the significant improvement over the original MIQCR approach. Finally, we show that MIQCR-CB is able to solve almost all of the considered instances, whereas Couenne and Cplex12.6 are not able to solve half of them.
This paper presents an algorithmic strategy to nonstationary policy search for finite-horizon, discrete-time Markovian decision problems with large state spaces, constrained action sets, and a risk-sensitive optimality criterion. The methodology relies on modeling time-variant policy parameters by a nonparametric response surface model for an indirect parametrized policy motivated by Bellman’s equation. The policy structure is heuristic when the optimization of the risk-sensitive criterion does not admit a dynamic programming reformulation. Through the interpolating approximation, the level of nonstationarity of the policy, and consequently, the size of the resulting search problem can be adjusted. The computational tractability and the generality of the approach follow from a nested parallel implementation of derivative-free optimization in conjunction with Monte Carlo simulation. We demonstrate the efficiency of the approach on an optimal energy storage charging problem, and illustrate the effect of the risk functional on the improvement achieved by allowing a higher complexity in time variation for the policy.The online supplement is available at https://doi.org/10.1287/ijoc.2016.0733.
Robust optimization is a methodology that can be applied to problems that are affected by uncertainty in their parameters. The classical robust counterpart of a problem requires the solution to be feasible for all uncertain parameter values in a so-called uncertainty set and offers no guarantees for parameter values outside this uncertainty set. The globalized robust counterpart (GRC) extends this idea by allowing controlled constraint violations in a larger uncertainty set. The constraint violations are controlled by the distance of the parameter from the original uncertainty set. We derive tractable GRCs that extend the initial GRCs in the literature: our GRC is applicable to nonlinear constraints instead of only linear or conic constraints, and the GRC is more flexible with respect to both the uncertainty set and distance measure function, which are used to control the constraint violations. In addition, we present a GRC approach that can be used to provide an extended trade-off overview between the objective value and several robustness measures.
In this paper we demonstrate how a key adjustment to known numerically exact methods for evaluating time-dependent moments of the number of entities in the Pht/Pht/∞ queueing system and [Pht/Pht/∞]K queueing network may be implemented to capture the effect of autocorrelation that may be present in arrivals to the more general MAPt/Pht/∞ queueing system and multiclass [MAPt/Pht/∞]K queueing network. The MAPt is more general than the Pht arrival process in that it allows for stationary nonrenewal point processes, as well as the time-dependent generalization of nonrenewal point processes. Modeling real-world systems with bursty arrival processes such as those in telecommunications and transportation, for example, necessitate the use of nonrenewal processes. Finally, we show that the covariance of the number of entities at different nodes and times may be described by a single closed differential equation.The online supplement is available at https://doi.org/10.1287/ijoc.2016.0736.
We consider the inventory routing problem, in which a supplier has to replenish a set of customers by means of a limited fleet of capacitated vehicles over a discrete time horizon. The goal is to minimize the total cost of the distribution that comprises the inventory cost at the supplier and at the customers and the routing cost. We present a matheuristic that combines a tabu search and mathematical programming formulations. When compared with two exact methods on 640 small instances, the matheuristic finds 192 (48%) optima over the 402 instances with known optima and improves 125 upper bounds. Tested on 240 large instances (with up to 200 customers) for which no optimal solutions are known, it improves the best solution for 220 (92%) of the 240 instances.The online supplement is available at https://doi.org/10.1287/ijoc.2016.0737.
We consider the dynamic facility location problem with generalized modular capacities, a multiperiod facility location problem in which the costs for capacity changes may differ for all pairs of capacity levels. The problem embeds a complex cost structure and generalizes several existing facility location problems, such as those that allow temporary facility closing or capacity expansion and reduction. As the model may become very large, general-purpose mixed-integer programming (MIP) solvers are limited to solving instances of small to medium size. In this paper, we extend the generalized model to the case of multiple commodities. We propose Lagrangian heuristics, based on subgradient and bundle methods, to find good quality solutions for large-scale instances with up to 250 facility locations and 1,000 customers. To improve the final solution quality, a restricted MIP model is solved based on the information collected through the solution of the Lagrangian dual. Computational results show that the Lagrangian-based heuristics provide highly reliable results for all problem variants considered. They produce good quality solutions in short computing times even for instances where state-of-the-art MIP solvers do not find feasible solutions. The strength of the formulation also allows the method to provide tight bounds on the optimal value.Data and the online appendix are available at https://doi.org/10.1287/ijoc.2016.0738.
The success of a recommender system is generally evaluated with respect to the accuracy of recommendations. However, recently diversity of recommendations has also become an important aspect in evaluating recommender systems. One dimension of diversity is called aggregate diversity, which refers to the diversity of items in the recommendation lists of all users and can be defined with different metrics. The maximization of both accuracy and the aggregate diversity simultaneously renders a multiobjective optimization problem that can be handled by different approaches. In this paper, after providing a thorough analysis of the multiobjective optimization approaches for this problem, we propose a new model that takes into account both accuracy and aggregate diversity. Different from previous works, our model is specifically designed to incorporate distributional diversity metrics, which measure how evenly the items are distributed in the recommendation lists of users. To solve the large-scale instances, we propose a column generation algorithm and a Lagrangian relaxation approach based on the decomposition of the model. We present the results of the mathematical models and the performance of the proposed methodology that are obtained by computational experiments on real-world data sets. These results reveal that our model successfully captures the trade-off between the objectives and reaches very high levels of distributional diversity.
We present a spatiotemporally integrated formulation of the optimal fractionation problem using the standard log-linear-quadratic survival model. Our objective is to choose a fluence map and a number of fractions to maximize the biological effect of tumor dose averaged over its voxels subject to maximum dose, mean dose, and dose-volume constraints for various normal tissues. Constraints are expressed in biologically effective dose equivalents. We propose an efficient convex programming method to approximately solve the resulting computationally difficult model.Through extensive computer simulations on 10 head-and-neck and prostate cancer test cases with a broad range of radiobiological parameters, we compare the biological effect on tumors obtained by our integrated approach relative to that from two other models. The first is a traditional intensity modulated radiation therapy (IMRT) fluence map optimization model that does not optimize the number of fractions. The second assumes that a fluence map is available a priori from a traditional IMRT optimization model and then optimizes the number of fractions, thus separating the spatial and temporal components.The improvements in tumor biological effect over IMRT were 9%–52%, with an average of 22% for head-and-neck, and 53%–108%, with an average of 69% for prostate. The improvements in tumor biological effect over the spatiotemporally separated model were 15%–45%, with an average of 27%, and 17%–23%, with an average of 21%, for head-and-neck and prostate, respectively. This suggests that integrated optimization of the fluence map and the number of fractions could improve treatment efficacy, as measured within the linear-quadratic framework.
It is well documented in management literature that characteristics of collaboration processes strongly influence team performance in a business environment. However, little work has been done on how specific collaboration process patterns affect teamwork performance, leading to an open issue in collaboration management. To address this research gap, we develop a Collaboration Process Pattern (CPP) approach that analyzes teamwork performance by mining collaboration system logs from open source software development. Our research is novel in three ways. First, our research is fact-driven, as the result is based on teamwork tracking logs. Second, we develop a pattern mining approach based on sequence mining and graph mining. Third, using time-dependent Cox regression, our approach derives business insights from real-world collaboration data that are directly applicable to managerial actions. Our empirical study identifies collaboration patterns that can lead to more efficient teamwork. It also shows that the effects of collaboration patterns vary depending on the types of tasks. These findings are of significant business value since they suggest that managers should carefully prioritize their limited attention on certain types of tasks for intervention.Data and the online supplement are available at https://doi.org/10.1287/ijoc.2016.0739.
We study the knapsack problem with conflict graph (KPCG), an extension of the 0-1 knapsack problem, in which a conflict graph describing incompatibilities between items is given. The goal of the KPCG is to select the maximum profit set of compatible items while satisfying the knapsack capacity constraint. We present a new branch-and-bound approach to derive optimal solutions to the KPCG in short computing times. Extensive computational experiments are reported showing that, for instances with graph density of 10% and larger, the proposed method outperforms a state-of-the-art approach and mixed-integer programming formulations tackled through a general purpose solver.The online supplement is available at https://doi.org/10.1287/ijoc.2016.0742.
In this paper, we use a branch decomposition technique to improve approximations to the p-median problem. Starting from a support graph produced either by a combination of heuristics or by linear programming, we use dynamic programming guided by a branch decomposition of that support graph to find the best p-median solution on the support graph. Our results show that when heuristics are used to build the support graph and the support graph has branchwidth at most 7, our algorithm is able to provide a solution of lower cost than any of the heuristic solutions. When linear programming is used to build the support graph and the support graph has branchwidth at most 7, then our algorithm provides better solutions than popular heuristics and is faster than integer programming. Thus, our algorithm is a useful practical tool when support graphs have branchwidth at most 7.
The vehicle routing problem with time windows (VRPTW) consists of finding least-cost vehicle routes to satisfy the demands of customers that can be visited within specific time windows. We introduce two enhancements for the exact solution of the VRPTW by branch-price-and-cut (BPC). First, we develop a sharper form of the limited-memory subset-row inequalities by representing the memory as an arc subset rather than a node subset. Second, from the elementary inequalities introduced by Balas in 1977, we derive a family of inequalities that dominate them. These enhancements are embedded into an exact BPC algorithm that includes state-of-the-art features such as bidirectional labeling, decremental state-space relaxation, completion bounds, variable fixing, and route enumeration. Computational results show that these enhancements are particularly effective for the most difficult instances and that our BPC algorithm can solve all 56 Solomon instances with 100 customers and 51 of 60 Gehring and Homberger instances with 200 customers.
Oftentimes businesses face the challenge of requiring costly information to improve the accuracy of prediction tasks. One notable example is obtaining informative customer feedback (e.g., customer-product ratings via costly incentives) to improve the effectiveness of recommender systems. In this paper, we develop a novel active learning approach, which aims to intelligently select informative training instances to be labeled so as to maximally improve the prediction accuracy of a real-valued prediction model. We focus on large, heterogeneous, and dyadic data, and on localized modeling techniques, which have been shown to model such data particularly well, as compared to a single, “global” model. Importantly, dyadic data with covariates is pervasive in contemporary big data applications such as large-scale recommender systems and search advertising. A key benefit from incorporating dyadic information is their simple, meaningful representation of heterogeneous data, in contrast to alternative local modeling techniques that typically produce complex and incomprehensible predictive patterns. We develop a computationally efficient active learning policy specifically tailored to exploit multiple local prediction models to identify informative acquisitions. Existing active learning policies are often computationally prohibitive for the setting we explore, and our policy makes the application of active learning computationally feasible for this setting. We present comprehensive empirical evaluations that demonstrate the benefits of our approach and explore its performance in real world, challenging domains.
The coupled lot-sizing and cutting-stock problem has been a challenging and significant problem for industry, and has therefore received sustained research attention. The quality of the solution is a major determinant of cost performance in related production and inventory management systems, and therefore there is intense pressure to develop effective practical solutions. In the literature, a number of heuristics have been proposed for solving the problem. However, the heuristics are limited in obtaining high solution qualities. This paper proposes a new progressive selection algorithm that hybridizes heuristic search and extended reformulation into a single framework. The method has the advantage of generating a strong bound using the extended reformulation, which can provide good guidelines on partitioning and sampling in the heuristic search procedure to ensure an efficient solution process. We also analyze per-item and per-period Dantzig–Wolfe decompositions of the problem and present theoretical comparisons. The master problem of the per period Dantzig–Wolfe decomposition is often degenerate, which results in a tailing-off effect for column generation. We apply a hybridization of Lagrangian relaxation and stabilization techniques to improve the convergence. The discussion is followed by extensive computational tests, where we also perform detailed statistical analyses on various parameters. Comparisons with other methods indicate that our approach is computationally tractable and is able to obtain improved results.The online supplement is available at https://doi.org/10.1287/ijoc.2017.0746.
The resolution of integer programming problems is typically performed via branch and bound. Nodes of the branch-and-bound tree are pruned whenever the corresponding subproblem is proven not to contain a solution better than the best solution found so far. This is a key ingredient for achieving reasonable solution times. However, since subproblems are solved in floating-point arithmetic, numerical errors can occur and may lead to inappropriate pruning. As a consequence, optimal solutions may be cut off. We propose several methods for avoiding this issue, in the special case of a branch-cut-and-price formulation for the capacitated vehicle routing problem. The methods are based on constructing dual feasible solutions for the linear programming relaxations of the subproblems and obtaining, by weak duality, bounds on their objective function value. Such approaches have been proposed before for formulations with a small number of variables (dual constraints), but the problem becomes more complex when the number of variables is exponentially large, which is the case in consideration. We show that, in practice, along with being safe, our bounds are stronger than those usually employed, obtained with unsafe floating-point arithmetic plus some heuristic tolerance, and all of this at a negligible computational cost. We also discuss some potential advantages and other uses of our safe bounds derivation.The online supplement is available at https://doi.org/10.1287/ijoc.2017.0747.
Operating rooms (ORs) play a substantial role in hospital profitability, and their optimal utilization is conducive to containing the cost of surgical service delivery, shortening surgical patient wait times, and increasing patient admissions. We extend the OR planning and scheduling problem from a single independent hospital to a coalition of multiple hospitals in a strategic network, where a pool of patients, surgeons, and ORs are collaboratively planned. To solve the resulting mixed-integer dual resource constrained model, we develop a novel logic-based Benders’ decomposition approach that employs an allocation master problem, sequencing sub-problems for each hospital-day, and novel multistrategy Benders’ feasibility and optimality cuts. We investigate various patient-to-surgeon allocation flexibilities, as well as the impact of surgeon schedule tightness. Using real data obtained from the General Surgery Departments of the University Health Network (UHN) hospitals, consisting of Toronto General Hospital, Toronto Western Hospital, and Princess Margret Cancer Centre in Toronto, Ontario, Canada (who already engage in some collaborative resource sharing), we find that on average, collaborative OR scheduling with traditional patient-to-surgeon allocation flexibility results in 6% cost-savings, while flexible patient-to-surgeon allocation flexibility increases cost-savings to 40%, and surgeon schedule tightness can impact costs by 15%. The collective impact of our collaboration and patient flexibility results in between 45% and 63% savings per surgery. We also use a game theoretic approach to fairly redistribute the payoff acquired from a coalition of hospitals and to empirically show coalitional stability among hospitals.Data and the online supplement are available at https://doi.org/10.1287/ijoc.2017.0745.
We present the algorithm SOCEMO for optimization problems that have multiple conflicting computationally expensive black-box objective functions. The computational expense arising from the objective function evaluations considerably restricts the number of evaluations that can be done to find Pareto-optimal solutions. Frequently used multiobjective optimization methods are based on evolutionary strategies and generally require a prohibitively large number of function evaluations to find a good approximation of the Pareto front. SOCEMO, in contrast, employs surrogate models to approximate the expensive objective functions. These surrogate models are used in the iterative sampling process to decide at which points in the variable domain the next expensive evaluations should be done. Therefore, fewer expensive objective function evaluations are needed, and a good approximation of the Pareto front can be found efficiently. Previous algorithms have generally been tested on problems with few variables (up to 10) and few objective functions (up to 5). In our numerical study, we show that our algorithm performs well for benchmark problems with up to 35 dimensions and up to 10 objective functions, as well as two engineering application problems. We compared the performance of SOCEMO to a variant of NSGA-II and show that SOCEMO’s sophisticated search strategy is more efficient than NSGA-II when the number of allowable function evaluations is low.The online supplement is available at https://doi.org/10.1287/ijoc.2017.0749.
We study a problem that integrates buy-at-bulk network design into the classical facility location problem. We consider a generalization of the facility location problem where multiple clients may share a capacitated network to connect to open facilities instead of requiring direct links. In this problem, we wish to open facilities, build a routing network by installing access cables of different costs and capacities, and route every client demand to an open facility.We provide a path-based formulation and we compare it with the natural compact formulation for this problem. We then design an exact branch-price-and-cut algorithm for solving the path-based formulation. We study the effect of two families of valid inequalities. In addition to this, we present three different types of primal heuristics and employ a hybrid approach to effectively combine these heuristics in order to improve the primal bounds. We finally report the results of our approach that were tested on a set of real world instances, as well as two sets of benchmark instances and evaluate the effects of our valid inequalities and primal heuristics.
Multistage stochastic optimization leads to NLPs over scenario trees that become extremely large when many time stages or fine discretizations of the probability space are required. Interior-point methods are well suited for these problems if the arising huge, structured KKT systems can be solved efficiently, for instance, with a large scenario tree but a moderate number of variables per node. For this setting we develop a distributed implementation based on data parallelism in a depth-first distribution of the scenario tree over the processes. Our theoretical analysis predicts very low memory and communication overheads. Detailed computational experiments confirm this prediction and demonstrate the overall performance of the algorithm. We solve multistage stochastic quadratic programs with up to 400 × 106 variables and 8.59 × 109 KKT matrix entries or 136 × 106 variables and 12.6 × 109 entries on a compute cluster with 384 GB RAM. Data are available at https://doi.org/10.1287/ijoc.2017.0748.
This paper presents a new model for online decision making. Motivated by the healthcare delivery application of dynamically allocating patients to procedure rooms in outpatient procedure centers, the online stochastic extensible bin-packing problem is described. The objective is to minimize the combined costs of opening procedure rooms and utilizing overtime to complete a day’s procedures. The dynamic patient-allocation decisions are made in an uncertain environment where the number of patients scheduled and the procedure durations are not known in advance. The resulting optimization model’s tractability focuses the paper’s attention on approximation methods and a special case that is amenable to decomposition-based solution methods. Theoretical performance guarantees are presented for list-based approximation methods as well as an approximation that is common in practice, where procedure rooms are reserved for patient groups in advance. Numerical results based on a real outpatient procedure center demonstrate the favorable results of the list-based approximations based on their average and worst case performances, as well as their computational requirements. Further, the numerical experiments show that the policy of reserving procedure rooms for patient groups in advance can perform poorly. These results are contrary to common practice and favor alternative, and still easy-to-implement, policies.The online supplement is available at https://doi.org/10.1287/ijoc.2017.0750.
Most power grid systems are operated to be N-1 secure, meaning that the system can withstand the failure of any one component. There is increasing interest in more stringent security standards, where the power grid must be able to survive the (nearly) simultaneous failure of k components (i.e., N-k). However, this improved reliability criterion significantly increases the number of contingency scenarios that must be considered when solving the unit commitment problem. Additional computational complexity is introduced when taking into account transmission switching. This relatively inexpensive method of redirecting power flows in the grid has been proposed as a way of introducing flexibility to better survive failure events. We present an algorithm for solving the unit commitment problem that simultaneously addresses both the challenges of the N-k security requirement and the use of transmission switching during operation. We analyze the algorithmic performance and present computational results for the IEEE24 and RTS-96 test systems for k = 1 and 2. We also include a discussion of how this approach might be extended to solve problems with k ≥ 3.The online supplement, and data, are available at https://doi.org/10.1287/ijoc.2017.0751.
Recent years have witnessed a rapid increase in online data volume and the growing challenge of information overload for web use and applications. Thus, information diversity is of great importance to both information service providers and users of search services. Based on a diversity evaluation measure (namely, information coverage), a heuristic method—FastCovC+S-Select—with corresponding algorithms is designed on the greedy submodular idea. First, we devise the CovC+S-Select algorithm, which possesses the characteristic of asymptotic optimality, to optimize information coverage using a strategy in the spirit of simulated annealing. To accelerate the efficiency of CovC+S-Select, its fast approximation (i.e., FastCovC+S-Select) is then developed through a heuristic strategy to downsize the solution space with the properties of information coverage. Furthermore, ample experiments have been conducted to show the effectiveness, efficiency, and parameter robustness of the proposed method, along with comparative analyses revealing the performance’s advantages over other related methods.The online appendix is available at https://doi.org/10.1287/ijoc.2017.0753.
The quadratic assignment problem (QAP) is a combinatorial optimization problem that arises in many real-world applications, such as equipment allocation in industry. The QAP is NP-hard and, in practice, one of the hardest combinatorial optimization problems to solve to optimality. Exact solutions of QAP are typically obtained by the branch-and-bound method. This method, however, potentially requires a high computational effort, and the use of good lower bounds is essential to prune the search tree. Branch-and-bound algorithms that use the dual-ascent procedure based on the level-2 reformulation linearization technique (RLT2) belong to the state of the art on exactly solving QAP. In this work, we propose a parallel implementation of that branch-and-bound algorithm. Our approach uses the Auction Algorithm of Bertsekas and Castañon to solve the linear assignment problems of RLT2, which allows us to take advantage of the massive parallel environment of graphics processing units to speed up the lower bound computation and implement some memory optimization techniques to address large-size problems. We report experimental results that show significant execution time reductions compared to previous works and allow us to provide, for the first time, exact solutions for two instances of QAP: tai35b and tai40b.
This paper presents a novel and computationally efficient methodology for approximating the queue length (the number of customers in the system) distributions of time-varying non-Markovian many-server queues (e.g., Gt/Gt/nt queues), where the number of servers (nt) is large. Our methodology consists of two steps. The first step uses phase-type distributions to approximate the general interarrival and service times, thus generating an approximating Pht/Pht/nt queue. The second step develops strong approximation theory to approximate the Pht/Pht/nt queue with fluid and diffusion limits whose mean and variance can be computed using ordinary differential equations. However, by naively representing the Pht/Pht/nt queue as a Markov process by expanding the state space, we encounter the lingering phenomenoneven when the queue is overloaded. Lingering typically occurs when the mean queue length is equal or near the number of servers, however, in this case it also happens when the queue is overloaded and this time is not of zero measure. As a result, we develop an alternative representation for the queue length process that avoids the lingering problem in the overloaded case, thus allowing for the derivation of a Gaussian diffusion limit. Finally, we compare the effectiveness of our proposed method with discrete event simulation in a variety parameter settings and show that our approximations are very accurate.The online supplement is available at https://doi.org/10.1287/ijoc.2017.0760.
A novel extension of the classical vehicle routing and scheduling problems is introduced that integrates aspects of machine scheduling into vehicle routing. Associated with each customer order is a release date that defines the earliest time that the order is available to leave the depot for delivery and a due date that indicates the time by which the order should ideally be delivered to the customer. The objective is to minimize a convex combination of the operational costs and customer service level, represented by the total distance traveled and the total weighted tardiness of delivery, respectively. A path-relinking algorithm (PRA) is proposed to address the problem, and a variety of benchmark instances are generated to evaluate its performance. The PRA exploits the efficiency and aggressive improvement of neighborhood search but relies on a new path-relinking procedure and advanced population management strategies to navigate the search space effectively. To provide a comparator algorithm to the PRA, we embed the neighborhood search into a standard iterated local search algorithm (ILS). Extensive computational experiments on the benchmark instances show that the newly defined features have a significant and varied impact on the problem, and the performance of the PRA dominates that of the ILS algorithm.The online supplement is available at https://doi.org/10.1287/ijoc.2017.0756.
In this paper, we propose a new methodology for the speed-scaling problem based on its link to scheduling with controllable processing times and submodular optimization. It results in faster algorithms for traditional speed-scaling models, characterized by a common speed/energy function. Additionally, it efficiently handles the most general models with job-dependent speed/energy functions with single and multiple machines. To the best of our knowledge, this has not been addressed prior to this study. In particular, the general version of the single-machine case is solvable by the new technique in O(n2) time.The online appendix is available at https://doi.org/10.1287/ijoc.2017.0758.
In this paper, a heuristic procedure is proposed for the facility location problem with general Bernoulli demands. This is a discrete facility location problem with stochastic demands that can be formulated as a two-stage stochastic program with recourse. In particular, facility locations and customer assignments must be decided here and now, i.e., before knowing the customers who will actually require to be served. In a second stage, service decisions are made according to the actual requests. The heuristic proposed consists of a greedy randomized adaptive search procedure followed by a path relinking. The heterogeneous Bernoulli demands make prohibitive the computational effort for evaluating feasible solutions. Thus the expected cost of a feasible solution is simulated when necessary. The results of extensive computational tests performed for evaluating the quality of the heuristic are reported, showing that high-quality feasible solutions can be obtained for the problem in fairly small computational times.The online supplement is available at https://doi.org/10.1287/ijoc.2017.0755.
Whereas much research in the area of optimization is directed toward developing algorithms for optimization of feasible models, the diagnosis of infeasible models has not received as much attention. Identification of irreducible infeasible sets (IISs) can facilitate the process of correcting infeasible models. Several filtering algorithms have been proposed for IIS identification but efficient implementations are available only for linear programs. We propose a novel approach for IIS identification that is applicable to linear programs (LPs), nonlinear programs (NLPs), mixed-integer linear programs (MIPs), and mixed-integer nonlinear programs (MINLPs). The approach makes use of a deletion presolve procedure that exploits bounds tightening techniques to reduce the model to an infeasible set (IS) in a computationally efficient manner. The IS is subsequently reduced to an IIS by applying one of the currently available exact filtering algorithms for IIS identification. We implement the proposed deletion presolve along with four filtering algorithms for IIS identification within the global solver BARON. The effectiveness and usefulness of the proposed approach is demonstrated through computational experiments on a test set of 790 infeasible LPs, NLPs, MIPs, and MINLPs. Deletion presolve rapidly eliminates a large fraction of the problem constraints and speeds up the filtering algorithms by over forty times on average. Speedups of as high as 1,000 times are observed for some problems, while, for 40% of the test problems, the deletion presolve itself reduces the original model to an IIS.
This paper studies multilevel uncapacitated p-location problems, a general class of facility location problems. We use a combinatorial representation of the general problem where the objective function satisfies the submodular property, and we exploit this characterization to derive worst-case bounds for a greedy heuristic. We also obtain sharper bounds when the setup cost for opening facilities is zero and the allocation profits are nonnegative. Moreover, we introduce a mixed integer linear programming formulation for the problem based on the submodularity property. We present results of computational experiments to assess the performance of the greedy heuristic and that of the formulation. We compare the models with previously studied formulations.The online supplement and data are available at https://doi.org/10.1287/ijoc.2017.0757.
On behalf of the Editorial Board, I would like to thank the following people, who acted as Reviewers during the past year.David L. Woodruff, Editor-in-Chief
In this paper, we present examples of a class of Markov chains that occur frequently, but whose associated matrices are a challenge to construct efficiently. These are Markov chains that arise as a result of several identical Markov chains running in parallel. Specifically for the cases considered, both the infinitesimal generator matrix for the continuous case, and more so the transition probability matrix for the discrete equivalent, are complex to construct effectively and efficiently. We summarize the algorithms for constructing the associated matrices and present examples of applications, ranging from special queueing problems to reliability issues and order statistics. MATLAB subroutines are provided in an online supplement for the implementation of the algorithms.The online supplement is available at https://doi.org/10.1287/ijoc.2017.0759.
Recently, parallel computing environments have become significantly popular. In order to obtain the benefit of using parallel computing environments, we have to deploy our programs for these effectively. This paper focuses on a parallelization of SCIP (Solving Constraint Integer Programs), which is a mixed-integer linear programming solver and constraint integer programming framework available in source code. There is a parallel extension of SCIP named ParaSCIP, which parallelizes SCIP on massively parallel distributed memory computing environments. This paper describes FiberSCIP, which is yet another parallel extension of SCIP to utilize multi-threaded parallel computation on shared memory computing environments, and has the following contributions: First, we present the basic concept of having two parallel extensions, and the relationship between them and the parallelization framework provided by UG (Ubiquity Generator), including an implementation of deterministic parallelization. Second, we discuss the difficulties in achieving a good performance that utilizes all resources on an actual computing environment, and the difficulties of performance evaluation of the parallel solvers. Third, we present a way to evaluate the performance of new algorithms and parameter settings of the parallel extensions. Finally, we demonstrate the current performance of FiberSCIP for solving mixed-integer linear programs (MIPs) and mixed-integer nonlinear programs (MINLPs) in parallel.The online appendix is available at https://doi.org/10.1287/ijoc.2017.0762.
We introduce a novel scheme based on a blending of Fourier-Motzkin elimination (FME) and adjustable robust optimization techniques to compute the maximum volume inscribed ellipsoid (MVE) in a polytopic projection. It is well-known that deriving an explicit description of a projected polytope is NP-hard. Our approach does not require an explicit description of the projection, and can easily be generalized to find a maximally sized convex body of a polytopic projection. Our obtained MVE is an inner approximation of the projected polytope, and its center is a centralized relative interior point of the projection. Since FME may produce many redundant constraints, we apply an LP-based procedure to keep the description of the projected polytopes at its minimal size. Furthermore, we propose an upper bounding scheme to evaluate the quality of the inner approximations. We test our approach on a simple polytope and a color tube design problem, and observe that as more auxiliary variables are eliminated, our inner approximations and upper bounds converge to optimal solutions.The online supplement is available at https://doi.org/10.1287/ijoc.2017.0763.
We investigate the maximum induced matching problem (MIM), which is the problem of finding an induced matching having the largest cardinality on an undirected graph. The problem is known to be NP-hard for general graphs. We first propose a vertex-based integer programming formulation for MIM, which is more compact compared to an edge-based formulation found in the literature. We also introduce the maximum weight induced matching problem (MWIM), which generalizes MIM so that vertices and edges have weights. We adapt the edge-based formulation to MWIM, and propose a quadratic programming formulation of MWIM based on our vertex-based formulation. We then linearize our quadratic programming formulation, and devise a Benders decomposition algorithm that exploits a special structure of the linearized formulation. We also propose valid inequalities and formulation tightening procedures to improve the efficiency of our approach. Our computational tests on a large suite of randomly generated graphs show that our vertex-based formulation and decomposition approach significantly improve the solvability of MIM and MWIM, especially on dense graphs.The online appendix and data are available at https://doi.org/10.1287/ijoc.2017.0764.
We present a computational study of several strategies to solve two-stage stochastic linear programs by integrating the adaptive partition-based approach with level decomposition. A partition-based formulation is a relaxation of the original stochastic program, obtained by aggregating variables and constraints according to a scenario partition. Partition refinements are guided by the optimal second-stage dual vectors computed at certain first-stage solutions. The proposed approaches rely on the level decomposition with on-demand accuracy to dynamically adjust partitions until an optimal solution is found. Numerical experiments on a large set of test problems including instances with up to one hundred thousand scenarios show the effectiveness of the proposed approaches.The online supplement is available at https://doi.org/10.1287/ijoc.2017.0765.
An efficient algorithm is developed to calculate the periodic steady-state distribution and moments of the remaining workload Wy at time yc within a cycle of length c, 0 ≤ y < 1, in a single-server queue with a periodic arrival-rate function. The algorithm applies exactly to the GIt/GI/1 model, where the arrival process is a time-transformation of a renewal process. A new representation of Wy makes it possible to apply a modification of the classic rare-event simulation for the stationary GI/GI/1 model exploiting importance sampling using an exponential change of measure. We establish bounds between the periodic workload and the stationary workload with the average arrival rate that enable us to prove that the relative error in estimates of P(Wy > b) is uniformly bounded in b. With the aid of a recent heavy-traffic limit theorem, the algorithm also applies to compute the periodic steady-state distribution of (i) reflected periodic Brownian motion (RPBM) by considering appropriately scaled GIt/GI/1 models and (ii) a large class of general Gt/G/1 queues by approximating by GIt/GI/1 models with the same heavy-traffic limit. Simulation examples demonstrate the accuracy and efficiency of the algorithm for both GIt/GI/1 queues and RPBM.The online supplement is available at https://doi.org/10.1287/ijoc.2017.0766.
In this paper, we extend a recently proposed scenario decomposition algorithm for risk-neutral 0-1 stochastic programs to the risk-averse setting. Specifically, we consider two-stage risk-averse 0-1 stochastic programs with objective functions based on coherent risk measures. Using a dual representation of a coherent risk measure, we first derive an equivalent minimax reformulation of the considered problem. We then develop three variants of the scenario decomposition algorithm for this minimax formulation based on different relaxations of the nonanticipaticity constraints. The algorithms proceed by solving scenario subproblems to obtain candidate solutions and bounds and subsequently cutting off the candidate solutions from the search space to achieve convergence to an optimal solution. We design three parallelization schemes for implementing the algorithms with different tradeoffs between overhead time and computation time. Our computational results with risk-averse extensions of two standard stochastic 0-1 programming test instances demonstrate the scalability of the proposed decomposition and parallelization framework.
We present and benchmark an approximate dynamic programming algorithm that is capable of designing near-optimal control policies for a portfolio of heterogenous storage devices in a time-dependent environment, where wind supply, demand, and electricity prices may evolve stochastically. We found that the algorithm was able to design storage policies that are within 0.08% of optimal on deterministic models, and within 0.86% on stochastic models. We use the algorithm to analyze a dual-storage system with different capacities and losses, and show that the policy properly uses the low-loss device (which is typically much more expensive) for high-frequency variations. We close by demonstrating the algorithm on a five-device system. The algorithm easily scales to handle heterogeneous portfolios of storage devices distributed over the grid and more complex storage networks.The online supplement is available at https://doi.org/10.1287/ijoc.2017.0768.
We consider the supply chain problem of minimizing ordering, distribution, and inventory holding costs of a supply chain formed by a set of warehouses and retailers over a finite time horizon, which we call the production and distribution problem. This is a common generalization of the classical metric facility location problem and joint replenishment problem that coordinates the network design and inventory management decisions in an integrated manner. This coordination can represent significant economy for many applications, where network design and operational costs are normally considered separately. This problem is considered when the instances satisfy assumptions such as metric space of warehouse and retailer locations, and monotonic increasing inventory holding costs. In this work, we give a 2.77-approximation based on the randomized rounding of the natural mixed-integer programming relaxation. Also, we give a 5-approximation for the case that objective function includes retailer ordering setup costs.
The maximum clique problem (MaxClique for short) consists of searching for a maximum complete subgraph in a graph. A branch-and-bound (BnB) MaxClique algorithm computes an upper bound of the number of vertices of a maximum clique at every search tree node, to prune the subtree rooted at the node. Existing upper bounds are usually computed from scratch at every search tree node. In this paper, we define an incremental upper bound, called IncUB, which is derived efficiently from previous searches instead of from scratch. Then, we describe a new BnB MaxClique algorithm, called IncMC2, which uses graph coloring and MaxSAT reasoning to filter out the vertices that do not need to be branched on, and uses IncUB to prune the remaining branches. Our experimental results show that IncMC2 is significantly faster than algorithms such as BBMC and IncMaxCLQ. Finally, we carry out experiments to provide evidence that the performance of IncMC2 is due to IncUB.The online supplement is available at https://doi.org/10.1287/ijoc.2017.0770.
We extend the idea of model-based algorithms for deterministic optimization to simulation optimization over continuous space. Model-based algorithms iteratively generate a population of candidate solutions from a sampling distribution and use the performance of the candidate solutions to update the sampling distribution. By viewing the original simulation optimization problem as another optimization problem over the parameter space of the sampling distribution, we propose to use a direct gradient search on the parameter space to update the sampling distribution. To improve the computational efficiency, we further develop a two-timescale updating scheme that updates the parameter on a slow timescale and estimates the quantities involved in the parameter updating on the fast timescale. We analyze the convergence properties of our algorithms through techniques from stochastic approximation, and demonstrate the good empirical performance by comparing with two state-of-the-art model-based simulation optimization methods.The online supplement is available at https://doi.org/10.1287/ijoc.2017.0771.
We study a prize-collecting single-machine scheduling problem with hard deadlines, where the objective is to minimize the difference between the total tardiness and the total prize of the selected jobs. This problem is motivated by industrial applications, both as a stand-alone model and as a pricing subproblem in column-generation algorithms for parallel machine scheduling problems. A preprocessing rule is devised to identify jobs that cannot belong to any optimal schedule. The resulting reduced problem is solved to optimality by a branch-and-bound algorithm and two integer linear programming formulations. The algorithm and the formulations are experimentally compared on randomly generated benchmark instances.
We develop a high-fidelity simulation model of the patient arrival process to an endocrinology clinic by carefully examining appointment and arrival data from that clinic. The data include the time that the appointment was originally made as well as the time that the patient actually arrived, as well as if the patient did not arrive at all, in addition to the scheduled appointment time. We take a data-based approach, specifying the schedule for each day by its value at the end of the previous day. This data-based approach shows that the schedule for a given day evolves randomly over time. Indeed, in addition to three recognized sources of variability—(i) no-shows, (ii) extra unscheduled arrivals, and (iii) deviations in the actual arrival times from the scheduled times—we find that the primary source of variability in the arrival process is variability in the daily schedule itself. Even though service systems with arrivals by appointment can differ in many ways, we think that our data-based approach to modeling the clinic arrival process can be a guideline or template for constructing high-fidelity simulation models for other arrival processes generated by appointments.The online supplement is available at https://doi.org/10.1287/ijoc.2017.0773.
This paper proposes a simulated annealing variant for optimization problems in which the solution quality can only be estimated by sampling from a random distribution. The aim is to find the solution with the best expected performance, as, e.g., is typical for problems where solutions are evaluated using a stochastic simulation. Assuming Gaussian noise with known standard deviation, we derive a fully sequential sampling procedure and decision rule. The procedure starts with a single sample of the value of a proposed move to a neighboring solution and then continues to draw more samples until it is able to make a decision to accept or reject the move. Under constraints of equilibrium detailed balance at each draw, we find a decoupling between the acceptance criterion and the choice of the rejection criterion. We derive a universally optimal acceptance criterion in the sense of maximizing the acceptance probability per sample and thus the efficiency of the optimization process. We show that the choice of the move rejection criterion depends on expectations of possible alternative moves and propose a simple and practical (albeit more empirical) solution that preserves detailed balance. An empirical evaluation shows that the resulting approach is indeed more efficient than several previously proposed simulated annealing variants.
Connected dominating set (CDS) problem has been extensively studied in the literature due to its applications in many domains, including computer science and operations research. For example, CDS has been recommended to serve as a virtual backbone in wireless sensor networks (WSNs). Since sensor nodes in WSNs are prone to failures, it is important to build a fault-tolerant virtual backbone that maintains a certain degree of redundancy. A fault-tolerant virtual backbone can be modeled as a k-connected m-fold dominating set (k, m)-CDS. For a connected graph G = (V, E) and two fixed integers k and m, a node set C ⊆ V is a (k, m)-CDS of G if every node in V\C has at least m neighbors in C, and the subgraph of G induced by C is k-connected. Previous to this work, approximation algorithms with guaranteed performance ratio in a general graph were known only for k ≤ 3. This paper makes significant progress by presenting a (2k − 1)α0 approximation algorithm for general k and m with m ≥ k, where α0 is the performance ratio for the minimum (1, m)-CDS problem. Using a currently best-known ratio for α0, our algorithm has performance ratio O(lnΔ), where Δ is the maximum degree of the graph. Simulation results validate the effectiveness of our algorithm.
Finding a connected dominating set (CDS) in a given graph is a fundamental problem and has been studied intensively for a long time because of its application in computer science and operations research, e.g., connected facility location and wireless networks. In some cases, fault-tolerance is desirable. Taking wireless networks as an example, since wireless nodes may fail due to accidental damage or energy depletion, it is desirable that the virtual backbone has some fault-tolerance. Such a problem can be modeled as finding a minimum k-connected m-fold dominating set ((k, m)-CDS) of a graph G = (V, E), which is a node set D such that every node outside of D has at least m neighbors in D and the subgraph of G induced by D is k-connected. In this paper, we study the minimum weight (1, m)-CDS problem ((1, m)-MWCDS), and present an (H(δ + m) + 2H(δ − 1))-approximation algorithm, where δ is the maximum degree of the graph and H(·) is the Harmonic number. Notice that the state-of-the-art algorithm achieves O(ln(n))-approximation factor for the (1, 1)-MWCDS problem, where n is the number of nodes. Our work improves this ratio to O(ln δ) for an even more general problem: (1, m)-MWCDS. Such an improvement also enables us to obtain a (3.67 + α)-approximation for the (1, m)-MWCDS problem on unit disk graph, where α is the performance ratio for the minimum weight m-fold dominating set problem on unit disk graph.The online appendix is available at https://doi.org/10.1287/ijoc.2017.0775.
We study a capacitated multi-item lot sizing problem with nonidentical machines. For the problem, we propose several mathematical formulations and their per-item and per-period Dantzig–Wolfe decompositions, followed by exploring their relative efficiency in obtaining lower and upper bounds. Additionally, we observe that the optimum has a correlation with the solution values of the pricing subproblems of Dantzig–Wolfe decompositions, along with the solution values of the uncapacitated problems and linear programming (LP) relaxation. Using these solution values, we build statistical estimation models (i.e., generalized linear models) that give insight on the optimal values, as well as information about how likely a setup variable is to take a value of 1 at an optimal point. We then develop an analytics branching and selection method where the information is utilized for an analytics-based branching and selection procedure to fix setup variables, which is, to our knowledge, the first research using likelihood information to improve solution qualities. This method differs from approaches that use solution values of LP relaxation (e.g., relaxation induced neighborhood search, feasibility pump, and LP and fix). The application is followed by extensive computational tests. Comparisons with other methods indicate that the optimization method is computationally tractable and can obtain better results.The online appendix is available at https://doi.org/10.1287/ijoc.2017.0777.
The majority of cancer-related fatalities are due to metastatic disease. Chemotherapeutic agents are administered along with radiation in chemoradiotherapy (CRT) to control the primary tumor and systemic disease such as metastasis. This work introduces a mathematical model of CRT treatment scheduling to obtain optimal drug and radiation protocols with the objective of minimizing metastatic cancer cell populations at multiple potential sites while maintaining a desired level of control on the primary tumor. Dynamic programming framework is used to determine the optimal radiotherapy fractionation regimen and the drug administration schedule. We design efficient DP data structures and use structural properties of the optimal solution to reduce the complexity of the resulting DP algorithm. We derive closed-form expressions for optimal chemotherapy schedules in special cases. The results suggest that if there is only an additive and spatial cooperation between the chemotherapeutic drug and radiation with no interaction between them, then radiation and drug administration schedules can be decoupled. In that case, regardless of the chemo- and radio sensitivity parameters, the optimal radiotherapy schedule follows a hypofractionated scheme. However, the structure of the optimal chemotherapy schedule depends on model parameters such as chemotherapy-induced cell kill at primary and metastatic sites, as well as the ability of primary tumor cells to initiate successful metastasis at different body sites. In contrast, an interactive cooperation between the drug and radiation leads to optimal split-course concurrent CRT regimens. Additionally, under dynamic radio sensitivity parameters due to the reoxygenation effect during therapy, we observe that it is optimal to immediately start the chemotherapy and administer a few large radiation fractions at the beginning of the therapy, while scheduling smaller fractions in later sessions. We quantify the trade-off between the new and traditional objectives of minimizing the metastatic population size and maximizing the primary tumor control probability, respectively, for a cervical cancer case. The trade-off information indicates the potential for significant reduction in the metastatic population with minimal loss in the primary tumor control.The online supplement is available at https://doi.org/10.1287/ijoc.2017.0778.
Stochastic kriging (SK) and stochastic kriging with gradient estimators (SKG) are useful methods for effectively approximating the response surface of a simulation model. In this paper, we show that in a fully sequential setting when all model parameters are known, the mean squared errors of the optimal SK and SKG predictors are monotonically decreasing as the number of design points increases. In addition, we prove, under appropriate conditions, that the use of gradient information in the SKG framework generally improves the prediction performance of SK. Motivated by these findings, we propose a sequential procedure for adaptively choosing design points and simulation replications in obtaining SK (SKG) predictors with desired levels of fidelity. We justify the validity of the procedure and carry out numerical experiments to illustrate its performance.The online supplement is available at https://doi.org/10.1287/ijoc.2017.0779.
This paper presents a new local search approach, called randomized decomposition (RD), for solving nonlinear, nonconvex mathematical programs. Starting from a feasible solution, RD partitions the problem’s decision variables into a randomly ordered list of randomly generated subsets. RD then optimizes over the variables in each subset, keeping all other variables fixed. Unlike most other decomposition methods, no knowledge of the problem structure is required. RD has been combined with a metaheuristic RDPerturb, for escaping local optima, to create a generic framework for solving mathematical programs, especially hard combinatorial nonconvex problems. The framework has been implemented as an optimization platform we call RDSolver and successfully applied to over 400 instances of the quadratic assignment problem (QAP). The results obtained by RDSolver are competitive with the solutions obtained by heuristics specially tailored for those problems, even though RDSolver is a general purpose mathematical programming solver. In addition to a strong performance on previously solved problems, RDSolver has found two new best known solutions and provided solutions to 68 large QAP problems for which no solutions have been previously reported.The online supplement is available at https://doi.org/10.1287/ijoc.2017.0781.
Detailed modeling of gas transport problems leads to nonlinear and nonconvex mixed-integer optimization or feasibility models (MINLPs) because both the incorporation of discrete controls of the network and accurate physical and technical modeling are required to achieve practical solutions. Hence, ignoring certain parts of the physics model is not valid for practice. In the present contribution we extend an approach based on linear relaxations of the underlying nonlinearities by tailored model reformulation techniques yielding block-separable MINLPs. This combination of techniques allows us to apply a penalty alternating direction method and thus to solve highly detailed MINLPs for large-scale, real-world instances. The practical strength of the proposed method is demonstrated by a computational study in which we apply the method to instances from steady-state gas transport including both pooling effects with respect to the mixing of gases of different composition and a highly detailed compressor station model.
In biobjective mixed integer linear programs (BOMILPs), two linear objectives are minimized over a polyhedron while restricting some of the variables to be integer. Since many of the techniques for finding or approximating the Pareto set of a BOMILP use and update a subset of nondominated solutions, it is highly desirable to efficiently store this subset. We present a new data structure, a variant of a binary tree that takes as input points and line segments in ℝ2 and stores the nondominated subset of this input. When used within an exact solution procedure, such as branch and bound (BB), at termination this structure contains the set of Pareto optimal solutions.We compare the efficiency of our structure in storing solutions to that of a dynamic list, which updates via pairwise comparison. Then we use our data structure in two biobjective BB techniques available in the literature and solve three classes of instances of BOMILP, one of which is generated by us. The first experiment shows that our data structure handles up to 107 points or segments much more efficiently than a dynamic list. The second experiment shows that our data structure handles points and segments much more efficiently than a list when used in a BB.
The convergence of a column generation algorithm can be improved in practice by using stabilization techniques. Smoothing and proximal methods based on penalizing the deviation from the incumbent dual solution have become standards of the domain. Interpreting column generation as cutting plane strategies in the dual problem, we analyze the mechanisms on which stabilization relies. In particular, the link is established between smoothing and in-out separation strategies to derive generic convergence properties. For penalty function methods as well as for smoothing, we describe proposals for parameter self-adjusting schemes. Such schemes make initial parameter tuning less of an issue as corrections are made dynamically. Such adjustments also allow us to adapt the parameters to the phase of the algorithm. We provide extensive test reports that validate our self-adjusting parameter scheme and highlight their performances. Our results also show that using smoothing in combination with a penalty function yields a cumulative effect on convergence speed-ups.
Optimization algorithms and heuristic procedures for arc-routing problems often use benchmark instances to validate and demonstrate performance. Ideally, these benchmark instances capture the features of real-world street networks. Typically, benchmark instances are artificially generated and only approximate real-world networks. We develop a software tool that allows users to generate arc-routing instances directly from an open-source, user-driven map database. Our tool gives the user the ability to edit the instances by hand or by using configurable parameters. The instances generated by our tool can then be exported for use by researchers. In addition, our tool has a visualization capability that can produce images of routes overlaid on the instance.The online supplement is available at https://doi.org/10.1287/ijoc.2017.0785.
Influenza (flu) is a serious public health concern. The first line of defense is the flu shot, whose composition is updated annually to adjust for frequent mutations of the circulating viruses. The World Health Organization recommends which strains to include in the flu shot based on global surveillance. Vaccine manufacturers produce trivalent and quadrivalent flu shots. The design of the flu shot, however, affects the manufacturers’ capacity and profit. In return, production decisions of the manufacturers affect the societal vaccination benefit by determining coverage and timely availability. We model this two-level hierarchy using a bilevel multistage stochastic mixed-integer program. Calibrated with publicly available data, our model integrates the flu shot composition and manufacturing in a stochastic and dynamic environment. We derive a branch-and-price algorithm to find the global optimal solution. We also propose an effective heuristic to provide the public health planners with a decision aid tool. Finally, we perform numerical experiments to answer important public health policy questions and to quantify the impact of the proposed modeling extensions. A major conclusion of our work is that the vaccine strain of a category that is not expected to be very prevalent and/or that is unlikely to drift in the upcoming season should be selected as early as possible, especially when the selections for other strain categories have to be postponed to improve the flu shot design.The online appendix is available at https://doi.org/10.1287/ijoc.2017.0786.
Electric power generation expansion planning (GEP) is the problem of determining an optimal construction and generation plan of both new and existing electric power plants to meet future electricity demand. We consider a stochastic optimization approach for this capacity expansion problem under demand and fuel price uncertainty. In a two-stage stochastic optimization model for GEP, the capacity expansion plan for the entire planning horizon is decided prior to the uncertainty realized and hence allows no adaptivity to uncertainty evolution over time. In comparison, a multistage stochastic optimization model allows full adaptivity to the uncertainty evolution but is extremely difficult to solve. To reconcile the trade-off between adaptivity and tractability, we propose a partially adaptive stochastic mixed integer optimization model in which the capacity expansion plan is fully adaptive to the uncertainty evolution up to a certain period and follows the two-stage approach thereafter. Any solution to the partially adaptive model is feasible to the multistage model, and we provide analytical bounds on the quality of such a solution. We propose an efficient algorithm that solves a sequence of partially adaptive models, to recursively construct an approximate solution to the multistage problem. We identify sufficient conditions under which this algorithm recovers an optimal solution to the multistage problem. Finally, we conduct extensive test of our algorithm on a realistic GEP problem. Experiments show that, within a reasonable computation time limit, the proposed algorithm produces a significantly better solution than solving the multistage model directly.The online supplement is available at https://doi.org/10.1287/ijoc.2017.0782.
We present a branch-and-bound (B&B) framework for the asymmetric prize-collecting Steiner tree problem (APCSTP). Several well-known network design problems can be transformed to the APCSTP, including the Steiner tree problem (STP), prize-collecting Steiner tree problem (PCSTP), maximum-weight connected subgraph problem (MWCS), and node-weighted Steiner tree problem (NWSTP). The main component of our framework is a new dual ascent algorithm for the rooted APCSTP, which generalizes Wong’s dual ascent algorithm for the Steiner arborescence problem. The lower bounds and dual information obtained from the algorithm are exploited within powerful bound-based reduction tests and for guiding primal heuristics. The framework is complemented by additional alternative-based reduction tests. Extensive computational results on benchmark instances for the PCSTP, MWCS, and NWSTP indicate the framework’s effectiveness, as most instances from literature are solved to optimality within seconds, including most of the (previously unsolved) largest instances from the recent DIMACS Challenge on Steiner trees. Moreover, results on new asymmetric instances for the APCSTP are reported. Since the addressed network design problems are frequently used for modeling various real-world applications (e.g., in bioinformatics), the implementation of the presented B&B framework has been made publicly available.The online supplement is available at https://doi.org/10.1287/ijoc.2017.0788.
Binary programs with a quadratic objective function are NP-hard in general, even if the linear optimization problem over the same feasible set is tractable. In this paper, we address such problems by computing quadratic global underestimators of the objective function that are separable but not necessarily convex. Exploiting the binary constraint on the variables, a minimizer of the separable underestimator over the feasible set can be computed by solving an appropriate linear minimization problem over the same feasible set. Embedding the resulting lower bounds into a branch-and-bound framework, we obtain an exact algorithm for the original quadratic binary program. The main practical challenge is the fast computation of an appropriate underestimator, which in our approach reduces to solving a series of semidefinite programs. We exploit the special structure of the resulting problems to obtain a tailored coordinate-descent method for their solution. Our extensive experimental results on various quadratic combinatorial optimization problems show that our approach outperforms both CPLEX and the related QCR method as well as the SDP-based software BiqCrunch on instances of the quadratic shortest path problem and the quadratic assignment problem.
Privacy-preserving data publishing has received much attention in recent years. Prior studies have developed various algorithms such as generalization, anatomy, and L-diversityslicing to protect individuals’ privacy when transactional data are published for public use. These existing algorithms, however, all have certain limitations. For instance, generalization protects identity privacy well but loses a considerable amount of information. Anatomy prevents attribute disclosure and lowers information loss, but fails to protect membership privacy. The more recent probability L-diversity slicing algorithm overcomes some shortcomings of generalization and anatomy, but cannot shield data from more subtle types of attacks such as skewness attack and similarity attack. To meet the demand of data owners with high privacy-preserving requirement, this study develops a novel method named t-closeness slicing (TCS) to better protect transactional data against various attacks. The time complexity of TCS is log-linear, hence the algorithm scales well with large data. We conduct experiments using three transactional data sets and find that TCS not only effectively protects membership privacy, identity privacy, and attribute privacy, but also preserves better data utility than benchmarking algorithms.The online supplement is available at https://doi.org/10.1287/ijoc.2017.0791.
In this paper, we investigate a portfolio optimization methodology using nonparametric value at risk (VaR). In particular, we adopt kernel VaR and quadratic VaR as risk measures. As the resulting models are nonconvex and nonsmooth optimization problems, albeit with some special structures, we propose some specially devised block coordinate descent (BCD) methods for finding approximate or local optimal solutions. Computational results show that the BCD methods are efficient for finding local solutions with good quality and they compare favorably with the branch-and-bound method-based global optimal solution procedures. From the simulation test and empirical analysis that we carry out, we are able to conclude that the mean-VaR models using kernel VaR and quadratic VaR are more robust compared to those using historical VaR or parametric VaR under the normal distribution assumption, especially when the information of the return distribution is limited.The online supplement is available at https://doi.org/10.1287/ijoc.2017.0793.
In this paper, we propose a problem-driven scenario-generation approach to the single-period portfolio selection problem that uses tail risk measures such as conditional value-at-risk. Tail risk measures are useful for quantifying potential losses in worst cases. However, for scenario-based problems, these are problematic: because the value of a tail risk measure only depends on a small subset of the support of the distribution of asset returns, traditional scenario-based methods, which spread scenarios evenly across the whole support of the distribution, yield very unstable solutions unless we use a very large number of scenarios. The proposed approach works by prioritizing the construction of scenarios in the areas of a probability distribution that correspond to the tail losses of feasible portfolios. The proposed approach can be applied to difficult instances of the portfolio selection problem characterized by high dimensions, nonelliptical distributions of asset returns, and the presence of integer variables. It is also observed that the methodology works better as the feasible set of portfolios becomes more constrained. Based on this fact, a heuristic algorithm based on the sample average approximation method is proposed. This algorithm works by adding artificial constraints to the problem that are gradually tightened, allowing one to telescope onto high-quality solutions.The online supplement is available at https://doi.org/10.1287/ijoc.2017.0790.
We present an extended mixed-integer programming formulation of the stochastic lot-sizing problem for the static-dynamic uncertainty strategy. The proposed formulation is significantly more time efficient as compared to existing formulations in the literature and it can handle variants of the stochastic lot-sizing problem characterized by penalty costs and service level constraints, as well as backorders and lost sales. Also, besides being capable of working with a predefined piecewise linear approximation of the cost function—as is the case in earlier formulations—it has the functionality of finding an optimal cost solution with an arbitrary level of precision by means of a novel dynamic cut generation approach.The online appendix is available at https://doi.org/10.1287/ijoc.2017.0792.
Motivated to apply sustainable supply chain principles to air-pollution control systems, this paper presents a dynamic inventory-management approach where substitution is possible to maintain these systems’ equipment. An air-pollution control system’s subsequent reliability depends on the replacement equipment selected. The corresponding problem is formulated as a stochastic dynamic program. Because the state and action space are prohibitively large, the approximate policy iteration algorithm is adapted to generate high-quality solutions. Therefore, this work replaces the value function with an affine combination of nonlinear basis functions and shows that a relaxation of the policy improvement step requires the solving of a mixed integer linear program. This approach helps in the designing of an algorithm that improves the quality of the approximation by solving a convex optimization problem. To assess the quality of resulting solutions, a lower bound is developed by considering a relaxation of the problem. In addition, two classes of heuristics are proposed based on a rolling-horizon two-stage stochastic programming formulation of the problem and a standard base-stock ordering policy. The performance of proposed policies is tested on a variety of settings, and results show that the approximate dynamic programming policies are near-optimal in the settings of interest and significantly outperform available benchmarks. The following analysis reveals that the proposed inventory replenishment policies resemble a base-stock policy with occasional deviations, and assignment and substitution decisions are determined by balancing the reliability with ordering, holding, and shortage costs.The online supplement is available at https://doi.org/10.1287/ijoc.2017.0794.
A network is given whose edges need to be constructed (or restored after a disaster). The lengths of edges represent the required construction/restoration times given available resources, and one unit of length of the network can be constructed per unit of time. All points of the network are accessible for construction at any time. For each pair of vertices, a due date is given. It is required to find a construction schedule that minimizes the maximum lateness of all pairs of vertices, where the lateness of a pair is the difference between the time when the pair becomes connected by an already constructed path and the pair’s due date. We introduce the problem and analyze its structural properties, present a mixed-integer linear programming formulation, develop a number of lower bounds that are integrated in a branch-and-bound algorithm, and discuss results of computational experiments both for instances based on randomly generated networks and for instances based on 2010 Chilean earthquake data.The online appendix is available at https://doi.org/10.1287/ijoc.2017.0796.
In this work we consider the shortest path problem with uncertainty in arc lengths and convex risk measure objective. We explore efficient implementations of sample average approximation (SAA) methods to solve shortest path problems when the conditional value at risk and entropic risk measures are used and there is correlation present in the uncertain arc lengths. Our work explores the use of different decomposition techniques to achieve an efficient implementation of SAA methods for these nonlinear convex integer optimization problems. A computational study shows the effect of geometry, uncertainty correlation and variance, and risk measure parameters on efficiency and accuracy of the methods developed.Data and the online supplement are available at https://doi.org/10.1287/ijoc.2017.0795.
This paper addresses a generalization of the vehicle routing problem in which the pick-up locations of the targets are nonstationary. We refer to this problem as the vehicle routing problem with floating targets and the main characteristic is that targets are allowed to move from their initial home locations while waiting for a vehicle. This problem models new applications in drone routing, ridesharing, and logistics where a vehicle agrees to meet another vehicle or a customer at a location that is away from the designated home location. We propose a Mixed Integer Second Order Cone Program (MISOCP) formulation for the problem, along with valid inequalities for strengthening the continuous relaxation. We further exploit the problem structure using a Lagrangian decomposition and propose an exact branch-and-price algorithm. Computational results on instances with varying characteristics are presented and the results are compared to the solution of the full problem using CPLEX. The proposed valid inequalities reduce the computational time of CPLEX by up to 30% on average while the proposed branch and price is capable of solving instances where CPLEX fails in finding the optimal solution within the imposed time limit.
Despite vast improvements in computational power, many large-scale optimization problems involving integer variables remain difficult to solve. Certain classes, however, can be efficiently solved by exploiting special structure. One such structure is the singly bordered block-diagonal (BBD) structure that lends itself to Dantzig-Wolfe decomposition, Lagrangian relaxation, and branch and price. We start by introducing a new measure of goodness to capture desired features in BBD structures such as granularity of the structure, homogeneity of the block sizes, and isomorphism of the blocks. We then use it to propose a new approach to identify the best BBD structure inherent in the constraint matrix. The main building block of the proposed approach is the use of a community detection methodology in lieu of graph/hypergraph partitioning methods to alleviate one major drawback of the existing approaches in the literature: predefining the number of blocks. When tested on MIPLIB2003/2010 instances and compared against the state-of-the-art technique, the proposed algorithm is found to identify very good structures and require shorter CPU time to reach comparable bounds, in most cases.
Fluctuations in emergency department (ED) patient arrivals during the day are one of the main causes of the long waiting times that are frequently encountered, and ED staffing is one of the key drivers of ED service quality improvement. This paper first proposes discrete-time models for approximating the patient waiting times for any given ED staffing. The waiting time approximation is based on three simple ideas: the separation of patients served in a period and patients overflowed, the combination of M/M/c approximation for patients served and waiting time analysis of overflow patients, and the transformation of the performance evaluation into an optimization problem with the number of overflow patients as decision variables. The resulting waiting time approximations are then integrated into ED staffing optimization models, and variable neighborhood search algorithms are developed to solve the ED staffing models. Numerical experiments with real-life data from Chinese hospitals are performed to validate the proposed models and algorithms. The results show that the proposed methodology is able to significantly reduce the total waiting time of patients without increasing staff capacity.The online appendix is available at https://doi.org/10.1287/ijoc.2017.0799.
Though empirical testing is broadly used to evaluate heuristics, there are shortcomings with how it is often applied in practice. In a systematic review of Max-Cut and quadratic unconstrained binary optimization (QUBO) heuristics papers, we found only 4% publish source code, only 14% compare heuristics with identical termination criteria, and most experiments are performed with an artificial, homogeneous set of problem instances. To address these limitations, we implement and release as open-source a code-base of 10 Max-Cut and 27 QUBO heuristics. We perform heuristic evaluation using cloud computing on a library of 3,296 instances. This large-scale evaluation provides insight into the types of problem instances for which each heuristic performs well or poorly. Because no single heuristic outperforms all others across all problem instances, we use machine learning to predict which heuristic will work best on a previously unseen problem instance, a key question facing practitioners.The online supplement is available at https://doi.org/10.1287/ijoc.2017.0798.
In this paper, I view and present the multiobjective discrete optimisation problem as a particular case of disjunctive programming where one seeks to identify efficient solutions from within a disjunction formed by a set of systems. The proposed approach lends itself to a simple yet effective iterative algorithm that is able to yield the set of all nondominated points, both supported and nonsupported, for a multiobjective discrete optimisation problem. Each iteration of the algorithm is a series of feasibility checks and requires only one formulation to be solved to optimality that has the same number of integer variables as that of the single objective formulation of the problem. The application of the algorithm shows that it is particularly effective when solving constrained multiobjective discrete optimisation problem instances.The online supplement is available at https://doi.org/10.1287/ijoc.2017.0804.
The prices of Asian options, which are among the most important options in financial engineering, can often be written in terms of Laplace transforms. However, computable error bounds of the Laplace inversions are rarely available to guarantee their accuracy. We conduct a thorough analysis of the inversion of the Laplace transforms for continuously and discretely monitored Asian option prices under general continuous-time Markov chains (CTMCs), which can be used to approximate any one-dimensional Markov process. More precisely, we derive computable bounds for the discretization and truncation errors involved in the inversion of Laplace transforms. Numerical results indicate that the algorithm is fast and easy to implement, and the computable error bounds are especially suitable to provide benchmark prices under CTMCs.The online supplement is available at https://doi.org/10.1287/ijoc.2017.0805.
Cutting and packing (C&P) is a fundamental research area that models a large number of managerial and industrial optimization issues. A solution to a C&P problem basically consists of a set of one-dimensional or multidimensional items packed in/cut from one or more bins, by satisfying problem constraints and minimizing a given objective function. Normal patterns are a well-known C&P technique used to build solutions where each item is aligned to the bottom of the bin along each dimension. They are used in several C&P techniques because they can reduce the search space while preserving optimality, but their limit is that their number grows consistently when number of items and size of the bin increase. In this paper we propose a new set of patterns, called meet in the middle, that preserves optimality and leads to several interesting results. Their computation is achieved with the same time complexity as that of the normal patterns, but their number is never higher, and in practical applications it frequently shows reductions of about 50%. These new patterns are applied to improve some exact state-of-the-art C&P techniques, including arc-flow formulations, combinatorial branch-and-bound algorithms, and mixed-integer linear programs. The efficacy of the improved techniques is assessed by extensive computational tests on a number of relevant applications.The online appendix is available at https://doi.org/10.1287/ijoc.2018.0806.
The β-robust machine scheduling has attracted increasing attention as an effective method to hedge against uncertainty. However, existing β-robust scheduling models rely on the normality assumption of uncertain parameters, and existing solution methods are based on branch and bound, which cannot solve problems of 45 jobs within 3,600 seconds. This paper proposes distributionally β-robust scheduling (DRS) models to handle uncertain processing times. The DRS models only require the lower bound, mean, and covariance information of processing times, and have the capability of handling both single and parallel machine problems. Another key contribution of this paper is to devise efficient parametric search (PS) methods for the DRS models. Specifically, we show that there exists a parameterized assignment problem (PAP), such that its optimal solutions are also optimal for the original problem. The proposed methods only need to perform a one-dimensional PS and solve a series of PAPs. We further propose a bidirectional PS to reduce the number of PAPs needed to be solved, and we design a speedup shortest augmentation path algorithm for these PAPs. Experimental results on both single and identical parallel machine problems show that the improved PS method outperforms existing algorithms by more than three orders of magnitude improvement in computation time for problems of 45 jobs, and it is able to solve problems of 500 jobs within 0.5 seconds.
We propose an adaptive search algorithm for solving simulation optimization problems with Lipschitz continuous objective functions. The method combines the strength of several popular strategies in simulation optimization. It employs the shrinking ball method to estimate the performance of sampled solutions and uses the performance estimates to fit a surrogate model that iteratively approximates the response surface of the objective function. The search for improved solutions at each iteration is then based on sampling from a promising region (a subset of the decision space) adaptively constructed to contain the point that optimizes the surrogate model. Under appropriate conditions, we show that the algorithm converges to the set of local optimal solutions with probability one. A computational study is also carried out to illustrate the algorithm and to compare its performance with some of the existing procedures.
Classic vehicle routing models usually treat fuel cost as input data, but fuel consumption heavily depends on the travel speed, which leads to the study of optimizing speeds over a route to improve fuel efficiency. In this paper, we propose a joint vehicle routing and speed optimization problem to minimize the total operating cost including fuel cost. The only assumption made on the dependence between fuel cost and travel speed is that it is a strictly convex differentiable function. This problem is very challenging, with medium-sized instances already difficult for a general mixed-integer convex optimization solver. We propose a novel set-partitioning formulation and a branch-cut-and-price algorithm to solve this problem. We introduce new dominance rules for the labeling algorithm so that the pricing problem can be solved efficiently. Our algorithm clearly outperforms the off-the-shelf optimization solver, and is able to solve some benchmark instances to optimality for the first time.The online supplement is available at https://doi.org/10.1287/ijoc.2018.0810.
We consider a combinatorial optimization problem for spatial information cloaking. The problem requires computing one or several disjoint arborescences on a graph from a predetermined root or subset of candidate roots, so that the number of vertices in the arborescences is minimized but a given threshold on the overall weight associated with the vertices in each arborescence is reached. For a single arborescence case, we solve the problem to optimality by designing a branch-and-cut exact algorithm. Then we adapt this algorithm for the purpose of pricing out columns in an exact branch-and-price algorithm for the multiarborescence version. We also propose a branch-and-price-based heuristic algorithm, where branching and pricing, respectively, act as diversification and intensification mechanisms. The heuristic consistently finds optimal or near optimal solutions within a computing time, which can be three to four orders of magnitude smaller than that required for exact optimization. From an application point of view, our computational results are useful to calibrate the values of relevant parameters, determining the obfuscation level that is achieved.The online supplement is available at https://doi.org/10.1287/ijoc.2018.0813.
We propose two sequential, indifference-zone procedures for the comparison of simulated systems. Comparisons and selection of the best system are based on the mean and variance of a performance metric estimated by simulation. The mean represents the central tendency while the variance is the surrogate for the system’s inherent systematic risk. The first procedure identifies the system(s) with the largest expected value and smallest variance. The second procedure uses the variance of a reference system as a risk threshold, and selects the system with the largest mean from among those with an acceptable level of risk not above the threshold. Numerical experiments demonstrate the validity and efficacy of the proposed procedures.The online appendix is available at https://doi.org/10.1287/ijoc.2018.0808.
We present a perfect formulation for a single generator in the unit commitment problem, inspired by the dynamic programming approach taken by Frangioni and Gentile. This generator can have characteristics such as ramp-up/ramp-down constraints, time-dependent start-up costs, and start-up/shut-down limits. To develop this perfect formulation, we extend the result of Balas on unions of polyhedra to present a framework allowing for flexible combinations of polyhedra using indicator variables. We use this perfect formulation to create a cut-generating linear program, similar in spirit to lift-and-project cuts, and demonstrate computational efficacy of these cuts in a utility-scale unit commitment problem.The online supplement is available at https://doi.org/10.1287/ijoc.2017.0802.
We present a sparse knowledge gradient (SpKG) algorithm for adaptively selecting the targeted regions within a large RNA molecule to identify which regions are most amenable to interactions with other molecules. Experimentally, such regions can be inferred from fluorescence measurements obtained by binding a complementary probe with fluorescence markers to the targeted regions. We perform a regularized, sparse linear model with a log link function where the marginal contribution to the thermodynamic cycle of each nucleotide is purely additive. The SpKG algorithm uniquely combines the Bayesian ranking and selection problem with the frequentist l1 regularized regression approach Lasso. We use this algorithm to identify the sparsity pattern of the linear model as well as sequentially decide the best regions to test before exhausting an experimental budget. We also develop two new algorithms: batch SpKG and batch SpKG-LM. The first algorithm generates more suggestions sequentially to run parallel experiments. The second one dynamically adds new alternatives, in the form of types of probes, which are created by inserting, deleting, or mutating nucleotides within existing probes. In simulation, we demonstrate these algorithms on the Tetrahymena Group I intron (a midsize RNA molecule), showing that they efficiently learn the correct sparsity pattern, identify the most accessible region, and outperform several other policies.
We study the parallel machine scheduling problem to minimize the sum of the weighted completion times of the jobs to be scheduled  (problem Pm∥∑wjCj in the standard three-field notation). We use the set  covering formulation that was introduced by van den Akker et al. [van den Akker J, Hoogeveen J, van de Velde S (1999) Parallel machine scheduling by column generation. Oper. Res. 47(6):862–872.] for this problem, and we improve the computational performance of their branch-and-price (B&P) algorithm by a number of techniques, including a different generic branching scheme, zero-suppressed binary decision diagrams (ZDDs) to solve the pricing problem, dual-price smoothing as a stabilization method, and Farkas pricing to handle infeasibilities. We report computational results that show the effectiveness of the algorithmic enhancements, which depends on the characteristics of the instances. To the best of our knowledge, we are also the first to use ZDDs to solve the pricing problem in a B&P algorithm for a scheduling problem.The online supplement is available at https://doi.org/10.1287/ijoc.2018.0809.
On behalf of the Editorial Board, I would like to thank the following people, who acted as Reviewers during the past year.David L. Woodruff, Editor-in-Chief
This study develops a data-driven approach to solve constrained optimization problems in which the decision maker does not have an analytic form for the objective function but knows what decision variables affect the function. The approach makes direct use of the available data, rather than first using the data to estimate the objective function and then solving the problem as a traditional optimization problem. The difficulty in first estimating the unknown objective function is that the decision maker needs to have sufficient knowledge of its properties that are necessary to guide the estimation process. Thus, our approach is appropriate for situations where such structural knowledge is absent, either because the domain is very complex or because the knowledge is deliberately hidden by a partner firm that has a vested interest in the outcome of the decision. Our approach comes with a worst-case performance guarantee that improves with the characteristics (size, pervasiveness) of the available data. We illustrate our technique on a traffic-stream mixing problem encountered by a supply side Internet advertising network that wishes to optimize the click revenue earned from ads. A head-to-head comparison (with the existing method used) on real data shows a significant increase (≥10%, on average) in the revenue. We also demonstrate the value of our approach under more general conditions.The online supplement is available at https://doi.org/10.1287/ijoc.2018.0818.
We propose an asymptotically optimal set (AOS) approach for solving stochastic optimization problems with discrete or continuous feasible regions. Our AOS approach is a framework for designing provably convergent algorithms that are adaptive in seeking new points and in resampling or discarding already sampled points. The framework is an improvement over the adaptive search with resampling (ASR) method for stochastic optimization in that it spends less effort on inferior points and uses a more robust estimate of the optimal solution. We present conditions guaranteeing that the AOS approach is globally convergent and will eventually discard suboptimal sampled points with probability one, compare the algorithms, and analyze when (additional) resampling (beyond the minimum) is desirable. Our theoretical results show that AOS has stronger performance guarantees than ASR. Our numerical results suggest that AOS makes substantial improvements over ASR, especially for difficult problems with large numbers of local optima.The online supplement is available at https://doi.org/10.1287/ijoc.2018.0811.
The growing dependence of electric power systems on gas-fired generators to balance fluctuating and intermittent production by renewable energy sources has increased the variation and volume of flows withdrawn from natural gas transmission pipelines. Adapting pipeline operations to maintain efficiency and security under these dynamic conditions requires optimization methods that account for substantial intraday transients and can rapidly compute solutions in reaction to generator re-dispatch. Here, we present a computationally efficient method for minimizing gas compression costs under dynamic conditions where deliveries to customers are described by time-dependent mass flows. The optimization method uses a simplified representation of gas flow physics, provides a choice of discretization schemes in time and space, and exploits a two-stage approach to minimize energy costs and ensure smooth and physically meaningful solutions. The resulting large-scale NLPs are solved using an interior point method. The optimization scheme is validated by comparing the solutions with an integration of the dynamic equations using an adaptive timestepping differential equation solver, as well as a different, recently proposed optimal control scheme. The comparison shows that solutions to the discretized problem are feasible for the continuous problem and also practical from an operational standpoint. The results also indicate that our scheme produces at least an order of magnitude reduction in computation time relative to the state of the art and scales to large gas transmission networks with more than 6,000 kilometers of total pipeline.The online supplement is available at https://doi.org/10.1287/ijoc.2018.0821.
Real-time bidding (RTB) for digital advertising is becoming the norm for improving advertisers’ campaigns. Unlike traditional advertising practices, in the process of RTB, the advertisement slots of a mobile application or a website are mapped to a particular advertiser through a real-time auction. The auction is triggered and is held for a few milliseconds after an application is launched. As one of the key components of the RTB ecosystem, the demand-side platform gives the advertisers a full pledge window to bid for available impressions. Because of the fast-growing market of mobile applications and websites, the selection of the most pertinent target audience for a particular advertiser is not a simple human-mediated process. The real-time programmatic approach has become popular instead. To address the complexity and dynamic nature of the RTB process, we propose an auto pricing strategy (APS) approach to determine the applications to bid for and their respective bid prices from the advertising agencies’ perspective. We apply the APS to actual RTB data and demonstrate how it outperforms the existing RTB approaches with a higher conversion rate for a lower target spend.A video abstract is available at https://doi.org/10.1287/ijoc.2018.0812.
In this paper, we study a revenue management model over a single flight leg, where the customers are allowed to lock an available fare. Each customer arrives into the system with an interest in purchasing a ticket for a particular fare class. If this fare class is available, the customer immediately purchases the ticket by paying the fare or locks the fare by paying a fee. If the customer locks the fare, then the airline reserves the capacity for the customer for a certain duration of time. At the end of this duration of time, the customer makes her ultimate purchase decision at the locked fare. The goal of the airline is to find a policy to decide which set of fare classes to make available at each time period to maximize the total expected revenue. Such fare locking options are commonly offered by airlines; the dynamic programming formulation of the revenue management problem with the option to lock an available fare has a high-dimensional state variable that keeps track of the locked fares. We develop an approximate policy that is guaranteed to obtain at least half of the optimal total expected revenue. Our approach is based on leveraging a linear programming approximation to decompose the problem by the seats on the flight and solving a dynamic program that separately controls the capacity on each seat. We also show that our results continue to hold when the airline makes pricing decisions instead of fare class availability decisions. Our numerical experiments show that the practical performance of our approximate policy is remarkably good compared to a tractable upper bound on the optimal total expected revenue.The online supplement is available at https://doi.org/10.1287/ijoc.2018.0816.
In radiation therapy planning, uncertainties in the definition of the target volume yield a risk of underdosing the tumor. The traditional corrective action in the context of external beam radiotherapy (EBRT) expands the clinical target volume (CTV) with an isotropic margin to obtain the planning target volume (PTV). However, the EBRT-based PTV concept is not directly applicable to brachytherapy (BT) since it can lead to undesirable dose escalation. Here, we present a treatment plan optimization model that uses worst-case robust optimization to account for delineation uncertainties in interstitial high-dose-rate BT of the prostate. A scenario-based method was developed that handles uncertainties in index sets. Heuristics were included to reduce the calculation times to acceptable proportions. The approach was extended to account for delineation uncertainties of an organ at risk (OAR) as well. The method was applied on data from prostate cancer patients and evaluated in terms of commonly used dosimetric performance criteria for the CTV and relevant OARs. The robust optimization approach was compared against the classical PTV margin concept and against a scenario-based CTV margin approach. The results show that the scenario-based margin and the robust optimization method are capable of reducing the risk of underdosage to the tumor. As expected, the scenario-based CTV margin approach leads to dose escalation within the target, whereas this can be prevented with the robust model. For cases where rectum sparing was a binding restriction, including uncertainties in rectum delineation in the planning model led to a reduced risk of a rectum overdose, and in some cases, to reduced targetcoverage.The online supplement is available at https://doi.org/10.1287/ijoc.2018.0815.
We present a new heuristic algorithm to approximately generate the nondominated frontier of bi-objective pure integer linear programs. The proposed algorithm employs a customized version of several existing algorithms in the literature of both single-objective and bi-objective optimization. Our proposed method has two desirable characteristics: (1) there is no parameter to be tuned by users other than the time limit; (2) it can naturally exploit parallelism. An extensive computational study shows the efficacy of the proposed method on some existing standard test instances in which the true frontier is known, and also some large randomly generated instances. We show that even a basic version of our algorithm can significantly outperform the Nondominated Sorting Genetic Algorithm II (Deb et al. 2002), and the sophisticated version of our algorithm is competitive with Multidirectional Local Search (Tricoire 2012). We also show the value of parallelization on the proposed approach.
While the joint optimization of production and outbound distribution decisions in a manufacturing context have been intensively studied in the past decade, the integration of production, inventory, and inbound transportation from suppliers have received much less attention despite its practical relevance. This paper aims to fill the gap by introducing a general model for the assembly routing problem (ARP), which consists of simultaneously planning the assembly of a finished product at a plant and the routing of vehicles collecting materials from suppliers to meet the inventory requirements imposed by the production. We formulate the problem as a mixed-integer linear program and we propose a three-phase decomposition matheuristic that relies on the iterative solution of different subproblems. The first phase determines a setup schedule while the second phase optimizes production quantities, supplier visit schedules and shipment quantities. The third phase solves a vehicle routing problem for each period in the planning horizon. The algorithm is flexible, and we show how it can also be used to solve two well-known outbound distribution problems related to the ARP: the production routing problem and the inventory routing problem. Using the same parameter setting for all problems and instances, we obtain 781 new best-known solutions out of 2,628 standard IRP and PRP test instances. In particular, on large-scale multivehicle instances, the new algorithm outperforms specialized state-of-the-art heuristics for these two problems.The online appendix is available at https://doi.org/10.1287/ijoc.2018.0817.
We study the value of deterministic solutions, in particular their quality and upgradability, in addressing stochastic network design problems, by analyzing their time-dependent formulations known as scheduled service network design problems in freight transportation planning. We study several problem variants and models and investigate, for each case, the immediate quality of the deterministic solutions stemming from the 50th and the 75th percentile of the demand distributions. We then show that for all models, but in different ways, we are able to make effective use of parts of the deterministic solution, confirming the value of the deterministic solution in the stochastic environment, even when the deterministic solution itself performs badly.We also investigate what makes the optimal stochastic solution better in the stochastic environment than other feasible solutions, particularly those obtained by addressing deterministic versions of the problem. We do this by quantitatively analyzing the structures of different solutions. A measurement scheme is proposed to evaluate the level of potentially beneficial structural properties (multipath usage and path sharing) in different solutions. We show that these structural properties are important and correlated with the performance of a solution in the stochastic environment.Data and the online appendix are available at https://doi.org/10.1287/ijoc.2018.0819.
In this article we consider the network design problem with relays (NDPR), which gives answers to some important strategic design questions in telecommunication network design. Given a family of origin-destination pairs and a set of existing links these questions are as follows: (1) What are the optimal locations for signal regeneration devices (relays) and how many of them are needed? (2) Could the available infrastructure be enhanced by installing additional links in order to reduce the travel distance and therefore reduce the number of necessary relays?In contrast to previous work on the NDPR, which mainly focused on heuristic approaches, we discuss exact methods based on different mixed-integer linear programming formulations for the problem. We develop branch-and-price and branch-price-and-cut algorithms that build upon models with an exponential number of variables (and constraints). In an extensive computational study, we analyze the performance of these approaches for instances that reflect different real-world settings. Finally, we also point out the relevance of the NDPR in the context of electric mobility.
Understanding citywide transit patterns is important for transportation management, including city planning and route optimization. The wide deployment of automated fare collection (AFC) systems in public transit vehicles has enabled us to collect massive amounts of transit records, which capture passengers’ traveling activities. Based on such transit records, origin–destination associations have been studied extensively in the literature. However, the identification of transit patterns that establish the origin–transfer–destination (OTD) associations, in spite of its importance, is underdeveloped. In this paper, we propose a framework based on transit tensor factorization (TTF) to identify citywide travel patterns. In particular, we create a transit tensor, which summarizes the citywide OTD information of all passenger trips captured in the AFC records. The TTF framework imposes spatial regularization in the formulation to group nearby stations into meaningful regions and uses multitask learning to identify traffic flows among these regions at different times of the day and days of the week. Evaluated with large-scale, real-world data, our results show that the proposed TTF framework can effectively identify meaningful citywide transit patterns.The online supplement is available at https://doi.org/10.1287/ijoc.2018.0824.
In this paper, we present a method for comparing and evaluating different collections of machine learning algorithms on the basis of a given performance measure (e.g., accuracy, area under the curve (AUC), F-score). Such a method can be used to compare standard machine learning platforms such as SAS, IBM SPSS, and Microsoft Azure ML. A recent trend in automation of machine learning is to exercise a collection of machine learning algorithms on a particular problem and then use the best performing algorithm. Thus, the proposed method can also be used to compare and evaluate different collections of algorithms for automation on a certain problem type and find the best collection. In the study reported here, we applied the method to compare six machine learning platforms – R, Python, SAS, IBM SPSS Modeler, Microsoft Azure ML, and Apache Spark ML. We compared the platforms on the basis of predictive performance on classification problems because a significant majority of the problems in machine learning are of that type. The general question that we addressed is the following: Are there platforms that are superior to others on some particular performance measure? For each platform, we used a collection of six classification algorithms from the following six families of algorithms – support vector machines, multilayer perceptrons, random forest (or variant), decision trees/gradient boosted trees, Naive Bayes/Bayesian networks, and logistic regression. We compared their performance on the basis of classification accuracy, F-score, and AUC. We used F-score and AUC measures to compare platforms on two-class problems only. For testing the platforms, we used a mix of data sets from (1) the University of California, Irvine (UCI) library, (2) the Kaggle competition library, and (3) high-dimensional gene expression problems. We performed some hyperparameter tuning on algorithms wherever possible.The online supplement is available at https://doi.org/10.1287/ijoc.2018.0825.
Generalized geometric programming (GGP) problems consist of a signomial being minimized in the objective function subject to signomial constraints, and such problems have been utilized in various fields. After modeling numerous applications as GGP problems, solving them has become a significant requirement. A convex underestimator is considered an important concept to solve GGP problems for obtaining the global minimum. Among convex underestimators, variable transformation is one of the most popular techniques. This study utilizes an estimator to solve the difficulty of selecting an appropriate transformation between the exponential transformation and power convex transformation techniques and considers all popular types of transformation techniques to develop a novel and efficient convexification strategy for solving GGP problems. This proposed convexification strategy offers a guide for selecting the most appropriate transformation techniques on any condition of a signomial term to obtain the tightest convex underestimator. Several numerical examples in the online supplement are presented to investigate the effects of different convexification strategies on GGP problems and demonstrate the effectiveness of the proposed convexification strategy with regard to both solution quality and computation efficiency.The online supplement is available at https://doi.org/10.1287/ijoc.2018.0850.
In this paper, we propose the continuous variable neighborhood search method for finding all the solutions to a nonlinear system of equations (NSEs). We transform the NSE problem into an equivalent optimization problem, and we use a new objective function that allows us to find all the zeros. Instead of the usual sum-of-squares objective function, our objective function is presented as the sum of absolute values. Theoretical investigation confirms that our objective function provides more accurate solutions regardless of the optimization method used. In addition, we achieve a trade-off (i.e., increased precision at the expense of reduced smoothness). Computational analysis of standard test instances shows that the proposed method is more precise and much faster than two recently developed methods. Similar conclusions are drawn by comparing the proposed method with many other methods in the literature.The online supplement is available at https://doi.org/10.1287/ijoc.2018.0876.
Primal heuristics have become essential components in mixed integer programming (MIP) solvers. Extending MIP-based heuristics, our study outlines generic procedures to build primal solutions in the context of a branch-and-price approach and reports on their performance. Our heuristic decisions carry on variables of the Dantzig–Wolfe reformulation, the motivation being to take advantage of a tighter linear programming relaxation than that of the original compact formulation and to benefit from the combinatorial structure embedded in these variables. We focus on the so-called diving methods that use reoptimization after each linear programming rounding. We explore combinations with diversification-intensification paradigms such as limited discrepancy search, sub-MIP, local branching, and strong branching. The dynamic generation of variables inherent to a column generation approach requires specific adaptation of heuristic paradigms. We manage to use simple strategies to get around these technical issues. Our numerical results on generalized assignment, cutting stock, and vertex-coloring problems set new benchmarks, highlighting the performance of diving heuristics as generic procedures in a column generation context and producing better solutions than state-of-the-art specialized heuristics in some cases.
The problem of designing a spanning tree on an underlying graph to minimize the flow costs of a given set of traffic demands is considered. Several new classes of valid inequalities are developed for the problem. Tests on 10-node problem instances show that the addition of these inequalities results in integer solutions for a significant majority of the instances without requiring any branching. In the remaining cases, root gaps of less than 1% from the optimal solutions are realized. For 30-node problem instances, the inequalities substantially reduce the number of nodes explored in the branch-and-bound tree, resulting in significantly reduced computational times. Optimal solutions are reported for problems with 30 nodes, 60 edges, fully dense traffic matrices, and Euclidean distance-based flow costs. Problems with such flow costs are well-known to be a very difficult class of problems to solve. Using the inequalities substantially improves the performance of a variable-fixing heuristic. Some polyhedral issues relating to the strength of these inequalities are also discussed.The e-companion is available at https://doi.org/10.1287/ijoc.2018.0827.
The most common approach to generate cuts in integer programming is to derive them from the linear programming relaxation. We study an alternative approach that extracts cuts from discrete relaxations known as relaxed decision diagrams. Through a connection between decision diagrams and polarity, the algorithm generates cuts that are facet defining for the convex hull of a decision diagram relaxation. As proof of concept, we provide computational evidence that this algorithm generates strong cuts for the maximum independent set problem and the minimum set covering problem.The online appendices are available at https://doi.org/10.1287/ijoc.2018.0830.
Solving a large batch of linear programs (LPs) with varying parameters is needed in stochastic programming and sensitivity analysis, among other modeling frameworks. Solving the LPs for all combinations of given parameter values, called the brute-force approach, can be computationally infeasible when the parameter space is high-dimensional and/or the underlying LP is computationally challenging. This paper introduces a computationally efficient approach for solving a large number of LPs that differ only in the right-hand side of the constraints (b of Ax=b). The computational approach builds on theoretical properties of the geometry of the space of critical regions, where a critical region is defined as the set of b’s for which a basis is optimal. To formally support our computational approach we provide proofs of geometric properties of neighboring critical regions. We contribute to the existing theory of parametric programming by establishing additional results, providing deeper geometric understanding of critical regions. On the basis of the geometric properties of critical regions, we develop an algorithm that solves the LPs in batches by finding critical regions that contain multiple b’s. Moreover, we suggest a data-driven version of our algorithm that uses the distribution (e.g., shape) of a sample of b’s for which the LPs need to be solved. We empirically compared our approach and three other methods on various instances. The results show the efficiency of our approach in comparison with the other methods but also indicate some limitations of the algorithm.The online supplement is available at https://doi.org/10.1287/ijoc.2018.0838.
Preferences provide a means for specifying the desires of a decision maker (DM) in a declarative way. In this paper, based on a DM’s pairwise preferences, we infer the DM’s unique decision model. We capture (a) the attitudinal character, (b) relative criteria importance, and (c) the criteria interaction, all of which are specific to the DM. We make use of the preference-learning (PL) technique to induce predictive preference models from empirical data. Because PL is emerging as a new subfield of machine learning, we could use standard machine-learning methods to accomplish our learning objective. We consider the DM’s exemplary preference information in the form of pairwise comparisons between alternatives as the training information. The DM’s decision model is captured in terms of (a), (b), and (c), through the parameters of an attitudinal Choquet integral operator. The proposed learning approach is validated through an experimental study on 16 standard data sets. The superiority of the proposed method in terms of predictive accuracy and easier interpretability is shown both theoretically as well as empirically.
The Traveling Salesman Problem with a Drone (TSP-D) is a hybrid truck and drone model of delivery, in which the drone rides on the truck and launches from the truck to deliver packages. Our approach to the TSP-D uses branch and bound, whereby each node of the branch-and-bound tree corresponds with a potential order to deliver a subset of packages. An approximate lower bound at each node is given by solving a dynamic program. We provide additional variants of our heuristic approach and compare solution quality and computation times. Consideration is given to various input parameters and distance metrics.The online supplement is available at https://doi.org/10.1287/ijoc.2018.0826.
We develop CIATA, a combined inversion-and-thinning approach for modeling a nonstationary non-Poisson process (NNPP), where the target arrival process is described by a given rate function and its associated mean-value function together with a given asymptotic variance-to-mean (dispersion) ratio. CIATA is based on the following: (i) a piecewise-constant majorizing rate function that closely approximates the given rate function from above; (ii) the associated piecewise-linear majorizing mean-value function; and (iii) an equilibrium renewal process (ERP) whose noninitial interrenewal times have mean 1 and variance equal to the given dispersion ratio. Transforming the ERP by the inverse of the majorizing mean-value function yields a majorizing NNPP whose arrival epochs are then thinned to deliver an NNPP having the specified properties. CIATA-Ph is a simulation algorithm that implements this approach based on an ERP whose noninitial interrenewal times have a phase-type distribution. Supporting theorems establish that CIATA-Ph can generate an NNPP having the desired mean-value function and asymptotic dispersion ratio. Extensive simulation experiments substantiated the effectiveness of CIATA-Ph with various rate functions and dispersion ratios. In all cases, we found approximate convergence of the dispersion ratio to its asymptotic value beyond a relatively short warm-up period.The online supplement is available at https://doi.org/10.1287/ijoc.2018.0828.
Closeness centrality is a class of distance-based measures in the network analysis literature to quantify reachability of a given vertex (or a group of vertices) by other network agents. In this paper, we consider a new class of critical edge detection problems, in which given a group of vertices that represent an important subset of network elements of interest (e.g., servers that provide an essential service to the network), the decision maker is interested in identifying a subset of critical edges whose removal maximally degrades the closeness centrality of those vertices. We develop a general optimization framework, in which the closeness centrality measure can be based on any nonincreasing function of distances between vertices, which, in turn, can be interpreted as communication efficiency between them. Our approach includes three well-known closeness centrality measures as special cases: harmonic centrality, decay centrality, and k𝑘-step reach centrality. Furthermore, for quantifying the centrality of a group of vertices we consider three different approaches for measuring the reachability of the group from any vertex in the network: minimum distance to a vertex in the group, maximum distance to a vertex in the group, and the average centrality of vertices in the group. We study the theoretical computational complexity of the proposed models and describe the corresponding mixed integer programming formulations. For solving medium- and large-scale instances of the problem, we first develop an exact algorithm that exploits the fact that real-life networks often have rather small diameters. Then we propose two conceptually different heuristic algorithms. Finally, we conduct computational experiments with real-world and synthetic network instances under various settings, which reveal interesting insights and demonstrate the advantages and limitations of the proposed models and algorithms.The online appendices are available at https://doi.org/10.1287/ijoc.2018.0829.
Two-person interdiction games represent an important modeling concept for applications in marketing, defending critical infrastructure, stopping nuclear weapons projects, or preventing drug smuggling. We present an exact branch-and-cut algorithm for interdiction games under the assumption that feasible solutions of the follower problem satisfy a certain monotonicity property. Prominent examples from the literature that fall into this category are knapsack interdiction, matching interdiction, and packing interdiction problems. We also show how practically relevant interdiction variants of facility location and prize-collecting problems can be modeled in our setting. Our branch-and-cut algorithm uses a solution scheme akin to Benders decomposition based on a family of so-called interdiction cuts. We present modified and lifted versions of these cuts along with exact and heuristic procedures for the separation of interdiction cuts and heuristic separation procedures for the other versions. In addition, we derive further valid inequalities and present a new heuristic procedure. We computationally evaluate the proposed algorithm on a benchmark of 360 knapsack interdiction instances from literature, including 27 instances for which the optimal solution was not known. Our approach is able to solve each of them to optimality within about one minute of computing time on a standard PC (in most cases, within just seconds), and it is up to some orders of magnitude faster than any previous approach from the literature. To further assess the effectiveness of our branch-and-cut algorithm, an additional computational study is performed on 144 randomly generated instances based on 0/1 multidimensional knapsack problems.
We introduce and investigate the problem of scheduling a single lock with parallel chambers. Special cases of this problem are related to interval scheduling. We focus on the existence of no-wait schedules and characterize their feasibility for a lock consisting of two chambers using new graph-theoretical concepts. We obtain a linear time algorithm for this special case. We also provide an efficient algorithm for the case where all chambers of the lock are identical. Furthermore, we describe a dynamic programming algorithm for the general case with arbitrary chambers. Finally, we indicate how our methods for the no-wait case can be applied to practical settings where waiting time is unavoidable.
Existing approaches to identify multiple solutions to combinatorial problems in practice are at best limited in their ability to simultaneously incorporate both diversity among generated solutions and problem-specific desires that may only be discovered or articulated by the user after further analysis of solver output. We propose a general framework for problems of a combinatorial nature that can generate a set of of multiple (near-)optimal, diverse solutions that are further infused with desirable features. We call our approach solution engineering. A key novelty is that desirable solution properties need not be explicitly modeled in advance. We customize the framework to both the mathematical programming and constraint programming technologies, and we subsequently demonstrate its practicality by implementing and then conducting computational experiments on existing test instances from the literature. Our computational results confirm the very real possibility of generating sets of solutions infused with features that might otherwise remain undiscovered.
In many applications, statistical estimators serve to derive conclusions from data, for example, in finance, medical decision making, and clinical trials. However, the conclusions are typically dependent on uncertainties in the data. We use robust optimization principles to provide robust maximum likelihood estimators that are protected against data errors. Both types of input data errors are considered: (a) the adversarial type, modeled using the notion of uncertainty sets, and (b) the probabilistic type, modeled by distributions. We provide efficient local and global search algorithms to compute the robust estimators and discuss them in detail for the case of multivariate normally distributed data. The estimator performance is demonstrated on two applications. First, using computer simulations, we demonstrate that the proposed estimators are robust against both types of data uncertainty and provide more accurate estimates compared with classical estimators, which degrade significantly, when errors are encountered. We establish a range of uncertainty sizes for which robust estimators are superior. Second, we analyze deviations in cancer radiation therapy planning. Uncertainties among plans are caused by patients’ individual anatomies and the trial-and-error nature of the process. When analyzing a large set of past clinical treatment data, robust estimators lead to more reliable decisions when applied to a large set of past treatment plans.
Eye tracking is an increasingly common technology with a variety of practical uses. Eye-tracking data, or gaze data, can be categorized into two main events: fixations represent focused eye movement, indicative of awareness and attention, whereas saccades are higher-velocity movements that occur between fixation events. Common methods to identify fixations in gaze data can lack sensitivity to peripheral points and may misrepresent positional and durational properties of fixations. To address these shortcomings, we introduce the notion of inner density for fixation identification, which concerns both the duration of the fixation and the proximity of its constituent gaze points. Moreover, we demonstrate how to identify fixations in a sequence of gaze data by optimizing for inner density. After decomposing the clustering of a temporal gaze data sequence into successive regions (chunks), we use nonlinear and linear 0–1 optimization formulations to identify the densest fixations within a given data chunk. Our approach is parametrized by a unique constant that adjusts the degree of desired density, allowing decision makers to have fine-tuned control over density during the process. Computational experiments on real data sets demonstrate the efficiency of our approach and its effectiveness in identifying fixations with greater density than existing methods, thereby enabling the refinement of key gaze metrics such as fixation duration and fixation center.
Knapsack problems play a pivotal role in the operations research literature, with various generalizations proposed and studied over the last century. Of recent interest is the quadratic multiknapsack problem (QMKP). Despite a plethora of heuristics, no exact methods for the QMKP have been published in the literature. This paper presents an exact branch-and-price algorithm for the QMKP. Experimental results indicate that the proposed algorithm is far superior, both in terms of solution times and objective function bounds, to state-of-the-art optimization technology solving a standard encoding of the problem. In addition to the algorithmic contribution, this paper studies the optimization problem of seating attendees at events, an operational challenge faced by event organizers. An optimization model for table event seating is shown to be closely related to the QMKP, and computational testing indicates that the proposed algorithm is particularly well suited for this application.
We study integrated production- and delivery-scheduling problems that arise in practical make-to-order settings in several industries. In these problems, make-to-order products are first processed in a plant and then delivered to customer sites through two stages of shipping: first, from the plant to a pool point (e.g., a port, a distribution, or a consolidation center) and, second, from the pool point to customer sites. The objective is to obtain a joint schedule of job processing at the plant and two-stage shipping of completed jobs to customer sites to optimize a performance measure that takes into account both delivery timeliness and total transportation costs. We consider two problems in which delivery timeliness is measured by total or maximum lead time of the jobs and study both offline and online versions of these problems. For the offline problems involving a single production line at the plant, we provide optimal dynamic programming algorithms. For the more general offline problems involving multiple production lines at the plant, we propose fast heuristics and analyze their worst-case and asymptotic performances. For the online problems, we propose online algorithms and analyze their competitive ratios. By comparing our offline heuristics with lower bounds using randomly generated test instances, it is shown that these heuristics are capable of generating near-optimal solutions quickly. Using real data from Baosteel’s Meishan plant, we also show that our corresponding offline heuristic generates significantly better solutions than Baosteel’s rule-based approach. In addition, our computational results on the performance of the online algorithms relative to the offline heuristics generate important methodological insights that can be used by practitioners in choosing a specific solution approach.
A polling model is defined as a queueing station with a single server serving multiple queues following a certain rule to determine when to stop serving one queue and which queue to serve next. The key to much of the analysis of these systems is to obtain the first two moments of queue lengths at polling instants, which are defined as the instants when the server starts to serve each queue. This study develops a novel approach, in which the generating functions of queue lengths at polling instants are computed iteratively. Transform inversion is then applied to invert these generating functions to obtain their first two moments efficiently. Because the expected waiting time of each queue depends on some basic distributions, such as the service time distributions, the switchover distributions, and the arrival batch distributions only through their first two moments, this study proposes a new transform inversion technique that is called “the pseudotransform inversion algorithm” by introducing “pseudotransforms” of these distributions, which are simply approximations to the transforms with Taylor’s expansions that agree up to a certain order. Then, the expected waiting times can be computed using transform inversion, because they are simply functions of two moments. This study also discusses the special case involving zero switchover times. The strongest benefit of the pseudotransform inversion algorithm over current approaches is that it significantly decreases the computation effort when the number of queues is large. Several numerical examples are devised to validate our approach and show its efficiency in analyzing polling models. The results show that the pseudotransform inversion algorithm is indeed quite efficient for analyzing polling stations. Although polling models are the best example that we have found to date, in any situation where the quantity to be computed depends only on the first several moments of the input distributions, the pseudotransform inversion algorithm holds promise as a computational algorithm.
Phase unwrapping is the process of recovering a continuous phase signal from an original signal wrapped in the (−π,π−𝜋,𝜋] interval. It is a critical step of coherent signal processing, with applications such as synthetic aperture radar, acoustic imaging, magnetic resonance, X-ray crystallography, and seismic processing. In the field of computational optics, this problem is classically treated as a norm-minimization problem, in which one seeks to minimize the differences between the gradients of the original wrapped signal and those of the continuous unwrapped signal. When the L0L0–norm is considered, the number of differences should be minimized, leading to a difficult combinatorial optimization problem. We propose an approximate model for the L0–norm phase unwrapping problem in two dimensions (2D), in which the singularities of the wrapped phase image are associated with a graph where the vertices have −1 or +1 polarities. The objective is to find a minimum-cost balanced spanning forest where the sum of the polarities is equal to zero in each tree. We introduce a set of primal and dual heuristics, a branch-and-cut algorithm, and a hybrid metaheuristic to efficiently find exact or heuristic solutions. These approaches move us one step closer to optimal solutions for 2D L0–norm phase unwrapping; such solutions were previously viewed, in the signal processing literature, as highly desirable but not achievable.
Radiation therapy is widely used in cancer treatment; however, plans necessarily involve tradeoffs between tumor coverage and mitigating damage to healthy tissue. Although current hardware can deliver custom-shaped beams from any angle around the patient, choosing (from all possible beams) an optimal set of beams that maximizes tumor coverage while minimizing collateral damage and treatment time is intractable. Furthermore, even though planning algorithms used in practice consider highly restricted sets of candidate beams, the time per run combined with the number of runs required to explore clinical tradeoffs results in planning times of hours to days. We propose a suite of cluster and bound methods that we hypothesize will (1) yield higher-quality plans by optimizing over much (i.e., 100-fold) larger sets of candidate beams, and/or (2) reduce planning time by allowing clinicians to search through candidate plans in real time. Our methods hinge on phrasing the treatment-planning problem as a convex problem. To handle large-scale optimizations, we form and solve compressed approximations to the full problem by clustering beams (i.e., columns of the dose deposition matrix used in the optimization) or voxels (rows of the matrix). Duality theory allows us to bound the error incurred when applying an approximate problem’s solution to the full problem. We observe that beam clustering and voxel clustering both yield excellent solutions while enabling a 10- to 200-fold speedup.
Surgery planning decisions include which operating rooms (ORs) to open, allocation of surgeries to ORs, sequence, and time to start each surgery. They are often made under uncertain surgery durations with limited data that lead to unknown distributional information. Moreover, cost parameters for criteria such as overtime and surgery delays are often difficult or impossible to estimate in practice. In this paper, we formulate distributionally robust (DR) chance constraints on surgery waiting and OR overtime, which recognize practical limitations on data availability and cost parameter accuracy. We use ϕ𝜙-divergence measures to build an ambiguity set of possible distributions of random surgery durations, and derive a branch-and-cut algorithm for optimizing a mixed-integer linear programming reformulation based on finite samples of the random surgery durations. We test instances generated from real hospital-based surgery data. The results show computational efficacy of our approaches, and provide insights for DR surgery planning.
“Virtual statistics,” as we define them, are estimators of performance measures that are conditional on the occurrence of an event; virtual waiting time of a customer arriving to a queue at time τ0𝜏0 is one example of virtual performance. In this paper, we describe a k-nearest-neighbor method for estimating virtual performance postsimulation from the retained sample paths, examining both its small-sample and asymptotic properties and providing two approaches for measuring the error of the k-nearest-neighbor estimator. We implement leave-one-replication-out cross-validation for tuning a single parameter k to use for any time (or times) of interest and evaluate the prediction performance of the k-nearest-neighbor estimator via controlled studies. As a by-product, this paper motivates a different way of thinking about how to process the output from dynamic, discrete-event simulation.
Inspired by the widespread and increasing usage of natural gas, we study the power consumption minimization problem associated with natural gas pipeline transmission in gunbarrel networks with nonidentical compressors. To accurately and flexibly model both gas flow dynamics and compressor working domains, we formulate the problem as a dynamic programming problem. Then we propose an approximate solution approach based on state dimension reduction. We analyze the problem properties and characterize conditions under which optimality is not compromised by the proposed solution approach. Next, we conduct numerical experiments using two data sets based on real networks in China and a data set from the public library GasLib. Numerical results demonstrate that the proposed solution approach significantly reduces computation time without compromising optimality in most cases. Specifically, the proposed solution approach obtains optimal solutions more than a 100 times faster than the exhaustive search when gas pressures are discretized at 0.01 MPa. Further, the optimality gaps do not exceed 0.4%.
The basis matrices corresponding to consecutive iterations of the simplex method only differ in a single column. This fact is commonly exploited in current linear programming solvers to avoid having to compute a new factorization of the basis at every iteration. Instead, a previous factorization is updated to reflect the modified column. Several methods are known for performing the update, most prominently the Forrest–Tomlin method. We present an alternative algorithm for the special case where the update can be performed purely by permuting rows and columns of the factors. In our experiments, this occurred for about half of the basis updates, and the new algorithm provides a modest reduction in computation time for the dual simplex method.
The single transferable vote (STV) is a system of preferential voting for multiseat elections. Each ballot cast by a voter is a (potentially partial) ranking over a set of candidates. No techniques currently exist for computing the margin of victory (MOV) in STV elections. The MOV is the smallest number of ballot manipulations (changes, additions, and deletions) required to bring about a change in the set of elected candidates. Knowing the MOV gives insight into how much time and money should be spent on auditing the election, and whether uncovered mistakes (such as ballot box losses) throw the election result into doubt—requiring a costly repeat election—or can be safely ignored. We present algorithms for computing lower and upper bounds on the MOV in STV elections. In small instances, these algorithms are able to compute exact margins.
We study how to model and handle correlated travel times in two-stage stochastic vehicle-routing problems. We allow these travel times to be correlated in time and space; that is, the travel time on one link in one period can be correlated to travel times on the same link in the next and previous periods as well as travel times on neighboring links (links sharing a node) in both the same and the following periods. Hence, we are handling a very high-dimensional dependent random vector. We discuss how such vehicle-routing problems should be modeled in time and space, how the random vector can be represented, and how scenarios (discretizations) can meaningfully be generated to be used in a stochastic program. We assume that the stochastic vehicle-routing problem is being solved by a search heuristic and focus on the objective function evaluation for any given solution. Numerical procedures are given and tested. As an example, our largest case has 142 nodes, 418 road links, and 60 time periods, leading to 25,080 dependent random variables. To achieve an objective function evaluation stability of 1%, we need only 15 scenarios for problem instances with 64 customer nodes and nine vehicles.
Many real-world data sets are modeled as entity relationship graphs or heterogeneous information networks. In these graphs, nodes represent entities and edges mimic relationships. ObjectRank extends the well-known PageRank authority flow–based ranking method to entity relationship graphs using an authority flow weight vector (W). The vector W assigns a different authority flow–based importance (weight) to each edge type based on domain knowledge or personalization. In this paper, our contribution is a framework for Learning to Rank in entity relationship graphs to learn W, in the context of authority flow. We show that the problem is similar to learning a recursive scoring function. We present a two-phase iterative solution and multiple variants of learning. In pointwise learning, we learn W, and hence the scoring function, from the scores of a sample of nodes. In pairwise learning, we learn W from given preferences for pairs of nodes. To demonstrate our contribution in a real setting, we apply our framework to learn the rank, with high accuracy, for a real-world challenge of predicting future citations in a bibliographic archive—that is, the FutureRank score. Our extensive experiments show that with a small amount of training data, and a limited number of iterations, our Learning to Rank approach learns W with high accuracy. Learning works well with pairwise training data in large graphs.
We introduce the algorithm SHEBO (surrogate optimization of problems with hidden constraints and expensive black-box objectives), an efficient optimization algorithm that employs surrogate models to solve computationally expensive black-box simulation optimization problems that have hidden constraints. Hidden constraints are encountered when the objective function evaluation does not return a value for a parameter vector. These constraints are often encountered in optimization problems in which the objective function is computed by a black-box simulation code. SHEBO uses a combination of local and global search strategies together with an evaluability prediction function and a dynamically adjusted evaluability threshold to iteratively select new sample points. We compare the performance of our algorithm with that of the mesh-based algorithms mesh adaptive direct search (MADS, NOMAD [nonlinear optimization by mesh adaptive direct search] implementation) and implicit filtering and SNOBFIT (stable noisy optimization by branch and fit), which assigns artificial function values to points that violate the hidden constraints. Our numerical experiments for a large set of test problems with 2–30 dimensions and a 31-dimensional real-world application problem arising in combustion simulation show that SHEBO is an efficient solver that outperforms the other methods for many test problems.
In this work, we propose a novel centrality metric, referred to as star centrality, which incorporates information from the closed neighborhood of a node, rather than solely from the node itself, when calculating its topological importance. More specifically, we focus on degree centrality and show that in the complex protein–protein interaction networks, it is a naive metric that can lead to misclassifying protein importance. For our extension of degree centrality when considering stars, we derive its computational complexity, provide a mathematical formulation, and propose two approximation algorithms that are shown to be efficient in practice. We portray the success of this new metric in protein–protein interaction networks when predicting protein essentiality in several organisms, including the well-studied Saccharomyces cerevisiae, Helicobacter pylori, and Caenorhabditis elegans, where star centrality is shown to significantly outperform other nodal centrality metrics at detecting essential proteins. We also analyze the average and worst-case performance of the two approximation algorithms in practice and show that they are viable options for computing star centrality in very large-scale protein–protein interaction networks, such as the human proteome, where exact methodologies are bound to be time and memory intensive.
We consider a maintenance planner problem to dynamically allocate the available repairmen to a system of unreliable production facilities. Each facility has several machines that incur a linear production loss due to stochastic degradation, which we model as a continuous time Markov process with fully observable states. The objective is to schedule group maintenance interventions, in discrete time epochs, so as to minimize production losses over an infinite horizon. Direct solution procedures, such as dynamic programming value or policy iteration, are impractical due to the curse of dimensionality. An approximate scheduling procedure is developed following Whittle’s restless bandits approach. In particular, we decompose the Whittle’s relaxation of our scheduling problem by production facility (i.e., bandit) using the Lagrangian technique. Based on the structural investigation of a single-bandit problem, we prove indexability and propose a novel index computational algorithm. Our numerical study shows that, for systems with three or four facilities, the index policy has a near-zero optimality gap. For systems with 10 or more facilities, the index policy expected cost remains fairly close to a lower bound that we compute using the known linear programming (LP) formulation of Whittle’s relaxation. Furthermore, the numerical study also shows that our policy yields substantial expected cost improvements relative to a benchmark LP-based heuristic when the states are partially observable and can handle large-scale systems unlike LP-based heuristics, which have excessive memory requirements.
Scheduling physicians is a key success factor in hospitals. Heterogeneous demand and 24/7 service make the problem challenging. Approaches in the literature use flexible shift patterns to match demand with scarce resources. In these approaches, demand is usually assumed to be deterministic. However, surgery durations and emergency arrivals are both uncertain, leading to massive staff overtime. We introduce stochastic demand for physicians using a scenario-based approach. To incorporate this in scheduling, we allow variable shift extensions. If a variable shift extension is scheduled, the physician knows that with a given probability he or she may have to work a few periods longer. Thus, we ensure a matching of supply with demand, and at the same time we increase predictability of working hours. We propose a mixed-integer model and a column generation heuristic to solve our problem and provide experimental data from a German university hospital. Our approach reduces unplanned overtime by more than 80%, given a constant workforce. In cases of similar levels of unplanned overtime, the required workforce level can be decreased by 20%. Our approach aims at improving physicians’ work–life balance and provides insights for hospitals’ contract design processes.
In this paper, we study the methodological underpinnings of the Morris elementary effects method, a model-free factor-screening technique originally proposed for deterministic simulation experiments, and develop an efficient Morris method–based framework (EMM) for simulation factor screening. Equipped with an efficient cluster-sampling procedure, EMM can simultaneously screen the main and interaction (or nonlinear) effects of all factors and control the overall false discovery rate at a prescribed level. Despite focusing on deterministic simulation experiments, we reveal the connections between EMM (also the Morris method) and other factor-screening methods, such as sequential bifurcation, and examine the resulting implications in the stochastic simulation setting under some commonly stipulated assumptions in design of experiments. Numerical experiments are presented to demonstrate the efficiency and efficacy of EMM.
This paper discusses efficient parallel algorithms for obtaining strong lower bounds and exact solutions for large instances of the quadratic assignment problem (QAP). Our parallel architecture is comprised of both multicore processors and compute unified device architecture–enabled NVIDIA graphics processing units (GPUs) on the Blue Waters Supercomputing Facility at the University of Illinois at Urbana–Champaign. We propose novel parallelization of the Lagrangian dual ascent algorithm on the GPUs, which is used for solving a QAP formulation based on the level-2 reformulation linearization technique. The linear assignment subproblems in this procedure are solved using our accelerated Hungarian algorithm [Date K, Rakesh N (2016) GPU-accelerated Hungarian algorithms for the linear assignment problem. Parallel Computing 57:52–72.]. We embed this accelerated dual-ascent algorithm in a parallel branch-and-bound scheme and conduct extensive computational experiments on single and multiple GPUs, using problem instances with up to 42 facilities from the quadratic assignment problem library (QAPLIB). The experiments suggest that our GPU-based approach is scalable, and it can be used to obtain tight lower bounds on large QAP instances. Our accelerated branch-and-bound scheme is able to comfortably solve Nugent and Taillard instances (up to 30 facilities) from the QAPLIB, using a modest number of GPUs.
Most real-world optimization problems are multi-objective by nature, with conflicting and incomparable objectives. Solving a multi-objective optimization problem requires a method that can generate all rational compromises between the objectives. This paper proposes two distinct bound set-based branch-and-cut algorithms for general bi-objective combinatorial optimization problems based on implicit and explicit lower-bound sets. The algorithm based on explicit lower-bound sets computes, for each branching node, a lower-bound set and compares it with an upper-bound set. The other fathoms branching nodes by generating a single point on the lower-bound set for each local nadir point. We outline several approaches for fathoming branching nodes, and we propose an updating scheme for the lower-bound sets that prevents us from solving the bi-objective linear programming relaxation of each branching node. To strengthen the lower-bound sets, we propose a bi-objective cutting-plane algorithm that adjusts the weights of the objective functions such that different parts of the feasible set are strengthened by cutting planes. In addition, we suggest an extension of the branching strategy “Pareto branching.” We prove the effectiveness of the algorithms through extensive computational results.
In bi-objective integer optimization the optimal result corresponds to a set of nondominated solutions. We propose a generic bi-objective branch-and-bound algorithm that uses a problem-independent branching rule exploiting available integer solutions and takes advantage of integer objective coefficients. The developed algorithm is applied to bi-objective facility location problems and the bi-objective set covering problem, as well as to the bi-objective team orienteering problem with time windows. In the latter case, lower bound sets are computed by means of column generation. Comparison with state-of-the-art exact algorithms shows the effectiveness of the proposed branch-and-bound algorithm.
We present the first (criterion space search) algorithm for optimizing a linear function over the set of efficient solutions of biobjective mixed integer linear programs. The proposed algorithm is developed based on the triangle splitting method [Boland N, Charkhgard H, Savelsbergh M (2015) A criterion space search algorithm for biobjective mixed integer programming: The triangle splitting method. INFORMS J. Comput. 27(4):597–618.], which can find a full representation of the nondominated frontier of any biobjective mixed integer linear program. The proposed algorithm is easy to implement and converges quickly to an optimal solution. An extensive computational study shows the efficacy of the algorithm. We numerically show that the proposed algorithm can be used to quickly generate a provably high-quality approximate solution because it maintains a lower and an upper bound on the optimal value of the linear function at any point in time.
On behalf of the Editorial Board, I would like to thank the following people, who acted as Reviewers during the past year.Alice Smith, Editor-in-Chief
We present a general method for obtaining strong bounds for discrete optimization problems that is based on a concept of branching duality. It can be applied when no useful integer programming model is available, and we illustrate this with the minimum bandwidth problem. The method strengthens a known bound for a given problem by formulating a dual problem whose feasible solutions are partial branching trees. It solves the dual problem with a “worst-bound” local search heuristic that explores neighboring partial trees. After proving some optimality properties of the heuristic, we show that it substantially improves known combinatorial bounds for the minimum bandwidth problem with a modest amount of computation. It also obtains significantly tighter bounds than depth-first and breadth-first branching, demonstrating that the dual perspective can lead to better branching strategies when the object is to find valid bounds.
Despite recent interest in multiobjective integer programming, few algorithms exist for solving biobjective mixed integer programs. We present such an algorithm: the boxed line method. For one of its variants, we prove that the number of single-objective integer programs solved is bounded by a linear function of the number of nondominated line segments in the nondominated frontier. This is the first such complexity result. An extensive computational study demonstrates that the box line method is also efficient in practice and that it outperforms existing algorithms on a diverse set of instances.
We reformulate a (indefinite) quadratic program (QP) as a mixed-integer linear programming (MILP) problem by first reformulating a QP as a linear complementary problem, and then using binary variables and big-M constraints to model its complementary constraints. To obtain such reformulation, we use fundamental results on the solution of perturbed linear systems to impose bounds on the QP’s dual variables without eliminating any of its (globally) optimal primal solutions. Reformulating a nonconvex QP as a MILP problem allows the use of current state-of-the-art MILP solvers to find its global optimal solution. To illustrate this, we compare the performance of this MILP-based solution approach, labeled quadprogIP, with quadprogBB, BARON, and CPLEX. In practice, quadprogIP is shown to typically outperform by orders of magnitude quadprogBB, BARON, and CPLEX on standard QPs. Also, unlike quadprogBB, quadprogIP is able to solve QP instances in which the dual feasible set is unbounded. The MATLAB code quadprogIP and the instances used to perform the reported numerical experiments are publicly available at https://github.com/xiawei918/quadprogIP.
This paper deals with a class of biobjective mixed binary linear programs having a multiple-choice constraint, which are found in applications such as Pareto set–reduction problems, single-supplier selection, and investment decisions, among others. Two objective space–search algorithms are presented. The first algorithm, termed line search and linear programming filtering, is a two-phase procedure. Phase 1 searches for supported Pareto outcomes using the parametric weighted sum method, and Phase 2 searches for unsupported Pareto outcomes by solving a sequence of auxiliary mixed binary linear programs. An effective linear programming filtering procedure excludes any previous outcomes found to be dominated. The second algorithm, termed linear programming decomposition and filtering, decomposes the mixed binary problem by iteratively fixing binary variables and uses the linear programming filtering procedure to prune out any dominated outcomes. Computational experiments show the effectiveness of the linear programming filtering and suggest that both algorithms run faster than existing general-purpose objective space–search procedures.
The bipartite Boolean quadratic programming problem (BBQP) is a generalization of the well-studied NP-hard Boolean quadratic programming problem and can be regarded as a unified model for many graph theoretic optimization problems, including maximum weight-induced subgraph problems, maximum weight biclique problems, matrix factorization problems, and maximum cut problems on bipartite graphs. This paper introduces three main algorithms for solving the BBQP, based on three variants of tabu search, the first two consisting of strategic oscillation–tabu search (SO-TS) algorithms, which use destructive and constructive procedures to guide the search into unexplored and promising areas. The third algorithm, whichDoes also incorporates the SO-TS algorithms as solution improvement methods, uses a path relinking (PR) algorithm that is capable of further enhancing search performance. Experimental results demonstrate that all three algorithms perform very effectively compared with the best methods in the literature, and the PR algorithm joined with tabu search is able to discover new best solutions for two-thirds of the large problem instances and match the previous best known solutions for the other instances. Additional analysis discloses the contributions of the key ingredients of each of the proposed algorithms.
This work presents an improved branch-cut-and-price algorithm for the identical parallel machine scheduling problem minimizing a generic function of the job completion times. A new family of cuts is proposed to strengthen the arc-time-indexed formulation, along with an efficient separation algorithm. Also, the projection of the arc-time-indexed into a time-indexed formulation is introduced to take advantage of the variable fixings performed in the larger variable space. The improved algorithm was capable of solving 146 out of 150 instances in the literature, with 12 being solved for the first time. Also, the running time for the 134 previously solved instances decreased by 95.7% on the average.
We study pseudo-polynomial formulations for the classical bin packing and cutting stock problems. We first propose an overview of dominance and equivalence relations among the main pattern-based and pseudo-polynomial formulations from the literature. We then introduce reflect, a new formulation that uses just half of the bin capacity to model an instance and needs significantly fewer constraints and variables than the classical models. We propose upper- and lower-bounding techniques that make use of column generation and dual information to compensate reflect weaknesses when bin capacity is too high. We also present nontrivial adaptations of our techniques that solve two interesting problem variants, namely the variable-sized bin packing problem and the bin packing problem with item fragmentation. Extensive computational tests on benchmark instances show that our algorithms achieve state of the art results on all problems, improving on previous algorithms and finding several new proven optimal solutions.
Given a graph, a set of origin-destination (OD) pairs with communication requirements, and an integer k ≥ 2, the network design problem with vulnerability constraints (NDPVC) is to identify a subgraph with the minimum total edge costs such that, between each OD pair, there exist a hop-constrained primary path and a hop-constrained backup path after any k − 1 edges of the graph fail. Formulations exist for single-edge failures (i.e., k = 2). To solve the NDPVC for an arbitrary number of edge failures, we develop two natural formulations based on the notion of length-bounded cuts. We compare their strengths and flexibilities in solving the problem for k ≥ 3. We study different methods to separate infeasible solutions by computing length-bounded cuts of a given size. Experimental results show that, for single-edge failures, our formulation increases the number of solved benchmark instances from 61% (obtained within a two-hour limit by the best published algorithm) to more than 95%, thus increasing the number of solved instances by 1,065. Our formulation also accelerates the solution process for larger hop limits and efficiently solves the NDPVC for general k. We test our best algorithm for two to five simultaneous edge failures and investigate the impact of multiple failures on the network design.
We study the lattice structure of random number generators of the MIXMAX family, a class of matrix linear congruential generators that produces a vector of random numbers at each step. The design of these generators was inspired by Kolmogorov K-systems over the unit torus in the real space, for which the transition function is measure preserving and produces a chaotic behavior. In actual implementations, however, the state space is a finite set of rational vectors, and the MIXMAX has a lattice structure just like linear congruential and multiple recursive generators. Its matrix entries were also selected in a special way to allow a fast implementation, and this has an impact on the lattice structure. We study this lattice structure for vectors of successive and nonsuccessive output values in various dimensions. We show in particular that for coordinates at specific lags not too far apart, in three dimensions, or if we construct points of k+2 or more successive values from the beginning of an output vector of size k, all the nonzero points lie in only two hyperplanes. This is reminiscent of the behavior of lagged-Fibonacci and add-with-carry/subtract-with-borrow generators. And even if we skip the output coordinates involved in this bad structure, other highly structured projections often remain, depending on the choice of parameters. We show that empirical statistical tests can easily detect this structure.
We consider multistage stochastic programming problems in which the random parameters have finite support, leading to optimization over a finite scenario set. There has been recent interest in dual bounds for such problems, of two types. One, known as expected group subproblem objective (EGSO) bounds, require solution of a group subproblem, which optimizes over a subset of the scenarios, for all subsets of the scenario set that have a given cardinality. Increasing the subset cardinality in the group subproblem improves bound quality, (EGSO bounds form a hierarchy), but the number of group subproblems required to compute the bound increases very rapidly. Another is based on partitions of the scenario set into subsets. Combining the values of the group subproblems for all subsets in a partition yields a partition bound. In this paper, we consider partitions into subsets of (nearly) equal cardinality. We show that the expected value of the partition bound over all such partitions also forms a hierarchy. To make use of these bounds in practice, we propose random sampling of partitions and suggest two enhancements to the approach: sampling partitions that align with the multistage scenario tree structure and use of an auxiliary optimization problem to discover new best bounds based on the values of group subproblems already computed. We establish the effectiveness of these ideas with computational experiments on benchmark problems. Finally, we give a heuristic to save computational effort by ceasing computation of a partition partway through if it appears unpromising.
The consecutive hit probability of antiaircraft artillery corresponds to the multivariate normal probability distribution. The computational complexity depends on the length of the firing error sequence (i.e., the integral dimension may exceed 100). The traditional numerical integration and the Monte Carlo method are too slow for this calculation. This paper established the state equation of the firing error sequence, which was the bridge between the multivariate normal probability distribution and stochastic process theory. The recursive calculation model was given after the rigorous derivation process. The accuracy and computational complexity of the model were quantified by theoretical analysis and expressed intuitively by examples. The model shows the upper bound for the absolute error, and the computational efficiency is significantly improved.
The context of this work is the well-studied dissemination of information in large-scale distributed networks through pairwise interactions. This problem, originally called rumor mongering, and then rumor spreading, has mainly been investigated in the synchronous model. This model relies on the assumption that all the nodes of the network act in synchrony; that is, at each round of the protocol, each node is allowed to contact a random neighbor. In this paper, we drop this assumption under the argument that it is not realistic in large-scale systems. We, thus, consider the asynchronous variant, with which, at random times, nodes successively interact by pairs, exchanging their information on the rumor. In a previous paper, we performed a study of the total number of interactions needed for all the nodes of the network to discover the rumor. Although most of the existing results involve huge constants that do not allow us to compare different protocols, we provided a thorough analysis of the distribution of this total number of interactions together with its asymptotic behavior. In this paper, we extend this discrete-time analysis by solving a conjecture proposed previously, and we consider the continuous-time case, in which a Poisson process is associated to each node to determine the instants at which interactions occur. The rumor-spreading time is, thus, more realistic because it is the real time needed for all the nodes of the network to discover the rumor. Once again, as most of the existing results involve huge constants, we provide tight bound and equivalent of the complementary distribution of the rumor-spreading time. We also give the exact asymptotic behavior of the complementary distribution of the rumor-spreading time around its expected value when the number of nodes tends to infinity.
Feature selection is at the heart of machine learning, and it is effective at facilitating data interpretability and improving prediction performance by defying the curse of dimensionality. Group feature selection is often used to reveal relationships in structured data and provide better predictive power compared with the standard feature selection methods without consideration of the grouped structure. We study a group feature selection problem in networked data in which edge weights are considered as features, while each node in the network is regarded as a group feature. This problem is particularly useful in feature selection for neuroimaging data, where the data are high dimensional and the intrinsic networked structure among the features (i.e., connectivities between regions) in brain data has to be captured properly. We propose a mathematical model based on the support vector machines (SVM), which entails the ℓ0 norm regularization to restrict the number of nodes (i.e., groups). To cope with the computational challenge of the ℓ0 norm regularization, we develop a convex relaxation reformulation of the proposed model as a convex semiinfinite programming (SIP). We then introduce a new iterative algorithm that achieves an optimal solution for this convex SIP. Experimental results for synthetic and real brain network data sets show that our approach gives better predictive performance compared with the state-of-the-art group feature selection and the standard feature selection methods. Our technique additionally yields a sparse subnetwork solution that is easier to interpret than those obtained by other methods.
We study robust convex quadratic programs where the uncertain problem parameters can contain both continuous and integer components. Under the natural boundedness assumption on the uncertainty set, we show that the generic problems are amenable to exact copositive programming reformulations of polynomial size. These convex optimization problems are NP-hard but admit a conservative semidefinite programming (SDP) approximation that can be solved efficiently. We prove that the popular approximate S-lemma method—which is valid only in the case of continuous uncertainty—is weaker than our approximation. We also show that all results can be extended to the two-stage robust quadratic optimization setting if the problem has complete recourse. We assess the effectiveness of our proposed SDP reformulations and demonstrate their superiority over the state-of-the-art solution schemes on instances of least squares, project management, and multi-item newsvendor problems.
The quadratic shortest path problem is the problem of finding a path in a directed graph such that the sum of interaction costs over all pairs of arcs on the path is minimized. We derive several semidefinite programming relaxations for the quadratic shortest path problem with a matrix variable of order m + 1, where m is the number of arcs in the graph. We use the alternating direction method of multipliers to solve the semidefinite programming relaxations. Numerical results show that our bounds are currently the strongest bounds for the quadratic shortest path problem. We also present computational results on solving the quadratic shortest path problem using a branch and bound algorithm. Our algorithm computes a semidefinite programming bound in each node of the search tree, and solves instances with up to 1,300 arcs in less than an hour.
In accordance with the National Organ Transplant Act, which requires the efficient and equitable allocation of donated organs, the United Network for Organ Sharing (UNOS) prioritizes patients on the liver transplant waiting list within given geographic areas based mainly on their most recently reported health status. Accordingly, the UNOS requires patients to update their health status at a frequency that depends on their last reported health status. However, patients may elect to update any time within the required timeframe, which creates opportunities to game the system, leading to information asymmetries between the UNOS and the patients on the waiting list. This information asymmetry can be alleviated through more frequent updating requirements but at the price of an increased update burden (e.g., data collection costs and patient inconvenience). We propose a model that determines health reporting requirements that simultaneously minimize these two (possibly conflicting) criteria (i.e., inequity due to information asymmetry and update burden). Calibrating the model with clinical data, we examine (i) the degree to which an individual patient can benefit from the flexibility inherent to the current health reporting requirements and (ii) alternative recommendations that dominate the current requirements with respect to the two criteria of interest.
The mothership and drone routing problem (MDRP) considers the routing of a two-vehicle tandem. The larger vehicle, which may be a ship or an airplane, is called the mothership; the smaller vehicle, which may be a small boat or unmanned aerial vehicle, is called the drone. We assume that there exists a set of target locations T. For each t in T, the drone must launch from the mothership, visit t, and then return to the mothership to refuel. The drone has a limited range of R time units. In the MDRP, we assume that both mothership and drone operate in the “open seas” (i.e., using the Euclidean metric). We also introduce the mothership and infinite-capacity drone routing problem (MDRP-IC), where a drone launches from the mothership and visits one or more targets consecutively before returning to the mothership. Our exact approach uses branch and bound, where each node of the branch-and-bound tree corresponds to a potential subsequence of the order of target visits. A lower bound at each node is given by solving a second-order cone program, which optimally chooses a launch point and landing point for each target in the subsequence. A set of heuristics that also uses a second-order cone program as an embedded procedure is presented. We show that our schemes are flexible to accommodate a variety of additional constraints and/or objective functions. Computational results and interesting variants of the MDRP and MDRP-IC are also presented.
We address the one-to-one multicommodity pickup-and-delivery traveling salesman problem, a challenging variant of the traveling salesman problem that includes the transportation of commodities between locations. The goal is to find a minimum cost tour such that each commodity is delivered to its destination and the maximum capacity of the vehicle is never exceeded. We propose an exact approach that uses a discrete relaxation based on multivalued decision diagrams (MDDs) to better represent the combinatorial structure of the problem. We enhance our relaxation by using the MDDs as a subproblem to a Lagrangian relaxation technique, leading to significant improvements in both bound quality and run-time performance. Our work extends the use of MDDs for solving routing problems by presenting new construction methods and filtering rules based on capacity restrictions. Experimental results show that our approach outperforms state-of-the-art methodologies, closing 33 open instances from the literature, with 27 of those closed by our best variant.
We introduce a simple modification to the repeated shortest-path algorithm for the all-pairs shortest-path problem that adds a cumulative distance label update at each iteration based on the shortest-path tree from the prior iteration. We have implemented and tested our update using several shortest-path algorithms on a range of test networks of varying size, degree, and “skewness” (i.e., asymmetry) of costs on antisymmetric arcs, and we find that it provides a significant speedup to any such algorithm, except for cases either in which the underlying graph is extremely sparsely connected (or even disconnected) or when the arc costs are highly nonsymmetric. An added charm is that our best-modified method preserves the polynomial worst case runtime of its label-correcting antecedent. As with other repeated shortest-path algorithms, it is significantly faster than the Floyd–Warshall algorithm on sparsely connected networks and even some fairly densely connected networks.
Viral-marketing strategies are of significant interest in the online economy. Roughly, in these problems, one seeks to identify which individuals to strategically target in a social network so that a given proportion of the network is influenced at minimum cost. Earlier literature has focused primarily on problems where a fixed inducement is provided to those targeted. In contrast, resembling the practical viral-marketing setting, we consider this problem where one is allowed to “partially influence” (by the use of monetary inducements) those selected for targeting. We thus focus on the “least-cost influence problem (LCIP)”: an influence-maximization problem where the goal is to find the minimum total amount of inducements (individuals to target and associated tailored incentive) required to influence a given proportion of the population. Motivated by the desire to develop a better understanding of fundamental problems in social-network analytics, we seek to develop (exact) optimization approaches for the LCIP. Our paper makes several contributions, including (i) showing that the problem is NP-complete in general as well as under a wide variety of special conditions; (ii) providing an influence greedy algorithm to solve the problem polynomially on trees, where we require 100% adoption and all neighbors exert equal influence on a node; and (iii) a totally unimodular formulation for this tree case.
In this study, we investigate how individual users’ rating characteristics affect the user-level performance of recommendation algorithms. We measure users’ rating characteristics from three perspectives: rating value, rating structure, and neighborhood network embeddedness. We study how these three categories of measures influence the predictive accuracy of popular recommendation algorithms for each user. Our experiments use five real-world data sets with varying characteristics. For each individual user, we estimate the predictive accuracy of three recommendation algorithms. We then apply regression-based models to uncover the relationships between rating characteristics and recommendation performance at the individual user level. Our experimental results show consistent and significant effects of several rating measures on recommendation accuracy. Understanding how rating characteristics affect the recommendation performance at the individual user level has practical implications for the design of recommender systems.
Network function virtualization enables efficient cloud-resource planning by virtualizing network services and applications into software running on commodity servers. A cloud-service provider needs to manage and ensure service availability of a network of concurrent virtualized network functions (VNFs). The downtime distribution of a network of VNFs can be estimated using sample-path randomization on the underlying birth–death process. An integrated modeling approach for this purpose is limited by its scalability and computational load because of the high dimensionality of the integrated birth–death process. We propose a generalized convex decomposition of the integrated birth–death process, which transforms the high-dimensional multi-VNF process into a series of interlinked, low-dimensional, single-VNF processes. We theoretically show the statistical equivalence between the transition probabilities of the integrated birth–death process and those resulting from interlinking the decomposed system of processes. We further develop a decomposition algorithm that yields scalable and fast estimation of the system downtime distribution. Our algorithmic framework can be easily adapted to any logical definition of overall system availability. It can also be easily extended to various realistic VNF network configurations and characteristics including heterogeneous VNF failure distributions, effects of both node and link failures on the overall system downtime of fully or partially connected networks, and resource sharing across multiple VNFs. Our extensive computational results demonstrate the computational efficiency of the proposed algorithms while ensuring statistical consistency with the integrated-network model and the superior performance of the decomposition strategy over the integrated modeling approach.
The advances in conic optimization have led to its increased utilization for modeling data uncertainty. In particular, conic mean-risk optimization gained prominence in probabilistic and robust optimization. Whereas the corresponding conic models are solved efficiently over convex sets, their discrete counterparts are intractable. In this paper, we give a highly effective successive quadratic upper-bounding procedure for discrete mean-risk minimization problems. The procedure is based on a reformulation of the mean-risk problem through the perspective of its convex quadratic term. Computational experiments conducted on the network interdiction problem with stochastic capacities show that the proposed approach yields near-optimal solutions in a small fraction of the time required by exact-search algorithms. We demonstrate the value of the proposed approach for constructing efficient frontiers of flow at risk versus interdiction cost for varying confidence levels.
Estimating portfolio risk measures and classifying portfolio risk levels in real time are important yet challenging tasks. In this paper, we propose to build a logistic regression model using data generated in past simulation experiments and to use the model to predict portfolio risk measures and classify risk levels at any time. We further explore regularization techniques, simulation model structure, and additional simulation budget to enhance the estimators of the logistic regression model to make its predictions more precise. Our numerical results show that the proposed methods work well. Our work may be viewed as an example of the recently proposed idea of simulation analytics, which treats a simulation model as a data generator and proposes to apply data analytics tools to the simulation outputs to uncover conditional statements. Our work shows that the simulation analytics idea is viable and promising in the field of financial risk management.
Consider a real-valued function that can be only observed with stochastic noise at a finite set of design points within a Euclidean space. We wish to determine whether there exists a convex function that goes through the true function values at the design points. We develop an asymptotically consistent Bayesian sequential sampling procedure that estimates the posterior probability of this being true. In each iteration, the posterior probability is estimated using Monte Carlo simulation. We offer three variance reduction methods: change of measure, acceptance-rejection, and conditional Monte Carlo. Numerical experiments suggest that the conditional Monte Carlo method is preferred.
We analyze the variance of single-run unbiased stochastic derivative estimators. The distribution of a specific conditional expectation characterizes an intrinsic distributional property of the derivative estimators in a given class, which, in turn, separates two of the most popular single-run unbiased derivative estimators, infinitesimal perturbation analysis and the likelihood ratio method, into disjoint classes. In addition, a necessary and sufficient condition for the estimators to achieve the lowest variance in a certain class is provided, as well as insights into finding an estimator with lower variance. We offer a sufficient condition to substantiate the rule of thumb that the infinitesimal perturbation analysis estimator has a smaller variance than does the likelihood ratio method estimator and to provide a counterexample when the sufficient condition is not satisfied.
Robust optimization has emerged in the operations research literature as a tractable and practical way to model uncertainty in optimization problems. Early approaches focused on relative worst-case objective functions, where the value of a solution is measured versus the best-possible solution over an uncertainty set of scenarios. However, over the past ten years the focus has primarily been on absolute worst-case objective functions, which have generally been considered to be more tractable. In this paper, we demonstrate that for many problems of interest, including some adaptive robust optimization problems, that considering relative objective functions does not significantly increase the computational cost over absolute objective functions. We use combinations of absolute and relative worst-case objective functions to find Pareto-efficient solutions that combine aspects of both, which suggests an approach to distinguish between otherwise very similar robust solutions. We provide reformulation and cutting plane approaches for these problems and demonstrate their efficacy with experiments on minimum-cost flow, inventory control, and facility location problems. These case studies show that solutions corresponding to relative objective functions may be a better match for a decision maker’s risk preferences than absolute.
In this paper, a new branch-and-price-and-cut algorithm is proposed to solve the one-dimensional bin-packing problem (1D-BPP). The 1D-BPP is one of the most fundamental problems in combinatorial optimization and has been extensively studied for decades. Recently, a set of new 500 test instances were proposed for the 1D-BPP, and the best exact algorithm proposed in the literature can optimally solve 167 of these new instances, with a time limit of 1 hour imposed on each execution of the algorithm. The exact algorithm proposed in this paper is based on the classical set-partitioning model for the 1DBPPs and the subset row inequalities. We describe an ad hoc label-setting algorithm to solve the pricing problem, dominance, and fathoming rules to speed up its computation and a new primal heuristic. The exact algorithm can easily handle some practical constraints, such as the incompatibility between the items, and therefore, we also apply it to solve the one-dimensional bin-packing problem with conflicts (1D-BPPC). The proposed method is tested on a large family of 1D-BPP and 1D-BPPC classes of instances. For the 1D-BPP, the proposed method can optimally solve 237 instances of the new set of difficult instances; the largest instance involves 1,003 items and bins of capacity 80,000. For the 1D-BPPC, the experiments show that the method is highly competitive with state-of-the-art methods and that it successfully closed several open 1D-BPPC instances.
The capacitated p-center problem requires one to select p facilities from a set of candidates to service a number of customers, subject to facility capacity constraints, with the aim of minimizing the maximum distance between a customer and its associated facility. The problem is well known in the field of facility location, because of the many applications that it can model. In this paper, we solve it by means of search algorithms that iteratively seek the optimal distance by solving tailored subproblems. We present different mathematical formulations for the subproblems and improve them by means of several valid inequalities, including an effective one based on a 0–1 disjunction and the solution of subset sum problems. We also develop an alternative search strategy that finds a balance between traditional sequential search and binary search. This strategy limits the number of feasible subproblems to be solved and, at the same time, avoids large overestimates of the solution value, which are detrimental for the search. We evaluate the proposed techniques by means of extensive computational experiments on benchmark instances from the literature and new larger test sets. All instances from the literature with up to 402 vertices and integer distances are solved to proven optimality, including 13 open cases, and feasible solutions are found in 10 minutes for instances with up to 3,038 vertices.
Exactly solving multiobjective integer programming (MOIP) problems is often a very time-consuming process, especially for large and complex problems. Parallel computing has the potential to significantly reduce the time taken to solve such problems but only if suitable algorithms are used. The first of our new algorithms follows a simple technique that demonstrates impressive performance for its design. We then go on to introduce new theory for developing more efficient parallel algorithms. The theory utilises elements of the symmetric group to apply a permutation to the objective functions to assign different workloads and applies to algorithms that order the objective functions lexicographically. As a result, information and updated bounds can be shared in real time, creating a synergy between threads. We design and implement two algorithms that take advantage of such a theory. To properly analyse the running time of our three algorithms, we compare them against two existing algorithms from the literature and against using multiple threads within our chosen integer programming solver, CPLEX. This survey of six different parallel algorithms, to our knowledge the first of its kind, demonstrates the advantages of parallel computing. Across all problem types tested, our new algorithms are on par with existing algorithms on smaller cases and massively outperform the competition on larger cases. These new algorithms, and freely available implementations, allow the investigation of complex MOIP problems with four or more objectives.
Mixed integer programming has become a very powerful tool for modeling and solving real-world planning and scheduling problems, with the breadth of applications appearing to be almost unlimited. A critical component in the solution of these mixed integer programs is a set of routines commonly referred to as presolve. Presolve can be viewed as a collection of preprocessing techniques that reduce the size of and, more importantly, improve the “strength” of the given model formulation, that is, the degree to which the constraints of the formulation accurately describe the underlying polyhedron of integer-feasible solutions. As our computational results will show, presolve is a key factor in the speed with which we can solve mixed integer programs and is often the difference between a model being intractable and solvable, in some cases easily solvable. In this paper we describe the presolve functionality in the Gurobi commercial mixed integer programming code. This includes an overview, or taxonomy of the different methods that are employed, as well as more-detailed descriptions of several of the techniques, with some of them appearing, to our knowledge, for the first time in the literature.
Piecewise linear (PWL) functions are used in a variety of applications. Computing such continuous PWL functions, however, is a challenging task. Software packages and the literature on PWL function fitting are dominated by heuristic methods. This is true for both fitting discrete data points and continuous univariate functions. The only exact methods rely on nonconvex model formulations. Exact methods compute continuous PWL function for a fixed number of breakpoints minimizing some distance function between the original function and the PWL function. An optimal PWL function can only be computed if the breakpoints are allowed to be placed freely and are not fixed to a set of candidate breakpoints. In this paper, we propose the first convex model for optimal continuous univariate PWL function fitting. Dependent on the metrics chosen, the resulting formulations are either mixed-integer linear programming or mixed-integer quadratic programming problems. These models yield optimal continuous PWL functions for a set of discrete data. On the basis of these convex formulations, we further develop an exact algorithm to fit continuous univariate functions. Computational results for benchmark instances from the literature demonstrate the superiority of the proposed convex models compared with state-of-the-art nonconvex models.
We propose mixed-integer programming models for fitting univariate discrete data points with continuous piecewise linear (PWL) functions. The number of approximating function segments and the locations of break points are optimized simultaneously. The proposed models include linear constraints and convex objective function and, thus, are computationally more efficient than previously proposed mixed-integer nonlinear programming models. We also show how the proposed models can be extended to approximate univariate functions with PWL functions with the minimum number of segments subject to bounds on the pointwise error.
This article describes two versions of the chance-constrained stochastic bin-packing (CCSBP) problem that consider item-to-bin allocation decisions in the context of chance constraints on the total item size within the bins. The first version is a stochastic CCSBP (SP-CCSBP) problem, which assumes that the distributions of item sizes are known. We present a two-stage stochastic mixed-integer program (SMIP) for this problem and a Dantzig–Wolfe formulation suited to a branch-and-price (B&P) algorithm. We further enhance the formulation using coefficient strengthening and reformulations based on probabilistic packs and covers. The second version is a distributionally robust CCSBP (DR-CCSBP) problem, which assumes that the distributions of item sizes are ambiguous. Based on a closed-form expression for the DR chance constraints, we approximate the DR-CCSBP problem as a mixed-integer program that has significantly fewer integer variables than the SMIP of the SP-CCSBP problem, and our proposed B&P algorithm can directly solve its Dantzig–Wolfe formulation. We also show that the approach for the DR-CCSBP problem, in addition to providing robust solutions, can obtain near-optimal solutions to the SP-CCSBP problem. We implement a series of numerical experiments based on real data in the context of surgery scheduling, and the results demonstrate that our proposed B&P algorithm is computationally more efficient than a standard branch-and-cut algorithm, and it significantly improves upon the performance of a well-known bin-packing heuristic.
For general multiobjective optimization problems, the usual goal is finding the set of solutions not dominated by any other solutions, that is, a set of solutions as good as any other solution in all objectives and strictly better in at least one objective. In this paper, we propose a novel performance metric called the domination measure to measure the quality of a solution, which can be intuitively interpreted as the probability that an arbitrary solution in the solution space dominates that solution with respect to a predefined probability measure. We then reformulate the original problem as a stochastic and single-objective optimization problem. We further propose a model-based approach to solve it, which leads to an ideal version algorithm and an implementable version algorithm. We show that the ideal version algorithm converges to a set representation of the global optima of the reformulated problem; we demonstrate the numerical performance of the implementable version algorithm by comparing it with numerous existing multiobjective optimization methods on popular benchmark test functions. The numerical results show that the proposed approach is effective in generating a finite and uniformly spread approximation of the Pareto optimal set of the original multiobjective problem and is competitive with the tested existing methods. The concept of domination measure opens the door for potentially many new algorithms, and our proposed algorithm is an instance that benefits from domination measure.
The discrete ordered median problem (DOMP) is formulated as a set-partitioning problem using an exponential number of variables. Each variable corresponds to a set of demand points allocated to the same facility with the information of the sorting position of their corresponding costs. We develop a column generation approach to solve the continuous relaxation of this model. Then we apply a branch-price-and-cut algorithm to solve small- to large-sized instances of DOMP in competitive computational time.
The redundancy allocation problem (RAP) aims to find an optimal allocation of redundant components subject to resource constraints. In this paper, mixed integer linear programming (MILP) models and MILP-based algorithms are proposed for complex system reliability redundancy allocation problem with mixed components, where the system have bridges or interconnecting subsystems and each subsystem can have mixed types of components. Unlike the other algorithms in the literature, the proposed MILP models view the problem from a different point of view and approximate the nonconvex nonlinear system reliability function of a complex system using random samples. The solution to the MILP converges to the optimal solution of the original problem as sample size increases. In addition, data aggregation-based algorithms are proposed to improve the solution time and quality based on the proposed MILP models. A computational experiment shows that the proposed models and algorithms converge to the optimal or best-known solution as sample size increases. The proposed algorithms outperform popular metaheuristic algorithms in the literature.
In this paper, we consider a redundancy allocation problem for a series parallel system with uncertain component lifetimes that minimizes system costs while safeguarding system reliability over a given threshold level. We consider mixed redundancy strategies of cold standby and active redundancy with multiple types of components. We address lifetime uncertainty in the framework of distributionally robust optimization. In particular, we assume the probability distributions of the component lifetimes are not exactly known with only limited distributional information (e.g., mean, dispersion, and support) being available. We protect the worst-case system reliability constraint over all the possible component lifetime distributions that are consistent with the given distributional characteristics. The proposed modeling framework enjoys computationally attractive structures. The evaluation of the worst-case system reliability in our redundancy allocation problem can be transformed into a linear program, and the resulting overall redundancy allocation optimization problem can be cast as a mixed integer linear program that does not induce any additional integer variables (other than original allocation variables). In addition, the extreme joint distribution of component lifetimes can be efficiently recovered by solving a linear program. Our modeling framework can also be extended to incorporate the startup failures and common-cause failures for cold standbys and active parallels, respectively, to cater to more computationally complex settings. Finally, the computational experiments positively demonstrate the performance of the proposed approach in protecting system reliability.
In recent years, several universities have adopted an algorithmic approach to the allocation of seats in courses, for which students place bids (typically by ordering or scoring desirable courses), and then seats are awarded according to a predetermined procedure or mechanism. Designing the appropriate mechanism for translating bids into student schedules has received attention in the literature, but there is currently no consensus on the best mechanism in practice. In this paper, we introduce five new algorithms for this course-allocation problem, using various combinations of matching algorithms, second-price concepts, and optimization, and compare our new methods with the natural benchmarks from the literature: the (proxy) draft mechanism and the (greedy) bidding-point mechanism. Using simulation, we compare the algorithms on metrics of fairness, efficiency, and incentive compatibility, measuring their ability to encourage truth telling among boundedly rational agents. We find good results for all of our methods and that a two-stage, full-market optimization performs best in measures of fairness and efficiency but with slightly worse incentives to act strategically compared with the best of the mechanisms. We also find generally negative results for the bidding-point mechanism, which performs poorly in all categories. These results can help guide the decision of selecting a mechanism for course allocation or for similar assignment problems, such as project team assignments or sports drafts, for example, in which efficiency and fairness are of utmost importance but incentives must also be considered. Additional robustness checks and comparisons are provided in the online supplement.
This paper studies robust variants of an extended model of the classical heterogeneous vehicle routing problem (HVRP), where a mixed fleet of vehicles with different capacities, availabilities, fixed costs, and routing costs is used to serve customers with uncertain demand. This model includes, as special cases, all variants of the HVRP studied in the literature with fixed and unlimited fleet sizes, accessibility restrictions at customer locations, and multiple depots. Contrary to its deterministic counterpart, the goal of the robust HVRP is to determine a minimum cost set of routes and fleet composition that remains feasible for all demand realizations from a prespecified uncertainty set. To solve this problem, we develop robust versions of classical node and edge exchange neighborhoods that are commonly used in local search and establish that efficient evaluation of the local moves can be achieved for five popular classes of uncertainty sets. The proposed local search is then incorporated in a modular fashion within two metaheuristic algorithms to determine robust HVRP solutions. The quality of the metaheuristic solutions is quantified using an integer programming model that provides lower bounds on the optimal solution. An extensive computational study on literature benchmarks shows that the proposed methods allow us to obtain high-quality robust solutions for different uncertainty sets and with minor additional effort compared with deterministic solutions.
This paper studies mixed-integer nonlinear programs featuring disjunctive constraints and trigonometric functions and presents a strengthened version of the convex quadratic relaxation of the optimal transmission switching problem. We first characterize the convex hull of univariate quadratic on/off constraints in the space of original variables using perspective functions. We then introduce new tight quadratic relaxations for trigonometric functions featuring variables with asymmetrical bounds. These results are used to further tighten recent convex relaxations introduced for the optimal transmission switching problem in power systems. Using the proposed improvements, along with bound propagation, on 23 medium-sized test cases in the PGLib benchmark library with a relaxation gap of more than 1%, we reduce the gap to less than 1% on five instances. The tightened model has promising computational results when compared with state-of-the-art formulations.
We study the energy consumption minimization problems of natural gas transmission in gunbarrel structured networks. In particular, we consider the transient-state dynamics of natural gas and the compressor’s nonlinear working domain and min-up-and-down constraints. We formulate the problem as a two-level dynamic program (DP), where the upper-level DP problem models each compressor station as a decision stage and each station’s optimization problem is further formulated as a lower-level DP by setting each time period as a stage. The upper-level DP faces the curse of high dimensionality. We propose an approximate dynamic programming (ADP) approach for the upper-level DP using appropriate basis functions and an exact approach for the lower-level DP by exploiting the structure of the problem. We validate the superior performance of the proposed ADP approach on both synthetic and real networks compared with the benchmark simulated annealing (SA) heuristic and the commonly used myopic policy and steady-state policy. On the synthetic networks (SNs), the ADP reduces the energy consumption by 5.8%–6.7% from the SA and 12% from the myopic policy. On the test gunbarrel network with 21 compressor stations and 28 pipes calibrated from China National Petroleum Corporation, the ADP saves 4.8%–5.1% (with an average of 5.0%) energy consumption compared with the SA and the currently deployed steady-state policy, which translates to cost savings of millions of dollars a year. Moreover, the proposed ADP algorithm requires 18.4%–61.0% less computation time than the SA. The advantages in both solution quality and computation time strongly support the proposed ADP algorithm in practice.
Cross-site account correlation correlates users who have multiple accounts but the same identity across online social networks (OSNs). Being able to identify cross-site users is important for a variety of applications in social networks, security, and electronic commerce, such as social link prediction and cross-domain recommendation. Because of either heterogeneous characteristics of platforms or some unobserved but intrinsic individual factors, the same individuals are likely to behave differently across OSNs, which accordingly causes many challenges for correlating accounts. Traditionally, account correlation is measured by analyzing user-generated content, such as writing style, rules of naming user accounts, or some existing metadata (e.g., account profile, account historical activities). Accounts can be correlated by de-anonymizing user behaviors, which is sometimes infeasible since such data are not often available. In this work, we propose a method, called ACCount eMbedding (ACCM), to go beyond text data and leverage semantics of network structures, a possibility that has not been well explored so far. ACCM aims to correlate accounts with high accuracy by exploiting the semantic information among accounts through random walks. It models and understands latent representations of accounts using an embedding framework similar to sequences of words in natural language models. It also learns a transformation matrix to project node representations into a common dimensional space for comparison. With evaluations on both real-world and synthetic data sets, we empirically demonstrate that ACCM provides performance improvement compared with several state-of-the-art baselines in correlating user accounts between OSNs.
The bilinear assignment problem (BAP) is a generalization of the well-known quadratic assignment problem. In this paper, we study the problem from the computational analysis point of view. Several classes of neighborhood structures are introduced for the problem along with some theoretical analysis. These neighborhoods are then explored within a local search and variable neighborhood search frameworks with multistart to generate robust heuristic algorithms. In addition, we present several very fast construction heuristics. Our systematic experimental analysis disclosed some interesting properties of the BAP, different from those of comparable models. We have also introduced benchmark test instances that can be used for future experiments on exact and heuristic algorithms for the problem.
This paper explores the connections between the classical maximum clique problem and its edge-weighted generalization, the maximum edge weight clique (MEWC) problem. As a result, a new analytic upper bound on the clique number of a graph is obtained and an exact algorithm for solving the MEWC problem is developed. The bound on the clique number is derived using a Lagrangian relaxation of an integer (linear) programming formulation of the MEWC problem. Furthermore, coloring-based bounds on the clique number are used in a novel upper-bounding scheme for the MEWC problem. This scheme is employed within a combinatorial branch-and-bound framework, yielding an exact algorithm for the MEWC problem. Results of computational experiments demonstrate a superior performance of the proposed algorithm compared with existing approaches.
A γ-quasi-clique in a simple undirected graph refers to a subset of vertices that induces a subgraph with edge density at least γ. When γ equals one, this definition corresponds to a classical clique. When γ is less than one, it relaxes the requirement of all possible edges by the clique definition. Quasi-clique detection has been used in graph-based data mining to find dense clusters, especially in large-scale error-prone data sets in which the clique model can be overly restrictive. The maximum γ-quasi-clique problem, seeking a γ-quasi-clique of maximum cardinality in the given graph, can be formulated as an optimization problem with a linear objective function and a single quadratic constraint in binary variables. This article investigates the Lagrangian dual of this formulation and develops an upper-bounding technique using the geometry of ellipsoids to bound the Lagrangian dual. The tightness of the upper bound is compared with those obtained from multiple mixed-integer programming formulations of the problem via experiments on benchmark instances.
The rich data used to train learning models increasingly tend to be distributed and private. It is important to efficiently perform learning tasks without compromising individual users’ privacy even considering untrusted learning applications and, furthermore, understand how privacy-preservation mechanisms impact the learning process. To address the problem, we design a differentially private distributed algorithm based on the stochastic variance reduced gradient (SVRG) algorithm, which prevents the learning server from accessing and inferring private training data with a theoretical guarantee. We quantify the impact of the adopted privacy-preservation measure on the learning process in terms of convergence rate, by which it indicates noises added at each gradient update results in a bounded deviation from the optimum. To further evaluate the impact on the trained models, we compare the proposed algorithm with SVRG and stochastic gradient descent using logistic regression and neural nets. The experimental results on benchmark data sets show that the proposed algorithm has minor impact on the accuracy of trained models under a moderate amount of privacy budget.
We propose the first budget-limited multi-armed bandit (BMAB) algorithm subject to a union of matroid constraints in arm pulling, while at the same time achieving differential privacy. Our model generalizes the arm-pulling models studied in prior BMAB schemes, and it can be used to address many practical problems such as network backbone construction and dynamic pricing in crowdsourcing. We handle the exploitation versus exploration tradeoff in our BMAB problem by exploiting the combinatorial structures of matroids, and reduce the searching complexity of arm selection based on a divide-and-conquer approach. Our algorithm achieves a uniform logarithmic regret bound with respect to B and ɛ-differential privacy, where B is the budget for pulling the arms with random costs. Without differential privacy, our algorithm achieves a uniform logarithmic regret bound with respect to B, which advances the asymptotic regret bounds achieved by prior BMAB algorithms. We performed side-by-side comparisons with prior schemes in our experiments. Experimental results show that our purely-combinatorial algorithm not only achieves significantly better regret performance, but also is more than 20 times faster than prior BMAB schemes, which use time-consuming LP-solving techniques.
Scenario decomposition algorithms for stochastic programs compute bounds by dualizing all nonanticipativity constraints and solving individual scenario problems independently. We develop an approach that improves on these bounds by reinforcing a carefully chosen subset of nonanticipativity constraints, effectively placing scenarios into groups. Specifically, we formulate an optimization problem for grouping scenarios that aims to improve the bound by optimizing a proxy metric based on information obtained from evaluating a subset of candidate feasible solutions. We show that the proposed grouping problem is NP-hard in general, identify a polynomially solvable case, and present two formulations for solving the problem: a matching formulation for a special case and a mixed-integer programming formulation for the general case. We use the proposed grouping scheme as a preprocessing step for a particular scenario decomposition algorithm and demonstrate its effectiveness in solving standard test instances of two-stage 0–1 stochastic programs. Using this approach, we are able to prove optimality for all previously unsolved instances of a standard test set. Additionally, we implement this scheme as a preprocessing step for PySP, a publicly available and widely used implementation of progressive hedging, and compare this grouping approach with standard grouping approaches on large-scale stochastic unit commitment instances. Finally, the idea is extended to propose a finitely convergent algorithm for two-stage stochastic programs with a finite feasible region.
In this paper, we present a method to determine if a lift-and-project cut for a mixed-integer linear program is irregular, in which case the cut is not equivalent to any intersection cut from the bases of the linear relaxation. This is an important question due to the intense research activity for the past decade on cuts from multiple rows of simplex tableau as well as on lift-and-project cuts from nonsplit disjunctions. Although it has been known for a while that lift-and-project cuts from split disjunctions are always equivalent to intersection cuts and consequently to such multirow cuts, it has been recently shown that there is a necessary and sufficient condition in the case of arbitrary disjunctions: a lift-and-project cut is regular if, and only if, it corresponds to a regular basic solution of the Cut Generating Linear Program (CGLP). This paper has four contributions. First, we state a result that simplifies the verification of regularity for basic CGLP solutions. Second, we provide a mixed-integer formulation that checks whether there is a regular CGLP solution for a given cut that is regular in a broader sense, which also encompasses irregular cuts that are implied by the regular cut closure. Third, we describe a numerical procedure based on such formulation that identifies irregular lift-and-project cuts. Finally, we use this method to evaluate how often lift-and-project cuts from simple t-branch split disjunctions are irregular, and thus not equivalent to multirow cuts, on 74 instances of the Mixed Integer Programming Library (MIPLIB) benchmarks.
We consider assortment optimization problems, where the choice process of a customer takes place in multiple stages. There is a finite number of stages. In each stage, we offer an assortment of products that does not overlap with the assortments offered in the earlier stages. If the customer makes a purchase within the offered assortment, then the customer leaves the system with the purchase. Otherwise, the customer proceeds to the next stage, where we offer another assortment. If the customer reaches the end of the last stage without a purchase, then the customer leaves the system without a purchase. The choice of the customer in each stage is governed by a multinomial logit model. The goal is to find an assortment to offer in each stage to maximize the expected revenue obtained from a customer. For this assortment optimization problem, it turns out that the union of the optimal assortments to offer in each stage is nested by revenue in the sense that this union includes a certain number of products with the largest revenues. However, it is still difficult to figure out the stage in which a certain product should be offered. In particular, the problem of finding an assortment to offer in each stage to maximize the expected revenue obtained from a customer is NP hard. We give a fully polynomial time approximation scheme for the problem when the number of stages is fixed.
We provide a comprehensive overview of mixed-integer programming formulations for the unit commitment (UC) problem. UC formulations have been an especially active area of research over the past 12 years due to their practical importance in power grid operations, and this paper serves as a capstone for this line of work. We additionally provide publicly available reference implementations of all formulations examined. We computationally test existing and novel UC formulations on a suite of instances drawn from both academic and real-world data sources. Driven by our computational experience from this and previous work, we contribute some additional formulations for both generator production upper bounds and piecewise linear production costs. By composing new UC formulations using existing components found in the literature and new components introduced in this paper, we demonstrate that performance can be significantly improved—and in the process, we identify a new state-of-the-art UC formulation.
Multiarmed bandit (MAB) problems, typically modeled as Markov decision processes (MDPs), exemplify the learning versus earning trade-off. An area that has motivated theoretical research in MAB designs is the study of clinical trials, where the application of such designs has the potential to significantly improve patient outcomes. However, for many practical problems of interest, the state space is intractably large, rendering exact approaches to solving MDPs impractical. In particular, settings that require multiple simultaneous allocations lead to an expanded state and action-outcome space, necessitating the use of approximation approaches. We propose a novel approximation approach that combines the strengths of multiple methods: grid-based state discretization, value function approximation methods, and techniques for a computationally efficient implementation. The hallmark of our approach is the accurate approximation of the value function that combines linear interpolation with bounds on interpolated value and the addition of a learning component to the objective function. Computational analysis on relevant datasets shows that our approach outperforms existing heuristics (e.g., greedy and upper confidence bound family of algorithms) and a popular Lagrangian-based approximation method, where we find that the average regret improves by up to 58.3%. A retrospective implementation on a recently conducted phase 3 clinical trial shows that our design could have reduced the number of failures by 17% relative to the randomized control design used in that trial. Our proposed approach makes it practically feasible for trial administrators and regulators to implement Bayesian response-adaptive designs on large clinical trials with potential significant gains.
We provide a novel regret-based robust formulation of the Dorfman group size problem considering the realistic setting where the prevalence rate is uncertain, establish key structural properties of the optimal solution, and provide an exact algorithm. Our analysis also leads to exact closed-form expressions for the optimal Dorfman group size under a deterministic prevalence rate, which is the problem studied in the extant literature. Thus, our structural results not only unify existing, and mostly empirical, results on the Dorfman group size problem under a deterministic prevalence rate, but, more importantly, enable us to efficiently solve the robust version of this problem to optimality. We demonstrate the value of robust testing schemes with a case study on disease screening using realistic data. Our case study indicates that robust testing schemes can significantly outperform their deterministic counterparts, by not only substantially reducing the maximum regret value, but, in the majority of the cases, reducing testing costs as well. Our findings have important implications on public health screening practices.
The transmission of multidrug-resistant organisms (MDROs) in the healthcare setting is an ongoing challenge affecting at least 2 million patients in the United States each year via infection and leading to over 20,000 deaths. Many mathematical models have been developed to approximate MDRO transmission dynamics, focusing most often on evaluating the impact of various infection-control strategies. However, although the insights derived from these studies are useful, the models do not typically have the ability to support decision making for infection-control practitioners in real time. In this study, we design a detailed agent-based model of MDRO transmission—focusing on methicillin-resistant Staphylococcus aureus in the intensive care unit setting—and validate its transmission dynamics using data collected during a multisite randomized, controlled trial. We leverage this model to develop and evaluate the effectiveness of a prediction-driven approach for targeting patients for contact precautions (i.e., requiring all visiting healthcare workers to wear personal protective equipment) in a simulated intensive care unit based on their daily likelihood of becoming colonized by the organism. We show that we can predict these outcomes with moderate to high accuracy across a broad range of scenarios and that these predictions can be used to efficiently target patients for intervention and, ultimately, to reduce the overall acquisition rate in the unit.
Recent theoretical research has employed the linear-quadratic model of dose-response in stochastic control formulations for spatiobiologically integrated radiotherapy. The goal is to maximize the expected tumor kill while limiting the biologically effective dose administered to nearby organs at risk under tolerable limits. This is attempted by adapting fluence maps to the uncertain evolution of tumor-cell densities observed in functional images acquired at the beginning of each treatment session. One limitation of this research is that the treatment planner is assumed to know the probability distribution of a crucial dose-response parameter in the linear-quadratic model. This paper proposes a Bayesian stochastic control framework to relax this assumption. An algorithm rooted in certainty-equivalent control is devised to simultaneously learn this probability distribution while adapting fluence maps based on dose-response data collected from functional images over the treatment course. This algorithm’s performance is compared via numerical simulations with two other solution procedures that are also rooted in certainty equivalent control. The first one is a clairvoyant method. This assumes that the treatment planner knows the probability distribution, and hence serves as an idealized gold standard. The other one uses a fixed value of the dose-response parameter as available from the literature, and hence provides a natural benchmark without learning. The tumor kill achieved by the learning algorithm is statistically indistinguishable from the clairvoyant approach, whereas it can be about 20% higher than the no-learning benchmark. Both these conclusions bode well for individualized spatiobiologically integrated radiotherapy using functional images, at least in theory.
Two nodes of a wireless network may not be able to communicate with each other directly, perhaps because of obstacles or insufficient signal strength. This necessitates the use of intermediate nodes to relay information. Often, one designates a (preferably small) subset of them to relay these messages (i.e., to serve as a virtual backbone for the wireless network), which can be seen as a connected dominating set (CDS) of the associated graph. Ideally, these communication paths should be short, leading to the notion of a latency-constrained CDS. In this paper, we point out several shortcomings of a previously studied formalization of a latency-constrained CDS and propose an alternative one. We introduce an integer programming formulation for the problem that has a variable for each node and imposes the latency constraints via an exponential number of cut-like inequalities. Two nice properties of this formulation are that (1) it applies when distances are hop-based and when they are weighted and (2) it easily generalizes to ensure fault tolerance. We provide a branch-and-cut implementation of this formulation and compare it with a new polynomial-size formulation. Computational experiments demonstrate the superiority of the cut-like formulation. We also study related questions from computational complexity, such as approximation hardness, and answer an open problem regarding the fault diameter of graphs.
Distributed generation and remotely controlled switches have emerged as important technologies to improve the resiliency of distribution grids against extreme weather-related disturbances. Therefore it becomes important to study how best to place them on the grid in order to meet a resiliency criteria, while minimizing costs and capturing their dependencies on the associated communication systems that sustain their distributed operations. This paper introduces the Optimal Resilient Design Problem for Distribution and Communication Systems (ORDPDC) to address this need. The ORDPDC is formulated as a two-stage stochastic mixed-integer program that captures the physical laws of distribution systems, the communication connectivity of the smart grid components, and a set of scenarios that specifies which components are affected by potential disasters. The paper proposes an exact branch-and-price algorithm for the ORDPDC that features a strong lower bound and a variety of acceleration schemes to address degeneracy. The ORDPDC model and branch-and-price algorithm were evaluated on a variety of test cases with varying disaster intensities and network topologies. The results demonstrate the significant impact of the network topologies on the expansion plans and costs, as well as the computational benefits of the proposed approach.
We propose and analyze a generalized splitting method to sample approximately from a distribution conditional on the occurrence of a rare event. This has important applications in a variety of contexts in operations research, engineering, and computational statistics. The method uses independent trials starting from a single particle. We exploit this independence to obtain asymptotic and nonasymptotic bounds on the total variation error of the sampler. Our main finding is that the approximation error depends crucially on the relative variability of the number of points produced by the splitting algorithm in one run and that this relative variability can be readily estimated via simulation. We illustrate the relevance of the proposed method on an application in which one needs to sample (approximately) from an intractable posterior density in Bayesian inference.
Online reviews are playing an increasingly important role in understanding and predicting users’ rating behavior, which brings great opportunities for users and organizations to make better decisions. In recent years, rating prediction has become a research hotspot. Existing research primarily focuses on generating content representation based on context information and using the overall rating score to optimize the semantics of the content, which largely ignores aspect ratings reflecting users’ feelings about more specific attributes of a product and semantic associations among aspect ratings, words, and sentences. Cognitive theory research has shown that users evaluate and rate products following the part–whole pattern; namely, they use aspect ratings to explicitly express sentiments toward aspect attributes of products and then describe those attributes in detail through the corresponding opinion words and sentences. In this paper, we develop a deep learning-based method for understanding and predicting users’ rating behavior, which adopts the hierarchical attention mechanism to unify the explicit aspect ratings and review contents. We conducted experiments using data collected from two real-world review sites and found that our proposed approach significantly outperforms existing methods. Experiments also show that the performance advantage of the proposed approach mainly comes from the high-quality representation of review content and the effective integration of aspect ratings. A user study empirically shows that aspect ratings influence users’ perceived review helpfulness and reduce users’ cognitive effort in understanding the overall score given for a product. The research contributes to the rating behavior analysis literature and has significant practical implications.
We study an integrated production and transportation problem for a make-to-order manufacturing company that operates under the commit-to-delivery mode and uses third-party logistics service providers to deliver products to customers on or before certain committed delivery dates. Such third-party logistics service providers often provide various shipping modes with quantity discounts and different guaranteed shipping times. As a result, the company’s shipping costs need to be represented by general shipping cost functions that are typically nondecreasing, subadditive, and piecewise linear with shipping quantities, and nonincreasing with guaranteed shipping times. To the best of our knowledge, this paper is the first attempt to solve such an integrated production and transportation problem for the commit-to-delivery mode with general shipping costs. We prove that with general shipping costs, the problem is strongly NP-hard when the planning horizon consists of an arbitrary number of days. For the two-day problem, we show that it is ordinarily NP-hard, but is unlikely to have a fully polynomial time approximation scheme (FPTAS) unless NP=P. Interestingly, we find that when the unit inventory holding cost is relatively small, which is often true in practice, there exists an FPTAS for the two-day problem, the development of which hinges on a newly discovered property for minimizing the sum of two general piecewise linear functions. For the multiday problem, we develop a heuristic algorithm based on column generation, which novelly uses a dynamic program for a variant of the problem with a single customer. Results from computational experiments demonstrate that the heuristic algorithm can find near-optimal solutions with optimality gaps less than 1% in a short running time.
This paper addresses the close-enough traveling salesman problem, a variant of the Euclidean traveling salesman problem, in which the traveler visits a node if it passes through the neighborhood set of that node. We apply an effective strategy to discretize the neighborhoods of the nodes and the carousel greedy algorithm to appropriately select the neighborhoods that, step by step, are added to the partial solution until a feasible solution is generated. Our heuristic, based on these ingredients, is able to compute tight upper and lower bounds on the optimal solution relatively quickly. The computational results, carried out on benchmark instances, show that our heuristic often finds the optimal solution, on the instances where it is known, and in general, the upper bounds are more accurate than those from other algorithms available in the literature.Summary of Contribution: In this paper, we focus on the close-enough traveling salesman problem. This is a problem that has attracted research attention over the last 10 years; it has numerous real-world applications. For instance, consider the task of meter reading for utility companies. Homes and businesses have meters that measure the usage of gas, water, and electricity. Each meter transmits signals that can be read by a meter reader vehicle via radio-frequency identification (RFID) technology if the distance between the meter and the reader is less than r units. Each meter plays the role of a target point and the neighborhood is a disc of radius r centered at each target point. Now, suppose the meter reader vehicle is a drone and the goal is to visit each disc while minimizing the amount of energy expended by the drone. To solve this problem, we develop a metaheuristic approach, called (lb/ub)Alg, which computes both upper and lower bounds on the optimal solution value. This metaheuristic uses an innovative discretization scheme and the Carousel Greedy algorithm to obtain high-quality solutions. On benchmark instances where the optimal solution is known, (lb/ub)Alg obtains this solution 83% of the time. Over the remaining 17% of these instances, the deviation from the optimality is 0.05%, on average. On the instances with the highest overlap ratio, (lb/ub)Alg does especially well.
The simple pattern minimality problem (SPMP) represents a central problem in the logical analysis of data and association rules mining, and it finds applications in several fields as logic synthesis, reliability analysis, and automated reasoning. It consists of determining the minimum number of patterns explaining all the observations of a data set, that is, a Boolean logic formula that is true for all the elements of the data set and false for all the unseen observations. We refer to this problem as covering SPMP (C-SPMP), because each observation can be explained (covered) by more than one pattern. Starting from a real industrial application, we also define a new version of the problem, and we refer to it as partitioning SPMP (P-SPMP), because each observation has to be covered just once. Given a propositional formula or a truth table, C-SPMP and P-SPMP coincide exactly with the problem of determining the minimum disjunctive and minimum exclusive disjunctive normal form, respectively. Both problems are known to be NP-hard and have been generally tackled by heuristic methods. In this context, the contribution of this work is twofold. On one side, it provides two original integer linear programming formulations for the two variants of the SPMP. These formulations exploit the concept of Boolean hypercube to build a graph representation of the problems and allow to exactly solve instances with more than 1,000 observations by using an MIP solver. On the other side, two effective and fast heuristics are proposed to solve relevant size instances taken from literature (SeattleSNPs) and from the industrial database. The proposed methods do not suffer from the same dimensional drawbacks of the methods present in the literature and outperform either existing commercial and freeware logic tools or the available industrial solutions in the number of generated patterns and/or in the computational burden.
In the multidimensional multiple choice knapsack problem (MMKP), items with nonnegative profits are partitioned into groups. Each item consumes a predefined nonnegative amount of a set of resources with given availability. The problem looks for a subset of items consisting of exactly one item for each group that maximizes the overall profit without violating the resource constraints. The MMKP is among the most complex problems in the knapsack family. In the literature, although a plethora of heuristic approaches have been proposed, very few exact methods can be found, and all of them work only on limited size instances. In this paper, we propose a new exact approach for the problem. The method exactly solves subproblems of increasing size by means of a recursive variable-fixing process until an optimality condition is satisfied. The algorithm has several properties. Memory requirement remains almost constant during computation, and the method is general enough to be easily adapted to other knapsack problems. Finally, it can be converted at no cost into a heuristic approach. We close to optimality 10 open benchmark instances and improve the best-known values for many of the remaining ones. Interesting enough, our algorithm is able to find, within three minutes, better solutions than the ones found by Gurobi in one hour.
We consider multiobjective simulation optimization (MOSO) problems on integer lattices, that is, nonlinear optimization problems in which multiple simultaneous objective functions can only be observed with stochastic error, for example, as output from a Monte Carlo simulation model. The solution to a MOSO problem is the efficient set, which is the set of all feasible decision points that map to nondominated points in the objective space. For problems with two objectives, we propose the retrospective partitioned epsilon-constraint with relaxed local enumeration (R-PERLE) algorithm. R-PERLE is designed for simulation efficiency and provably converges to a local efficient set under appropriate regularity conditions. It uses a retrospective approximation (RA) framework and solves each resulting biobjective sample-path problem only to an error tolerance commensurate with the sampling error. R-PERLE uses the subalgorithm RLE to certify it has found a sample-path approximate local efficient set. We also propose R-MinRLE, which is a provably convergent benchmark algorithm for problems with two or more objectives. R-PERLE performs favorably relative to R-MinRLE and the current state of the art, MO-COMPASS, in our numerical experiments. This work points to a family of RA algorithms for MOSO on integer lattices that employ RLE to certify sample-path approximate local efficient sets and for which we provide the convergence guarantees.
We present the PyMOSO software package for (1) solving multiobjective simulation optimization (MOSO) problems on integer lattices and (2) implementing and testing new simulation optimization (SO) algorithms. First, for solving MOSO problems on integer lattices, PyMOSO implements R-PERLE, a state-of-the-art algorithm for two objectives, and R-MinRLE, a competitive benchmark algorithm for three or more objectives. Both algorithms use pseudogradients, are designed for sampling efficiency, and return solutions that, under appropriate regularity conditions, provably converge to a local efficient set with probability 1 as the simulation budget increases. PyMOSO can interface with existing simulation software and can obtain simulation replications in parallel. Second, for implementing and testing new SO algorithms, PyMOSO includes pseudorandom number stream management, implements algorithm testing with independent pseudorandom number streams run in parallel, and computes the performance of algorithms with user-defined metrics. For convenience, we also include an implementation of R-SPLINE for problems with one objective. The PyMOSO source code is available under a permissive open-source license.
Although robust optimization is a powerful technique in dealing with uncertainty in optimization, its solutions can be too conservative. More specifically, it can lead to an objective value much worse than the nominal solution or even to infeasibility of the robust problem. In practice, this can lead to robust solutions being disregarded in favor of the nominal solution. This conservatism is caused by both the constraint-wise approach of robust optimization and its core assumption that all constraints are hard for all scenarios in the uncertainty set. This paper seeks to alleviate this conservatism by proposing an alternative robust formulation that condenses all uncertainty into a single constraint, binding the worst-case expected violation in the original constraints from above. Using recent results in distributionally robust optimization, the proposed formulation is shown to be tractable for both right- and left-hand side uncertainty. A computational study is performed with problems from the NETLIB library. For some problems, the percentage of uncertainty is magnified fourfold in terms of increase in objective value of the standard robust solution compared with the nominal solution, whereas we find solutions that safeguard against over half the violation at only a 10th of the cost in objective value. For problems with an infeasible standard robust counterpart, the suggested approach is still applicable and finds both solutions that safeguard against most of the uncertainty at a low price in terms of objective value.
In this paper, we present a new computation scheme for the pessimistic bilevel optimization problem, which so far does not have any computational methods generally applicable. We first develop a tight relaxation and then design a simple scheme to ensure a feasible and optimal solution. Then we discuss using this scheme to analyze and compute a linear pessimistic bilevel problem and several extensions. We also provide demonstrations on illustrative examples and a systematic numerical study on instances of two practical problems. Because of its simple structure and strong computational capacity, we believe that the developed scheme is of critical value in studying and solving pessimistic bilevel optimization problems arising from practice.
We study strategically missing data problems in predictive analytics with regression. In many real-world situations, such as financial reporting, college admission, job application, and marketing advertisement, data providers often conceal certain information on purpose in order to gain a favorable outcome. It is important for the decision-maker to have a mechanism to deal with such strategic behaviors. We propose a novel approach to handle strategically missing data in regression prediction. The proposed method derives imputation values of strategically missing data based on the Support Vector Regression models. It provides incentives for the data providers to disclose their true information. We show that with the proposed method imputation errors for the missing values are minimized under some reasonable conditions. An experimental study on real-world data demonstrates the effectiveness of the proposed approach.
In this paper, we address two models of nondeterministic discrete time finite-horizon dynamic programs (DPs): implicit stochastic DPs (the information about the random events is given by value oracles to their cumulative distribution functions) and sample-based DPs (the information about the random events is deduced by drawing random samples). Such data-driven models frequently appear in practice, where the cumulative distribution functions of the underlying random variables are either unavailable or too complicated to work with. In both models, the single-period cost functions are accessed via value oracle calls and assumed to possess either monotone or convex structure. We develop the first near-optimal relative approximation schemes for each of the two models. Applications in stochastic inventory control (that is, several variants of the so-called newsvendor problem) are discussed in detail. Our results are achieved by a combination of Bellman equation calculations, density estimation results, and extensions of the technique of K-approximation sets and functions introduced by Halman et al. (2009) [Halman N, Klabjan D, Mostagir M, Orlin J, Simchi-Levi D (2009) A fully polynomial time approximation scheme for single-item stochastic inventory control with discrete demand. Math. Oper. Res. 34(3):674–685.].
Military medical planners must consider how aerial medical evacuation (MEDEVAC) assets will be dispatched when preparing for and supporting high-intensity combat operations. The dispatching authority seeks to dispatch MEDEVAC assets to prioritized requests for service, such that battlefield casualties are effectively and efficiently transported to nearby medical-treatment facilities. We formulate and solve a discounted, infinite-horizon Markov decision process (MDP) model of the MEDEVAC dispatching problem. Because the high dimensionality and uncountable state space of our MDP model renders classical dynamic programming solution methods intractable, we instead apply approximate dynamic programming (ADP) solution methods to produce high-quality dispatching policies relative to the currently practiced closest-available dispatching policy. We develop, test, and compare two distinct ADP solution techniques, both of which utilize an approximate policy iteration (API) algorithmic framework. The first algorithm uses least-squares temporal differences (LSTD) learning for policy evaluation, whereas the second algorithm uses neural network (NN) learning. We construct a notional, yet representative planning scenario based on high-intensity combat operations in southern Azerbaijan to demonstrate the applicability of our MDP model and to compare the efficacies of our proposed ADP solution techniques. We generate 30 problem instances via a designed experiment to examine how selected problem features and algorithmic features affect the quality of solutions attained by our ADP policies. Results show that the respective policies determined by the NN-API and LSTD-API algorithms significantly outperform the closest-available benchmark policies in 27 (90%) and 24 (80%) of the problem instances examined. Moreover, the NN-API policies significantly outperform the LSTD-API policies in each of the problem instances examined. Compared with the closest-available policy for the baseline problem instance, the NN-API policy decreases the average response time of important urgent (i.e., life-threatening) requests by 39 minutes. These research models, methodologies, and results inform the implementation and modification of current and future MEDEVAC tactics, techniques, and procedures, as well as the design and purchase of future aerial MEDEVAC assets.
We present SDDP.jl, an open-source library for solving multistage stochastic programming problems using the stochastic dual dynamic programming algorithm. SDDP.jl is built on JuMP, an algebraic modeling language in Julia. JuMP provides SDDP.jl with a solver-agnostic, user-friendly interface. In addition, we leverage unique features of Julia, such as multiple dispatch, to provide an extensible framework for practitioners to build on our work. SDDP.jl is well tested, and accessible documentation is available at https://github.com/odow/SDDP.jl.
Prototyping algorithms in algebraic modeling languages has a long tradition. Despite the convenient prototyping platform that modeling languages offer, they are typically seen as rather inefficient with regard to repeatedly solving mathematical programming problems, a concept on which many algorithms are based. The most prominent examples of such algorithms are decomposition methods, such as the Benders decomposition, column generation, and the Dantzig–Wolfe decomposition. In this work, we discuss the underlying reasons for repeated solve deficiency with regard to speed in detail and provide an insider’s look into the algebraic modeling language GAMS. Further, we present recently added features in GAMS that mitigate some of the efficiency drawbacks inherent to the way modeling languages represent model data and ultimately solve a model. In particular, we demonstrate the grid-enabled gather-update-solve-scatter facility and the GAMS object-oriented application programming interface on a large-scale case study that involves a Benders decomposition–type algorithm for a power-expansion planning problem.
Stochastic decomposition (SD) has been a computationally effective approach to solve large-scale stochastic programming (SP) problems arising in practical applications. By using incremental sampling, this approach is designed to discover an appropriate sample size for a given SP instance, thus precluding the need for either scenario reduction or arbitrary sample sizes to create sample average approximations (SAA). When compared with the solutions obtained using the SAA procedure, SD provides solutions of similar quality in far less computational time using ordinarily available computational resources. However, previous versions of SD were not applicable to problems with randomness in second-stage cost coefficients. In this paper, we extend its capabilities by relaxing this assumption on cost coefficients in the second stage. In addition to the algorithmic enhancements necessary to achieve this, we also present the details of implementing these extensions, which preserve the computational edge of SD. Finally, we illustrate the computational results obtained from the latest implementation of SD on a variety of test instances generated for problems from the literature. We compare these results with those obtained from the regularized L-shaped method applied to the SAA function of these problems with different sample sizes.
In this paper, we propose a generic algorithm to compute exactly the set of nondominated points for multiobjective discrete optimization problems. Our algorithm extends the ε-constraint method, originally designed for the biobjective case only, to solve problems with two or more objectives. For this purpose, our algorithm splits the search space into zones that can be investigated separately by solving an integer program. We also propose refinements, which provide extra information on several zones, allowing us to detect, and discard, empty parts of the search space without checking them by solving the associated integer programs. This results in a limited number of calls to the integer solver. Moreover, we can provide a feasible starting solution before solving every program, which significantly reduces the time spent for each resolution. The resulting algorithm is fast and simple to implement. It is compared with previous state-of-the-art algorithms and is seen to outperform them significantly on the experimented problem instances.
Most existing facility location models assume that the facility cost is either a fixed setup cost or made up of a fixed setup and a problem-specific concave or submodular cost term. This structural property plays a critical role in developing fast branch-and-price, Lagrangian relaxation, constant ratio approximation, and conic integer programming reformulation approaches for these NP-hard problems. Many practical considerations and complicating factors, however, can make the facility cost no longer concave or submodular. By removing this restrictive assumption, we study a new location model that considers general nonlinear costs to operate facilities in the facility location framework. The general model does not even admit any approximation algorithms unless P = NP because it takes the unsplittable hard-capacitated metric facility location problem as a special case. We first reformulate this general model as a set-partitioning model and then propose a branch-and-price approach. Although the corresponding pricing problem is NP-hard, we effectively analyze its structural properties and design an algorithm to solve it efficiently. The numerical results obtained from two implementation examples of the general model demonstrate the effectiveness of the solution approach, reveal the managerial implications, and validate the importance to study the general framework.
We study the resource loading problem, which arises in tactical capacity planning. In this problem, one has to plan the intensity of execution of a set of orders to minimize a cost function that penalizes the resource use above given capacity limits and the completion of the orders after their due dates. Our main contributions include a novel mixed-integer linear-programming (MIP)‐based formulation, the investigation of the polyhedra associated with the feasible intensity assignments of individual orders, and a comparison of our branch-and-cut algorithm based on the novel formulation and the related polyhedral results with other MIP formulations. The computational results demonstrate the superiority of our approach. In our formulation and in one of the proofs, we use fundamental results of Egon Balas on disjunctive programming.
We propose a data aggregation-based algorithm with monotonic convergence to a global optimum for a generalized version of the L1-norm error fitting model with an assumption of the fitting function. The proposed algorithm generalizes the recent algorithm in the literature, aggregate and iterative disaggregate (AID), which selectively solves three specific L1-norm error fitting problems. With the proposed algorithm, any L1-norm error fitting model can be solved optimally if it follows the form of the L1-norm error fitting problem and if the fitting function satisfies the assumption. The proposed algorithm can also solve multidimensional fitting problems with arbitrary constraints on the fitting coefficients matrix. The generalized problem includes popular models, such as regression and the orthogonal Procrustes problem. The results of the computational experiment show that the proposed algorithms are faster than the state-of-the-art benchmarks for L1-norm regression subset selection and L1-norm regression over a sphere. Furthermore, the relative performance of the proposed algorithm improves as data size increases.
This paper addresses a class of two-stage robust optimization models with an exponential number of scenarios given implicitly. We apply Dantzig–Wolfe decomposition to exploit the structure of these models and show that the original problem reduces to a single-stage robust problem. We propose a Benders algorithm for the reformulated single-stage problem. We also develop a heuristic algorithm that dualizes the linear programming relaxation of the inner maximization problem in the reformulated model and iteratively generates cuts to shape the convex hull of the uncertainty set. We combine this heuristic with the Benders algorithm to create a more effective hybrid Benders algorithm. Because the master problem and subproblem in the Benders algorithm are mixed-integer programs, it is computationally demanding to solve them optimally at each iteration of the algorithm. Therefore, we develop novel stopping conditions for these mixed-integer programs and provide the relevant convergence proofs. Extensive computational experiments on a nurse planning problem and a two-echelon supply chain problem are performed to evaluate the efficiency of the proposed algorithms.
We study perspective reformulations (PRs) of semicontinuous quadratically constrained quadratic programs (SQCQPs) in this paper. Based on perspective functions, we first propose a class of PRs for SQCQPs and discuss how to find the best PR in this class via strong duality and lifting techniques. We then study the properties of the PR class and relate them to alternative formulations that are used to derive lower bounds for SQCQPs. Finally, we embed the PR bounds in branch-and-bound algorithms and conduct computational experiments to illustrate the effectiveness of the proposed approach.
In this paper, we consider the so-called worst-case linear optimization (WCLO) with uncertainties on the right-hand side of the constraints. Such a problem often arises in applications such as in systemic risk estimation in finance and stochastic optimization. We first show that the WCLO problem with the uncertainty set corresponding to the l𝓁p-norm ((WCLOp)) is NP-hard for p ɛ (1,∞). Second, we combine several simple optimization techniques, such as the successive convex optimization method, quadratic convex relaxation, initialization, and branch-and-bound (B&B), to develop an algorithm for (WCLO2) that can find a globally optimal solution to (WCLO2) within a prespecified ε-tolerance. We establish the global convergence of the algorithm and estimate its complexity. We also develop a finite B&B algorithm for (WCLO∞) to identify a global optimal solution to the underlying problem, and establish the finite convergence of the algorithm. Numerical experiments are reported to illustrate the effectiveness of our proposed algorithms in finding globally optimal solutions to medium and large-scale WCLO instances.
Bilevel problems are highly challenging optimization problems that appear in many applications of energy market design, critical infrastructure defense, transportation, pricing, and so on. Often these bilevel models are equipped with integer decisions, which makes the problems even harder to solve. Typically, in such a setting in mathematical optimization, one develops primal heuristics in order to obtain feasible points of good quality quickly or to enhance the search process of exact global methods. However, there are comparably few heuristics for bilevel problems. In this paper, we develop such a primal heuristic for bilevel problems with a mixed-integer linear or quadratic upper level and a linear or quadratic lower level. The heuristic is based on a penalty alternating direction method, which allows for a theoretical analysis. We derive a convergence theory stating that the method converges to a stationary point of an equivalent single-level reformulation of the bilevel problem and extensively test the method on a test set of more than 2,800 instances—which is one of the largest computational test sets ever used in bilevel programming. The study illustrates the very good performance of the proposed method in terms of both running times and solution quality. This renders the method a suitable subroutine in global bilevel solvers as well as a reasonable standalone approach.Summary of Contribution: Bilevel optimization problems form a very important class of optimization problems in the field of operations research, which is mainly due to their capability of modeling hierarchical decision processes. However, real-world bilevel problems are usually very hard to solve—especially in the case in which additional mixed-integer aspects are included in the modeling. Hence, the development of fast and reliable primal heuristics for this class of problems is very important. This paper presents such a method.
The extant literature on first passage problems of reflected hyperexponential jump diffusion processes (RHEPs) lacks efficiently computable formulae for the Laplace transform of the joint distribution of the RHEP and its first passage time, cumulative distribution function of the overshoot, expected cumulative value of the discounted increments of the local time up to the first passage time, expected cumulative discounted value of the RHEP up to the first passage time, and expectation of the first passage time. We combine numerical solutions to ordinary integro-differential equations and martingale methods in a novel manner to derive such expressions. For some of these quantities, our approach can deal with the subtle case in which both the RHEP’s overall drift and the discount rate equal zero. As a by-product, we obtain a formula for the Laplace transform of the RHEP transition density. We illustrate the numerical performance of our methodology through a few examples. We observe that, when the RHEP’s overall drift and the discount rate are very close to zero, rounding errors can make the evaluation of some of our formulae unreliable. In these situations our exact expression for the case in which the RHEP’s overall drift and discount rate are both zero can be an effective approximation for the quantities in question that is substantially more efficient than reliably calculating them using their exact expressions and “multiprecision computing.” Our research has applications in financial engineering, insurance, economics, and queueing.
We propose a dynamic sampling allocation and selection paradigm for finding the alternative with the optimal quantile in a Bayesian framework. Myopic allocation policies (MAPs), analogous to existing methods in classic ranking and selection for selecting the alternative with the optimal mean, and computationally efficient selection policies are derived for selecting the alternative with the optimal quantile. Under certain conditions, we prove that the proposed MAPs and selection procedures are consistent, which means that the best quantile would be eventually correctly selected as the sample size goes to infinity. Numerical experiments demonstrate that the proposed schemes can significantly improve the performance.
Voting mechanisms are widely adopted for evaluating the quality and credibility of user-generated content, such as online product reviews. For the reviews that do not receive sufficient votes, techniques and models are developed to automatically assess their helpfulness levels. Existing methods serving this purpose are mostly centered on feature analysis, ignoring the information conveyed in the frequencies and patterns of user votes. Consequently, the accuracy of helpfulness measurement is limited. Inspired by related findings from prediction theories and consumer behavior research, we propose a novel approach characterized by the technique of iterative Bayesian distribution estimation, aiming to more accurately measure the helpfulness levels of reviews used for training prediction models. Using synthetic data and a real-world data set involving 1.67 million reviews and 5.18 million votes from Amazon, a simulation experiment and a two-stage data experiment show that the proposed approach outperforms existing methods on accuracy measures. Moreover, an out-of-sample user study is conducted on Amazon Mechanical Turk. The results further illustrate the predictive power of the new approach. Practically, the research contributes to e-commerce by providing an enhanced method for exploiting the value of user-generated content. Academically, we contribute to the design science literature with a novel approach that may be adapted to a wide range of research topics, such as recommender systems and social media analytics.
We consider the problem of best k𝑘-subset convex regression using n observations in d variables. For the case without sparsity, we develop a scalable algorithm for obtaining high quality solutions in practical times that compare favorably with other state of the art methods. We show that by using a cutting plane method, the least squares convex regression problem can be solved for sizes (n,d)=(104,10) in minutes and (n,d)=(105,102) in hours. Our algorithm can be adapted to solve variants such as finding the best convex or concave functions with coordinate-wise monotonicity, norm-bounded subgradients, and minimize the ℓ1 loss—all with similar scalability to the least squares convex regression problem. Under sparsity, we propose algorithms which iteratively solve for the best subset of features based on first order and cutting plane methods. We show that our methods scale for sizes (n,d,k=104,102,10) in minutes and (n,d,k=105,102,10) in hours. We demonstrate that these methods control for the false discovery rate effectively.
Concerning the information overload of online reviews, this paper models a new review selection problem called the Informative Review Subset Selection problem (namely, IRSS) and demonstrates that it is NP-hard to solve and approximate. Furthermore, a novel heuristic method (namely, Combined Search-ComS) is proposed for seeking the solution to the problem and selecting a subset of reviews, which is consistent with the original review corpus in light of mutual information entropy. The proposed method is then comprehensively examined via extensive data experiments and a user study on Amazon data. Experimental results reveal the overall superiority of the proposed method in comparison with other extant methods of concern, showing that it is an effective way to select an informative subset of online reviews. The proposed method is deemed desirable and useful for online consumers and service providers.
Recommender systems have become one of the main components of web technologies that help people to cope with information overload. Based on the analysis of past user behavior, these systems filter items according to users’ likes and interests. Two of the most important metrics used to analyze the performance of these systems are the accuracy and diversity of the recommendation lists. Whereas all the efforts exerted in the prediction of the user interests aim at maximizing the former, the latter emerges in various forms, such as diversity in the lists across all user recommendation lists, referred to as aggregate diversity, and diversity in the lists of individuals, known as individual diversity. In this paper, we tackle the combination of these three objectives and justify this approach by showing through experiments that handling these objectives in pairs does not yield satisfactory results in the third one. To that end, we develop a mathematical model that is formulated using multiobjective optimization approaches. To cope with the intractability of this nonlinear integer programming model, its special structure is exploited by a decomposition technique. For the solution of the resulting formulation, we propose an iterative framework that is composed of a clique-generating genetic algorithm, a constructive heuristic, and an improvement heuristic. The former is designed to incorporate all objective functions into the generated cliques and specifically impose a certain level of individual diversity, whereas the latter chooses one clique for each user such that the desired aggregate diversity level is fulfilled. We conduct experiments on three data sets and show that the proposed modeling approach successfully handles all objectives according to the needs of the system and that the proposed methodology is capable of yielding good upper bounds.
Apiculture has gained worldwide interest because of its contributions to economic incomes and environmental conservation. In view of these, migratory beekeeping, as a high-yielding technique, is extensively adopted. However, because of the lack of an overall routing plan, beekeepers who follow the experiential migratory routes frequently encounter unexpected detours and suffer losses when faced with problems such as those related to nectar source capacities and the production of bee products. The migratory beekeeping routing problem (MBRP) is proposed based on the practical background of the commercial apiculture industry to optimize the global revenue for beekeepers by comprehensively considering nectar source allocation, migration, production and sales of bee products, and corresponding time decisions. The MBRP is a new variant of the vehicle routing problem but with significantly different production time decisions at the vertices (i.e., nectar sources). That is, only the overlaps between residence durations and flowering periods generate production benefits. Different sales visits cause different gains from the same products; in turn, these lead to different production time decisions at previously visited nectar source locations and even change the visits for production. To overcome the difficulty resulting from the complicated time decisions, we utilize the Dantzig–Wolfe decomposition method and propose a revised labeling algorithm for the pricing subproblems. The tests, performed on instances and a real-world case, demonstrate that the column generation method with the revised labeling algorithm is efficient for solving the MBRP. Compared with traditional routes, a more efficient overall routing schedule for migratory beekeepers is proposed.Summary of Contribution. Based on the practical background of commercial apiculture industry, this paper proposes a new type of routing problem named the migratory beekeeping routing problem (MBRP), which incorporates the selection of productive nodes and sales nodes as well as the production time decision at the productive nodes on a migratory beekeeping network. To overcome the difficulty resulting from the complicated time decisions, we utilize the Dantzig–Wolfe decomposition method and propose a revised labeling algorithm for the pricing subproblems. The tests, performed on instances and a real-world case, demonstrate that the column generation method with the revised labeling algorithm is efficient for solving the MBRP. Compared with traditional routes, a more efficient overall routing schedule for migratory beekeepers is proposed. Therefore, this paper is congruent with, and contributes to, the scope and mission of INFORMS Journal on Computing, especially the area of Network Optimization: Algorithms & Applications.
We consider a robust optimization problem in an electric power system under uncertain demand and availability of renewable energy resources. Solving the deterministic alternating current (AC) optimal power flow (ACOPF) problem has been considered challenging since the 1960s due to its nonconvexity. Linear approximation of the AC power flow system sees pervasive use, but does not guarantee a physically feasible system configuration. In recent years, various convex relaxation schemes for the ACOPF problem have been investigated, and under some assumptions, a physically feasible solution can be recovered. Based on these convex relaxations, we construct a robust convex optimization problem with recourse to solve for optimal controllable injections (fossil fuel, nuclear, etc.) in electric power systems under uncertainty (renewable energy generation, demand fluctuation, etc.). We propose a cutting-plane method to solve this robust optimization problem, and we establish convergence and other desirable properties. Experimental results indicate that our robust convex relaxation of the ACOPF problem can provide a tight lower bound.
With the increasing penetration of intermittent renewable energy and fluctuating electricity loads, power system operators are facing significant challenges in maintaining system load balance and reliability. In addition to traditional energy markets that are designed to balance power generation and load, ancillary service markets have been recently introduced to help manage the considerable uncertainty by reserving certain generation capacities against unexpected events. In this paper, we develop a multistage stochastic optimization model for system operators to efficiently schedule power-generation assets to co-optimize power generation and regulation reserve service (a critical ancillary service product) under uncertainty. In addition, to improve the computational efficiency of the proposed multistage stochastic integer program, we explore its polyhedral structure by investigating physical characteristics of individual generators, the system-wide requirements that couple all of the generators, and the scenario tree structure for our proposed multistage model. We start with the single-generator polytope and provide convex hull descriptions for the two-period case under different parameter settings. We then provide several families of multiperiod strong valid inequalities linking different scenarios and covering decision variables that represent both power generation and regulation reserve amounts. We further extend our study by exploring the multigenerator polytope and derive strong valid inequalities linking different generators and covering multiple periods. To enhance computational performance, polynomial-time separation algorithms are developed for the exponential number of inequalities. Finally, we verify the effectiveness of our proposed strong valid inequalities by applying them as user cuts under the branch-and-cut scheme to solve multistage stochastic network-constrained power generation scheduling problems.
Computing and minimizing the worst-case bound on the expected shortfall risk of a portfolio given partial information on the distribution of the asset returns is an important problem in risk management. One such bound that been proposed is for the worst-case distribution that is “close” to a reference distribution where closeness in distance among distributions is measured using 𝜑φ-divergence. In this paper, we advocate the use of such ambiguity sets with a tree structure on the univariate and bivariate marginal distributions. Such an approach has attractive modeling and computational properties. From a modeling perspective, this provides flexibility for risk management applications where there are many more choices for bivariate copulas in comparison with multivariate copulas. Bivariate copulas form the basis of the nested tree structure that is found in vine copulas. Because estimating a vine copula is fairly challenging, our approach provides robust bounds that are valid for the tree structure that is obtained by truncating the vine copula at the top level. The model also provides flexibility in tackling instances when the lower dimensional marginal information is inconsistent that might arise when multiple experts provide information. From a computational perspective, under the assumption of a tree structure on the bivariate marginals, we show that the worst-case expected shortfall is computable in polynomial time in the input size when the distributions are discrete. The corresponding distributionally robust portfolio optimization problem is also solvable in polynomial time. In contrast, under the assumption of independence, the expected shortfall is shown to be #P-hard to compute for discrete distributions. We provide numerical examples with simulated and real data to illustrate the quality of the worst-case bounds in risk management and portfolio optimization and compare it with alternate probabilistic models such as vine copulas and Markov tree distributions.
Multistage problems with uncertain parameters and integer decisions variables are among the most difficult applications of robust optimization (RO). The challenge in these problems is to find optimal here-and-now decisions, taking into account that the wait-and-see decisions have to adapt to the revealed values of the uncertain parameters. An existing approach to solve these problems is to construct piecewise constant decision rules by adaptively partitioning the uncertainty set. The partitions of this set are iteratively updated by separating so-called criticial scenarios, and methods for identifying these critical scenarios are available. However, these methods are most suitable for problems with continuous decision variables and many uncertain constraints, providing no mathematically rigorous methodology for partitioning in case of integer decisions. In particular, they are not able to identify sets of critical scenarios for integer problems with uncertainty in the objective function only. In this paper, we address this shortcoming by introducing a general critical scenario detection method. The new method leverages the information embedded in the dual vectors of the LP relaxations at the nodes of the branch-and-bound tree used to solve the corresponding static problem. Numerical experiments on a route planning problem show that our general-purpose method outperforms a problem-specific approach from the literature.
In recent years the use of decision diagrams within the context of discrete optimization has proliferated. This paper continues this expansion by proposing the use of decision diagrams for modeling and solving binary optimization problems with quadratic constraints. The model proposes the use of multiple decision diagrams to decompose a quadratic matrix so that each individual diagram has provably limited size. The decision diagrams are then linked through channeling constraints to ensure that the solution represented is consistent across the decision diagrams and that the original quadratic constraints are satisfied. The resulting family of decision diagrams are optimized over by a dedicated cutting-plane algorithm akin to Benders decomposition. The approach is general, in that commercial integer programming solvers can readily apply the technique. A thorough experimental evaluation on both benchmark and synthetic instances exhibits that the proposed decision diagram reformulation provides significant improvements over current methods for quadratic constraints in state-of-the-art solvers.
The generalization of mixed integer program (MIP) techniques to deal with nonlinear, potentially nonconvex, constraints has been a fruitful direction of research for computational mixed integer nonlinear programs (MINLPs) in the last decade. In this paper, we follow that path in order to extend another essential subroutine of modern MIP solvers toward the case of nonlinear optimization: the analysis of infeasible subproblems for learning additional valid constraints. To this end, we derive two different strategies, geared toward two different solution approaches. These are using local dual proofs of infeasibility for LP-based branch-and-bound and the creation of nonlinear dual proofs for NLP-based branch-and-bound, respectively. We discuss implementation details of both approaches and present an extensive computational study, showing that both techniques can significantly enhance performance when solving MINLPs to global optimality.Summary of Contribution: This original article concerns the advancement of exact general-purpose algorithms for solving one of the largest and most prominent problem classes in optimization, mixed integer nonlinear programs (MINLPs). It demonstrates how methods for conflict analysis that learn from infeasible subproblems can be transferred to nonlinear optimization. Further, it develops theory for how nonlinear dual infeasibility proofs can be derived from a nonlinear relaxation. This paper features a thoroughly computational study regarding the impact of conflict analysis techniques on the overall performance of a state-of-the-art MINLP solver when solving MINLPs to global optimality.
The standard single-picker routing problem (SPRP) seeks the cost-minimal tour to collect a set of given articles in a rectangular single-block warehouse with parallel picking aisles and a dedicated storage policy, that is, each stock-keeping unit is only available from one storage location in the warehouse. We present a compact formulation that forgoes classical subtour elimination constraints by directly exploiting two of the properties of an optimal picking tour used in the dynamic programming algorithm published in the seminal paper of Ratliff and Rosenthal. We extend the formulation to three important settings prevalent in modern e-commerce warehouses: scattered storage, decoupling of picker and cart, and multiple end depots. In numerical studies, our formulation outperforms existing standard SPRP formulations from the literature and proves able to solve large instances within short runtimes. Realistically sized instances of the three problem extensions can also be solved with low computational effort. For scattered storage, we note a rough tendency that runtimes increase with longer pick lists or a higher degree of duplication. In addition, we find that decoupling of picker and cart can lead to substantial cost savings depending on the speed and capacity of the picker when traveling alone, whereas additional end depots have rather limited benefits in a single-block warehouse.Summary of Contribution: Efficiently routing order pickers is of great practical interest because picking costs make up a substantial part of operational warehouse costs. For the prevalent case of a rectangular warehouse with parallel picking aisles, we present a highly effective modeling approach that covers—in addition to the standard setting—several important storage and order-picking strategies employed in modern e-commerce warehouses: scattered storage, decoupling of picker and cart, and multiple end depots. In this way, we provide practitioners as well as scientists with an easy and quick way of implementing a high-quality solution approach for routing pickers in the described settings. In addition, we shed some light on the cost benefits of the different storage and picking strategies in numerical experiments.
A solar power plant is a large-scale photovoltaic (PV) system designed to supply usable solar power to the electricity grid. Building a solar power plant needs consideration of arrangements of several important components, such as PV arrays, solar inverters, combiner boxes, cables, and other electrical accessories. The design of solar power plants is very complex because of various optimization parameters and design regulations. In this study, we address the cable-routing problem arising in the planning of large-scale solar power plants, which aims to determine the partition of the PV arrays, the location of combiner boxes, and cable routing such that the installation cost of the cables connecting the components is minimized. We formulate the problem as a mathematical programming problem, which can be viewed as a generalized capacitated minimum spanning tree (CMST) problem, and then devise a branch-and-price-and-cut (BPC) algorithm to solve it. The BPC algorithm uses two important valid inequalities, namely the capacity inequalities and the subset-row inequalities, to tighten the lower bounds. We also adopt several acceleration strategies to speed up the algorithm. Using real-world data sets, we show by numerical experiments that our BPC algorithm is superior to the typical manual-based planning approach used by many electric power planning companies. In addition, when solving the CMST problem with unitary demands, our algorithm is highly competitive compared with the best exact algorithm in the literature.
Operational planning at transshipment nodes is a wide and challenging field of research that covers a vast number of distinct relevant applications, spanning from seaport container terminals to rail terminals to cross-docks. In this work, we study the feasibility version of a fundamental synchronization problem that assigns incoming vehicles to docking resources subject to handover relations. We carry out a comprehensive analysis of computational complexity of various problem variants and establish structural connections to famous decision problems in graph theory. We further propose an exact solution algorithm for finding feasible dock assignments, if vehicles can visit the node only once and evaluate its performance in a comprehensive computational study.
The time window assignment vehicle routing problem (TWAVRP) is the problem of assigning time windows for delivery before demand volume becomes known. This implies that vehicle routes in different demand scenarios have to be synchronized such that the same client is visited around the same time in each scenario. For TWAVRP instances that are relatively difficult to solve, we observe many similar solutions in which one or more routes have a different orientation, that is, the clients are visited in the reverse order. We introduce an edge-based branching method combined with additional components to eliminate orientation symmetry from the search tree, and we present enhancements to make this method efficient in practice. Next, we present a branch-price-and-cut algorithm based on this branching method. Our computational experiments show that addressing orientation symmetry significantly improves our algorithm: The number of nodes in the search tree is reduced by 92.6% on average, and 25 additional benchmark instances are solved to optimality. Furthermore, the resulting algorithm is competitive with the state of the art. The main ideas of this paper are not TWAVRP specific and can be applied to other vehicle routing problems with consistency considerations or synchronization requirements.
Finding sparse solutions to a system of equations and/or inequalities is an important topic in many application areas such as signal processing, statistical regression and nonparametric modeling. Various continuous relaxation models have been proposed and widely studied to deal with the discrete nature of the underlying problem. In this paper, we propose a quadratically constrained ℓq (0 < q < 1) minimization model for finding sparse solutions to a quadratic system. We prove that solving the proposed model is strongly NP-hard. To tackle the computation difficulty, a first order necessary condition for local minimizers is derived. Various properties of the proposed model are studied for designing an active-set-based descent algorithm to find candidate solutions satisfying the proposed condition. In addition to providing a theoretical convergence proof, we conduct extensive computational experiments using synthetic and real-life data to validate the effectiveness of the proposed algorithm and to show the superior capability in finding sparse solutions of the proposed model compared with other known models in the literature. We also extend our results to a quadratically constrained ℓq (0 < q < 1) minimization model with multiple convex quadratic constraints for further potential applications.Summary of Contribution: In this paper, we propose and study a quadratically constrained ℓq minimization (0 < q < 1) model for finding sparse solutions to a quadratic system which has wide applications in sparse signal recovery, image processing and machine learning. The proposed quadratically constrained ℓq minimization model extends the linearly constrained ℓq and unconstrained ℓ2-ℓq models. We study various properties of the proposed model in aim of designing an efficient algorithm. Especially, we propose an unrelaxed KKT condition for local/global minimizers. Followed by the properties studied, an active-set based descent algorithm is then proposed with its convergence proof being given. Extensive numerical experiments with synthetic and real-life Sparco datasets are conducted to show that the proposed algorithm works very effectively and efficiently. Its sparse recovery capability is superior to that of other known models in the literature.
The sparse inverse covariance matrix is used to model conditional dependencies between variables in a graphical model to fit a multivariate Gaussian distribution. Estimating the matrix from data are well known to be computationally expensive for large-scale problems. Sparsity is employed to handle noise in the data and to promote interpretability of a learning model. Although the use of a convex ℓ1 regularizer to encourage sparsity is common practice, the combinatorial ℓ0 penalty often has more favorable statistical properties. In this paper, we directly constrain sparsity by specifying a maximally allowable number of nonzeros, in other words, by imposing an ℓ0 constraint. We introduce an efficient approximate Newton algorithm using warm starts for solving the nonconvex ℓ0-constrained inverse covariance learning problem. Numerical experiments on standard data sets show that the performance of the proposed algorithm is competitive with state-of-the-art methods.Summary of Contribution: The inverse covariance estimation problem underpins many domains, including statistics, operations research, and machine learning. We propose a scalable optimization algorithm for solving the nonconvex ℓ0-constrained problem.
We consider the best subset selection problem in linear regression—that is, finding a parsimonious subset of the regression variables that provides the best fit to the data according to some predefined criterion. We are primarily concerned with alternatives to cross-validation methods that do not require data partitioning and involve a range of information criteria extensively studied in the statistical literature. We show that the problem of interest can be modeled using fractional mixed-integer optimization, which can be tackled by leveraging recent advances in modern optimization solvers. The proposed algorithms involve solving a sequence of mixed-integer quadratic optimization problems (or their convexifications) and can be implemented with off-the-shelf solvers. We report encouraging results in our computational experiments, with respect to both the optimization and statistical performance.Summary of Contribution: This paper considers feature selection problems with information criteria. We show that by adopting a fractional optimization perspective (a well-known field in nonlinear optimization and operations research), it is possible to leverage recent advances in mixed-integer quadratic optimization technology to tackle traditional statistical problems long considered intractable. We present extensive computational experiments, with both synthetic and real data, illustrating that the new fractional optimization approach is orders of magnitude faster than existing approaches in the literature.
We propose a novel supervised dimension-reduction method called supervised t-distributed stochastic neighbor embedding (St-SNE) that achieves dimension reduction by preserving the similarities of data points in both feature and outcome spaces. The proposed method can be used for both prediction and visualization tasks with the ability to handle high-dimensional data. We show through a variety of data sets that when compared with a comprehensive list of existing methods, St-SNE has superior prediction performance in the ultrahigh-dimensional setting in which the number of features p exceeds the sample size n and has competitive performance in the p ≤ n setting. We also show that St-SNE is a competitive visualization tool that is capable of capturing within-cluster variations. In addition, we propose a penalized Kullback–Leibler divergence criterion to automatically select the reduced-dimension size k for St-SNE.Summary of Contribution: With the fast development of data collection and data processing technologies, high-dimensional data have now become ubiquitous. Examples of such data include those collected from environmental sensors, personal mobile devices, and wearable electronics. High-dimensionality poses great challenges for data analytics routines, both methodologically and computationally. Many machine learning algorithms may fail to work for ultrahigh-dimensional data, where the number of the features p is (much) larger than the sample size n. We propose a novel method for dimension reduction that can (i) aid the understanding of high-dimensional data through visualization and (ii) create a small set of good predictors, which is especially useful for prediction using ultrahigh-dimensional data.
The learning of predictive models for data-driven decision support has been a prevalent topic in many fields. However, construction of models that would capture interactions among input variables is a challenging task. In this paper, we present a new preference learning approach for multiple criteria sorting with potentially interacting criteria. It employs an additive piecewise-linear value function as the basic preference model, which is augmented with components for handling the interactions. To construct such a model from a given set of assignment examples concerning reference alternatives, we develop a convex quadratic programming model. Because its complexity does not depend on the number of training samples, the proposed approach is capable for dealing with data-intensive tasks. To improve the generalization of the constructed model on new instances and to overcome the problem of overfitting, we employ the regularization techniques. We also propose a few novel methods for classifying nonreference alternatives in order to enhance the applicability of our approach to different data sets. The practical usefulness of the proposed approach is demonstrated on a problem of parametric evaluation of research units, whereas its predictive performance is studied on several monotone classification problems. The experimental results indicate that our approach compares favourably with the classical UTilités Additives DIScriminantes (UTADIS) method and the Choquet integral-based sorting model.Summary of Contribution. The paper tackles vital challenges at the intersections of multiple criteria decision analysis and machine learning, showing how computationally advanced techniques can be used for faithfully representing human preferences and dealing with complex decision problems. Specifically, we propose a novel preference learning method for multiple criteria sorting problems. The introduced approach incorporates convex quadratic programming to construct a value-based preference model based on large sets of preference statements. In this way, we extend the applicability of decision analysis methods to preferences derived from historical data or observation of users' behavior in addition to the preference judgments explicitly revealed by the decision-makers. The method's practical usefulness is illustrated on a variety of real-world datasets from fields such as higher education, medicine, human resources, and housing market. Its potential for supporting better decision-making is enhanced by both an interpretable form of the assumed model handling interactions between criteria as well as a high predictive performance demonstrated in the extensive computational experiments.
We explore the power of semidefinite programming (SDP) for finding additive ɛ-approximate Nash equilibria in bimatrix games. We introduce an SDP relaxation for a quadratic programming formulation of the Nash equilibrium problem and provide a number of valid inequalities to improve the quality of the relaxation. If a rank-1 solution to this SDP is found, then an exact Nash equilibrium can be recovered. We show that, for a strictly competitive game, our SDP is guaranteed to return a rank-1 solution. We propose two algorithms based on the iterative linearization of smooth nonconvex objective functions whose global minima by design coincide with rank-1 solutions. Empirically, we demonstrate that these algorithms often recover solutions of rank at most 2 and ɛ close to zero. Furthermore, we prove that if a rank-2 solution to our SDP is found, then a 511-Nash equilibrium can be recovered for any game, or a 13-Nash equilibrium for a symmetric game. We then show how our SDP approach can address two (NP-hard) problems of economic interest: finding the maximum welfare achievable under any Nash equilibrium, and testing whether there exists a Nash equilibrium where a particular set of strategies is not played. Finally, we show the connection between our SDP and the first level of the Lasserre/sum of squares hierarchy.
Scatter search (SS) is a well-established metaheuristic solution methodology that has seen most of its success in single-objective optimization. The literature includes a few examples of the SS methodology adapted to multiobjective optimization, almost all dealing with continuous, nonlinear problems. We describe an SS design that we believe has general applicability in the area of multiobjective combinatorial optimization and show its effectiveness by applying it to a facility location problem. Facility location consists of identifying the best locations for a set of facilities. The set of best locations may vary substantially according to the objective function employed to solve the optimization problem. We employ a facility location problem with multiple objectives (mo-FLP) to test our design ideas for a multiobjective optimization scatter search. We focus on the objective functions associated with three well-known location problems in the literature: the p-Median Problem (pMP), the Maximal Coverage Location Problem (MCLP), and the p-Center Problem (pCP). Our computational experiments are configured to show that the proposed SS design is capable of producing high-quality Pareto-front approximations.Summary of Contribution: Metaheuristic optimization is at the heart of the intersection between computer science and operations research. The INFORMS Journal on Computing has been fundamental in advancing the ideas behind metaheuristic methodologies. Fred Glover's Tabu Search–Part I was published more than 30 years ago in the first volume of the then ORSA Journal on Computing. This article, one of the most cited in the area of heuristic optimization, paved the way for many contributions to the methodology and practice of operations research. As a continuation of this stream of research, we describe a new scatter search design for multiobjective optimization. The design includes a short-term memory tabu search and a path relinking combination method. We show how the strategies and mechanisms within scatter search and tabu search can be combined to produce a highly effective approach to multiobjective optimization.
This paper proposes a solution approach for the multicommodity capacitated fixed-charge network design problem with uncertain demand modeled as a two-stage stochastic program. The proposed learning-based matheuristic combines heuristic search techniques with mathematical programming. It provides a systematic approach to identifying structures of good-quality solutions by gradually considering scenarios and their influences on design decisions. Extensive computational experiments illustrate the efficiency of the proposed matheuristic in obtaining high-quality solutions with limited computational efforts.
A value-at-risk, or quantile, is widely used as an appropriate investment selection measure for risk-conscious decision makers. We present two quantile-based sequential procedures—with and without consideration of equivalency between alternatives—for selecting the best alternative from a set of simulated alternatives. These procedures asymptotically guarantee a user-defined target probability of correct selection within a prespecified indifference zone. Experimental results demonstrate the trade-off between the indifference-zone size and the number of simulation iterations needed to render a correct selection while satisfying a desired probability of correct selection.
Input uncertainty is an aspect of simulation model risk that arises when the driving input distributions are derived or “fit” to real-world, historical data. Although there has been significant progress on quantifying and hedging against input uncertainty, there has been no direct attempt to reduce it via better input modeling. The meaning of “better” depends on the context and the objective: Our context is when (a) there are one or more families of parametric distributions that are plausible choices; (b) the real-world historical data are not expected to perfectly conform to any of them; and (c) our primary goal is to obtain higher-fidelity simulation output rather than to discover the “true” distribution. In this paper, we show that frequentist model averaging can be an effective way to create input models that better represent the true, unknown input distribution, thereby reducing model risk. Input model averaging builds from standard input modeling practice, is not computationally burdensome, requires no change in how the simulation is executed nor any follow-up experiments, and is available on the Comprehensive R Archive Network (CRAN). We provide theoretical and empirical support for our approach.
We consider a class of min-max robust problems in which the functions that need to be “robustified” can be decomposed as the sum of arbitrary functions. This class of problems includes many practical problems, such as the lot-sizing problem under demand uncertainty. By considering a Lagrangian relaxation of the uncertainty set, we derive a tractable approximation, called the dual Lagrangian approach, that we relate with both the classical dualization approximation approach and an exact approach. Moreover, we show that the dual Lagrangian approach coincides with the affine decision rule approximation approach. The dual Lagrangian approach is applied to a lot-sizing problem, in which demands are assumed to be uncertain and to belong to the uncertainty set with a budget constraint for each time period. Using the insights provided by the interpretation of the Lagrangian multipliers as penalties in the proposed approach, two heuristic strategies, a new guided iterated local search heuristic, and a subgradient optimization method are designed to solve more complex lot-sizing problems in which additional practical aspects, such as setup costs, are considered. Computational results show the efficiency of the proposed heuristics that provide a good compromise between the quality of the robust solutions and the running time required in their computation.Summary of Contribution: The paper includes both theoretical and algorithmic contributions for a class of min-max robust optimization problems where the objective function includes the maximum of a sum of affine functions. From the theoretical point of view, a tractable Lagrangian dual model resulting from a relaxation of the well-known adversarial problem is proposed, providing a new perspective of well-known models, such as the affinely adjustable robust counterpart (AARC) and the dualization technique introduced by Bertsimas and Sim. These results are particularized to lot-sizing problems. From the algorithm point of view, efficient heuristic schemes—which exploit the information based on the interpretation of the Lagrangian multipliers to solve large size robust problems—are proposed, and their performance is evaluated through extensive computational results based on the lot-sizing problem. In particular, a guided iterated local search and a subgradient optimization method are proposed and compared against the dualization approach proposed by Bertsimas and Sim and with several heuristics based on the AARC approach, which include an iterated local search heuristic and a Benders decomposition approach. Computational results show the efficiency of the proposed heuristics, which provide a good compromise between the quality of the robust solutions and the running time.
Two essential ingredients of modern mixed-integer programming solvers are diving heuristics, which simulate a partial depth-first search in a branch-and-bound tree, and conflict analysis, which learns valid constraints from infeasible subproblems. So far, these techniques have mostly been studied independently: primal heuristics for finding high-quality feasible solutions early during the solving process and conflict analysis for fathoming nodes of the search tree and improving the dual bound. In this paper, we pose the question of whether and how the orthogonal goals of proving infeasibility and generating improving solutions can be pursued in a combined manner such that a state-of-the-art solver can benefit. To do so, we integrate both concepts in two different ways. First, we develop a diving heuristic that simultaneously targets the generation of valid conflict constraints from the Farkas dual and the generation of improving solutions. We show that, in the primal, this is equivalent to the optimistic strategy of diving toward the best bound with respect to the objective function. Second, we use information derived from conflict analysis to enhance the search of a diving heuristic akin to classic coefficient diving. In a detailed computational study, both methods are evaluated on the basis of an implementation in the source-open-solver SCIP. The experimental results underline the potential of combining both diving heuristics and conflict analysis.Summary of Contribution. This original article concerns the advancement of exact general-purpose algorithms for solving one of the largest and most prominent problem classes in optimization, mixed-integer linear programs. It demonstrates how methods for conflict analysis that learn from infeasible subproblems can be combined successfully with diving heuristics that aim at finding primal solutions. For two newly designed diving heuristics, this paper features a thoroughly computational study regarding their impact on the overall performance of a state-of-the-art MIP solver.
This paper develops an exact solution algorithm for the multiple sequence alignment (MSA) problem. In the first step, we design a dynamic programming model and use it to construct a novel multivalued decision diagram (MDD) representation of all pairwise sequence alignments (PSA). PSA MDDs are then synchronized using side constraints to model the MSA problem as a mixed-integer program (MIP), for the first time, in polynomial space complexity. Two bound-based filtering procedures are developed to reduce the size of the MDDs, and the resulting MIP is solved using logic-based Benders decomposition. For a more effective algorithm, we develop a two-phase solution approach. In the first phase, we use optimistic filtering to quickly obtain a near-optimal bound, which we then use for exact filtering in the second phase to prove or obtain an optimal solution. Numerical results on benchmark instances show that our algorithm solves several instances to optimality for the first time, and, in case optimality cannot be proven, considerably improves upon a state-of-the-art heuristic MSA solver. Comparison with an existing state-of-the-art exact MSA algorithm shows that our approach is more time efficient and yields significantly smaller optimality gaps.
Security-constrained unit commitment (SCUC) is a fundamental problem in power systems and electricity markets. In practical settings, SCUC is repeatedly solved via mixed-integer linear programming (MIP), sometimes multiple times per day, with only minor changes in input data. In this work, we propose a number of machine learning techniques to effectively extract information from previously solved instances in order to significantly improve the computational performance of MIP solvers when solving similar instances in the future. Based on statistical data, we predict redundant constraints in the formulation, good initial feasible solutions, and affine subspaces where the optimal solution is likely to lie, leading to a significant reduction in problem size. Computational results on a diverse set of realistic and large-scale instances show that using the proposed techniques, SCUC can be solved on average 4.3 times faster with optimality guarantees and 10.2 times faster without optimality guarantees, with no observed reduction in solution quality. Out-of-distribution experiments provide evidence that the method is somewhat robust against data-set shift.Summary of Contribution. The paper describes a novel computational method, based on a combination of mixed-integer linear programming (MILP) and machine learning (ML), to solve a challenging and fundamental optimization problem in the energy sector. The method advances the state-of-the-art, not only for this particular problem, but also, more generally, in solving discrete optimization problems via ML. We expect that the techniques presented can be readily used by practitioners in the energy sector and adapted, by researchers in other fields, to other challenging operations research problems that are solved routinely.
A lower bound for a finite-scenario-based chance-constrained program is the quantile value corresponding to the sorted optimal objective values of scenario subproblems. This quantile bound can be improved by grouping subsets of scenarios at the expense of solving larger subproblems. The quality of the bound depends on how the scenarios are grouped. In this paper, we formulate a mixed-integer bilevel program that optimally groups scenarios to tighten the quantile bounds. For general chance-constrained programs, we propose a branch-and-cut algorithm to optimize the bilevel program, and for chance-constrained linear programs, a mixed-integer linear-programming reformulation is derived. We also propose several heuristics for grouping similar or dissimilar scenarios. Our computational results demonstrate that optimal grouping bounds are much tighter than heuristic bounds, resulting in smaller root-node gaps and better performance of scenario decomposition for solving chance-constrained 0-1 programs. Also, the optimal grouping bounds can be greatly strengthened using larger group size.Summary of Contribution: Chance-constrained programs are in general NP-hard but widely used in practice for lowering the risk of undesirable outcomes during decision making under uncertainty. Assuming finite scenarios of uncertain parameter, chance-constrained programs can be reformulated as mixed-integer linear programs with binary variables representing whether or not the constraints are satisfied in corresponding scenarios. A useful quantile bound for solving chance-constrained programs can be improved by grouping subsets of scenarios at the expense of solving larger subproblems. In this paper, we develop algorithms for optimally and heuristically grouping scenarios to tighten the quantile bounds. We aim to improve both the computation and solution quality of a variety of chance-constrained programs formulated for different Operations Research problems.
Given an element set E of order n, a collection of subsets S⊆2E, a cost cS on each set S∈S, a covering requirement re for each element e∈E, and an integer k, the goal of a minimum partial set multicover problem (MinPSMC) is to find a subcollection F⊆S to fully cover at least k elements such that the cost of F is as small as possible and element e is fully covered by F if it belongs to at least re sets of F. This problem generalizes the minimum k-union problem (MinkU) and is believed not to admit a subpolynomial approximation ratio. In this paper, we present a (4 lognH(Δ)In k+2log n√n)-approximation algorithm for MinPSMC, in which Δ is the maximum size of a set in S. And when k=Ω(n), we present a bicriteria algorithm fully covering at least (1−ε2 log n) k elements with approximation ratio O(1ε(log n)2 H(Δ)), where 0<ε<1 is a fixed number. These results are obtained by studying the minimum density subcollection problem with (or without) cardinality constraint, which might be of interest by itself.
We consider a dynamic, stochastic extension to the transportation problem. For the deterministic problem, there are known necessary and sufficient conditions under which a greedy algorithm achieves the optimal solution. We define a distribution-free type of optimality and provide analogous necessary and sufficient conditions under which a greedy policy achieves this type of optimality in the dynamic, stochastic setting. These results are used to prove that a greedy algorithm is optimal when planning a type of air-traffic management initiative. We also provide weaker conditions under which it is possible to strengthen an existing policy. These results can be applied to the problem of matching passengers with drivers in an on-demand taxi service. They specify conditions under which a passenger and driver should not be left unassigned.
Emerald ash borer (EAB), a wood-boring insect native to Asia and invading North America, has killed untold millions of high-value ash trees that shade streets, homes, and parks and caused significant economic damage in cities of the United States. Local actions to reduce damage include surveillance to find EAB and control to slow its spread. We present a multistage stochastic mixed-integer programming (M-SMIP) model for the optimization of surveillance, treatment, and removal of ash trees in cities. Decision-dependent uncertainty is modeled by representing surveillance decisions and the realizations of the uncertain infestation parameter contingent on surveillance as branches in the M-SMIP scenario tree. The objective is to allocate resources to surveillance and control over space and time to maximize public benefits. We develop a new cutting-plane algorithm to strengthen the M-SMIP formulation and facilitate an optimal solution. We calibrate and validate our model of ash dynamics using seven years of observational data and apply the optimization model to a possible infestation in Burnsville, Minnesota. Proposed cutting planes improve the solution time by an average of seven times over solving the original M-SMIP model without cutting planes. Our comparative analysis shows that the M-SMIP model outperforms six different heuristic approaches proposed for the management of EAB. Results from optimally solving our M-SMIP model imply that under a belief of infestation, it is critical to apply surveillance immediately to locate EAB and then prioritize treatment of minimally infested trees followed by removal of highly infested trees.Summary of Contributions: Emerald ash borer (EAB) is one of the most damaging invasive species ever to reach the United States, damaging millions of ash trees. Much of the economic impact of EAB occurs in cities, where high-value ash trees grow in abundance along streets and in yards and parks. This paper addresses the joint optimization of surveillance and control of the emerald ash borer invasion, which is a novel application for the INFORMS society because, to our knowledge, this specific problem of EAB management has not been published before in any OR/MS journals. We develop a new multi-stage stochastic mixed-integer programming (MSS-MIP) formulation, and we apply our model to surveillance and control of EAB in cities. Our MSS-MIP model aims to help city managers maximize the net benefits of their healthy ash trees by determining the optimal timing and target population for surveying, treating, and removing infested ash trees while taking into account the spatio-temporal stochastic growth of the EAB infestation. We develop a new cutting plane methodology motivated by our problem, which could also be applied to other stochastic MIPs. Our cutting plane approach provides significant computational benefit in solving the problem. Specifically, proposed cutting planes improve the solution time by an average of seven times over solving the original M-SMIP model without cutting planes. We calibrate and validate our model using seven years of ash infestation observations in forests near Toledo, Ohio. We then apply our model to an urban forest in Burnsville, Minnesota, that is threatened by EAB. Our results provide insights into the optimal timing and location of EAB surveillance and control strategies.
This note corrects an error in our paper “N. Boland, R. Clement, and H. Waterer. A bucket indexed formulation for nonpreemptive single machine scheduling problems. INFORMS Journal on Computing 28(1):14–30, 2016.”
We address a dynamic covering location problem of an unmanned aerial vehicle base station (UAV-BS), in which the location sequence of a single UAV-BS in a wireless communication network is determined to satisfy data demand arising from ground users. This problem is especially relevant in the context of smart grid and disaster relief. The vertical movement ability of the UAV-BS and nonconvex covering functions in wireless communication restrict utilizing classical planar covering location approaches. Therefore, we develop new formulations to this emerging problem for a finite time horizon to maximize the total coverage. In particular, we develop a mixed-integer nonlinear programming formulation that is nonconvex in nature and propose a Lagrangean decomposition algorithm (LDA) to solve this formulation. Because of the high complexity of the problem, the LDA is still unable to find good local solutions to large-scale problems. Therefore, we develop a continuum approximation (CA) model and show that CA would be a promising approach in terms of both computational time and solution accuracy. Our numerical study also shows that the CA model can be a remedy to build efficient initial solutions for exact solution algorithms.Summary of Contribution: This paper addresses a facet of mixed integer nonlinear programming formulations. Dynamic facility location problems (DFLPs) arise in a wide range of applications. However, classical DFLPs typically focus on the two-dimensional spaces. Emerging technologies in wireless communication and some other promising application areas, such as smart grids, have brought new location problems that cannot be solved with classical approaches. For practical reasons, many research attempts to solve this new problem, especially by researchers whose primary research area is not OR, have seemed far from analyzing the characteristics of the formulations. Rather, solution-oriented greedy heuristics have been proposed. This paper has two main objectives: (i) to close the gap between practical and theoretical sides of this new problem with the help of current knowledge that OR possesses to solve facility location problems and (ii) to support the findings with an exhaustive computational study to show how these findings can be applied to practice.
Kidney exchange programs aim at matching end-stage renal disease patients who have a willing but incompatible kidney donor with another donor. The programs comprise a pool of such incompatible patient-donor pairs and, whenever a donor from one pair is compatible with the patient of another pair, and vice versa, the pairs may be matched and exchange kidneys. This is typically a two-step process in which, first, a set of pairs is matched based on preliminary compatibility tests and, second, the matched pairs are notified and more accurate compatibility tests are performed to verify that actual transplantation can take place. These additional tests may reveal incompatibilities not previously detected. When that happens, the planned exchange will not proceed. Furthermore, pairs may drop out before the transplant, and thus the planned exchange is canceled. In this paper, we study the case in which a new set of pairs may be matched if incompatibilities are discovered or a pair withdraws from the program. The new set should be as close as possible to the initial set in order to minimize the material and emotional costs of the changes. Various recourse policies that determine the admissible second-stage actions are investigated. For each recourse policy, we propose a novel adjustable robust integer programming model. We also propose solution approaches to solve this model exactly. The approaches are validated through thorough computational experiments.Summary of Contribution: In the paper, we present an original work related to the modeling and optimization approaches for Kidney Exchange Programs (KEPs). Currently, KEPs represent an alternative way for patients suffering from renal failure to find a compatible (living) donor. The problem of determining an assignment of patients to (compatible) donors that maximizes the number of transplants in a KEP can be seen as a vertex-disjoint cycle packing problem. Thus, KEPs have been extensively studied in the literature of integer programming. In practice, the assignment determined to a KEP might not be implemented due to withdraws from the program (e.g., a more accurate compatible test shows a new incompatibility or a patient health condition unable him/her to participate on the KEP). In our paper, we model the problem of determining a robust solution to the KEP, i.e., a solution that minimizes the material and emotional costs of changing an assignment. In this way, we propose and design solution approaches for three recourse policies that anticipate withdraws. Through computational experiments we compare the three recourse policies and validate the practical interest of robust solutions.
Tags have been adopted by many online services as a method to manage their online resources. Effective tagging benefits both users and firms. In real applications providing a user tagging mechanism, only a small portion of tags are usually provided by users. Therefore, an automatic tagging method, which can assign tags to different items automatically, is urgently needed. Previous works on automatic tagging focus on exploring the tagging behavior of users or the content information of items. In online service platforms, users frequently browse items related to their interests, which implies users’ judgment about the underlying features of items and is helpful for automatic tagging. Browsing-behavior records are much more plentiful compared with tagging behavior and easy to collect. However, existing studies about automatic tagging ignore this kind of information. To properly integrate both browsing behaviors and content information for automatic tagging, we propose a novel probabilistic graphical model and develop a new algorithm for the model parameter inference. We conduct thorough experiments on a real-world data set to evaluate and analyze the performance of our proposed method. The experimental results demonstrate that our approach achieves better performance than state-of-the-art automatic tagging methods.Summary of Contribution. In this paper, we study how to automatically assign tags to items in an e-commerce background. Our study is about how to perform item tagging for e-commerce and other online service providers so that consumers can easily find what they need and firms can manage their resources effectively. Specifically, we study if consumer browsing behavior can be utilized to perform the tagging task automatically, which can save efforts of both firms and consumers. Additionally, we transform the problem into how to find the most proper tags for items and propose a novel probabilistic graphical model to model the generation process of tags. Finally, we develop a variational inference algorithm to learn the model parameters, and the model shows superior performance over competing benchmark models. We believe this study contributes to machine learning techniques.
Maintenance optimization has been extensively studied in the past decades. However, most of the existing maintenance models focus on single-component systems and are not applicable to complex systems consisting of multiple components, due to various interactions among the components. The multicomponent maintenance optimization problem, which joins the stochastic processes regarding the failures of components with the combinatorial problems regarding the grouping of maintenance activities, is challenging in both modeling and solution techniques, and has remained an open issue in the literature. In this paper, we study the multicomponent maintenance problem over a finite planning horizon and formulate the problem as a multistage stochastic integer program with decision-dependent uncertainty. There is a lack of general efficient methods to solve this type of problem. To address this challenge, we use an alternative approach to model the underlying failure process and develop a novel two-stage model without decision-dependent uncertainty. Structural properties of the two-stage problem are investigated, and a progressive-hedging-based heuristic is developed based on the structural properties. Our heuristic algorithm demonstrates a significantly improved capacity to handle large-size two-stage problems comparing to three conventional methods for stochastic integer programming, and solving the two-stage model by our heuristic in a rolling horizon provides a good approximation of the multistage problem. The heuristic is further benchmarked with a dynamic programming approach and a structural policy, which are two commonly adopted approaches in the literature. Numerical results show that our heuristic can lead to significant cost savings compared with the benchmark approaches.
Inference-based optimization via simulation, which substitutes Gaussian process (GP) learning for the structural properties exploited in mathematical programming, is a powerful paradigm that has been shown to be remarkably effective in problems of modest feasible-region size and decision-variable dimension. The limitation to “modest” problems is a result of the computational overhead and numerical challenges encountered in computing the GP conditional (posterior) distribution on each iteration. In this paper, we substantially expand the size of discrete-decision-variable optimization-via-simulation problems that can be attacked in this way by exploiting a particular GP—discrete Gaussian Markov random fields—and carefully tailored computational methods. The result is the rapid Gaussian Markov Improvement Algorithm (rGMIA), an algorithm that delivers both a global convergence guarantee and finite-sample optimality-gap inference for significantly larger problems. Between infrequent evaluations of the global conditional distribution, rGMIA applies the full power of GP learning to rapidly search smaller sets of promising feasible solutions that need not be spatially close. We carefully document the computational savings via complexity analysis and an extensive empirical study.Summary of Contribution: The broad topic of the paper is optimization via simulation, which means optimizing some performance measure of a system that may only be estimated by executing a stochastic, discrete-event simulation. Stochastic simulation is a core topic and method of operations research. The focus of this paper is on significantly speeding-up the computations underlying an existing method that is based on Gaussian process learning, where the underlying Gaussian process is a discrete Gaussian Markov Random Field. This speed-up is accomplished by employing smart computational linear algebra, state-of-the-art algorithms, and a careful divide-and-conquer evaluation strategy. Problems of significantly greater size than any other existing algorithm with similar guarantees can solve are solved as illustrations.
This paper considers a single-resource allocation problem for multiple items with random, independent resource consumption values, known as the static stochastic knapsack problem (SSKP). Whereas the existing SSKP literature generally assumes a risk-neutral objective using an expected value approach, such an approach can maximize expected profit while admitting the possibility of very high losses in some unfavorable scenarios. Because of this, we consider two popular risk measures, conditional value-at-risk (CVaR) and a mean-standard deviation trade-off, in order to address risk within this problem class. Optimizing the trade-offs associated with these risk measures presents significant modeling and computational challenges. To address these challenges, we first provide mixed-integer linear programming models using a scenario-based approach, which can be exploited to provide exact solutions for discrete distributions. For general distributions, a sample average approximation method provides approximate solutions. We then propose a general mixed integer nonlinear optimization modeling approach for the special case of normally distributed resource requirements. This modeling approach incorporates a new class of normalized penalty functions that account for both the expected costs and risks associated with uncertainty, and it can be specialized to a broad class of risk measures, including CVaR and mean-standard deviation. Our results characterize key optimality properties for the associated continuous relaxation of the proposed general model and provide insights on valuable rank-ordering mechanisms for items with uncertain resource needs under different risk measures. For this broadly applicable case, we present a class of efficient and high-performing asymptotically optimal heuristic methods based on these optimality conditions. An extensive numerical study evaluates the efficiency and quality of the proposed solution methods, identifies optimal item selection strategies, and examines the sensitivity of the solution to varying levels of risk, excess weight penalty values, and knapsack capacity values.Summary of Contribution: This research proposes and analyzes new models for a stochastic resource allocation problem that arises in a variety of operations contexts. One of the primary contributions of the paper lies in providing a succinct, robust, and general model that can address a range of different risk-based objectives and cost assumptions under uncertainty. While the model expression is relatively simple, it embeds a reasonably high degree of underlying complexity, as the analysis shows. In addition, in-depth analysis of the model, both in its general form and under various specific risk measures, uncovers some interesting and powerful insights regarding the problem tradeoffs. Furthermore, this analysis leads to a highly efficient class of heuristic algorithms for solving the problem, which we demonstrate via numerical experimentation to provide close-to-optimal solutions. This computational benefit is a critical element for solving a class of broadly applicable larger problems for which our problem arises as a subproblem that requires repeated solution.
The quadratic multiknapsack problem consists of packing a set of items of various weights into knapsacks of limited capacities with profits being associated with pairs of items packed into the same knapsack. This problem has been solved by various heuristics since its inception, and more recently it has also been solved with an exact method. We introduce a generalization of this problem that includes pairwise conflicts as well as balance constraints, among other particularities. We present and compare constraint programming and integer programming approaches for solving this generalized problem.Summary of Contribution: The quadratic multiknapsack problem consists of packing a set of items of various weights into knapsacks of limited capacities -- with profits being associated with pairs of items packed into the same knapsack. This problem has been solved by various heuristics since its inception, and more recently it has also been solved with an exact method. We introduce a generalization of this problem which includes pairwise conflicts as well as balance constraints, among other particularities. We present and compare constraint programming and integer programming approaches for solving this generalized problem. The problem we address is clearly in the core of the operations research applications in which subsets have to be built and, in particular, we add the concept of fairness to the modeling and solution process by computationally evaluating techniques to take fairness into account. This is clearly at the core of computational evaluation of algorithms.
The two-dimensional bin packing problem calls for packing a set of rectangular items into a minimal set of larger rectangular bins. Items must be packed with their edges parallel to the borders of the bins, cannot be rotated, and cannot overlap among them. The problem is of interest because it models many real-world applications, including production, warehouse management, and transportation. It is, unfortunately, very difficult, and instances with just 40 items are unsolved to proven optimality, despite many attempts, since the 1990s. In this paper, we solve the problem with a combinatorial Benders decomposition that is based on a simple model in which the two-dimensional items and bins are just represented by their areas, and infeasible packings are imposed by means of exponentially many no-good cuts. The basic decomposition scheme is quite naive, but we enrich it with a number of preprocessing techniques, valid inequalities, lower bounding methods, and enhanced algorithms to produce the strongest possible cuts. The resulting algorithm behaved very well on the benchmark sets of instances, improving on average on previous algorithms from the literature and solving for the first time a number of open instances.Summary of Contribution: We address the two-dimensional bin packing problem (2D-BPP), which calls for packing a set of rectangular items into a minimal set of larger rectangular bins. The 2D-BPP is a very difficult generalization of the standard one-dimensional bin packing problem, and it has been widely studied in the past because it models many real-world applications, including production, warehouse management, and transportation. We solve the 2D-BPP with a combinatorial Benders decomposition that is based on a model in which the two-dimensional items and bins are represented by their areas, and infeasible packings are imposed by means of exponentially many no-good cuts. The basic decomposition scheme is quite naive, but it is enriched with a number of preprocessing techniques, valid inequalities, lower bounding methods, and enhanced algorithms to produce the strongest possible cuts. The algorithm we developed has been extensively tested on the most well-known benchmark set from the literature, which contains 500 instances. It behaved very well, improving on average upon previous algorithms, and solving for the first time a number of open instances. We analyzed in detail several configurations before obtaining the best one and discussed several insights from this analysis in the manuscript.
In this paper, we study the quadratic assignment problem with a rank-one cost matrix (QAP-R1). Four integer-programming formulations are introduced of which three are assumed to have partial integer data. Unlike the standard quadratic assignment problem, some of our formulations can solve reasonably large instances of QAP-R1 with impressive running times and are faster than some metaheuristics. Pairwise relative strength of the LP relaxations of these formulations are also analyzed from theoretical and experimental points of view. Finally, we present a new metaheuristic algorithm to solve QAP-R1 along with its computational analysis. Our study offers the first systematic experimental analysis of integer-programming models and heuristics for QAP-R1. The benchmark instances with various characteristics generated for our study are made available to the public for future research work. Some new polynomially solvable special cases are also introduced.Summary of Contribution: This paper aims to advance our knowledge and ability in solving an important special case of the quadratic assignment problem. It shows how to exploit inherent properties of an optimization problem to achieve computational advantages, a strategy that was followed by researchers in model building and algorithm developments for decades. Our computational results attest to this time-tested general philosophy. The paper presents the first systematic computational study of the rank one quadratic assignment problem, along with new mathematical programming models and complexity analysis. We believe the theoretical and computational results of this paper will inspire further research on the topic and will be of significant value to practitioners using rank one quadratic assignment models.
Waiting at the right location at the right time can be critically important in certain variants of time-dependent shortest path problems. We investigate the computational complexity of time-dependent shortest path problems in which there is either a penalty on waiting or a limit on the total time spent waiting at a given subset of the nodes. We show that some cases are nondeterministic polynomial-time hard, and others can be solved in polynomial time, depending on the choice of the subset of nodes, on whether waiting is penalized or constrained, and on the magnitude of the penalty/waiting limit parameter.Summary of Contributions: This paper addresses simple yet relevant extensions of a fundamental problem in Operations Research: the Shortest Path Problem (SPP). It considers time-dependent variants of SPP, which can account for changing traffic and/or weather conditions. The first variant that is tackled allows for waiting at certain nodes but at a cost. The second variant instead places a limit on the total waiting. Both variants have applications in transportation, e.g., when it is possible to wait at certain locations if the benefits outweigh the costs. The paper investigates these problems using complexity analysis and algorithm design, both tools from the field of computing. Different cases are considered depending on which of the nodes contribute to the waiting cost or waiting limit (all nodes, all nodes except the origin, a subset of nodes…). The computational complexity of all cases is determined, providing complexity proofs for the variants that are NP-Hard and polynomial time algorithms for the variants that are in P.
We propose a novel partial sample average approximation (PSAA) framework to solve the two main types of chance-constrained linear matrix inequality (CCLMI) problems: CCLMI with random technology matrix and CCLMI with random right-hand side. We propose a series of computationally tractable PSAA-based approximations for CCLMI problems, analyze their properties, and derive sufficient conditions that ensure convexity for the two most popular—normal and uniform—continuous distributions. We derive several semidefinite programming PSAA reformulations efficiently solved by off-the-shelf solvers and design a sequential convex approximation method for the PSAA formulations containing bilinear matrix inequalities. The proposed methods can be generalized to other continuous random variables whose cumulative distribution function can be easily computed. We carry out a comprehensive numerical study on three practical CCLMI problems: robust truss topology design, calibration, and robust control. The tests attest to the superiority of the PSAA reformulation and algorithmic framework over the scenario and sample average approximation methods.Summary of Contribution: In line with the mission and scope of IJOC, we study an important type of optimization problems, chance-constrained linear matrix inequality (CCLMI) problems, which require stochastic linear matrix inequality (LMI) constraints to be satisfied with high probability. To solve CCLMI problems, we propose a novel partial sample average approximation (PSAA) framework: (i) develop a series of computationally tractable PSAA-based approximations for CCLMI problems, (ii) analyze their properties, (iii) derive sufficient conditions ensuring convexity, and (iv) design a sequential convex approximation method. We evaluate our proposed method via a comprehensive numerical study on three practical CCLMI problems. The tests attest the superiority of the PSAA reformulation and algorithmic framework over standard benchmarks.
The unit commitment problem with uncertainty is considered one of the most challenging power system scheduling problems. Different stochastic models have been proposed to solve the problem, but such approaches have yet to be applied in industry practice because of computational challenges. In practice, the problem is formulated as a deterministic model with reserve requirements to hedge against uncertainty. However, simply requiring a certain level of reserves cannot ensure power system reliability as the procured reserves may be nondispatchable because of transmission limitations. In this paper, we derive a set of feasibility cuts (constraints) for managing the unit commitment problem with uncertainty. These cuts eliminate unreliable scheduling solutions and reallocate reserves in the power system; they are induced by the extreme rays of a polyhedral dual cone. This paper shows that, with the proposed reformulation, the extreme rays of the dual cone can be characterized by combinatorial selections of transmission lines (arcs) and buses (nodes) of the power system. As a result, the cuts can then be characterized using engineering insights. The unit commitment problem with uncertainty is formulated as a deterministic model with the identified extreme ray feasibility cuts. Test results show that, with the proposed extreme ray feasibility cuts, the problem can be solved more efficiently, and the resulting scheduling decision is also more reliable.
Joint chance-constrained optimization problems under discrete distributions arise frequently in financial management and business operations. These problems can be reformulated as mixed-integer programs. The size of reformulated integer programs is usually very large even though the original problem is of medium size. This paper studies an augmented Lagrangian decomposition method for finding high-quality feasible solutions of complex optimization problems, including nonconvex chance-constrained problems. Different from the current augmented Lagrangian approaches, the proposed method allows randomness to appear in both the left-hand-side matrix and the right-hand-side vector of the chance constraint. In addition, the proposed method only requires solving a convex subproblem and a 0-1 knapsack subproblem at each iteration. Based on the special structure of the chance constraint, the 0-1 knapsack problem can be computed in quasi-linear time, which keeps the computation for discrete optimization subproblems at a relatively low level. The convergence of the method to a first-order stationary point is established under certain mild conditions. Numerical results are presented in comparison with a set of existing methods in the literature for various real-world models. It is observed that the proposed method compares favorably in terms of the quality of the best feasible solution obtained within a certain time for large-size problems, particularly when the objective function of the problem is nonconvex or the left-hand-side matrix of the constraints is random.
We study the family of problems of partitioning and covering a graph into/with a minimum number of relaxed cliques. Relaxed cliques are subsets of vertices of a graph for which a clique-defining property—for example, the degree of the vertices, the distance between the vertices, the density of the edges, or the connectivity between the vertices—is relaxed. These graph partitioning and covering problems have important applications in many areas such as social network analysis, biology, and disease-spread prevention. We propose a unified framework based on branch-and-price techniques to compute optimal decompositions. For this purpose, new, effective pricing algorithms are developed, and new branching schemes are invented. In extensive computational studies, we compare several algorithmic designs, such as structure-preserving versus dichotomous branching, and their interplay with different pricing algorithms. The final chosen branch-and-price setup produces results that demonstrate the effectiveness of all components of the newly developed framework and the validity of our approach when applied to social network instances.
In this paper, we discuss the one-machine scheduling problem with release and delivery times with the minimum makespan objective. Both heuristics and branch-and-bound algorithms have been formulated for the problem. One such branch-and-bound algorithm solves the problem and a variation that requires a delay between the completion of one job and the start of another (delayed precedence constraints). This paper analyzes key components of this branch-and-bound algorithm and proposes an improved heuristic to be used in conjunction with a different search strategy. Computational experiments demonstrate that the modifications lead to substantial improvements in running time and number of iterations on the one-machine problem instances both with and without delayed precedence constraints.
Decision trees usefully represent sparse, high-dimensional, and noisy data. Having learned a function from these data, we may want to thereafter integrate the function into a larger decision-making problem, for example, for picking the best chemical process catalyst. We study a large-scale, industrially relevant mixed-integer nonlinear nonconvex optimization problem involving both gradient-boosted trees and penalty functions mitigating risk. This mixed-integer optimization problem with convex penalty terms broadly applies to optimizing pretrained regression tree models. Decision makers may wish to optimize discrete models to repurpose legacy predictive models or they may wish to optimize a discrete model that accurately represents a data set. We develop several heuristic methods to find feasible solutions and an exact branch-and-bound algorithm leveraging structural properties of the gradient-boosted trees and penalty functions. We computationally test our methods on a concrete mixture design instance and a chemical catalysis industrial instance.
We investigate a class of fractional distributionally robust optimization problems with uncertain probabilities. They consist in the maximization of ambiguous fractional functions representing reward-risk ratios and have a semi-infinite programming epigraphic formulation. We derive a new fully parameterized closed-form to compute a new bound on the size of the Wasserstein ambiguity ball. We design a data-driven reformulation and solution framework. The reformulation phase involves the derivation of the support function of the ambiguity set and the concave conjugate of the ratio function. We design modular bisection algorithms which enjoy the finite convergence property. This class of problems has wide applicability in finance, and we specify new ambiguous portfolio optimization models for the Sharpe and Omega ratios. The computational study shows the applicability and scalability of the framework to solve quickly large, industry-relevant-size problems, which cannot be solved in one day with state-of-the-art mixed-integer nonlinear programming (MINLP) solvers.
In this article, we discuss an alternative method for deriving conservative approximation models for two-stage robust optimization problems. The method mainly relies on a linearization scheme employed in bilinear programming; therefore, we will say that it gives rise to the linearized robust counterpart models. We identify a close relation between this linearized robust counterpart model and the popular affinely adjustable robust counterpart model. We also describe methods of modifying both types of models to make these approximations less conservative. These methods are heavily inspired by the use of valid linear and conic inequalities in the linearization process for bilinear models. We finally demonstrate how to employ this new scheme in location-transportation and multi-item newsvendor problems to improve the numerical efficiency and performance guarantees of robust optimization.
The team formation problem (TFP) aims to construct a capable team that can communicate and collaborate effectively. The cost of communication is quantified using the proximity of the potential members in a social network. We study a TFP with two measures for communication effectiveness; namely, we minimize the sum of communication costs, and we impose an upper bound on the largest communication cost. This problem can be formulated as a constrained quadratic set covering problem. Our experiments show that a general-purpose solver is capable of solving small and medium-sized instances to optimality. We propose a branch-and-bound algorithm to solve larger sizes: we reformulate the problem and relax it in such a way that it decomposes into a series of linear set covering problems, and we impose the relaxed constraints through branching. Our computational experiments show that the algorithm is capable of solving large-size instances, which are intractable for the solver.Summary of Contribution: This paper presents an exact algorithm for the Team Formation Problem (TFP), in which the aim is, given a project and its required skills, to construct a capable team that can communicate and collaborate effectively. This combinatorial optimization problem is modeled as a quadratic set covering problem. The study provides a novel branch-and-bound algorithm where a reformulation of the problem is relaxed so that it decomposes into a series of linear set covering problems and the relaxed constraints are imposed through branching. The algorithm is able to solve instances that are intractable for commercial solvers. The study illustrates an efficient usage of algorithmic methods and modelling techniques for an operations research problem. It contributes to the field of computational optimization by proposing a new application as well as a new algorithm to solve a quadratic version of a classical combinatorial optimization problem.
Learning the customers’ experience and behavior creates competitive advantages for any company over its rivals. The insurance industry is an essential sector in any developed economy and a better understanding of customers’ risk profile is critical to decision making in all aspects of insurance operations. In this paper, we explore the idea of using copula-based dependence models to learn the hidden risk of policyholders in property insurance. Specifically, we build a novel copula model to accommodate the dependence over time and over space among spatially clustered property risks. To tackle the computational challenge caused by the discreteness feature of large-scale insurance data, we propose an efficient multilevel composite likelihood approach for parameter estimation. Provided that latent risk induces correlation, the proposed customer learning method offers improved predictive analytics by allowing insurers to borrow strength from related risks in predicting new risks and also helps reveal the relative importance of the multiple sources of unobserved heterogeneity in updating policyholders’ risk profile. In the empirical study, we examine the loss cost of a portfolio of entities insured by a government property insurance program in Wisconsin. We find both significant temporal and spatial association among property risks. However, their effects on the predictive distribution of loss cost are different for the new and renewal policyholders. The two sources of dependence are complements for the former and substitutes for the latter. These findings are shown to have substantial managerial implications in key insurance operations such as experience rating, capital allocation, and reinsurance arrangement.
The separable convex resource allocation problem with nested bound constraints aims to allocate B units of resources to n activities to minimize a separable convex cost function, with lower and upper bounds on the total amount of resources that can be consumed by nested subsets of activities. We develop a new combinatorial algorithm to solve this model exactly. Our algorithm is capable of solving instances with millions of activities in several minutes. The running time of our algorithm is at most 73% of the running time of the current best algorithm for benchmark instances with three classes of convex objectives. The efficiency of our algorithm derives from a combination of constraint relaxation and divide and conquer based on infeasibility information. In particular, nested bound constraints are relaxed first; if the solution obtained violates some bound constraints, we show that the problem can be divided into two subproblems of the same structure and smaller sizes according to the bound constraint with the largest violation.Summary of Contribution. The resource allocation problem is a collection of optimization models with a wide range of applications in production planning, logistics, portfolio management, telecommunications, statistical surveys, and machine learning. This paper studies the resource allocation model with prescribed lower and upper bounds on the total amount of resources consumed by nested subsets of activities. These nested bound constraints are motivated by storage limits, time-window requirements, and budget constraints in various applications. The model also appears as a subproblem in models for green logistics and machine learning, and it has to be solved repeatedly. The model belongs to the class of computationally challenging convex mixed-integer nonlinear programs. We develop a combinatorial algorithm to solve this model exactly. Our algorithm is faster than the algorithm that currently has the best theoretical complexity in the literature on an extensive set of test instances. The efficiency of our algorithm derives from the combination of an infeasibility-guided divide-and-conquer framework and a scaling-based greedy subroutine for resource allocation with submodular constraints. This paper also showcases the prevalent mismatch between the theoretical worst-case time complexity of an algorithm and its practical efficiency. We have offered some explanations of this mismatch through the perspectives of worst-case analysis, specially designed instances, and statistical metrics of numerical experiments. The implementation of our algorithm is available on an online repository.
We study a large-scale resource allocation problem with a convex, separable, not necessarily differentiable objective function that includes uncertain parameters falling under an interval uncertainty set, considering a set of deterministic constraints. We devise an exact algorithm to solve the minimax regret formulation of this problem, which is NP-hard, and we show that the proposed Benders-type decomposition algorithm converges to an ɛ-optimal solution in finite time. We evaluate the performance of the proposed algorithm via an extensive computational study, and our results show that the proposed algorithm provides efficient solutions to large-scale problems, especially when the objective function is differentiable. Although the computation time takes longer for problems with nondifferentiable objective functions as expected, we show that good quality, near-optimal solutions can be achieved in shorter runtimes by using our exact approach. We also develop two heuristic approaches, which are partially based on our exact algorithm, and show that the merit of the proposed exact approach lies in both providing an ɛ-optimal solution and providing good quality near-optimal solutions by laying the foundation for efficient heuristic approaches.
This paper studies a multiproduct newsvendor problem with customer-driven demand substitution, where each product, once run out of stock, can be proportionally substituted by the others. This problem has been widely studied in the literature; however, because of nonconvexity and intractability, only limited analytical properties have been reported and no efficient approaches have been proposed. This paper first completely characterizes the optimal order policy when the demand is known and reformulates this nonconvex problem as a binary quadratic program. When the demand is random, we formulate the problem as a two-stage stochastic integer program, derive several necessary optimality conditions, prove the submodularity of the profit function, and also develop polynomial-time approximation algorithms and show their performance guarantees. We further propose a tight upper bound via nonanticipativity dual, which is proven to be very close to the optimal value and can yield a good-quality feasible solution under a mild condition. Our numerical investigation demonstrates effectiveness of the proposed algorithms. Moreover, several useful findings and managerial insights are revealed from a series of sensitivity analyses.
Local governments inspect roads to decide which segments and intersections to repair. Videos are taken using a camera mounted on a vehicle. The vehicle taking the videos proceeds straight or takes a left turn to cover an intersection fully. We introduce the intersection inspection rural postman problem (IIRPP), which is a new variant of the rural postman problem (RPP) that involves turns. We develop integer programming formulations of the IIRPP based on two different graph transformations to generate least-cost vehicle routes. One formulation is based on a new idea of transforming a graph. A second formulation is based on a graph transformation idea from the literature. Computational experiments show that the formulation involving the new graph transformation idea performs much better than the other formulation. We also develop an RPP-based heuristic and a heuristic based on a modified RPP. Heuristic solutions are improved by solving integer programming formulations on an induced subgraph. Computational experiments show that the heuristics based on the modified RPP perform much better than the RPP-based heuristics. The best-performing heuristic generates very good quality IIRPP-feasible routes on large street networks quickly.Summary of Contribution. Our paper addresses a real-world problem faced by local governments during road inspections. The real-world problem that we solve and the methodologies that we use fall at the intersection of computing and operations research. We introduce the intersection inspection rural postman problem, which is a new variant of the rural postman problem that involves turns to capture this real-world scenario. The rural postman problem is an important problem in vehicle routing. Studying new variants of this problem is key to extending the practice and theory of vehicle routing. We develop an integer programming formulation based on a new idea of transforming a graph and also develop heuristics based on the rural postman problem.
We study the quadratic cycle cover problem (QCCP), which aims to find a node-disjoint cycle cover in a directed graph with minimum interaction cost between successive arcs. We derive several semidefinite programming (SDP) relaxations and use facial reduction to make these strictly feasible. We investigate a nontrivial relationship between the transformation matrix used in the reduction and the structure of the graph, which is exploited in an efficient algorithm that constructs this matrix for any instance of the problem. To solve our relaxations, we propose an algorithm that incorporates an augmented Lagrangian method into a cutting-plane framework by utilizing Dykstra’s projection algorithm. Our algorithm is suitable for solving SDP relaxations with a large number of cutting-planes. Computational results show that our SDP bounds and efficient cutting-plane algorithm outperform other QCCP bounding approaches from the literature. Finally, we provide several SDP-based upper bounding techniques, among which is a sequential Q-learning method that exploits a solution of our SDP relaxation within a reinforcement learning environment.Summary of Contribution: The quadratic cycle cover problem (QCCP) is the problem of finding a set of node-disjoint cycles covering all the nodes in a graph such that the total interaction cost between successive arcs is minimized. The QCCP has applications in many fields, among which are robotics, transportation, energy distribution networks, and automatic inspection. Besides this, the problem has a high theoretical relevance because of its close connection to the quadratic traveling salesman problem (QTSP). The QTSP has several applications, for example, in bioinformatics, and is considered to be among the most difficult combinatorial optimization problems nowadays. After removing the subtour elimination constraints, the QTSP boils down to the QCCP. Hence, an in-depth study of the QCCP also contributes to the construction of strong bounds for the QTSP. In this paper, we study the application of semidefinite programming (SDP) to obtain strong bounds for the QCCP. Our strongest SDP relaxation is very hard to solve by any SDP solver because of the large number of involved cutting-planes. Because of that, we propose a new approach in which an augmented Lagrangian method is incorporated into a cutting-plane framework by utilizing Dykstra’s projection algorithm. We emphasize an efficient implementation of the method and perform an extensive computational study. This study shows that our method is able to handle a large number of cuts and that the resulting bounds are currently the best QCCP bounds in the literature. We also introduce several upper bounding techniques, among which is a distributed reinforcement learning algorithm that exploits our SDP relaxations.
Electric vehicles offer a pathway to more sustainable transportation, but their adoption entails new challenges not faced by their petroleum-based counterparts. A difficult task in vehicle routing problems addressing these challenges is determining how to make good charging decisions for an electric vehicle traveling a given route. This is known as the fixed route vehicle charging problem. An exact and efficient algorithm for this task exists, but its implementation is sufficiently complex to deter researchers from adopting it. In this work we introduce frvcpy, an open-source Python package implementing this algorithm. Our aim with the package is to make it easier for researchers to solve electric vehicle routing problems, facilitating the development of optimization tools that may ultimately enable the mass adoption of electric vehicles.Summary of Contribution: This work describes a novel software tool for the vehicle routing community. The tool, frvcpy, addresses one of the primary challenges faced by the vehicle routing community when considering problems involving the adoption of electric vehicles (EVs): how to make optimal charging decisions. The state-of-the-art algorithm for solving these problems is sufficiently complex to deter researchers from using it, leading them to adopt less robust methods. frvcpy offers an easy-to-use, lightweight implementation of this algorithm, providing optimal solutions in low (∼5 ms) runtime. It is designed to be easily embedded in larger solution schemes for general EV routing problems, requiring minimal input, offering compatibility with the community standard file types, and offering access both through the command line and a Python API. The tool has thus far proven adaptable, having been used by researchers studying EV routing problems with novel constraints. Our aim with frvcpy is to make it easier for researchers to solve EV routing problems, facilitating the development of optimization tools that may contribute toward the mass adoption of electric vehicles.
Algorithms for approximating the nondominated set of multiobjective optimization problems are reviewed. The approaches are categorized into general methods that are applicable under mild assumptions and, thus, to a wide range of problems, and into algorithms that are specifically tailored to structured problems. All in all, this survey covers 52 articles published within the last 41 years, that is, between 1979 and 2020.Summary of Contribution: In many problems in operations research, several conflicting objective functions have to be optimized simultaneously, and one is interested in finding Pareto optimal solutions. Because of the high complexity of finding Pareto optimal solutions and their usually very large number, however, the exact solution of such multiobjective problems is often very difficult, which motivates the study of approximation algorithms for multiobjective optimization problems. This research area uses techniques and methods from algorithmics and computing in order to efficiently determine approximate solutions to many well-known multiobjective problems from operations research. Even though approximation algorithms for multiobjective optimization problems have been investigated for more than 40 years and more than 50 research articles have been published on this topic, this paper provides the first survey of this important area at the intersection of computing and operations research.
Microgrids are frequently employed in remote regions, in part because access to a larger electric grid is impossible, difficult, or compromises reliability and independence. Although small microgrids often employ spot generation, in which a diesel generator is attached directly to a load, microgrids that combine these individual loads and augment generators with photovoltaic cells and batteries as a distributed energy system are emerging as a safer, less costly alternative. We present a model that seeks the minimum-cost microgrid design and ideal dispatched power to support a small remote site for one year with hourly fidelity under a detailed battery model; this mixed-integer nonlinear program (MINLP) is intractable with commercial solvers but loosely coupled with respect to time. A mixed-integer linear program (MIP) approximates the model, and a partitioning scheme linearizes the bilinear terms. We introduce a novel policy for loosely coupled MIPs in which the system reverts to equivalent conditions at regular time intervals; this separates the problem into subproblems that we solve in parallel. We obtain solutions within 5% of optimality in at most six minutes across 14 MIP instances from the literature and solutions within 5% of optimality to the MINLP instances within 20 minutes.
Viewers often use social media platforms like Twitter to express their views about televised programs and events like the presidential debate, the Oscars, and the State of the Union speech. Although this promises tremendous opportunities to analyze the feedback on a program or an event using viewer-generated content on social media, there are significant technical challenges to doing so. Specifically, given a televised event and related tweets about this event, we need methods to effectively align these tweets and the corresponding event. In turn, this will raise many questions, such as how to segment the event and how to classify a tweet based on whether it is generally about the entire event or specifically about one particular event segment. In this paper, we propose and develop a novel joint Bayesian model that aligns an event and its related tweets based on the influence of the event’s topics. Our model allows the automated event segmentation and tweet classification concurrently. We present an efficient inference method for this model and a comprehensive evaluation of its effectiveness compared with the state-of-the-art methods. We find that the topics, segments, and alignment provided by our model are significantly more accurate and robust.
Local polynomial regression is an important class of methods for nonparametric density estimation and regression problems. However, straightforward implementation of local polynomial regression has quadratic time complexity which hinders its applicability in large-scale data analysis. In this paper, we significantly accelerate the computation of local polynomial estimates by novel applications of multidimensional binary indexed trees. Both time and space complexity of our proposed algorithm is nearly linear in the number of input data points. Simulation results confirm the efficiency and effectiveness of our proposed approach.Summary of Contribution. Big data analytics has become essential for modern operations research and operations management applications. Statistics methods, such as nonparametric density and function estimation, play important roles in predictive and exploratory data analysis for economics and operations management problems. In this paper, we concentrate on efficiently computing local polynomial regression estimates. We significantly accelerate the computation of such local polynomial estimates by novel applications of multidimensional binary indexed trees and lazy memory allocation via hashing. Both time and space complexity of our proposed algorithm are nearly linear in the number of inputs. Simulation results confirm the efficiency and effectiveness of our proposed methods.
We explore the benefits of multivariable branching schemes for linear-programming-based branch-and-bound algorithms for the 0-1 knapsack problem—that is, the benefits of branching on sets of variables rather than on a single variable (the current default in integer-programming solvers). We present examples where multivariable branching has advantages over single-variable branching and partially characterize situations in which this happens. Chvátal shows that for a specific class of 0-1 knapsack instances, a linear-programming-based branch-and-bound algorithm (employing a single-variable branching scheme) must explore exponentially many nodes. We show that for this class of 0-1 knapsack instances, a linear-programming-based branch-and-bound algorithm employing an appropriately chosen multivariable branching scheme explores either three or seven nodes. Finally, we investigate the performance of various multivariable branching schemes for 0-1 knapsack instances computationally and demonstrate their potential; the multivariable branching schemes explored result in smaller search trees (some in search trees that are an order of magnitude smaller), and some also result in shorter solution times.Summary of Contribution: As a powerful modeling tool, mixed-integer programming (MIP) is ubiquitous in Operations Research and is usually solved via the branch-and-bound framework. However, solving MIPs is computationally challenging in general, where branching affects the performance of solvers dramatically. In this paper, we explore the benefits of branching on multiple variables, which can be viewed as a generalization of the standard single-variable branching. We analyze its theoretical behavior on a special instance introduced by Chvátal, which is proved to be hard for single-variable branching. We also partially characterize situations in which branching on multiple variables is superior to its single-variable counterpart. Lastly, we demonstrate its potential in reducing the overall computational time and possible memory usage for storing unexplored nodes through numerical experiments on 0-1 knapsack problems.
We consider the classical convex constrained nonconvex quadratic programming problem where the Hessian matrix of the objective to be minimized has r negative eigenvalues, denoted by (QPr). Based on a biconvex programming reformulation in a slightly higher dimension, we propose a novel branch-and-bound algorithm to solve (QP1) and show that it returns an ɛ-approximate solution of (QP1) in at most O(1/√ɛ) iterations. We further extend the new algorithm to solve the general (QPr) with r > 1. Computational comparison shows the efficiency of our proposed global optimization method for small r. Finally, we extend the explicit relaxation approach for (QP1) to (QPr) with r > 1.Summary of Contribution: Nonconvex quadratic program (QP) is a classical optimization problem in operations research. This paper aims at globally solving the QP where the Hessian matrix of the objective to be minimized has r negative eigenvalues. It is known to be nondeterministic polynomial-time hard even when r = 1. This paper presents a novel algorithm to globally solve the QP for r = 1 and then extends to general r. Numerical results demonstrate the superiority of the proposed algorithm in comparison with state-of-the-art algorithms/software for small r.
Zero forcing is a graph coloring process based on the following color change rule: all vertices of a graph G𝐺 are initially colored either blue or white; in each timestep, a white vertex turns blue if it is the only white neighbor of some blue vertex. A zero forcing set of G is a set of blue vertices such that all vertices eventually become blue after iteratively applying the color change rule. The zero forcing number Z(G) is the cardinality of a minimum zero forcing set. In this paper, we propose novel exact algorithms for computing Z(G) based on formulating the zero forcing problem as a two-stage Boolean satisfiability problem. We also propose several heuristics for zero forcing based on iteratively adding blue vertices which color a large part of the remaining white vertices. These heuristics are used to speed up the exact algorithms and can also be of independent interest in approximating Z(G). Computational results on various types of graphs show that, in many cases, our algorithms offer a significant improvement on the state-of-the-art algorithms for zero forcing.Summary of Contribution: This paper proposes novel algorithms and heuristics for an NP-hard graph coloring problem that has numerous applications. Our exact methods combine Boolean satisfiability modeling with a constraint generation framework commonly used in operations research. The paper also includes an analysis of the facets of the polytope associated with this problem and decomposition techniques which can reduce the size of the problem. Our computational approaches are implemented and tested on a wide variety of graphs and are compared with the state-of-the-art algorithms from the literature. We show that our proposed algorithms based on Boolean satisfiability, in conjunction with the heuristics and order-reduction techniques, yield a significant speedup in some cases.
Recent advances in network representation learning have enabled significant improvement in the link prediction task, which is at the core of many downstream applications. As an increasing amount of mobility data become available because of the development of location-based technologies, we argue that this resourceful mobility data can be used to improve link prediction tasks. In this paper, we propose a novel link prediction framework that utilizes user offline check-in behavior combined with user online social relations. We model user offline location preference via a probabilistic factor model and represent user social relations using neural network representation learning. To capture the interrelationship of these two sources, we develop an anchor link method to align these two different user latent representations. Furthermore, we employ locality-sensitive hashing to project the aggregated user representation into a binary matrix, which not only preserves the data structure but also improves the efficiency of convolutional network learning. By comparing with several baseline methods that solely rely on social networks or mobility data, we show that our unified approach significantly improves the link prediction performance.Summary of Contribution: This paper proposes a novel framework that utilizes both user offline and online behavior for social link prediction by developing several machine learning algorithms, such as probabilistic factor model, neural network embedding, anchor link model, and locality-sensitive hashing. The scope and mission has the following aspects: (1) We develop a data and knowledge modeling approach that demonstrates significant performance improvement. (2) Our method can efficiently manage large-scale data. (3) We conduct rigorous experiments on real-world data sets and empirically show the effectiveness and the efficiency of our proposed method. Overall, our paper can contribute to the advancement of social link prediction, which can spur many downstream applications in information systems and computer science.
The exact solution of the NP-hard (nondeterministic polynomial-time hard) maximum cut problem is important in many applications across, for example, physics, chemistry, neuroscience, and circuit layout—which is also due to its equivalence to the unconstrained binary quadratic optimization problem. Leading solution methods are based on linear or semidefinite programming and require the separation of the so-called odd-cycle inequalities. In their groundbreaking research, F. Barahona and A. R. Mahjoub have given an informal description of a polynomial-time algorithm for this problem. As pointed out recently, however, additional effort is necessary to guarantee that the inequalities obtained correspond to facets of the cut polytope. In this paper, we shed more light on a so enhanced separation procedure and investigate experimentally how it performs in comparison with an ideal setting where one could even employ the sparsest, most violated, or geometrically most promising facet-defining odd-cycle inequalities.Summary of Contribution: This paper aims at a better capability to solve binary quadratic optimization or maximum cut problems and their various applications using integer programming techniques. To this end, the paper describes enhancements to a well-known algorithm for the central separation problem arising in this context; it is demonstrated experimentally that these enhancements are worthwhile from a computational point of view. The linear relaxations of the aforementioned problems are typically solved using fewer iterations and cutting planes than with a nonenhanced approach. It is also shown that the enhanced procedure is only slightly inferior to an ideal, enumerative, and, in practice, intractable global cutting-plane selection.
In a minimum load spanning tree (MLST) problem, we are given an undirected graph and nondecreasing load functions for nodes defined on nodes’ degrees in a spanning tree, and the objective is to find a spanning tree that minimizes the maximum load among all nodes. We propose the first O∗(2n)𝑂∗(2𝑛) time exact algorithm for the MLST problem, where n is the number of nodes and O∗ ignores polynomial factor. The algorithm is obtained by repeatedly querying whether a candidate objective value is feasible, where each query can be formulated as a bounded degree spanning tree problem (BDST). We propose a novel solution to BDST by extending an inclusion-exclusion based algorithm. To further enhance the time efficiency of the previous algorithm, we then propose a faster algorithm by generalizing the concept of branching walks. In addition, for the purpose of comparison, we give the first mixed integer linear programming formulation for MLST. In numerical analysis, we consider various load functions on a randomly generated network. The results verify the effectiveness of the proposed algorithms.Summary of Contribution: Minimum load spanning tree (MLST) plays an important role in various applications such as wireless sensor networks (WSNs). In many applications of WSNs, we often need to collect data from all sensors to some specified sink. In this paper, we propose the first exact algorithms for the MLST problem. Besides having theoretical guarantees, our algorithms have extraordinarily good performance in practice. We believe that our results make significant contributions to the field of graph theory, internet of things, and WSNs.
In this paper, we consider the maximum lifetime data gathering tree (MLDT) problem in sensor networks. A data gathering tree is a spanning tree rooted at a specified sink so that every node can send its messages to the sink along the tree. The lifetime of a tree is defined as the minimum lifetime among nodes where each node’s lifetime is determined by its initial energy and transmission load. The MLDT problem is NP-hard, and the state-of-the-art solution formulates a decision version of the problem as an integer linear program (ILP) and then solves it by conducting binary search over all possible lifetimes. In this paper, we first give an ILP for the optimization problem rather than its decision version, and then show that using ILP solvers to solve these programs could be highly inefficient. We then propose a branch-and-bound algorithm that incorporates two novel features. First, the bounding method takes into account integer flows, and contains a new set of constraints. Second, a special set of edges are deleted to reduce the number of subproblems generated by the branching process. Numerical simulations on randomly generated networks show that the proposed algorithm outperforms existing algorithms in terms of the number of solved problem instances in a fixed amount of time.Summary of Contribution: We study the maximum lifetime data gathering tree (MLDT) problem in the context of wireless sensor network. MLDT is a fundamental problem in both computer science and operations research. Since sensor nodes are often resource limited, the data gathering tree must be carefully constructed to prolong the network lifetime. In this paper, we first give an integer linear program for the optimization problem rather than its decision version, and then show that using ILP solvers to solve these programs could be highly inefficient. We then propose a branch and bound algorithm that incorporates two novel features.
We consider a two-player interdiction problem staged over a graph where the attacker’s objective is to minimize the cost of removing edges from the graph so that the defender’s objective, that is, the weight of a minimum spanning tree in the residual graph, is increased up to a predefined level r. Standard approaches for graph interdiction frame this type of problems as bilevel formulations, which are commonly solved by replacing the inner problem by its dual to produce a single-level reformulation. In this paper, we study an alternative integer program derived directly from the attacker’s solution space and show that this formulation yields a stronger linear relaxation than the bilevel counterpart. Furthermore, we analyze the convex hull of the feasible solutions of the problem and identify several families of facet-defining inequalities that can be used to strengthen this integer program. We then proceed by introducing a different formulation defined by a set of so-called supervalid inequalities that may exclude feasible solutions, albeit solutions whose objective value is not better than that of an edge cut of minimum cost. We discuss several computational aspects required for an efficient implementation of the proposed approaches. Finally, we perform an extensive set of computational experiments to test the quality of these formulations, analyzing and comparing the benefits of each model, as well as identifying further enhancements.Summary of Contribution: Network interdiction has received significant attention over the last couple of decades, with a notable peak of interest in recent years. This paper provides an interesting balance between the theoretical and computational aspects of solving a challenging network interdiction problem via integer programming. We present several technical developments, including a detailed study of the problem's solution space, multiple formulations, and a polyhedral analysis of the convex hull of feasible solutions. We then analyze the results of an extensive set of computational experiments that were used to validate the effectiveness of the different methods we developed in this paper.
Network cascades represent a number of real-life applications: social influence, electrical grid failures, viral spread, and so on. The commonality between these phenomena is that they begin from a set of seed nodes and spread to other regions of the network. We consider a variant of a critical node detection problem dubbed the robust critical node fortification problem, wherein the decision maker wishes to fortify nodes (within a budget) to limit the spread of cascading behavior under uncertain conditions. In particular, the arc weights—how much influence one node has on another in the cascade process—are uncertain but are known to lie in some range bounded by a worst-case budget uncertainty. This problem is shown to be NP𝑁𝑃-hard even in the deterministic case. We formulate a mixed-integer program (MIP) to solve the deterministic problem and improve its continuous relaxation via nonlinear constraints and convexification. The robust problem is computationally more difficult, and we present an MIP-based expand-and-cut exact solution algorithm, in which the expansion is enhanced by cutting planes, which are themselves tied to the expansion process. Insights from these exact solutions motivate two novel (interrelated) centrality measures, and a centrality-based heuristic that obtains high-quality solutions within a few seconds. Finally, extensive computational results are given to validate our theoretical developments as well as provide insights into structural properties of the robust problem and its solution.
We consider a problem of ranking and selection via simulation in the context of personalized decision making, in which the best alternative is not universal, but varies as a function of some observable covariates. The goal of ranking and selection with covariates (R&S-C) is to use simulation samples to obtain a selection policy that specifies the best alternative with a certain statistical guarantee for subsequent individuals upon observing their covariates. A linear model is proposed to capture the relationship between the mean performance of an alternative and the covariates. Under the indifference-zone formulation, we develop two-stage procedures for both homoscedastic and heteroscedastic simulation errors, respectively, and prove their statistical validity in terms of average probability of correct selection. We also generalize the well-known slippage configuration and prove that the generalized slippage configuration is the least favorable configuration for our procedures. Extensive numerical experiments are conducted to investigate the performance of the proposed procedures, the experimental design issue, and the robustness to the linearity assumption. Finally, we demonstrate the usefulness of R&S-C via a case study of selecting the best treatment regimen in the prevention of esophageal cancer. We find that by leveraging disease-related personal information, R&S-C can substantially improve patients’ expected quality-adjusted life years by providing a patient-specific treatment regimen.
Distortion risk measure, defined by an integral of a distorted tail probability, has been widely used in behavioral economics and risk management as an alternative to expected utility. The sensitivity of the distortion risk measure is a functional of certain distribution sensitivities. We propose a new sensitivity estimator for the distortion risk measure that uses generalized likelihood ratio estimators for distribution sensitivities as input and establish a central limit theorem for the new estimator. The proposed estimator can handle discontinuous sample paths and distortion functions.
We consider a significant problem that arises in the planning of many projects. Project companies often use outsourced providers that require capacity reservations that must be contracted before task durations are realized. We model these decisions for a company that, given partially characterized distributional information, assumes the worst-case distribution for task durations. Once task durations are realized, the project company makes decisions about fast tracking and outsourced crashing, to minimize the total capacity reservation, fast tracking, crashing, and makespan penalty costs. We model the company’s objective using the target-based measure of minimizing an underperformance riskiness index. We allow for correlation in task performance, and for piecewise linear costs of crashing and makespan penalties. An optimal solution of the discrete, nonlinear model is possible for small to medium size projects. We compare the performance of our model against the best available benchmarks from the robust optimization literature, and show that it provides lower risk and greater robustness to distributional information. Our work thus enables more effective risk minimization in projects, and provides insights about how to make more robust capacity reservation decisions.Summary of Contribution: This work studies a financially significant planning problem that arises in project management. Companies that face uncertainties in project execution may need to reserve capacity with outsourced providers. Given that decision, they further need to plan their operational decisions to protect against a bad outcome. We model and solve this problem via adjustable distributionally robust optimization. While this problem involves two-stage decision making, which is computationally challenging in general, we develop a computationally efficient algorithm to find the exact optimal solution for instances of practical size.
The distributed operating room (OR) scheduling problem aims to find an assignment of surgeries to ORs across collaborating hospitals that share their waiting lists and ORs. We propose a stochastic extension of this problem where surgery durations are considered to be uncertain. In order to obtain solutions for the challenging stochastic model, we use sample average approximation and develop two enhanced decomposition frameworks that use logic-based Benders (LBBD) optimality cuts and binary decision diagram based Benders cuts. Specifically, to the best of our knowledge, deriving LBBD optimality cuts in a stochastic programming context is new to the literature. Our computational experiments on a hospital data set illustrate that the stochastic formulation generates robust schedules and that our algorithms improve the computational efficiency.Summary of Contribution: We propose a new model for an important problem in healthcare scheduling, namely, stochastic distributed operating room scheduling, which is inspired by a current practice in Toronto, Ontario, Canada. We develop two decomposition methods that are computationally faster than solving the model directly via a state-of-the-art solver. We present both some theoretical results for our algorithms and numerical results for the evaluation of the model and algorithms. Compared with its deterministic counterpart in the literature, our model shows improvement in relevant evaluation metrics for the underlying scheduling problem. In addition, our algorithms exploit the structure of the model and improve its solvability. Those algorithms also have the potential to be used to tackle other planning and scheduling problems with a similar structure.
We study a class of sequential defender-attacker optimization problems where the defender’s objective is uncertain and depends on the operations of the attacker, which are represented by a mixed-integer uncertainty set. The defender seeks to hedge against the worst possible data realization, resulting in a robust optimization problem with a mixed-integer uncertainty set, which requires the solution of a challenging mixed-integer problem, which can be seen as a saddle-point problem over a nonconvex domain. We study two exact solution algorithms and present two feature applications for which the uncertainty is naturally modeled as a mixed-integer set. Our computational experiments show that the considered algorithms greatly outperform standard algorithms both in terms of computational time and solution quality. Moreover, our results show that modeling uncertainty with mixed-integer sets, instead of approximating the data using convex sets, results in less conservative solutions, which translates to a lower cost for the defender to protect from uncertainty.Summary of Contribution: We consider a class of defender-attacker problems where the defender has to make operational decisions that depend on uncertain actions from an adversarial attacker. Due to the type of information available to the defender, neither probabilistic modeling, nor robust optimization methods with convex uncertainty sets, are well suited to address the defender’s decision-making problem. Consequently, we frame the defender’s problem as a class of robust optimization problems with a mixed-integer uncertainty sets, and devise two exact algorithms that solve this class of problems. A comprehensive computational study shows that for the considered applications, our algorithms improves the performance of existing robust optimization approaches that can be adapted to solve this class of problems. Moreover, we show how mixed-integer uncertainty sets can reduce the level of over-conservatism that is a known issue of robust optimization approaches.
Constrained submodular function maximization has been used in subset selection problems such as selection of most informative sensor locations. Although these models have been quite popular, the solutions obtained via this approach are unstable to perturbations in data defining the submodular functions. Robust submodular maximization has been proposed as a richer model that aims to overcome this discrepancy as well as increase the modeling scope of submodular optimization. In this work, we consider robust submodular maximization with structured combinatorial constraints and give efficient algorithms with provable guarantees. Our approach is applicable to constraints defined by single or multiple matroids and knapsack as well as distributionally robust criteria. We consider both the offline setting where the data defining the problem are known in advance and the online setting where the input data are revealed over time. For the offline setting, we give a general (nearly) optimal bicriteria approximation algorithm that relies on new extensions of classical algorithms for submodular maximization. For the online version of the problem, we give an algorithm that returns a bicriteria solution with sublinear regret.Summary of Contribution: Constrained submodular maximization is one of the core areas in combinatorial optimization with a wide variety of applications in operations research and computer science. Over the last decades, both communities have been interested on the design and analysis of new algorithms with provable guarantees. Sensor location, influence maximization and data summarization are some of the applications of submodular optimization that lie at the intersection of the aforementioned communities. Particularly, our work focuses on optimizing several submodular functions simultaneously. We provide new insights and algorithms to the offline and online variants of the problem which significantly expand the related literature. At the same time, we provide a computational study that supports our theoretical results.
Providing real-time product recommendations based on consumer profiles and purchase history is a successful marketing strategy in online retailing. However, brick-and-mortar (BAM) retailers have yet to utilize this important promotional strategy because it is difficult to predict consumer preferences as they travel in a physical space but remain anonymous and unidentifiable until checkout. In this paper, we develop such a recommender approach by leveraging the consumer shopping path information generated by radio frequency identification technologies. The system relies on spatial-temporal pattern discovery that measures the similarity between paths and recommends products based on measured similarity. We use a real-world retail data set to demonstrate the feasibility of this real-time recommender system and show that our approach outperforms benchmark methods in key recommendation metrics. Conceptually, this research provides generalizable insights on the correlation between spatial movement and consumer preference. It makes a strong case that the emerging location and path data and the spatial-temporal pattern discovery methods can be effectively utilized for implementable marketing strategies. Managerially, it provides one of the first real-time recommender systems for BAM retailers. Our approach can potentially become the core of the next-generation intelligent shopping environment in which the stores customize marketing efforts to provide real-time, location-aware recommendations.
When generating multirow intersection cuts for mixed-integer linear optimization problems, an important practical question is deciding which intersection cuts to use. Even when restricted to cuts that are facet defining for the corner relaxation, the number of potential candidates is still very large, especially for instances of large size. In this paper, we introduce a subset of intersection cuts based on the infinity norm that is very small, works for relaxations having arbitrary number of rows and, unlike many subclasses studied in the literature, takes into account the entire data from the simplex tableau. We describe an algorithm for generating these inequalities and run extensive computational experiments in order to evaluate their practical effectiveness in real-world instances. We conclude that this subset of inequalities yields, in terms of gap closure, around 50% of the benefits of using all valid inequalities for the corner relaxation simultaneously, but at a small fraction of the computational cost, and with a very small number of cuts.Summary of Contribution: Cutting planes are one of the most important techniques used by modern mixed-integer linear programming solvers when solving a variety of challenging operations research problems. The paper advances the state of the art on general-purpose multirow intersection cuts by proposing a practical and computationally friendly method to generate them.
We develop an exact value function-based approach to solve a class of bilevel integer programs with stochastic right-hand sides. We first study structural properties and design two methods to efficiently construct the value function of a bilevel integer program. Most notably, we generalize the integer complementary slackness theorem to bilevel integer programs. We also show that the value function of a bilevel integer program can be characterized by its values on a set of so-called bilevel minimal vectors. We then solve the value function reformulation of the original bilevel integer program with stochastic right-hand sides using a branch-and-bound algorithm. We demonstrate the performance of our solution methods on a set of randomly generated instances. We also apply the proposed approach to a bilevel facility interdiction problem. Our computational experiments show that the proposed solution methods can efficiently optimize large-scale instances. The performance of our value function-based approach is relatively insensitive to the number of scenarios, but it is sensitive to the number of constraints with stochastic right-hand sides.Summary of Contribution: Bilevel integer programs arise in many different application areas of operations research including supply chain, energy, defense, and revenue management. This paper derives structural properties of the value functions of bilevel integer programs. Furthermore, it proposes exact solution algorithms for a class of bilevel integer programs with stochastic right-hand sides. These algorithms extend the applicability of bilevel integer programs to a larger set of decision-making problems under uncertainty.
We study the chance-constrained bin packing problem, with an application to hospital operating room planning. The bin packing problem allocates items of random sizes that follow a discrete distribution to a set of bins with limited capacity, while minimizing the total cost. The bin capacity constraints are satisfied with a given probability. We investigate a big-M and a 0-1 bilinear formulation of this problem. We analyze the bilinear structure of the formulation and use the lifting techniques to identify cover, clique, and projection inequalities to strengthen the formulation. We show that in certain cases these inequalities are facet-defining for a bilinear knapsack constraint that arises in the reformulation. An extensive computational study is conducted for the operating room planning problem that minimizes the number of open operating rooms. The computational tests are performed using problems generated based on real data from a hospital. A lower-bound improvement heuristic is combined with the cuts proposed in this paper in a branch-and-cut framework. The computations illustrate that the techniques developed in this paper can significantly improve the performance of the branch-and-cut method. Problems with up to 1,000 scenarios are solved to optimality in less than an hour. A safe approximation based on conditional value at risk (CVaR) is also solved. The computations show that the CVaR approximation typically leaves a gap of one operating room (e.g., six instead of five) to satisfy the chance constraint.Summary of Contribution: This paper investigates a branch-and-cut algorithm for a chance-constrained bin packing problem with multiple bins. The chance-constrained bin packing provides a modeling framework for applied operations research problems, such as health care, scheduling, and so on. This paper studies alternative computational approaches to solve this problem. Moreover, this paper uses real data from a hospital operating room planning setting as an application to test the algorithmic ideas. This work, therefore, is at the intersection of computing and operations research. Several interesting ideas are developed and studied. These include a strengthened big-M reformulation, analysis of a bilinear reformulation, and identifying certain facet-defining inequalities for this formulation. This paper also gives a lower-bound generation heuristic for a model that minimizes the number of bins. Computational experiments for an operating room planning model that uses data from a hospital demonstrate the computational improvement and importance of the proposed approaches. The techniques proposed in this paper and computational experiments further enhance the interface of computing and operations research.
On behalf of the Editorial Board, I would like to thank the following people, who acted as Reviewers during the past year. Reviewers are the cornerstone of the peer review system and give their time unselfishly which has been especially difficult during the continuing pandemic. IJOC reviewers, please know you are greatly appreciated!Alice Smith, Editor-in-Chief
We present a spatiotemporal data set of all out-of-hospital sudden cardiac arrests (OHCA) dispatches for the City of Virginia Beach. We also develop a modular toolkit that can be used to process the data and generate problem instances based on user-defined input. The data were collected from multiple sources, and our analysis process was validated by Virginia Beach officials. The data set consists of detailed information about each dispatch made in response to an OHCA; it includes the time the call for service arrived, the response time of the first unit on scene, the address, and the coordinates of each OHCA incident. It also contains detailed spatial information for all existing first-responder stations and both the great-circle and the road distances between all first-responder stations and OHCA incidents. The raw data files were very large in size and were processed using SAS®, MATLAB, and QGIS. In conjunction with the database, we provide a MATLAB code that allows generating multiple random test instances based on user-defined input. The library of problems can be used in healthcare emergency problems and also for facility location models, bilocation problems, and drone studies. The data set was organized such that it can be readily used by researchers in the field of healthcare operations research and those studying the spatiotemporal distribution of OHCAs. Given the difficulty to access OHCA data at the level of detail we provide, the data set will facilitate the implementation of data-driven models to design emergency medical response networks and to study the distribution of OHCAs. Additionally, the provision of data and the toolkit will be very useful in benchmarking algorithms and solvers, which is valuable to the data-driven optimization community in general.Summary of Contribution: The paper provides a data set of spatiotemporal information out-of-hospital cardiac arrests (OHCAs) for the City of Virginia Beach. The complete data set also includes spatial information about all fire, emergency medical services, and police stations in the city and both the road and haversine distances between each pair of stations and OHCA incident. Additionally, we provide a toolkit to generate random instances based on user input. To the best of our knowledge, it is the first time that an OHCA database is made publicly available in such level of detail, and there is no precedent of such in IJOC. OHCAs are a leading cause of death worldwide, and emergency medical services still encounter difficulties in providing care in a timely manner. Given the criticality of OHCAs, we believe that making this data set publicly available can help the implementation of data-driven models by researchers in the field of operations research.
We present alfonso, an open-source Matlab package for solving conic optimization problems over nonsymmetric convex cones. The implementation is based on the authors’ corrected analysis of a method of Skajaa and Ye. It enables optimization over any convex cone as long as a logarithmically homogeneous self-concordant barrier is available for the cone or its dual. This includes many nonsymmetric cones, for example, hyperbolicity cones and their duals (such as sum-of-squares cones), semidefinite and second-order cone representable cones, power cones, and the exponential cone. Besides enabling the solution of problems that cannot be cast as optimization problems over a symmetric cone, algorithms for nonsymmetric conic optimization also offer performance advantages for problems whose symmetric cone programming representation requires a large number of auxiliary variables or has a special structure that can be exploited in the barrier computation. The worst-case iteration complexity of alfonso is the best known for nonsymmetric cone optimization: O(√νlog(1/ε)) iterations to reach an ε-optimal solution, where ν is the barrier parameter of the barrier function used in the optimization. Alfonso can be interfaced with a Matlab function (supplied by the user) that computes the Hessian of a barrier function for the cone. A simplified interface is also available to optimize over the direct product of cones for which a barrier function has already been built into the software. This interface can be easily extended to include new cones. Both interfaces are illustrated by solving linear programs. The oracle interface and the efficiency of alfonso are also demonstrated using an optimal design of experiments problem in which the tailored barrier computation greatly decreases the solution time compared with using state-of-the-art, off-the-shelf conic optimization software.Summary of Contribution: The paper describes an open-source Matlab package for optimization over nonsymmetric cones. A particularly important feature of this software is that, unlike other conic optimization software, it enables optimization over any convex cone as long as a suitable barrier function is available for the cone or its dual, not limiting the user to a small number of specific cones. Nonsymmetric cones for which such barriers are already known include, for example, hyperbolicity cones and their duals (such as sum-of-squares cones), semidefinite and second-order cone representable cones, power cones, and the exponential cone. Thus, the scope of this software is far larger than most current conic optimization software. This does not come at the price of efficiency, as the worst-case iteration complexity of our algorithm matches the iteration complexity of the most successful interior-point methods for symmetric cones. Besides enabling the solution of problems that cannot be cast as optimization problems over a symmetric cone, our software can also offer performance advantages for problems whose symmetric cone programming representation requires a large number of auxiliary variables or has a special structure that can be exploited in the barrier computation. This is also demonstrated in this paper via an example in which our code significantly outperforms Mosek 9 and SCS 2.
Many stochastic systems face a time-dependent demand. Especially in stochastic service systems, for example, in call centers, customers may leave the queue if their waiting time exceeds their personal patience. As discussed in the extant literature, it can be useful to use general distributions to model such customer patience. This paper analyzes the time-dependent performance of a multiserver queue with a nonhomogeneous Poisson arrival process with a time-dependent arrival rate, exponentially distributed processing times, and generally distributed time to abandon. Fast and accurate performance approximations are essential for decision support in such queueing systems, but the extant literature lacks appropriate methods for the setting we consider. To approximate time-dependent performance measures for small- and medium-sized systems, we develop a new stationary backlog-carryover (SBC) approach that allows for the analysis of underloaded and overloaded systems. Abandonments are considered in two steps of the algorithm: (i) in the approximation of the utilization as a reduced arrival stream and (ii) in the approximation of waiting-based performance measures with a stationary model for general abandonments. To improve the approximation quality, we discuss an adjustment to the interval lengths. We present a limit result that indicates convergence of the method for stationary parameters. The numerical study compares the approximation quality of different adjustments to the interval length. The new SBC approach is effective for instances with small numbers of time-dependent servers and gamma-distributed abandonment times with different coefficients of variation and for an empirical distribution of the abandonment times from real-world data obtained from a call center. A discrete-event simulation benchmark confirms that the SBC algorithm approximates the performance of the queueing system with abandonments very well for different parameter configurations.Summary of Contribution: The paper presents a fast and accurate numerical method to approximate the performance measures of a time‐dependent queueing system with generally distributed abandonments. The presented stationary backlog carryover approach with abandonment combines algorithmic ideas with stationary queueing models for generally distributed abandonment times. The reliability of the method is analyzed for transient systems and numerically studied with real‐world data.
We consider a large-scale service system where incoming tasks have to be instantaneously dispatched to one out of many parallel server pools. The user-perceived performance degrades with the number of concurrent tasks and the dispatcher aims at maximizing the overall quality of service by balancing the load through a simple threshold policy. We demonstrate that such a policy is optimal on the fluid and diffusion scales, while only involving a small communication overhead, which is crucial for large-scale deployments. In order to set the threshold optimally, it is important, however, to learn the load of the system, which may be unknown. For that purpose, we design a control rule for tuning the threshold in an online manner. We derive conditions that guarantee that this adaptive threshold settles at the optimal value, along with estimates for the time until this happens. In addition, we provide numerical experiments that support the theoretical results and further indicate that our policy copes effectively with time-varying demand patterns.Summary of Contribution: Data centers and cloud computing platforms are the digital factories of the world, and managing resources and workloads in these systems involves operations research challenges of an unprecedented scale. Due to the massive size, complex dynamics, and wide range of time scales, the design and implementation of optimal resource-allocation strategies is prohibitively demanding from a computation and communication perspective. These resource-allocation strategies are essential for certain interactive applications, for which the available computing resources need to be distributed optimally among users in order to provide the best overall experienced performance. This is the subject of the present article, which considers the problem of distributing tasks among the various server pools of a large-scale service system, with the objective of optimizing the overall quality of service provided to users. A solution to this load-balancing problem cannot rely on maintaining complete state information at the gateway of the system, since this is computationally unfeasible, due to the magnitude and complexity of modern data centers and cloud computing platforms. Therefore, we examine a computationally light load-balancing algorithm that is yet asymptotically optimal in a regime where the size of the system approaches infinity. The analysis is based on a Markovian stochastic model, which is studied through fluid and diffusion limits in the aforementioned large-scale regime. The article analyzes the load-balancing algorithm theoretically and provides numerical experiments that support and extend the theoretical results.
We study a problem of energy-efficiently connecting a symmetric wireless communication network: given an n-vertex graph with edge weights, find a connected spanning subgraph of minimum cost, where the cost is determined by each vertex paying the heaviest edge incident to it in the subgraph. The problem is known to be NP-hard. Strengthening this hardness result, we show that even o(log n)-approximating the difference d between the optimal solution cost and a natural lower bound is NP-hard. Moreover, we show that under the exponential time hypothesis, there are no exact algorithms running in 2o(n) time or in f(d)⋅nO(1)𝑓(𝑑)⋅𝑛𝑂(1) time for any computable function f. We also show that the special case of connecting c network components with minimum additional cost generally cannot be polynomial-time reduced to instances of size cO(1) unless the polynomial-time hierarchy collapses. On the positive side, we provide an algorithm that reconnects O(log n)-connected components with minimum additional cost in polynomial time. These algorithms are motivated by application scenarios of monitoring areas or where an existing sensor network may fall apart into several connected components because of sensor faults. In experiments, the algorithm outperforms CPLEX with known integer linear programming (ILP) formulations when n is sufficiently large compared with c.Summary of Contribution: Wireless sensor networks are used to monitor air pollution, water pollution, and machine health; in forest fire and landslide detection; and in natural disaster prevention. Sensors in wireless sensor networks are often battery-powered and disposable, so one may be interested in lowering the energy consumption of the sensors in order to achieve a long lifetime of the network. We study the min-power symmetric connectivity problem, which models the task of assigning transmission powers to sensors so as to achieve a connected communication network with minimum total power consumption. The problem is NP-hard. We provide perhaps the first parameterized complexity study of optimal and approximate solutions for the problem. Our algorithms work in polynomial time in the scenario where one has to reconnect a sensor network with n sensors and O(log n)-connected components by means of a minimum transmission power increase or if one can find transmission power lower bounds that already yield a network with O(log n)-connected components. In experiments, we show that, in this scenario, our algorithms outperform previously known exact algorithms based on ILP formulations.
This paper proposes an exact algorithm to solve the one-to-one multiobjective shortest path problem. The solution involves determining a set of nondominated paths between two given nodes in a graph that minimizes several objective functions. This study is motivated by the application of this solution method to determine cycling itineraries. The proposed algorithm improves upon a label-correcting algorithm to rapidly solve the problem on large graphs (i.e., up to millions of nodes and edges). To verify the performance of the proposed algorithm, we use computational experiments to compare it with the best-known methods in the literature. The numerical results confirm the efficiency of the proposed algorithm.Summary of Contribution: The paper deals with a classic operations research problem (the one-to-one multiobjective shortest path problem) and is motivated by a real application for cycling itineraries. An efficient method is proposed and is based on a label-correcting algorithm into which several additional improvement techniques are integrated. Computational experiments compare this algorithm with the best-known methods in the literature to validate the performance on large-size graphs (Center for Discrete Mathematics and Theoretical Computer Science (DIMACS) instances from the ninth DIMACS challenge). New instances from the context of cycling itineraries are also proposed.
We consider the problem of identifying the induced star with the largest cardinality open neighborhood in a graph. This problem, also known as the star degree centrality (SDC) problem, is shown to be NP𝒩𝒫-complete. In this work, we first propose a new integer programming (IP) formulation, which has a smaller number of constraints and nonzero coefficients in them than the existing formulation in the literature. We present classes of networks in which the problem is solvable in polynomial time and offer a new proof of NP-completeness that shows the problem remains NP-complete for both bipartite and split graphs. In addition, we propose a decomposition framework that is suitable for both the existing and our formulations. We implement several acceleration techniques in this framework, motivated by techniques used in Benders decomposition. We test our approaches on networks generated based on the Barabási–Albert, Erdös–Rényi, and Watts–Strogatz models. Our decomposition approach outperforms solving the IP formulations in most of the instances in terms of both solution time and quality; this is especially true for larger and denser graphs. We then test the decomposition algorithm on large-scale protein–protein interaction networks, for which SDC is shown to be an important centrality metric.Summary of Contribution: In this study, we first introduce a new integer programming (NIP) formulation for the star degree centrality (SDC) problem in which the goal is to identify the induced star with the largest open neighborhood. We then show that, although the SDC can be efficiently solved in tree graphs, it remains NP-complete in both split and bipartite graphs via a reduction performed from the set cover problem. In addition, we implement a decomposition algorithm motivated by Benders decomposition together with several acceleration techniques to both the NIP formulation and the existing formulation in the literature. Our experimental results indicate that the decomposition implementation on the NIP is the best solution method in terms of both solution time and quality.
We present a decision analytic framework that uses a mathematical model of Chlamydia trachomatis transmission dynamics in two interacting populations using ordinary differential equations. A public health survey informs model parametrization, and analytical findings guide the computational design of the decision-making process. The potential impact of jail-based screen-treat (S-T) programs on community health outcomes is presented. Numerical experiments are conducted for a case study population to quantify the effect and evaluate the cost-effectiveness of considered interventions. Numerical experiments show the effectiveness of increased jail S-T rates on community cases when resources for a community S-T program stays constant. Although this effect decreases when higher S-T rates are in place, jail-based S-T programs are cost-effective relative to community-based programs.Summary of Contribution: Public health programs have been developed to control community-wide infectious diseases and to reduce prevalence of sexually transmitted diseases (STD). These programs can consist of screening and treatment of diseases and behavioral interventions. Public correctional facilities play an important role in operational execution of these public health programs. However, because of lack of capacity and resources, public health programs using correctional facilities are questioned by policy-makers in terms of their costs and benefits. In this article, we present an analytical framework using a computational epidemiology model for supporting public health policy making. The system represents the dynamics of Chlamydia trachomatis transmission in two interacting populations, with an ordinary differential equations-based simulation model. The theoretical epidemic control conditions are derived and numerically tested, which guide the design of simulation experiments. Then cost-effectiveness of the potential policies is analyzed. We also present an extensive sensitivity analyses on model parameters. This study contributes to the computational epidemiology literature by presenting an analytical framework to guide effective simulation experimentation for policy decision making. The presented methodology can be applied to other complex policy and public health problems.
The joint optimization problem of multiresource capacity planning and multitype patient scheduling under uncertain demands and random capacity consumption poses a significant computational challenge. The common practice in solving this problem is to first identify capacity levels and then determine patient scheduling decisions separately, which typically leads to suboptimal decisions that often result in ineffective outcomes of care. In order to overcome these inefficiencies, in this paper, we propose a novel two-stage stochastic optimization model that integrates these two decisions, which can lower costs by exploring the coupling relationship between patient scheduling and capacity configuration. The patient scheduling problem is modeled as a Markov decision process. We first analyze the properties for the multitype patient case under specific assumptions and then establish structural properties of the optimal scheduling policy for the one-type patient case. Based on these findings, we propose optimal solution algorithms to solve the joint optimization problem for this special case. Because it is intractable to solve the original two-stage problem for a general multitype system with large state space, we propose a heuristic policy and a two-stage stochastic mixed-integer programming model solved by the Benders decomposition algorithm, which is further improved by combining an approximate linear program and the look-ahead strategy. To illustrate the efficiency of our approaches and draw managerial insights, we apply our solutions to a data set from the day surgery center of a large public hospital in Shanghai, China. The results show that the joint optimization of capacity planning and patient scheduling could significantly improve the performance. Furthermore, our model can be applied to a rolling-horizon framework to optimize dynamic patient scheduling decisions. Through extensive numerical analyses, we demonstrate that our approaches yield good performances, as measured by the gap against an upper bound, and that these approaches outperform several benchmark policies.Summary of Contribution: First, this paper investigates the joint optimization problem of multiresource capacity planning and multitype patient scheduling under uncertain demands and random capacity consumption, which poses a significant computational challenge. It belongs to the scope of computing and operations research. Second, this paper formulates a mathematical model, establishes optimality properties, proposes solution algorithms, and performs extensive numerical experiments using real-world data. This work includes aspects of dynamic stochastic control, computing algorithms, and experiments. Moreover, this paper is motivated by a practical problem (joint management of capacity planning and patient scheduling in the day surgery center) in our cooperative hospital, which is also key to numerous other applications, for example, the make-to-order manufacturing systems and computing facility systems. By using the optimality properties, solution algorithms, and management insights derived in this paper, the practitioners can be equipped with a decision support tool for efficient and effective operation decisions.
We study the design of large-scale group testing schemes under a heterogeneous population (i.e., subjects with potentially different risk) and with the availability of multiple tests. The objective is to classify the population as positive or negative for a given binary characteristic (e.g., the presence of an infectious disease) as efficiently and accurately as possible. Our approach examines components often neglected in the literature, such as the dependence of testing cost on the group size and the possibility of no testing, which are especially relevant within a heterogeneous setting. By developing key structural properties of the resulting optimization problem, we are able to reduce it to a network flow problem under a specific, yet not too restrictive, objective function. We then provide results that facilitate the construction of the resulting graph and finally provide a polynomial time algorithm. Our case study, on the screening of HIV in the United States, demonstrates the substantial benefits of the proposed approach over conventional screening methods.Summary of Contribution: This paper studies the problem of testing heterogeneous populations in groups in order to reduce costs and hence allow for the use of more efficient tests for high-risk groups. The resulting problem is a difficult combinatorial optimization problem that is NP-complete under a general objective. Using structural properties specific to our objective function, we show that the problem can be cast as a network flow problem and provide a polynomial time algorithm.
In this work, we study the optimal design of two-armed clinical trials to maximize the accuracy of parameter estimation in a statistical model, where the interaction between patient covariates and treatment are explicitly incorporated to enable precision medication decisions. Such a modeling extension leads to significant complexities for the produced optimization problems because they include optimization over design and covariates concurrently. We take a min-max optimization model and minimize (over design) the maximum (over population) variance of the estimated interaction effect between treatment and patient covariates. This results in a min-max bilevel mixed integer nonlinear programming problem, which is notably challenging to solve. To address this challenge, we introduce a surrogate optimization model by approximating the objective function, for which we propose two solution approaches. The first approach provides an exact solution based on reformulation and decomposition techniques. In the second approach, we provide a lower bound for the inner optimization problem and solve the outer optimization problem over the lower bound. We test our proposed algorithms with synthetic and real-world data sets and compare them with standard (re)randomization methods. Our numerical analysis suggests that the proposed approaches provide higher-quality solutions in terms of the variance of estimators and probability of correct selection. We also show the value of covariate information in precision medicine clinical trials by comparing our proposed approaches to an alternative optimal design approach that does not consider the interaction terms between covariates and treatment.Summary of Contribution: Precision medicine is the future of healthcare where treatment is prescribed based on each patient information. Designing precision medicine clinical trials, which are the cornerstone of precision medicine, is extremely challenging because sample size is limited and patient information may be multidimensional. This work proposes a novel approach to optimally estimate the treatment effect for each patient type in a two-armed clinical trial by reducing the largest variance of personalized treatment effect. We use several statistical and optimization techniques to produce efficient solution methodologies. Results have the potential to save countless lives by transforming the design and implementation of future clinical trials to ensure the right treatments for the right patients. Doing so will reduce patient risks and reduce costs in the healthcare system.
Having an interpretable, dynamic length-of-stay model can help hospital administrators and clinicians make better decisions and improve the quality of care. The widespread implementation of electronic medical record (EMR) systems has enabled hospitals to collect massive amounts of health data. However, how to integrate this deluge of data into healthcare operations remains unclear. We propose a framework grounded in established clinical knowledge to model patients’ lengths of stay. In particular, we impose expert knowledge when grouping raw clinical data into medically meaningful variables that summarize patients’ health trajectories. We use dynamic, predictive models to output patients’ remaining lengths of stay, future discharges, and census probability distributions based on their health trajectories up to the current stay. Evaluated with large-scale EMR data, the dynamic model significantly improves predictive power over the performance of any model in previous literature and remains medically interpretable.Summary of Contribution: The widespread implementation of electronic health systems has created opportunities and challenges to best utilize mounting clinical data for healthcare operations. In this study, we propose a new approach that integrates clinical analysis in generating variables and implementations of computational methods. This approach allows our model to remain interpretable to the medical professionals while being accurate. We believe our study has broader relevance to researchers and practitioners of healthcare operations.
In this paper, we consider uncertain second-order cone (SOC) and semidefinite programming (SDP) constraints with polyhedral uncertainty, which are in general computationally intractable. We propose to reformulate an uncertain SOC or SDP constraint as a set of adjustable robust linear optimization constraints with an ellipsoidal or semidefinite representable uncertainty set, respectively. The resulting adjustable problem can then (approximately) be solved by using adjustable robust linear optimization techniques. For example, we show that if linear decision rules are used, then the final robust counterpart consists of SOC or SDP constraints, respectively, which have the same computational complexity as the nominal version of the original constraints. We propose an efficient method to obtain good lower bounds. Moreover, we extend our approach to other classes of robust optimization problems, such as nonlinear problems that contain wait-and-see variables, linear problems that contain bilinear uncertainty, and general conic constraints. Numerically, we apply our approach to reformulate the problem on finding the minimum volume circumscribing ellipsoid of a polytope and solve the resulting reformulation with linear and quadratic decision rules as well as Fourier-Motzkin elimination. We demonstrate the effectiveness and efficiency of the proposed approach by comparing it with the state-of-the-art copositive approach. Moreover, we apply the proposed approach to a robust regression problem and a robust sensor network problem and use linear decision rules to solve the resulting adjustable robust linear optimization problems, which solve the problem to (near) optimality.Summary of Contribution: Computing robust solutions for nonlinear optimization problems with uncertain second-order cone and semidefinite programming constraints are of much interest in real-life applications, yet they are in general computationally intractable. This paper proposes a computationally tractable approximation for such problems. Extensive computational experiments on (i) computing the minimum volume circumscribing ellipsoid of a polytope, (ii) robust regressions, and (iii) robust sensor networks are conducted to demonstrate the effectiveness and efficiency of the proposed approach.
We derive computationally tractable formulations of the robust counterparts of convex quadratic and conic quadratic constraints that are concave in matrix-valued uncertain parameters. We do this for a broad range of uncertainty sets. Our results provide extensions to known results from the literature. We also consider hard quadratic constraints: those that are convex in uncertain matrix-valued parameters. For the robust counterpart of such constraints, we derive inner and outer tractable approximations. As an application, we show how to construct a natural uncertainty set based on a statistical confidence set around a sample mean vector and covariance matrix and use this to provide a tractable reformulation of the robust counterpart of an uncertain portfolio optimization problem. We also apply the results of this paper to norm approximation problems.Summary of Contribution: This paper develops new theoretical results and algorithms that extend the scope of a robust quadratic optimization problem. More specifically, we derive computationally tractable formulations of the robust counterparts of convex quadratic and conic quadratic constraints that are concave in matrix-valued uncertain parameters. We also consider hard quadratic constraints: those that are convex in uncertain matrix-valued parameters. For the robust counterpart of such constraints, we derive inner and outer tractable approximations.
This paper offers a methodological contribution at the intersection of machine learning and operations research. Namely, we propose a methodology to quickly predict expected tactical descriptions of operational solutions (TDOSs). The problem we address occurs in the context of two-stage stochastic programming, where the second stage is demanding computationally. We aim to predict at a high speed the expected TDOS associated with the second-stage problem, conditionally on the first-stage variables. This may be used in support of the solution to the overall two-stage problem by avoiding the online generation of multiple second-stage scenarios and solutions. We formulate the tactical prediction problem as a stochastic optimal prediction program, whose solution we approximate with supervised machine learning. The training data set consists of a large number of deterministic operational problems generated by controlled probabilistic sampling. The labels are computed based on solutions to these problems (solved independently and offline), employing appropriate aggregation and subselection methods to address uncertainty. Results on our motivating application on load planning for rail transportation show that deep learning models produce accurate predictions in very short computing time (milliseconds or less). The predictive accuracy is close to the lower bounds calculated based on sample average approximation of the stochastic prediction programs.
For a mixed-integer linear problem (MIP) with uncertain constraints, the radius of robust feasibility (RRF) determines a value for the maximal size of the uncertainty set such that robust feasibility of the MIP can be guaranteed. The approaches for the RRF in the literature are restricted to continuous optimization problems. We first analyze relations between the RRF of a MIP and its continuous linear (LP) relaxation. In particular, we derive conditions under which a MIP and its LP relaxation have the same RRF. Afterward, we extend the notion of the RRF such that it can be applied to a large variety of optimization problems and uncertainty sets. In contrast to the setting commonly used in the literature, we consider for every constraint a potentially different uncertainty set that is not necessarily full-dimensional. Thus, we generalize the RRF to MIPs and to include safe variables and constraints; that is, where uncertainties do not affect certain variables or constraints. In the extended setting, we again analyze relations between the RRF for a MIP and its LP relaxation. Afterward, we present methods for computing the RRF of LPs and of MIPs with safe variables and constraints. Finally, we show that the new methodologies can be successfully applied to the instances in the MIPLIB 2017 for computing the RRF.Summary of Contribution: Robust optimization is an important field of operations research due to its capability of protecting optimization problems from data uncertainties that are usually defined via so-called uncertainty sets. Intensive research has been conducted in developing algorithmically tractable reformulations of the usually semi-infinite robust optimization problems. However, in applications it also important to construct appropriate uncertainty sets (i.e., prohibiting too conservative, intractable, or even infeasible robust optimization problems due to the choice of the uncertainty set). In doing so, it is useful to know the maximal “size” of a given uncertainty set such that a robust feasible solution still exists. In this paper, we study one notion of “size”: the radius of robust feasibility (RRF). We contribute on the theoretical side by generalizing the RRF to MIPs as well as to include “safe” variables and constraints (i.e., where uncertainties do not affect certain variables or constraints). This allows to apply the RRF to many applications since safe variables and constraints exist in most applications. We also provide first methods for computing the RRF of LPs as well as of MIPs with safe variables and constraints. Finally, we show that the new methodologies can be successfully applied to the instances in the MIPLIB 2017 for computing the RRF.
This paper studies the team orienteering problem, where the arrival time and service time affect the collection of profits. Such interactions result in a nonconcave profit function. This problem integrates the aspect of time scheduling into the routing decision, which can be applied in humanitarian search and rescue operations where the survival rate declines rapidly. Rescue teams are needed to help trapped people in multiple affected sites, whereas the number of people who could be saved depends as well on how long a rescue team spends at each site. Efficient allocation and scheduling of rescue teams is critical to ensure a high survival rate. To solve the problem, we formulate a mixed-integer nonconcave programming model and propose a Benders branch-and-cut algorithm, along with valid inequalities for tightening the upper bound. To solve it more effectively, we introduce a hybrid heuristic that integrates a modified coordinate search (MCS) into an iterated local search. Computational results show that valid inequalities significantly reduce the optimality gap, and the proposed exact method is capable of solving instances where the mixed-integer nonlinear programming solver SCIP fails in finding an optimal solution. In addition, the proposed MCS algorithm is highly efficient compared with other benchmark approaches, whereas the hybrid heuristic is proven to be effective in finding high-quality solutions within short computing times. We also demonstrate the performance of the heuristic with the MCS using instances with up to 100 customers.Summary of Contribution: Motivated by search and rescue (SAR) operations, we consider a generalization of the well-known team orienteering problem (TOP) to incorporate a nonlinear time-varying profit function in conjunction with routing and scheduling decisions. This paper expands the envelope of operations research and computing in several ways. To address the scalability issue of this highly complex combinatorial problem in an exact manner, we propose a Benders branch-and-cut (BBC) algorithm, which allows us to efficiently deal with the nonconcave component. This BBC algorithm is computationally enhanced through valid inequalities used to strengthen the bounds of the BBC. In addition, we propose a highly efficient hybrid heuristic that integrates a modified coordinate search into an iterated local search. It can quickly produce high-quality solutions to this complex problem. The performance of our solution algorithms is demonstrated through a series of computational experiments.
Suppose a target is hidden in one of the vertices of an edge-weighted graph according to a known probability distribution. Starting from a fixed root node, an expanding search visits the vertices sequentially until it finds the target, where the next vertex can be reached from any of the previously visited vertices. That is, the time to reach the next vertex equals the shortest-path distance from the set of all previously visited vertices. The expanding search problem then asks for a sequence of the nodes, so as to minimize the expected time to finding the target. This problem has numerous applications, such as searching for hidden explosives, mining coal, and disaster relief. In this paper, we develop exact algorithms and heuristics, including a branch-and-cut procedure, a greedy algorithm with a constant-factor approximation guarantee, and a local search procedure based on a spanning-tree neighborhood. Computational experiments show that our branch-and-cut procedure outperforms existing methods for instances with nonuniform probability distributions and that both our heuristics compute near-optimal solutions with little computational effort.Summary of Contribution: This paper studies new algorithms for the expanding search problem, which asks to search a graph for a target hidden in one of the nodes according to a known probability distribution. This problem has applications such as searching for hidden explosives, mining coal, and disaster relief. We propose several new algorithms, including a branch-and-cut procedure, a greedy algorithm, and a local search procedure; and we analyze their performance both experimentally and theoretically. Our analysis shows that the algorithms improve on the performance of existing methods and establishes the first constant-factor approximation guarantee for this problem.
Given an integer dimension K and a simple, undirected graph G with positive edge weights, the Distance Geometry Problem (DGP) aims to find a realization function mapping each vertex to a coordinate in RKR𝐾 such that the distance between pairs of vertex coordinates is equal to the corresponding edge weights in G. The so-called discretization assumptions reduce the search space of the realization to a finite discrete one, which can be explored via the branch-and-prune (BP) algorithm. Given a discretization vertex order in G, the BP algorithm constructs a binary tree where the nodes at a layer provide all possible coordinates of the vertex corresponding to that layer. The focus of this paper is on finding optimal BP trees for a class of discretizable DGPs. More specifically, we aim to find a discretization vertex order in G that yields a BP tree with the least number of branches. We propose an integer programming formulation and three constraint programming formulations that all significantly outperform the state-of-the-art cutting-plane algorithm for this problem. Moreover, motivated by the difficulty in solving instances with a large and low-density input graph, we develop two hybrid decomposition algorithms, strengthened by a set of valid inequalities, which further improve the solvability of the problem.Summary of Contribution: We present a new model to solve a combinatorial optimization problem on graphs, MIN DOUBLE, which comes from the highly active area of distance geometry and has applications in a wide variety of fields. We use integer programming (IP) and present the first constraint programming (CP) models and hybrid decomposition methods, implemented as a branch-and-cut procedure, for MIN DOUBLE. Through an extensive computational study, we show that our approaches advance the state of the art for MIN DOUBLE. We accomplish this by not only combining generic techniques from IP and CP but also exploring the structure of the problem in developing valid inequalities and variable fixing rules. Our methods significantly improve the solvability of MIN DOUBLE, which we believe can also provide insights for tackling other problem classes and applications.
We solve the type-2 assembly line balancing problem in the presence of sequence-dependent setup times, denoted SUALBP-2. The problem consists of a set of tasks of a product, requiring to be processed in different assembly stations. Each task has a definite processing and setup times. The magnitude of setup times for each task is dependent on the processing sequence within each station. Processing and setup times of tasks assigned to each station constitute the station time. The goal is to minimize the cycle time (the maximum station time) by optimally (i) assigning tasks to assembly stations and (ii) sequencing these tasks within each station. To solve this challenging optimization problem, we first improve upon an existing mixed-integer programming (MIP) model by our proposed lower and upper bounds. These enhancements reduce the MIP model’s (solved CPLEX) average optimality gap from 41.61% to 20.77% on extra-large instances of the problem. To further overcome the intractability of the MIP model, we develop an exact logic-based Benders decomposition (LBBD) algorithm. The LBBD algorithm effectively incorporates a novel two-phase solution approach, the lower and upper bounds, various preprocessing techniques, relaxations, and valid inequalities. Using existing benchmarks in the literature, we demonstrate that our LBBD algorithm finds integer feasible solutions for 100% of all 788 instances (64% for the MIP), verifies optimality for 47% of instances (37% for the MIP), and achieves an average optimality gap of 5.04% (7.72% for the MIP obtained over 64% solved small instances). The LBBD algorithm also significantly reduces the computational time required to solve these benchmarks.Summary of Contribution: Assembly line balancing plays a crucial role in productivity enhancement in manufacturing and service companies. A balanced assembly line ensures higher throughput rate and fairer distribution of workload among assembly stations (workers). Assembly line balancing, in its simplest form, is one of the most challenging combinatorial optimization problems. Its complexity is further intensified when the sequence of executing tasks assigned to each station influences the magnitude of the setup performed between any two successive tasks. In view of such complexity, most assembly line balancing problems have been solved by randomized search techniques that do not provide any guarantee on the quality of solutions found. The mission of this paper is to understand whether there is any special structure within the existing mathematical models in the literature and, if so, exploit them toward developing computationally efficient exact techniques that can provide guarantee on the quality of solutions. Indeed, we demonstrate that such a special mathematical structure exists and we thus develop the first decomposition technique in form of a logic-based Benders decomposition (LBBD) to efficiently solve the type-2 sequence-dependent assembly line balancing problem. Specifically, we show that our LBBD significantly reduces cycle time and the time required for decision making. Our LBBD generalizes the scope of exact techniques for decision-making beyond the assembly line problems and is extendable to many other shop scheduling problems that arrange their stations (machines) serially and there are sequence-dependent setup times among their tasks.
Randomized decision making refers to the process of making decisions randomly according to the outcome of an independent randomization device, such as a dice roll or a coin flip. The concept is unconventional, and somehow counterintuitive, in the domain of mathematical programming, in which deterministic decisions are usually sought even when the problem parameters are uncertain. However, it has recently been shown that using a randomized, rather than a deterministic, strategy in nonconvex distributionally robust optimization (DRO) problems can lead to improvements in their objective values. It is still unknown, though, what is the magnitude of improvement that can be attained through randomization or how to numerically find the optimal randomized strategy. In this paper, we study the value of randomization in mixed-integer DRO problems and show that it is bounded by the improvement achievable through its continuous relaxation. Furthermore, we identify conditions under which the bound is tight. We then develop algorithmic procedures, based on column generation, for solving both single- and two-stage linear DRO problems with randomization that can be used with both moment-based and Wasserstein ambiguity sets. Finally, we apply the proposed algorithm to solve three classical discrete DRO problems: the assignment problem, the uncapacitated facility location problem, and the capacitated facility location problem and report numerical results that show the quality of our bounds, the computational efficiency of the proposed solution method, and the magnitude of performance improvement achieved by randomized decisions.Summary of Contribution: In this paper, we present both theoretical results and algorithmic tools to identify optimal randomized strategies for discrete distributionally robust optimization (DRO) problems and evaluate the performance improvements that can be achieved when using them rather than classical deterministic strategies. On the theory side, we provide improvement bounds based on continuous relaxation and identify the conditions under which these bound are tight. On the algorithmic side, we propose a finitely convergent, two-layer, column-generation algorithm that iterates between identifying feasible solutions and finding extreme realizations of the uncertain parameter. The proposed algorithm was implemented to solve distributionally robust stochastic versions of three classical optimization problems and extensive numerical results are reported. The paper extends a previous, purely theoretical work of the first author on the idea of randomized strategies in nonconvex DRO problems by providing useful bounds and algorithms to solve this kind of problems.
This paper considers a class of two-stage stochastic mixed-integer optimization problems where, for a given first-stage solution, we can determine the optimal values of recourse variables sequentially. This class of problems arises in a wide variety of applications. In the case of multivariate discrete distributions for uncertain parameters, a standard stochastic programming formulation of these problems involves an exponential number of scenarios, therefore an exponential number of variables and constraints. We propose a new mixed-integer programming modeling approach where the number of variables and constraints is independent of the number of scenarios and scales at most pseudopolynomially with the problem size. The proposed modeling approach relies on state variables that track the system’s state as the uncertainty realizes sequentially. We demonstrate the advantages of the proposed approach in two applications arising in project scheduling and operating room allocation.Summary of Contribution: This paper proposes a new modeling approach for a class of two-stage stochastic optimization problems that is computationally more efficient than the traditional scenario-based stochastic integer programming models. The proposed modeling approach relies on state variables that track the system's state as the uncertainty realizes sequentially. We demonstrated the efficiency of the proposed approach by computational results on two applications in project scheduling and operating room allocation.
We consider the deterministic inventory routing problem over a discrete finite time horizon. Given clients on a metric, each with daily demands that must be delivered from a depot and holding costs over the planning horizon, an optimal solution selects a set of daily tours through a subset of clients to deliver all demands before they are due and minimizes the total holding and tour routing costs over the horizon. In the capacitated case, a limited number of vehicles are available, where each vehicle makes at most one trip per day. Each trip from the depot is allowed to carry a limited amount of supply to deliver. We develop fast heuristics for both cases by solving a family of prize-collecting Steiner tree instances. Computational experiments show our heuristics can find near-optimal solutions for both cases and substantially reduce the runtime compared with a pure mixed integer programming formulation approach.
For capacitated multi-item lot sizing problems, we propose a predictive search method that integrates machine learning/advanced analytics, mathematical programming, and heuristic search into a single framework. Advanced analytics can predict the probability that an event will happen and has been applied to pressing industry issues, such as credit scoring, risk management, and default management. Although little research has applied such technique for lot sizing problems, we observe that advanced analytics can uncover optimal patterns of setup variables given properties associated with the problems, such as problem attributes, and solution values yielded by linear programming relaxation, column generation, and Lagrangian relaxation. We, therefore, build advanced analytics models that yield information about how likely a solution pattern is the same as the optimum, which is insightful information used to partition the solution space into incumbent, superincumbent, and nonincumbent regions where an analytics-driven heuristic search procedure is applied to build restricted subproblems. These subproblems are solved by a combined mathematical programming technique to improve solution quality iteratively. We prove that the predictive search method can converge to the global optimal solution point. The discussion is followed by computational tests, where comparisons with other methods indicate that our approach can obtain better results for the benchmark problems than other state-of-the-art methods.Summary of Contribution: In this study, we propose a predictive search method that integrates machine learning/advanced analytics, mathematical programming, and heuristic search into a single framework for capacitated multi-item lot sizing problems. The advanced analytics models are used to yield information about how likely a solution pattern is the same as the optimum, which is insightful information used to divide the solution space into incumbent, superincumbent, and nonincumbent regions where an analytics-driven heuristic search procedure is applied to build restricted subproblems. These subproblems are solved by a combined mathematical programming technique to improve solution quality iteratively. We prove that the predictive search method can converge to the global optimal solution point. Through computational tests based on benchmark problems, we observe that the proposed approach can obtain better results than other state-of-the-art methods.
This paper studies the reliable uncapacitated facility location problem in which facilities are subject to uncertain disruptions. A two-stage distributionally robust model is formulated, which optimizes the facility location decisions so as to minimize the fixed facility location cost and the expected transportation cost of serving customers under the worst-case disruption distribution. The model is formulated in a general form, where the uncertain joint distribution of disruptions is partially characterized and is allowed to have any prespecified dependency structure. This model extends several related models in the literature, including the stochastic one with explicitly given disruption distribution and the robust one with moment information on disruptions. An efficient cutting plane algorithm is proposed to solve this model, where the separation problem is solved respectively by a polynomial-time algorithm in the stochastic case and by a column generation approach in the robust case. Extensive numerical study shows that the proposed cutting plane algorithm not only outperforms the best-known algorithm in the literature for the stochastic problem under independent disruptions but also efficiently solves the robust problem under correlated disruptions. The practical performance of the robust models is verified in a simulation based on historical typhoon data in China. The numerical results further indicate that the robust model with even a small amount of information on disruption correlation can mitigate the conservativeness and improve the location decision significantly.Summary of Contribution: In this paper, we study the reliable uncapacitated facility location problem under uncertain facility disruptions. The problem is formulated as a two-stage distributionally robust model, which generalizes several related models in the literature, including the stochastic one with explicitly given disruption distribution and the robust one with moment information on disruptions. To solve this generalized model, we propose a cutting plane algorithm, where the separation problem is solved respectively by a polynomial-time algorithm in the stochastic case and by a column generation approach in the robust case. The efficiency and effectiveness of the proposed algorithm are validated through extensive numerical experiments. We also conduct a data-driven simulation based on historical typhoon data in China to verify the practical performance of the proposed robust model. The numerical results further reveal insights into the value of information on disruption correlation in improving the robust location decisions.
Quantum annealing (QA) can be used to quickly obtain near-optimal solutions for quadratic unconstrained binary optimization (QUBO) problems. In QA hardware, each decision variable of a QUBO should be mapped to one or more adjacent qubits in such a way that pairs of variables defining a quadratic term in the objective function are mapped to some pair of adjacent qubits. However, qubits have limited connectivity in existing QA hardware. This has spurred work on preprocessing algorithms for embedding the graph representing problem variables with quadratic terms into the hardware graph representing qubits adjacencies, such as the Chimera graph in hardware produced by D-Wave Systems. In this paper, we use integer linear programming to search for an embedding of the problem graph into certain classes of minors of the Chimera graph, which we call template embeddings. One of these classes corresponds to complete bipartite graphs, for which we show the limitation of the existing approach based on minimum odd cycle transversals (OCTs). One of the formulations presented is exact and thus can be used to certify the absence of a minor embedding using that template. On an extensive test set consisting of random graphs from five different classes of varying size and sparsity, we can embed more graphs than a state-of-the-art OCT-based approach, our approach scales better with the hardware size, and the runtime is generally orders of magnitude smaller.Summary of Contribution: Our work combines classical and quantum computing for operations research by showing that integer linear programming can be successfully used as a preprocessing step for adiabatic quantum optimization. We use it to determine how a quadratic unconstrained binary optimization problem can be solved by a quantum annealer in which the qubits are coupled as in a Chimera graph, such as in the quantum annealers currently produced by D-Wave Systems. The paper also provides a timely introduction to adiabatic quantum computing and related work on minor embeddings.
To reduce unproductive picker walking in traditional picker-to-parts warehousing systems, automated guided vehicles (AGVs) are used to support human order pickers. In an AGV-assisted order-picking system, each human order picker is accompanied by an AGV during the order-picking process. AGVs receive the picked items and, once a picking order is complete, autonomously bring the collected items to the shipping area. Meanwhile, a new AGV is requested to meet the picker at the first storage position of the next picking order. Thus, the picker does not have to return to a central depot and continuously picks order after order. This paper addresses both the routing of an AGV-assisted picker through a single-block, parallel-aisle warehouse and the sequencing of incoming orders. We present an exact polynomial time routing algorithm for the case of a given order sequence, which is an extension of the algorithm of Ratliff and Rosenthal [Ratliff HD, Rosenthal AS (1983) Order-picking in a rectangular warehouse: A solvable case of the traveling salesman problem. Oper. Res. 1(3):507–521], and a heuristic for the case in which order sequencing is part of the problem. In addition, we investigate the use of highly effective traveling salesman problem (TSP) solvers that can be applied after a transformation of both problem types into a standard TSP. The numerical studies address the performance of these methods and study the impact of AGV usage on picker travel: by using AGVs to avoid returns to the depot and by sequencing in (near-) optimal fashion, picker walking can be reduced by about 20% compared with a traditional setting. Sharing AGVs among the picker workforce enables a pooling effect so that, in larger warehouses, only about 1.5 AGVs per picker are required to avoid picker waiting.Summary of Contribution: New technologies, such as automatic guided vehicles (AGVs) are currently considered as options to increase the efficiency of the order-picking process in warehouses, which is responsible for a large part of operational warehousing costs. In addition, picker-routing decisions are more and more often based on algorithmic decision support because of their relevance for decreasing unproductive picker walking time. This paper addresses both aspects and investigates routing algorithms for AGV-assisted order picking in parallel-aisle warehouses. We present a dynamic programming routine with polynomial runtime to solve the problem variant in which the sequence of picking orders is fixed. For the variant in which this sequence is a decision, we show that the problem becomes NP-hard, and we propose a greedy heuristic and investigate the use of state-of-the-art exact and heuristic traveling salesman problem solution methods to address the problem. The numerical studies demonstrate the effectiveness of the algorithms and indicate that AGV assistance promises strong improvements in the order-fulfillment process. Because of the practical relevance of AGV-assisted order picking and the presented algorithmic contributions, we believe that the paper is relevant for practitioners and researchers alike.
In many engineered systems, optimization is used for decision making at time scales ranging from real-time operation to long-term planning. This process often involves solving similar optimization problems over and over again with slightly modified input parameters, often under tight latency requirements. We consider the problem of using the information available through this repeated solution process to learn important characteristics of the optimal solution as a function of the input parameters. Our proposed method is based on learning relevant sets of active constraints, from which the optimal solution can be obtained efficiently. Using active sets as features preserves information about the physics of the system, enables interpretable results, accounts for relevant safety constraints, and is easy to represent and encode. However, the total number of active sets is also very large, as it grows exponentially with system size. The key contribution of this paper is a streaming algorithm that learns the relevant active sets from training samples consisting of the input parameters and the corresponding optimal solution, without any restrictions on the problem type, problem structure or probability distribution of the input parameters. The algorithm comes with theoretical performance guarantees and is shown to converge fast for problem instances with a small number of relevant active sets. It can thus be used to establish simultaneously learn the relevant active sets and the practicability of the learning method. Through case studies in optimal power flow, supply chain planning, and shortest path routing, we demonstrate that often only a few active sets are relevant in practice, suggesting that active sets provide an appropriate level of abstraction for a learning algorithm to target.
This paper proposes a cluster-aware supervised learning (CluSL) framework, which integrates the clustering analysis with supervised learning. The objective of CluSL is to simultaneously find the best clusters of the data points and minimize the sum of loss functions within each cluster. This framework has many potential applications in healthcare, operations management, manufacturing, and so on. Because CluSL, in general, is nonconvex, we develop a regularized alternating minimization (RAM) algorithm to solve it, where at each iteration, we penalize the distance between the current clustering solution and the one from the previous iteration. By choosing a proper penalty function, we show that each iteration of the RAM algorithm can be computed efficiently. We further prove that the proposed RAM algorithm will always converge to a stationary point within a finite number of iterations. This is the first known convergence result in cluster-aware learning literature. Furthermore, we extend CluSL to the high-dimensional data sets, termed the F-CluSL framework. In F-CluSL, we cluster features and minimize loss function at the same time. Similarly, to solve F-CluSL, a variant of the RAM algorithm (i.e., F-RAM) is developed and proven to be convergent to an ∈∈-stationary point. Our numerical studies demonstrate that the proposed CluSL and F-CluSL can outperform the existing ones such as random forests and support vector classification, both in the interpretability of learning results and in prediction accuracy.Summary of Contribution: Aligned with the mission and scope of the INFORMS Journal on Computing, this paper proposes a cluster-aware supervised learning (CluSL) framework, which integrates the clustering analysis with supervised learning. Because CluSL is, in general, nonconvex, a regularized alternating projection algorithm is developed to solve it and is proven to always find a stationary solution. We further generalize the framework to the high-dimensional data set, F-CluSL. Our numerical studies demonstrate that the proposed CluSL and F-CluSL can deliver more interpretable learning results and outperform the existing ones such as random forests and support vector classification in computational time and prediction accuracy.
Numerical predictive modeling is widely used in different application domains. Although many modeling techniques have been proposed, and a number of different aggregate accuracy metrics exist for evaluating the overall performance of predictive models, other important aspects, such as the reliability (or confidence and uncertainty) of individual predictions, have been underexplored. We propose to use estimated absolute prediction error as the indicator of individual prediction reliability, which has the benefits of being intuitive and providing highly interpretable information to decision makers, as well as allowing for more precise evaluation of reliability estimation quality. As importantly, the proposed reliability indicator allows the reframing of reliability estimation itself as a canonical numeric prediction problem, which makes the proposed approach general-purpose (i.e., it can work in conjunction with any outcome prediction model), alleviates the need for distributional assumptions, and enables the use of advanced, state-of-the-art machine learning techniques to learn individual prediction reliability patterns directly from data. Extensive experimental results on multiple real-world data sets show that the proposed machine learning-based approach can significantly improve individual prediction reliability estimation as compared with a number of baselines from prior work, especially in more complex predictive scenarios.
Predicting stock return volatility is the key to investment and risk management. Traditional volatility-forecasting methods primarily rely on stochastic models. More recently, many machine-learning approaches, particularly text-mining techniques, have been implemented to predict stock return volatility, thus taking advantage of the availability of large amounts of unstructured data such as firm financial reports. Most existing studies develop simple but effective models to analyze text, such as dictionary-based matching algorithms that use a set of manually constructed keywords. However, the latent and deep semantics encoded in text are usually neglected. In this study, we build on recent progress in representation learning and propose a novel word-embedding method that incorporates external knowledge from a well-known finance-domain lexicon (the Loughran and McDonald (2011) word list), which helps us learn semantic relationships among words in firm reports for better volatility prediction. Using over 10 years of annual reports from Russell 3000 firms, we empirically show that, compared with cutting-edge benchmarks, our proposed method achieves significant improvement in terms of prediction error, for example, a 28.4% reduction on average. We also discuss the practical and methodological implications of our findings. Our financial-specific word-embedding program is available as open-source information so that researchers can use it to analyze financial reports and assess financial risks.Summary of Contribution: Predicting stock return volatility is the key to investment and risk management. Traditional volatility-forecasting methods primarily rely on stochastic models. More recently, many machine-learning, especially text-mining, techniques have been developed to predict stock return volatility given the availability of a large amount of unstructured data, such as firm annual reports. Most existing research develops simple but effective approaches, for example, manually constructing a set of keywords to analyze texts. However, the latent and deep semantics encoded in texts are usually ignored. In this research, we build on recent progress in representation learning and propose a novel word-embedding method that incorporates external knowledge from the finance-domain lexicon of Loughran and McDonald (2011), which helps us learn the semantic relationships among words in firm annual reports for better volatility prediction. In this study, we make the following contributions. First, methodologically, we are among the first to incorporate finance-specific lexicon into representation learning for stock volatility prediction. We propose a novel knowledge-driven text-embedding model that is trained on a large amount of unstructured textual data to learn high quality word embedding. Our proposed approach is effective in predicting stock return volatility, and the approach can potentially have broader applications. Second, substantively, we empirically show that the domain lexicon enhanced text representation learning can indeed significantly improve the performance, compared with bag-of-words models and generic word embedding for volatility prediction. Domain knowledge combined with text learning plays a critical enabling role in understanding financial reports. Third, our method adds on to existing literature on designing financial information systems by incorporating ontology knowledge, common-sense knowledge, and general prior knowledge.
Detecting product adoption intentions on social media could yield significant value in a wide range of applications, such as personalized recommendations and targeted marketing. In the literature, no study has explored the detection of product adoption intentions on social media, and only a few relevant studies have focused on purchase intention detection for products in one or several categories. Focusing on a product category rather than a specific product is too coarse-grained for precise advertising. Additionally, existing studies primarily focus on using one type of text representation in target social media posts, ignoring the major yet unexplored potential of fusing different text representations. In this paper, we first formulate the problem of product adoption intention mining and demonstrate the necessity of studying this problem and its practical value. To detect a product adoption intention for an individual product, we propose a novel and general multiview deep learning model that simultaneously taps into the capability of multiview learning in leveraging different representations and deep learning in learning latent data representations using a flexible nonlinear transformation. Specifically, the proposed model leverages three different text representations from a multiview perspective and takes advantage of local and long-term word relations by integrating convolutional neural network (CNN) and long short-term memory (LSTM) modules. Extensive experiments on three Twitter datasets demonstrate the effectiveness of the proposed multiview deep learning model compared with the existing benchmark methods. This study also significantly contributes research insights to the literature about intention mining and provides business value to relevant stakeholders such as product providers.
Monte Carlo simulation is a commonly used tool for evaluating the performance of complex stochastic systems. In practice, simulation can be expensive, especially when comparing a large number of alternatives, thus motivating the need to intelligently allocate simulation replications. Given a finite set of alternatives whose means are estimated via simulation, we consider the problem of determining the subset of alternatives that have means smaller than a fixed threshold. A dynamic sampling procedure that possesses not only asymptotic optimality, but also desirable finite-sample properties is proposed. Theoretical results show that there is a significant difference between finite-sample optimality and asymptotic optimality. Numerical experiments substantiate the effectiveness of the new method.Summary of Contribution: Simulation is an important tool to estimate the performance of complex stochastic systems. We consider a feasibility determination problem of identifying all those among a finite set of alternatives with mean smaller than a given threshold, in which the means are unknown but can be estimated by sampling replications via stochastic simulation. This problem appears widely in many applications, including call center design and hospital resource allocation. Our work considers how to intelligently allocate simulation replications to different alternatives for efficiently finding the feasible alternatives. Previous work focuses on the asymptotic properties of the sampling allocation procedures, whereas our contribution lies in developing a finite-budget allocation rule that possesses both asymptotic optimality and desirable finite-budget properties.
A quantile is a popular performance measure for a stochastic system to evaluate its variability and risk. To reduce the risk, selecting the actions that minimize the tail quantiles of some loss distributions is typically of interest for decision makers. When the loss distribution is observed via simulations, evaluating and optimizing its quantile can be challenging, especially when the simulations are expensive as it may cost a large number of simulation runs to obtain accurate quantile estimators. In this work, we propose a multilevel metamodel (cokriging)-based algorithm to optimize quantiles more efficiently. Utilizing nondecreasing properties of quantiles, we first search on cheaper and informative lower quantiles, which are more accurate and easier to optimize. The quantile level iteratively increases to the objective level, and the search has a focus on the possible promising regions identified by the previous levels. This enables us to leverage the accurate information from the lower quantiles to find the optimums faster and improve algorithm efficiency.
With the rapid development of computing technology, using parallel computing to solve large-scale ranking-and-selection (R&S) problems has emerged as an important research topic. However, direct implementation of traditionally fully sequential procedures in parallel computing environments may encounter various problems. First, the scheme of all-pairwise comparisons, which is commonly used in fully sequential procedures, requires a large amount of computation and significantly slows down the selection process. Second, traditional fully sequential procedures require frequent communication and coordination among processors, which are also not efficient in parallel computing environments. In this paper, we propose three modifications on one classical fully sequential procedure, Paulson’s procedure, to speed up its selection process in parallel computing environments. First, we show that if no common random numbers are used, then we can significantly reduce the computation spent on all-pairwise comparisons at each round. Second, by batching different alternatives, we show that we can reduce the communication cost among the processors, leading the procedure to achieve better performance. Third, to boost the procedure’s final-stage selection, when the number of surviving alternatives is less than the number of processors, we suggest to sample all surviving alternatives to the maximal number of observations that they should take. We show that, after these modifications, the procedure remains statistically valid and is more efficient compared with existing parallel procedures in the literature.Summary of Contribution: Ranking and selection (R&S) is a branch of simulation optimization, which is an important area of operations research. In recent years, using parallel computing to solve large-scale R&S problems has emerged as an important research topic, and this research topic is naturally situated in the intersection of computing and operations research. In this paper, we consider how to improve a fully sequential R&S procedure, namely, Paulson’s procedure, to reduce the high computational complexity of all-pairwise comparisons and the burden of frequent communications and coordination, so that the procedure is more suitable and more efficient in solving large-scale R&S problems using parallel computing environments that are becoming ubiquitous and accessible for ordinary users. The procedure designed in this paper appears more efficient than the ones available in the literature and is capable of solving R&S problems with over a million alternatives in a parallel computing environment with 96 processors. The paper also extended the theory of R&S by showing that the all-pairwise comparisons may be decomposed so that the computational complexity may be reduced significantly, which drastically improves the efficiency of all-pairwise comparisons as observed in numerical experiments.
Kriging or Gaussian process models are popular metamodels (surrogate models or emulators) of simulation models; these metamodels give predictors for input combinations that are not simulated. To validate these metamodels for computationally expensive simulation models, the analysts often apply computationally efficient cross-validation. In this paper, we derive new statistical tests for so-called leave-one-out cross-validation. Graphically, we present these tests as scatterplots augmented with confidence intervals that use the estimated variances of the Kriging predictors. To estimate the true variances of these predictors, we might use bootstrapping. Like other statistical tests, our tests—with or without bootstrapping—have type I and type II error probabilities; to estimate these probabilities, we use Monte Carlo experiments. We also use such experiments to investigate statistical convergence. To illustrate the application of our tests, we use (i) an example with two inputs and (ii) the popular borehole example with eight inputs.Summary of Contribution: Simulation models are very popular in operations research (OR) and are also known as computer simulations or computer experiments. A popular topic is design and analysis of computer experiments. This paper focuses on Kriging methods and cross-validation methods applied to simulation models; these methods and models are often applied in OR. More specifically, the paper provides the following; (1) the basic variant of a new statistical test for leave-one–out cross-validation; (2) a bootstrap method for the estimation of the true variance of the Kriging predictor; and (3) Monte Carlo experiments for the evaluation of the consistency of the Kriging predictor, the convergence of the Studentized prediction error to the standard normal variable, and the convergence of the expected experimentwise type I error rate to the prespecified nominal value. The new statistical test is illustrated through examples, including the popular borehole model.
Gaussian process (GP) model based optimization is widely applied in simulation and machine learning. In general, it first estimates a GP model based on a few observations from the true response and then uses this model to guide the search, aiming to quickly locate the global optimum. Despite its successful applications, it has several limitations that may hinder its broader use. First, building an accurate GP model can be difficult and computationally expensive, especially when the response function is multimodal or varies significantly over the design space. Second, even with an appropriate model, the search process can be trapped in suboptimal regions before moving to the global optimum because of the excessive effort spent around the current best solution. In this work, we adopt the additive global and local GP (AGLGP) model in the optimization framework. The model is rooted in the inducing points based GP sparse approximations and is combined with independent local models in different regions. With these properties, the AGLGP model is suitable for multimodal responses with relatively large data sizes. Based on this AGLGP model, we propose a combined global and local search for optimization (CGLO) algorithm. It first divides the whole design space into disjoint local regions and identifies a promising region with the global model. Next, a local model in the selected region is fit to guide detailed search within this region. The algorithm then switches back to the global step when a good local solution is found. The global and local natures of CGLO enable it to enjoy the benefits of both global and local search to efficiently locate the global optimum.Summary of Contribution: This work proposes a new Gaussian process based algorithm for stochastic simulation optimization, which is an important area in operations research. This type of algorithm is also regarded as one of the state-of-the-art optimization algorithms for black-box functions in computer science. The aim of this work is to provide a computationally efficient optimization algorithm when the baseline functions are highly nonstationary (the function values change dramatically across the design space). Such nonstationary surfaces are very common in reality, such as the case in the maritime traffic safety problem considered here. In this problem, agent-based simulation is used to simulate the probability of collision of one vessel with the others on a given trajectory, and the decision maker needs to choose the trajectory with the minimum probability of collision quickly. Typically, in a high-congestion region, a small turn of the vessel can result in a very different conflict environment, and thus the response is highly nonstationary. Through our study, we find that the proposed algorithm can provide safer choices within a limited time compared with other methods. We believe the proposed algorithm is very computationally efficient and has large potential in such operational problems.
We investigate a new approach to compute the gradients of artificial neural networks (ANNs), based on the so-called push-out likelihood ratio method. Unlike the widely used backpropagation (BP) method that requires continuity of the loss function and the activation function, our approach bypasses this requirement by injecting artificial noises into the signals passed along the neurons. We show how this approach has a similar computational complexity as BP, and moreover is more advantageous in terms of removing the backward recursion and eliciting transparent formulas. We also formalize the connection between BP, a pivotal technique for training ANNs, and infinitesimal perturbation analysis, a classic path-wise derivative estimation approach, so that both our new proposed methods and BP can be better understood in the context of stochastic gradient estimation. Our approach allows efficient training for ANNs with more flexibility on the loss and activation functions, and shows empirical improvements on the robustness of ANNs under adversarial attacks and corruptions of natural noises.Summary of Contribution: Stochastic gradient estimation has been studied actively in simulation for decades and becomes more important in the era of machine learning and artificial intelligence. The stochastic gradient descent is a standard technique for training the artificial neural networks (ANNs), a pivotal problem in deep learning. The most popular stochastic gradient estimation technique is the backpropagation method. We find that the backpropagation method lies in the family of infinitesimal perturbation analysis, a path-wise gradient estimation technique in simulation. Moreover, we develop a new likelihood ratio-based method, another popular family of gradient estimation technique in simulation, for training more general ANNs, and demonstrate that the new training method can improve the robustness of the ANN.
The maximum k-colorable subgraph (MkCS) problem is to find an induced k-colorable subgraph with maximum cardinality in a given graph. This paper is an in-depth analysis of the MkCS problem that considers various semidefinite programming relaxations, including their theoretical and numerical comparisons. To simplify these relaxations, we exploit the symmetry arising from permuting the colors, as well as the symmetry of the given graphs when applicable. We also show how to exploit invariance under permutations of the subsets for other partition problems and how to use the MkCS problem to derive bounds on the chromatic number of a graph. Our numerical results verify that the proposed relaxations provide strong bounds for the MkCS problem and that those outperform existing bounds for most of the test instances.Summary of Contribution: The maximum k-colorable subgraph (MkCS) problem is to find an induced k-colorable subgraph with maximum cardinality in a given graph. The MkCS problem has a number of applications, such as channel assignment in spectrum sharing networks (e.g., Wi-Fi or cellular), very-large-scale integration design, human genetic research, and so on. The MkCS problem is also related to several other optimization problems, including the graph partition problem and the max-k-cut problem. The two mentioned problems have applications in parallel computing, network partitioning, floor planning, and so on. This paper is an in-depth analysis of the MkCS problem that considers various semidefinite programming relaxations, including their theoretical and numerical comparisons. Further, our analysis relates the MkCS results with the stable set and the chromatic number problems. We provide extended numerical results that verify that the proposed bounding approaches provide strong bounds for the MkCS problem and that those outperform existing bounds for most of the test instances. Moreover, our lower bounds on the chromatic number of a graph are competitive with existing bounds in the literature.
We introduce MathOptInterface, an abstract data structure for representing mathematical optimization problems based on combining predefined functions and sets. MathOptInterface is significantly more general than existing data structures in the literature, encompassing, for example, a spectrum of problems classes from integer programming with indicator constraints to bilinear semidefinite programming. We also outline an automated rewriting system between equivalent formulations of a constraint. MathOptInterface has been implemented in practice, forming the foundation of a recent rewrite of JuMP, an open-source algebraic modeling language in the Julia language. The regularity of the MathOptInterface representation leads naturally to a general file format for mathematical optimization we call MathOptFormat. In addition, the automated rewriting system provides modeling power to users while making it easy to connect new solvers to JuMP.Summary of Contribution: This paper describes a new abstract data structure for representing mathematical optimization models with a corresponding file format and automatic transformation system. The advances are useful for algebraic modeling languages, allowing practitioners to model problems more naturally and more generally than before.
Contingency research to find optimal operations and postcontingency recovery plans in distribution networks has gained major attention in recent years. To this end, we consider a multiperiod optimal power flow problem in distribution networks, subject to the N – 1 contingency in which a line or distributed energy resource fails. The contingency can be modeled as a stochastic disruption, an event with random magnitude and timing. Assuming a specific recovery time, we formulate a multistage stochastic convex program and develop a decomposition algorithm based on stochastic dual dynamic programming. Realistic modeling features, such as linearized AC power flow physics, engineering limits, and battery devices with realistic efficiency curves, are incorporated. We present extensive computational tests to show the efficiency of our decomposition algorithm and out-of-samplex performance of our solution compared with its deterministic counterpart. Operational insights on battery utilization, component hardening, and length of recovery phase are obtained by performing analyses from stochastic disruption-aware solutions.Summary of Contribution: Stochastic disruptions are random in time and can significantly alter the operating status of a distribution power network. Most of the previous research focuses on the magnitude aspect with a fixed set of time points in which randomness is observed. Our paper provides a novel multistage stochastic programming model for stochastic disruptions, considering both the uncertainty in timing and magnitude. We propose a computationally efficient cutting-plane method to solve this large-scale model and prove the theoretical convergence of such a decomposition algorithm. We present computational results to substantiate and demonstrate the theoretical convergence and provide operational insights into how making infrastructure investments can hedge against stochastic disruptions via sensitivity analyses.
Targeted marketing strategies are of significant interest in the smartapp economy. Typically, one seeks to identify individuals to strategically target in a social network so that the network is influenced at a minimal cost. In many practical settings, the effects of direct influence predominate, leading to the positive influence dominating set with partial payments (PIDS-PP) problem that we discuss in this paper. The PIDS-PP problem is NP-complete because it generalizes the dominating set problem. We discuss several mixed integer programming formulations for the PIDS-PP problem. First, we describe two compact formulations on the payment space. We then develop a stronger compact extended formulation. We show that when the underlying graph is a tree, this compact extended formulation provides integral solutions for the node selection variables. In conjunction, we describe a polynomial-time dynamic programming algorithm for the PIDS-PP problem on trees. We project the compact extended formulation onto the payment space, providing an equivalently strong formulation that has exponentially many constraints. We present a polynomial time algorithm to solve the associated separation problem. Our computational experience on a test bed of 100 real-world graph instances (with up to approximately 465,000 nodes and 835,000 edges) demonstrates the efficacy of our strongest payment space formulation. It finds solutions that are on average 0.4% from optimality and solves 80 of the 100 instances to optimality.Summary of Contribution: The study of influence propagation is important in a number of applications including marketing, epidemiology, and healthcare. Typically, in these problems, one seeks to identify individuals to strategically target in a social network so that the entire network is influenced at a minimal cost. With the ease of tracking consumers in the smartapp economy, the scope and nature of these problems have become larger. Consequently, there is considerable interest across multiple research communities in computationally solving large-scale influence maximization problems, which thus represent significant opportunities for the development of operations research–based methods and analysis in this interface. This paper introduces the positive influence dominating set with partial payments (PIDS-PP) problem, an influence maximization problem where the effects of direct influence predominate, and it is possible to make partial payments to nodes that are not targeted. The paper focuses on model development to solve large-scale PIDS-PP problems. To this end, starting from an initial base optimization model, it uses several operations research model strengthening techniques to develop two equivalent models that have strong computational performance (and can be theoretically shown to be the best model for trees). Computational experiments on a test bed of 100 real-world graph instances (with up to approximately 465,000 nodes and 835,000 edges) attest to the efficacy of the best model, which finds solutions that are on average 0.4% from optimality and solves 80 of the 100 instances to optimality.
We introduce a new class of distributionally robust optimization problems under decision-dependent ambiguity sets. In particular, as our ambiguity sets, we consider balls centered on a decision-dependent probability distribution. The balls are based on a class of earth mover’s distances that includes both the total variation distance and the Wasserstein metrics. We discuss the main computational challenges in solving the problems of interest and provide an overview of various settings leading to tractable formulations. Some of the arising side results, such as the mathematical programming expressions for robustified risk measures in a discrete space, are also of independent interest. Finally, we rely on state-of-the-art modeling techniques from machine scheduling and humanitarian logistics to arrive at potentially practical applications, and present a numerical study for a novel risk-averse scheduling problem with controllable processing times.Summary of Contribution: In this study, we introduce a new class of optimization problems that simultaneously address distributional and decision-dependent uncertainty. We present a unified modeling framework along with a discussion on possible ways to specify the key model components, and discuss the main computational challenges in solving the complex problems of interest. Special care has been devoted to identifying the settings and problem classes where these challenges can be mitigated. In particular, we provide model reformulation results, including mathematical programming expressions for robustified risk measures, and describe how these results can be utilized to obtain tractable formulations for specific applied problems from the fields of humanitarian logistics and machine scheduling. Toward demonstrating the value of the modeling approach and investigating the performance of the proposed mixed-integer linear programming formulations, we conduct a computational study on a novel risk-averse machine scheduling problem with controllable processing times. We derive insights regarding the decision-making impact of our modeling approach and key parameter choices.
Given a finite number of stochastic systems, the goal of our problem is to dynamically allocate a finite sampling budget to maximize the probability of selecting the “best” system. Systems are encoded with the probability distributions that govern sample observations, which are unknown and only assumed to belong to a broad family of distributions that need not admit any parametric representation. The best system is defined as the one with the highest quantile value. The objective of maximizing the probability of selecting this best system is not analytically tractable. In lieu of that, we use the rate function for the probability of error relying on large deviations theory. Our point of departure is an algorithm that naively combines sequential estimation and myopic optimization. This algorithm is shown to be asymptotically optimal; however, it exhibits poor finite-time performance and does not lead itself to implementation in settings with a large number of systems. To address this, we propose practically implementable variants that retain the asymptotic performance of the former while dramatically improving its finite-time performance.
The emergence of online retailers has brought new opportunities to the design of their distribution networks. Notably, for online retailers that do not operate offline stores, their target customers are more sensitive to the quality of logistic services, such as delivery speed and reliability. This paper is motivated by a leading online retailer for cosmetic products on Taobao.com that aimed to improve its logistics efficiency by redesigning its centralized distribution network into a multilevel one. The multilevel distribution network consists of a layer of primary facilities to hold stocks from suppliers and transshipment and a layer of secondary facilities to provide last-mile delivery. There are two major challenges of designing such a facility network. First, online customers can respond significantly to the change of logistics efficiency with the redesigned network, thereby rendering the network optimized under the original demand distribution suboptimal. Second, because online retailers have relatively small sales volumes and are very flexible in choosing facility locations, the facility candidate set can be large, causing the facility location optimization challenging to solve. To this end, we propose an iterative prediction-and-optimization strategy for distribution network design. Specifically, we first develop an artificial neural network (ANN) to predict customer demands, factoring in the logistic service quality given the network and the city-level purchasing power based on demographic statistics. Then, a mixed integer linear programming (MILP) model is formulated to choose facility locations with minimum transportation, facility setup, and package processing costs. We further develop an efficient two-stage heuristic for computing high-quality solutions to the MILP model, featuring an agglomerative hierarchical clustering algorithm and an expectation and maximization algorithm. Subsequently, the ANN demand predictor and two-stage heuristic are integrated for iterative network design. Finally, using a real-world data set, we validate the demand prediction accuracy and demonstrate the mutual interdependence between the demand and network design.Summary of Contribution: We propose an iterative prediction-and-optimization algorithm for multilevel distribution network design for e-logistics and evaluate its operational value for online retailers. We address the issue of the interplay between distribution network design and the demand distribution using an iterative framework. Further, combining the idea in operational research and data mining, our paper provides an end-to-end solution that can provide accurate predictions of online sales distribution, subsequently solving large-scale optimization problems for distribution network design problems.
Residents often rely on newspapers and television to gather hyperlocal news for community awareness and engagement. More recently, social media have emerged as an increasingly important source of hyperlocal news. Thus far, the literature on using social media to create desirable societal benefits, such as civic awareness and engagement, is still in its infancy. One key challenge in this research stream is to timely and accurately distill information from noisy social media data streams to community members. In this work, we develop SHEDR (social media–based hyperlocal event detection and recommendation), an end-to-end neural event detection and recommendation framework with a particular use case for Twitter to facilitate residents’ information seeking of hyperlocal events. The key model innovation in SHEDR lies in the design of the hyperlocal event detector and the event recommender. First, we harness the power of two popular deep neural network models, the convolutional neural network (CNN) and long short-term memory (LSTM), in a novel joint CNN-LSTM model to characterize spatiotemporal dependencies for capturing unusualness in a region of interest, which is classified as a hyperlocal event. Next, we develop a neural pairwise ranking algorithm for recommending detected hyperlocal events to residents based on their interests. To alleviate the sparsity issue and improve personalization, our algorithm incorporates several types of contextual information covering topic, social, and geographical proximities. We perform comprehensive evaluations based on two large-scale data sets comprising geotagged tweets covering Seattle and Chicago. We demonstrate the effectiveness of our framework in comparison with several state-of-the-art approaches. We show that our hyperlocal event detection and recommendation models consistently and significantly outperform other approaches in terms of precision, recall, and F-1 scores.Summary of Contribution: In this paper, we focus on a novel and important, yet largely underexplored application of computing—how to improve civic engagement in local neighborhoods via local news sharing and consumption based on social media feeds. To address this question, we propose two new computational and data-driven methods: (1) a deep learning–based hyperlocal event detection algorithm that scans spatially and temporally to detect hyperlocal events from geotagged Twitter feeds; and (2) A personalized deep learning–based hyperlocal event recommender system that systematically integrates several contextual cues such as topical, geographical, and social proximity to recommend the detected hyperlocal events to potential users. We conduct a series of experiments to examine our proposed models. The outcomes demonstrate that our algorithms are significantly better than the state-of-the-art models and can provide users with more relevant information about the local neighborhoods that they live in, which in turn may boost their community engagement.
Business research practice is witnessing a surge in the integration of predictive modeling and prescriptive analysis. We describe a modeling framework JANOS that seamlessly integrates the two streams of analytics, allowing researchers and practitioners to embed machine learning models in an end-to-end optimization framework. JANOS allows for specifying a prescriptive model using standard optimization modeling elements such as constraints and variables. The key novelty lies in providing modeling constructs that enable the specification of commonly used predictive models within an optimization model, have the features of the predictive model as variables in the optimization model, and incorporate the output of the predictive models as part of the objective. The framework considers two sets of decision variables: regular and predicted. The relationship between the regular and the predicted variables is specified by the user as pretrained predictive models. JANOS currently supports linear regression, logistic regression, and neural network with rectified linear activation functions. In this paper, we demonstrate the flexibility of the framework through an example on scholarship allocation in a student enrollment problem and provide a numeric performance evaluation.Summary of Contribution. This paper describes a new software tool, JANOS, that integrates predictive modeling and discrete optimization to assist decision making. Specifically, the proposed solver takes as input user-specified pretrained predictive models and formulates optimization models directly over those predictive models by embedding them within an optimization model through linear transformations.
The minimum connected dominating set (MCDS) problem consists of selecting a minimum set of vertices from an undirected graph, such that each vertex not in this set is adjacent to at least one of the vertices in it, and the subgraph induced by this vertex set is connected. This paper presents a fast vertex weighting (FVW) algorithm for solving the MCDS problem, which integrates several distinguishing features, such as a vertex weighting-based local search with tabu and perturbation strategies to help the search to jump out of the local optima, as well as a search space reduction strategy to improve the search efficiency. Computational experiments on four sets of 112 commonly used public benchmark instances, as well as 15 newly introduced sparse instances, show that FVW is highly competitive compared with the state-of-the-art algorithms in the literature despite its simplicity. FVW improves the previous best-known results for 20 large public benchmark instances while matching the best-known results for all but 2 of the remaining ones. Several ingredients of FVW are investigated to demonstrate the importance of the proposed ideas and techniques.Summary of Contribution: As a challenging classical NP-hard problem, the minimum connected dominating set (MCDS) problem has been studied for decades in the areas of both operations research and computer science, although there does not exist an exact polynomial algorithm for solving it. Thus, the new breakthrough on this classical NP-hard problem in terms of the computational results on classical benchmark instances is significant. This paper presents a new fast vertex weighting local search for solving the MCDS problem. Computational experiments on four sets of 112 commonly used public benchmark instances show that fast vertex weighting (FVW) is able to improve the previous best-known results for 20 large instances while matching the best-known results for all but 2 of the remaining instances. Several ingredients of FVW are also investigated to demonstrate the importance of the proposed ideas and techniques.
On-demand air mobility services, often called air taxis, are on the way to revolutionize our urban/regional transportation sector by lifting transportation to the third dimension and thus possibly contribute to solving the congestion-induced transportation deadlock many metropolitan regions face today. Although existing research mainly focuses on the design of efficient vehicles and specifically battery technology, in the near future, a new question will arise: Where to locate the vertiports/landing pads for such air taxis? In this study, we propose a vertiport location selection problem. In contrast to existing studies, we allow the demand to be distributed over the whole metropolitan area, modeled as a grid, and exclude certain grid cells from becoming hubs, for example, because of safety/geographical constraints. The combination of these two contributions makes the problem intriguingly difficult to solve with standard solution techniques. We propose a novel variable neighborhood search heuristic, which is able to solve 12 × 12 grid instances within a few seconds of computation time and zero gaps in our experiments, whereas CPLEX needs up to 10 hours. We believe that our study contributes toward the scalable selection of vertiport locations for air taxis.Summary of Contribution: The increasing interest in opening the third dimension, that is, altitude, to transportation inside metropolitan regions raises new research challenges. Existing research mainly focuses on the design of efficient vehicles and control problems. In the near future, however, the actual operation of air taxis will lead to new set of operations research problems for so-called air taxi operations. Our contribution focuses on the optimization of vertiports for air taxi operations in a metropolitan region. We choose to model the problem over a grid-like demand structure, with a novel side constraint: selected grid cells are unavailable as hubs, for example, because of environmental, technical, cultural, or other reasons. This makes our model a special case in between the two traditional models: discrete location and continuous location. Our model is inherently difficult to solve for exact methods; for instance, solving a grid of 12 × 12 grid cells needs more than 10 hours with CPLEX, when modeled as a discrete location problem. We show that a straightforward application of existing neighborhood search heuristics is not suitable to solve this problem well. Therefore, we design an own variant of mixed variable neighborhood search, which consists of novel local search steps, tailored toward our grid structure. Our evaluation shows that by using our novel heuristic, almost all instances can be solved toward optimality.
In this paper, we study a class of two-stage robust binary optimization problems with objective uncertainty, where recourse decisions are restricted to be mixed-binary. For these problems, we present a deterministic equivalent formulation through the convexification of the recourse-feasible region. We then explore this formulation under the lens of a relaxation, showing that the specific relaxation we propose can be solved by using the branch-and-price algorithm. We present conditions under which this relaxation is exact and describe alternative exact solution methods when this is not the case. Despite the two-stage nature of the problem, we provide NP-completeness results based on our reformulations. Finally, we present various applications in which the methodology we propose can be applied. We compare our exact methodology to those approximate methods recently proposed in the literature under the name K−𝐾−adaptability. Our computational results show that our methodology is able to produce better solutions in less computational time compared with the K−𝐾−adaptability approach, as well as to solve bigger instances than those previously managed in the literature.Summary of Contribution: Our manuscript describes an exact solution approach for a class of robust binary optimization problems with mixed-binary recourse and objective uncertainty. Its development reposes first on a reformulation of the problem, then a carefully constructed relaxation of this reformulation. Our solution approach is designed to exploit the two-stage and binary structure of the problem for effective resolution. In its execution, it relies on the branch-and-price algorithm and its efficient implementation. With our computational experiments, we show that our proposed exact solution method outperforms the existing approximate methodologies and, therefore, pushes the computational envelope for the class of problems considered.
The prize-collecting Steiner tree problem (PCSTP) is a well-known generalization of the classic Steiner tree problem in graphs, with a large number of practical applications. It attracted particular interest during the 11th DIMACS Challenge in 2014, and since then, several PCSTP solvers have been introduced in the literature. Although these new solvers further, and often drastically, improved on the results of the DIMACS Challenge, many PCSTP benchmark instances have remained unsolved. The following article describes further advances in the state of the art in exact PCSTP solving. It introduces new techniques and algorithms for PCSTP, involving various new transformations (or reductions) of PCSTP instances to equivalent problems, for example, to decrease the problem size or to obtain a better integer programming formulation. Several of the new techniques and algorithms provably dominate previous approaches. Further theoretical properties of the new components, such as their complexity, are discussed. Also, new complexity results for the exact solution of PCSTP and related problems are described, which form the base of the algorithm design. Finally, the new developments also translate into a strong computational performance: the resulting exact PCSTP solver outperforms all previous approaches, both in terms of runtime and solvability. In particular, it solves several formerly intractable benchmark instances from the 11th DIMACS Challenge to optimality. Moreover, several recently introduced large-scale instances with up to 10 million edges, previously considered to be too large for any exact approach, can now be solved to optimality in less than two hours.Summary of Contribution: The prize-collecting Steiner tree problem (PCSTP) is a well-known generalization of the classic Steiner tree problem in graphs, with many practical applications. The article introduces and analyses new techniques and algorithms for PCSTP that ultimately aim for improved (practical) exact solution. The algorithmic developments are underpinned by results on theoretical aspects, such as fixed-parameter tractability of PCSTP. Computationally, we considerably push the limits of tractibility, being able to solve PCSTP instances with up to 10 million edges. The new solver, which also considerably outperforms the state of the art on smaller instances, will be made publicly available as part of the SCIP Optimization Suite.
We present a progressive approximation algorithm for the exact solution of several classes of interdiction games in which two noncooperative players (namely an attacker and a follower) interact sequentially. The follower must solve an optimization problem that has been previously perturbed by means of a series of attacking actions led by the attacker. These attacking actions aim at augmenting the cost of the decision variables of the follower’s optimization problem. The objective, from the attacker’s viewpoint, is that of choosing an attacking strategy that reduces as much as possible the quality of the optimal solution attainable by the follower. The progressive approximation mechanism consists of the iterative solution of an interdiction problem in which the attacker actions are restricted to a subset of the whole solution space and a pricing subproblem invoked with the objective of proving the optimality of the attacking strategy. This scheme is especially useful when the optimal solutions to the follower’s subproblem intersect with the decision space of the attacker only in a small number of decision variables. In such cases, the progressive approximation method can solve interdiction games otherwise intractable for classical methods. We illustrate the efficiency of our approach on the shortest path, 0-1 knapsack and facility location interdiction games.Summary of Contribution: In this article, we present a progressive approximation algorithm for the exact solution of several classes of interdiction games in which two noncooperative players (namely an attacker and a follower) interact sequentially. We exploit the discrete nature of this interdiction game to design an effective algorithmic framework that improves the performance of general-purpose solvers. Our algorithm combines elements from mathematical programming and computer science, including a metaheuristic algorithm, a binary search procedure, a cutting-planes algorithm, and supervalid inequalities. Although we illustrate our results on three specific problems (shortest path, 0-1 knapsack, and facility location), our algorithmic framework can be extended to a broader class of interdiction problems.
We present a generic branch-and-bound algorithm for finding all the Pareto solutions of a biobjective mixed-integer linear program. The main contributions are new algorithms for obtaining dual bounds at a node, checking node fathoming, presolve, and duality gap measurement. Our branch-and-bound is predominantly a decision space search method because the branching is performed on the decision variables, akin to single objective problems, although we also sometimes split gaps and branch in the objective space. The various algorithms are implemented using a data structure for storing Pareto sets. Computational experiments are carried out on literature instances and on a new set of instances that we generate using a benchmark library (MIPLIB2017) for single objective problems. We also perform comparisons against the triangle splitting method from literature, which is an objective space search algorithm.Summary of Contribution: Biobjective mixed-integer optimization problems have two linear objectives and a mixed-integer feasible region. Such problems have many applications in operations research, because many real-world optimization problems naturally comprise two conflicting objectives to optimize or can be approximated in such a manner and are even harder than single objective mixed-integer programs. Solving them exactly requires the computation of all the nondominated solutions in the objective space, whereas some applications may also require finding at least one solution in the decision space corresponding to each nondominated solution. This paper provides an exact algorithm for solving these problems using the branch-and-bound method, which works predominantly in the decision space. Of the many ingredients of this algorithm, some parts are direct extensions of the single-objective version, but the main parts are newly designed algorithms to handle the distinct challenges of optimizing over two objectives. The goal of this study is to improve solution quality and speed and show that decision-space algorithms perform comparably to, and sometimes better than, algorithms that work mainly in the objective-space.
This paper investigates the problem of estimating the size of branch-and-bound (B&B) trees for solving mixed-integer programs. We first prove that the size of the B&B tree cannot be approximated within a factor of 2 for general binary programs, unless P =NP𝒫=𝒩𝒫. Second, we review measures of progress of the B&B search, such as the well-known gap and the often-overlooked tree weight, and propose a new measure, which we call leaf frequency. We study two simple ways to transform these progress measures into B&B tree-size estimates, either as a direct projection or via double-exponential smoothing, a standard time-series forecasting technique. We then combine different progress measures and their trends into nontrivial estimates using machine learning techniques, which yield more precise estimates than any individual measure. The best method that we have identified uses all individual measures as features of a random forest model. In a large computational study, we train and validate all methods on the publicly available MIPLIB and Coral general purpose benchmark sets. On average, the best method estimates B&B tree sizes within a factor of 3 on the set of unseen test instances, even during the early stage of the search, and improves in accuracy as the search progresses. It also achieves a factor of 2 over the entire search on each of the six additional sets of homogeneous instances that we tested. All techniques are available in version 7 of the branch-and-cut framework SCIP.Summary of Contribution: This manuscript develops a method for online estimation of the size of branch-and-bound trees, thereby combining methods of mixed-integer programming and machine learning. We show that high-quality estimations can be obtained using the presented techniques. The methods are also useful in everyday use of branch-and-bound algorithms to obtain approximate search-completion information. The manuscript is accompanied by an extensive online supplement comprising the code used for our simulations and an implementation of all discussed methods in the academic solver SCIP, together with the tools and instructions to train estimators for custom instance sets.
We consider a variant of the vehicle routing problem (VRP) where each customer has a unit demand and the goal is to minimize the total cost of routing a fleet of capacitated vehicles from one or multiple depots to visit all customers. We propose two parallel algorithms to efficiently solve the column-generation-based linear-programming relaxation for this VRP. Specifically, we focus on algorithms for the “pricing problem,” which corresponds to the resource-constrained elementary shortest path problem. The first algorithm extends the pulse algorithm for which we derive a new bounding scheme on the maximum load of any route. The second algorithm is based on random coloring from parameterized complexity which can be also combined with other techniques in the literature for improving VRPs, including cutting planes and column enumeration. We conduct numerical studies using VRP benchmarks (with 50–957 nodes) and instances of a medical home care delivery problem using census data in Wayne County, Michigan. Using parallel computing, both pulse and random coloring can significantly improve column generation for solving the linear programming relaxations and we can obtain heuristic integer solutions with small optimality gaps. Combining random coloring with column enumeration, we can obtain improved integer solutions having less than 2% optimality gaps for most VRP benchmark instances and less than 1% optimality gaps for the medical home care delivery instances, both under a 30-minute computational time limit. The use of cutting planes (e.g., robust cuts) can further reduce optimality gaps on some hard instances, without much increase in the run time.Summary of Contribution: The vehicle routing problem (VRP) is a fundamental combinatorial problem, and its variants have been studied extensively in the literature of operations research and computer science. In this paper, we consider general-purpose algorithms for solving VRPs, including the column-generation approach for the linear programming relaxations of the integer programs of VRPs and the column-enumeration approach for seeking improved integer solutions. We revise the pulse algorithm and also propose a random-coloring algorithm that can be used for solving the elementary shortest path problem that formulates the pricing problem in the column-generation approach. We show that the parallel implementation of both algorithms can significantly improve the performance of column generation and the random coloring algorithm can improve the solution time and quality of the VRP integer solutions produced by the column-enumeration approach. We focus on algorithmic design for VRPs and conduct extensive computational tests to demonstrate the performance of various approaches.
This paper presents a new bidirectional search algorithm to solve the shortest path problem. The new algorithm uses an iterative deepening technique with a consistent heuristic to improve lower bounds on path costs. The new algorithm contains a novel technique of filtering nodes to significantly reduce the memory requirements. Computational experiments on the pancake problem, sliding tile problem, and Rubik’s cube show that the new algorithm uses significantly less memory and executes faster than A* and other state-of-the-art bidirectional algorithms.Summary of Contribution: Quickly solving single-source shortest path problems on graphs is important for pathfinding applications and is a core problem in both artificial intelligence and operations research. This paper attempts to solve large problems that do not easily fit into the available memory of a desktop computer, such as finding the optimal shortest set of moves to solve a Rubik’s cube, and solve them faster than existing algorithms.
This paper provides a novel framework for solving multiobjective discrete optimization problems with an arbitrary number of objectives. Our framework represents these problems as network models, in that enumerating the Pareto frontier amounts to solving a multicriteria shortest-path problem in an auxiliary network. We design techniques for exploiting network models in order to accelerate the identification of the Pareto frontier, most notably a number of operations to simplify the network by removing nodes and arcs while preserving the set of nondominated solutions. We show that the proposed framework yields orders-of-magnitude performance improvements over existing state-of-the-art algorithms on five problem classes containing both linear and nonlinear objective functions.Summary of Contribution: Multiobjective optimization has a long history of research with applications in several domains. Our paper provides an alternative modeling and solution approach for multiobjective discrete optimization problems by leveraging graphical structures. Specifically, we encode the decision space of a problem as a layered network and propose graph reduction operators to preserve only solutions whose image are part of the Pareto frontier. The nondominated solutions can then be extracted through shortest-path algorithms on such a network. Numerical results comparing our method with state-of-the-art approaches on several problem classes, including the knapsack, set covering, and the traveling salesperson problem (TSP), suggest orders-of-magnitude runtime speed-ups for exactly enumerating the Pareto frontier, especially when the number of objective functions grows.
In the context of the maximum stable set problem, rank inequalities impose that the cardinality of any set of vertices contained in a stable set be, at most, as large as the stability number of the subgraph induced by such a set. Rank inequalities are very general, as they subsume many classical inequalities such as clique, hole, antihole, web, and antiweb inequalities. In spite of their generality, the exact separation of rank inequalities has never been addressed without the introduction of topological restrictions on the induced subgraph and the tightness of their closure has never been investigated systematically. In this work, we propose a methodology for optimizing over the closure of all rank inequalities with a right-hand side no larger than a small constant without imposing any restrictions on the topology of the induced subgraph. Our method relies on the exact separation of a relaxation of rank inequalities, which we call relaxed k-rank inequalities, whose closure is as tight. We investigate the corresponding separation problem, a bilevel programming problem asking for a subgraph of maximum weight with a bound on its stability number, whose study could be of independent interest. We first prove that the problem is ΣP2-hard and provide some insights on its polyhedral structure. We then propose two exact methods for its solution: a branch-and-cut algorithm (which relies on a family of faced-defining inequalities which we introduce in this paper) and a purely combinatorial branch-and-bound algorithm. Our computational results show that the closure of rank inequalities with a right-hand side no larger than a small constant can yield a bound that is stronger, in some cases, than Lovász’s Theta function, and substantially stronger than bounds obtained with standard inequalities that are valid for the stable set problem, including odd-cycle inequalities and wheel inequalities.Summary of Contribution: This paper proposes two original methods for solving a challenging cut-separation problem (of bilevel type) for a large class of inequalities valid for one of the key operations research problems, namely, the max stable set problem. An extensive set of experimental results validates the proposed methods. All the source code and data sets are available online on GitHub.
We study the uncapacitated lot-sizing problem with uncertain demand and costs. The problem is modeled as a multistage stochastic mixed-integer linear program in which the evolution of the uncertain parameters is represented by a scenario tree. To solve this problem, we propose a new extension of the stochastic dual dynamic integer programming algorithm (SDDiP). This extension aims at being more computationally efficient in the management of the expected cost-to-go functions involved in the model, in particular by reducing their number and by exploiting the current knowledge on the polyhedral structure of the stochastic uncapacitated lot-sizing problem. The algorithm is based on a partial decomposition of the problem into a set of stochastic subproblems, each one involving a subset of nodes forming a subtree of the initial scenario tree. We then introduce a cutting plane–generation procedure that iteratively strengthens the linear relaxation of these subproblems and enables the generation of an additional strengthened Benders’ cut, which improves the convergence of the method. We carry out extensive computational experiments on randomly generated large-size instances. Our numerical results show that the proposed algorithm significantly outperforms the SDDiP algorithm at providing good-quality solutions within the computation time limit.Summary of Contribution: This paper investigates a combinatorial optimization problem called the uncapacitated lot-sizing problem. This problem has been widely studied in the operations research literature as it appears as a core subproblem in many industrial production planning problems. We consider a stochastic extension in which the input parameters are subject to uncertainty and model the resulting stochastic optimization problem as a multistage stochastic integer program. To solve this stochastic problem, we propose a novel extension of the recently published stochastic dual dynamic integer programming (SDDiP) algorithm. The proposed extension relies on two main ideas: the use of a partial decomposition of the scenario tree and the exploitation of existing knowledge on the polyhedral structure of the stochastic uncapacitated lot-sizing problem. We provide the results of extensive computational experiments carried out on large-size randomly generated instances. These results show that the proposed extended algorithm significantly outperforms the SDDiP at providing good-quality solutions for the stochastic uncapacitated lot-sizing problem. Although the paper focuses on a basic lot-sizing problem, the proposed algorithmic framework may be useful to solve more complex practical production planning problems.
The problem of fitting continuous piecewise linear (PWL) functions to discrete data has applications in pattern recognition and engineering, amongst many other fields. To find an optimal PWL function, the positioning of the breakpoints connecting adjacent linear segments must not be constrained and should be allowed to be placed freely. Although the univariate PWL fitting problem has often been approached from a global optimisation perspective, recently, two mixed-integer linear programming approaches have been presented that solve for optimal PWL functions. In this paper, we compare the two approaches: the first was presented by Rebennack and Krasko [Rebennack S, Krasko V (2020) Piecewise linear function fitting via mixed-integer linear programming. INFORMS J. Comput. 32(2):507–530] and the second by Kong and Maravelias [Kong L, Maravelias CT (2020) On the derivation of continuous piecewise linear approximating functions. INFORMS J. Comput. 32(3):531–546]. Both formulations are similar in that they use binary variables and logical implications modelled by big-Mℳ constructs to ensure the continuity of the PWL function, yet the former model uses fewer binary variables. We present experimental results comparing the time taken to find optimal PWL functions with differing numbers of breakpoints across 10 data sets for three different objective functions. Although neither of the two formulations is superior on all data sets, the presented computational results suggest that the formulation presented by Rebennack and Krasko is faster. This might be explained by the fact that it contains fewer complicating binary variables and sparser constraints.Summary of Contribution: This paper presents a comparison of the mixed-integer linear programming models presented in two recent studies published in the INFORMS Journal on Computing. Because of the similarity of the formulations of the two models, it is not clear which one is preferable. We present a detailed comparison of the two formulations, including a series of comparative experimental results across 10 data sets that appeared across both papers. We hope that our results will allow readers to take an objective view as to which implementation they should use.
We investigate the discrete parallel machine scheduling and location problem, which consists of locating multiple machines to a set of candidate locations, assigning jobs from different locations to the located machines, and sequencing the assigned jobs. The objective is to minimize the maximum completion time of all jobs, that is, the makespan. Though the problem is of theoretical significance with a wide range of practical applications, it has not been well studied as reported in the literature. For this problem, we first propose three new mixed-integer linear programs that outperform state-of-the-art formulations. Then, we develop a new logic-based Benders decomposition algorithm for practical-sized instances, which splits the problem into a master problem that determines machine locations and job assignments to machines and a subproblem that sequences jobs on each machine. The master problem is solved by a branch-and-cut procedure that operates on a single search tree. Once an incumbent solution to the master problem is found, the subproblem is solved to generate cuts that are dynamically added to the master problem. A generic no-good cut is first proposed, which is later improved by some strengthening techniques. Two optimality cuts are also developed based on optimality conditions of the subproblem and improved by strengthening techniques. Numerical results on small-sized instances show that the proposed formulations outperform state-of-the-art ones. Computational results on 1,400 benchmark instances with up to 300 jobs, 50 machines, and 300 locations demonstrate the effectiveness and efficiency of the algorithm compared with current approaches.Summary of Contribution: This paper employs operations research methods and computing techniques to address an NP-hard combinatorial optimization problem: the parallel discrete machine scheduling and location problem. The problem is of practical significance but has not been well studied in the literature. For the problem, we formulate three novel mixed-integer linear programs that outperform state-of-the-art formulations and develop a new logic-based Benders decomposition algorithm. Extensive computational experiments on 1,400 benchmark instances with up to 300 jobs, 50 machines, and 300 locations are conducted to evaluate the performance of the proposed models and algorithms.
In the bin-packing problem with minimum color fragmentation (BPPMCF), we are given a fixed number of bins and a collection of items, each associated with a size and a color, and the goal is to avoid color fragmentation by packing items with the same color within as few bins as possible. This problem emerges in areas as diverse as surgical scheduling and group event seating. We present several optimization models for the BPPMCF, including baseline integer programming formulations, alternative integer programming formulations based on two recursive decomposition strategies that utilize decision diagrams, and a branch-and-price algorithm. Using the results from an extensive computational evaluation on synthetic instances, we train a decision tree model that predicts which algorithm should be chosen to solve a given instance of the problem based on a collection of derived features. Our insights are validated through experiments on the aforementioned applications on real-world data.Summary of Contribution: In this paper, we investigate a colored variant of the bin-packing problem. We present and evaluate several exact mixed-integer programming formulations to solve the problem, including models that explore recursive decomposition strategies based on decision diagrams and a set partitioning model that we solve using branch and price. Our results show that the computational performance of the algorithms depends on features of the input data, such as the average number of items per bin. Our algorithms and featured applications suggest that the problem is of practical relevance and that instances of reasonable size can be solved efficiently.
Finding a shortest path in a network is a fundamental optimization problem. We focus on settings in which the travel time on an arc in the network depends on the time at which traversal of the arc begins. In such settings, reaching the destination as early as possible is not the only objective of interest. Minimizing the duration of the path, that is, the difference between the arrival time at the destination and the departure from the origin, and minimizing the travel time along the path from origin to destination, are also of interest. We introduce dynamic discretization discovery algorithms to efficiently solve such time-dependent shortest path problems with piecewise linear arc travel time functions. The algorithms operate on partially time-expanded networks in which arc costs represent lower bounds on the arc travel time over the subsequent time interval. A shortest path in this partially time-expanded network yields a lower bound on the value of an optimal path. Upper bounds are easily obtained as by-products of the lower bound calculations. The algorithms iteratively refine the discretization by exploiting breakpoints of the arc travel time functions. In addition to time discretization refinement, the algorithms permit time intervals to be eliminated, improving lower and upper bounds, until, in a finite number of iterations, optimality is proved. Computational experiments show that only a small fraction of breakpoints must be explored and that the fraction decreases as the length of the time horizon and the size of the network increases, making the algorithms highly efficient and scalable.Summary of Contribution: New data collection techniques have increased the availability and fidelity of time-dependent travel time information, making the time-dependent variant of the classic shortest path problem an extremely relevant problem in the field of operations research. This paper provides novel algorithms for the time-dependent shortest path problem with both the minimum duration and minimum travel time objectives, which aims to address the computational challenges faced by existing algorithms. A computational study shows that our new algorithm is indeed significantly more efficient than existing approaches.
We propose an analytic center cutting plane method to determine whether a matrix is completely positive and return a cut that separates it from the completely positive cone if not. This was stated as an open (computational) problem by Berman et al. [Berman A, Dur M, Shaked-Monderer N (2015) Open problems in the theory of completely positive and copositive matrices. Electronic J. Linear Algebra 29(1):46–58]. Our method optimizes over the intersection of a ball and the copositive cone, where membership is determined by solving a mixed-integer linear program suggested by Xia et al. [Xia W, Vera JC, Zuluaga LF (2020) Globally solving nonconvex quadratic programs via linear integer programming techniques. INFORMS J. Comput. 32(1):40–56]. Thus, our algorithm can, more generally, be used to solve any copositive optimization problem, provided one knows the radius of a ball containing an optimal solution. Numerical experiments show that the number of oracle calls (matrix copositivity checks) for our implementation scales well with the matrix size, growing roughly like O(d2)𝑂(𝑑2) for d × d matrices. The method is implemented in Julia and available at https://github.com/rileybadenbroek/CopositiveAnalyticCenter.jl.Summary of Contribution: Completely positive matrices play an important role in operations research. They allow many NP-hard problems to be formulated as optimization problems over a proper cone, which enables them to benefit from the duality theory of convex programming. We propose an analytic center cutting plane method to determine whether a matrix is completely positive by solving an optimization problem over the copositive cone. In fact, we can use our method to solve any copositive optimization problem, provided we know the radius of a ball containing an optimal solution. We emphasize numerical performance and stability in developing this method. A software implementation in Julia is provided.
In many statistical learning problems, it is desired that the optimal solution conform to an a priori known sparsity structure represented by a directed acyclic graph. Inducing such structures by means of convex regularizers requires nonsmooth penalty functions that exploit group overlapping. Our study focuses on evaluating the proximal operator of the latent overlapping group lasso developed by Jacob et al. in 2009. We implemented an alternating direction method of multiplier with a sharing scheme to solve large-scale instances of the underlying optimization problem efficiently. In the absence of strong convexity, global linear convergence of the algorithm is established using the error bound theory. More specifically, the paper contributes to establishing primal and dual error bounds when the nonsmooth component in the objective function does not have a polyhedral epigraph. We also investigate the effect of the graph structure on the speed of convergence of the algorithm. Detailed numerical simulation studies over different graph structures supporting the proposed algorithm and two applications in learning are provided.Summary of Contribution: The paper proposes a computationally efficient optimization algorithm to evaluate the proximal operator of a nonsmooth hierarchical sparsity-inducing regularizer and establishes its convergence properties. The computationally intensive subproblem of the proposed algorithm can be fully parallelized, which allows solving large-scale instances of the underlying problem. Comprehensive numerical simulation studies benchmarking the proposed algorithm against five other methods on the speed of convergence to optimality are provided. Furthermore, performance of the algorithm is demonstrated on two statistical learning applications related to topic modeling and breast cancer classification. The code along with the simulation studies and benchmarks are available on the corresponding author’s GitHub website for evaluation and future use.
Column generation (CG) algorithms are well known to suffer from convergence issues due, mainly, to the degenerate structure of their master problem and the instability associated with the dual variables involved in the process. In the literature, several strategies have been proposed to overcome this issue. These techniques rely either on the modification of the standard CG algorithm or on some prior information about the set of dual optimal solutions. In this paper, we propose a new stabilization framework, which relies on the dynamic generation of aggregated rows from the CG master problem. To evaluate the performance of our method and its flexibility, we consider instances of three different problems, namely, vehicle routing with time windows (VRPTW), bin packing with conflicts (BPPC), and multiperson pose estimation (MPPEP). When solving the VRPTW, the proposed stabilized CG method yields significant improvements in terms of CPU time and number of iterations with respect to a standard CG algorithm. Huge reductions in CPU time are also achieved when solving the BPPC and the MPPEP. For the latter, our method has shown to be competitive when compared with a tailored method.Summary of Contribution: Column generation (CG) algorithms are among the most important and studied solution methods in operations research. CG algorithms are suitable to cope with large-scale problems arising from several real-life applications. The present paper proposes a generic stabilization framework to address two of the main issues found in a CG method: degeneracy in the master problem and massive instability of the dual variables. The newly devised method, called dynamic separation of aggregated rows (dyn-SAR), relies on an extended master problem that contains redundant constraints obtained by aggregating constraints from the original master problem formulation. This new formulation is solved in a column/row generation fashion. The efficacy of the proposed method is tested through an extensive experimental campaign, where we solve three different problems that differ considerably in terms of their constraints and objective function. Despite being a generic framework, dyn-SAR requires the embedded CG algorithm to be tailored to the application at hand.
We study a new variant of the vehicle routing problem, which arises in hospital-wide scheduling of physical therapists. Multiple service locations exist for patients, and resource synchronization for the location capacities is required as only a limited number of patients can be treated at one location at a time. Additionally, operations synchronization between treatments is required as precedence relations exist. We develop an innovative exact branch-price-and-cut algorithm including two approaches targeting the synchronization constraints (1) based on branching on time windows and (2) based on adding combinatorial Benders cuts. We optimally solve realistic hospital instances with up to 120 treatments and find that branching on time windows performs better than adding cutting planes.Summary of Contribution: We present an exact branch-price-and-cut (BPC) algorithm for the therapist scheduling and routing problem (ThSRP), a daily planning problem arising at almost every hospital. The difficulty of this problem stems from its inherent structure that features routing and scheduling while considering multiple possible service locations with time-dependent location capacities. We model the ThSRP as a vehicle routing problem with time windows and flexible delivery locations and synchronization constraints, which are properties relevant to other vehicle routing problem variants as well. In our computational study, we show that the proposed exact BPC algorithm is capable of solving realistic hospital instances and can, thus, be used by hospital planners to derive better schedules with less manual work. Moreover, we show that time window branching can be a valid alternative to cutting planes when addressing synchronization constraints in a BPC algorithm.
Identifying the right dose is one of the most important decisions in drug development. Adaptive designs are promoted to conduct dose-finding clinical trials as they are more efficient and ethical compared with static designs. However, current techniques in response-adaptive designs for dose allocation are complex and need significant computational effort, which is a major impediment for implementation in practice. This study proposes a Bayesian nonparametric framework for estimating the dose-response curve, which uses a piecewise linear approximation to the curve by consecutively connecting the expected mean response at each dose. Our extensive numerical results reveal that a first-order Bayesian nonparametric model with a known correlation structure in prior for the expected mean response performs competitively when compared with the standard approach and other more complex models in terms of several relevant metrics and enjoys computational efficiency. Furthermore, structural properties for the optimal learning problem, which seeks to minimize the variance of the target dose, are established under this simple model.Summary of Contribution: In this work, we propose a methodology to derive efficient patient allocation rules in response-adaptive dose-finding clinical trials, where computational issues are the main concern. We show that our methodologies are competitive with the state-of-the-art methodology in terms of solution quality, are significantly more computationally efficient, and are more robust in terms of the shape of the dose-response curve, among other parameter changes. This research fits in “the intersection of computing and operations research” as it adapts operations research techniques to produce computationally attractive solutions to patient allocation problems in dose-finding clinical trials.
We introduce the problem of selecting patient-donor pairs in a kidney exchange program to undergo a crossmatch test, and we model this selection problem as a two-stage stochastic integer programming problem. The optimal solutions of this new formulation yield a larger expected number of realized transplants than previous approaches based on internal recourse or subset recourse. We settle the computational complexity of the selection problem by showing that it remains NP-hard even for maximum cycle length equal to two. Furthermore, we investigate to what extent different algorithmic approaches, including one based on Benders decomposition, are able to solve instances of the model. We empirically investigate the computational efficiency of this approach by solving randomly generated instances and study the corresponding running times as a function of maximum cycle length, and of the presence of nondirected donors.Summary of Contribution: This paper deals with an important and very complex issue linked to the optimization of transplant matchings in kidney exchange programs, namely, the inherent uncertainty in the assessment of compatibility between donors and recipients of transplants. Although this issue has previously received some attention in the optimization literature, most attempts to date have focused on applying recourse to solutions selected within restricted spaces. The present paper explicitly formulates the maximization of the expected number of transplants as a two-stage stochastic integer programming problem. The formulation turns out to be computationally difficulty, both from a theoretical and from a numerical perspective. Different algorithmic approaches are proposed and tested experimentally for its solution. The quality of the kidney exchanges produced by these algorithms compares favorably with that of earlier models.
Surgical practice administrators need to determine the sequence of surgeries and reserved operating room (OR) time for each surgery in the surgery scheduling process. Both decisions require coordination among multiple ORs and the recovery resource in the postanesthesia care unit (PACU) in a surgical suite. Although existing studies have addressed OR time reservation, surgery sequencing coordination is an open challenge in the stochastic surgical environment. In this paper, we propose an algorithmic solution to this problem based on stochastic optimization. The proposed methodology involves the development of a surrogate objective function that is highly correlated with the original one. The resulting surrogate model has network-structured subproblems after Lagrangian relaxation and decomposition, which makes it easier to solve than the impractically difficult original problem. We show that our proposed approach finds near-optimal solutions in small instances and outperforms benchmark methods by 13%–51% or equivalently an estimated saving of $760–$7,420 per day in surgical suites with 4–10 ORs. Our results illustrate a mechanism to alleviate congestion in the PACU. We also recommend that practice administrators prioritize sequencing coordination over the optimization of OR time reservation in an effort for performance improvement. Furthermore, we demonstrate how administrators should consider the impact of sequencing decisions when making strategic capacity adjustments for the PACU.Summary of Contribution: Our work provides an algorithmic solution to an open question in the field of healthcare operations management. This solution approach involves formulating a surrogate optimization model and exploiting its decomposability and network-structure. In computational experiments, we quantitatively benchmark its performance and assess its benefits. Our numerical results provide unique managerial insights for healthcare leadership.
This paper is devoted to the management of advance admission requests for obstetric care. Pregnant women in China select one hospital and request admission for both antenatal and postnatal care after nine weeks of pregnancy. Schedulers must make the admission decision instantly based on the availability of the most critical resource, that is, hospital beds for postnatal care. The random delay between admission requests and postnatal care has created a distinct advance admission control problem. To address this issue, we propose a basic model that assumes a unit bed requirement for one day. Each admission generates a unit of revenue and each unit of overcapacity use incurs an overcapacity cost. With the objective of maximizing the expected net revenue, we establish an optimal policy for unlimited requests, that is, an expected arrival time quota (EATQ) policy that accepts a fixed quota of advance admission requests with the same expected date of confinement. We then propose an extended model for general capacity requirements. Using the Poisson approximation, we establish the optimality of the EATQ policy, which is shown to be solvable by a simple linear programming model. We compare the numerical results from the different policies and conduct a sensitivity analysis. The EATQ policy is demonstrated to be the best option in all test instances and notably outperforms the current admission rules used in hospitals, which usually accept admission requests according to some empirical monthly quota of the expected delivery month. The Poisson approximation is shown to be effective for determining the optimal EATQ policy for both stationary and nonstationary arrivals. Summary of Contribution: First, this paper investigates the advance admission control problem for obstetric care. Pregnant women in China choose one hospital and request admission for both antenatal and postnatal care after nine weeks of pregnancy but the most critical resource is hospitalization beds needed for postnatal care. The random delay between admission request and postnatal care makes the problem unique and challenging to solve. It belongs to the scope of computing and operations research. Second, this paper formulates a dynamic programming model, analyzes the structural properties of the optimal control policy, and finally proposes a mathematical programming model to determine the optimal quota. Numerical experiments show the validity of the proposed approach. It covers the research contents of theories on dynamic stochastic control, mathematic programming model, and experiments. Moreover, this paper is motivated by the practical problem (advance admission control) in obstetric units of Shanghai. Using these optimality properties, solution approaches, and numerical results, this paper provides guidance on how to manage advance obstetric admission requests.
It has been recently shown that an additional therapeutic gain may be achieved if a radiotherapy plan is altered over the treatment course using a new treatment paradigm referred to in the literature as spatiotemporal fractionation. Because of the nonconvex and large-scale nature of the corresponding treatment plan optimization problem, the extent of the potential therapeutic gain that may be achieved from spatiotemporal fractionation has been investigated using stylized cancer cases to circumvent the arising computational challenges. This research aims at developing scalable optimization methods to obtain high-quality spatiotemporally fractionated plans with optimality bounds for clinical cancer cases. In particular, the treatment-planning problem is formulated as a quadratically constrained quadratic program and is solved to local optimality using a constraint-generation approach, in which each subproblem is solved using sequential linear/quadratic programming methods. To obtain optimality bounds, cutting-plane and column-generation methods are combined to solve the Lagrangian relaxation of the formulation. The performance of the developed methods are tested on deidentified clinical liver and prostate cancer cases. Results show that the proposed method is capable of achieving local-optimal spatiotemporally fractionated plans with an optimality gap of around 10%–12% for cancer cases tested in this study.Summary of Contribution: The design of spatiotemporally fractionated radiotherapy plans for clinical cancer cases gives rise to a class of nonconvex and large-scale quadratically constrained quadratic programming (QCQP) problems, the solution of which requires the development of efficient models and solution methods. To address the computational challenges posed by the large-scale and nonconvex nature of the problem, we employ large-scale optimization techniques to develop scalable solution methods that find local-optimal solutions along with optimality bounds. We test the performance of the proposed methods on deidentified clinical cancer cases. The proposed methods in this study can, in principle, be applied to solve other QCQP formulations, which commonly arise in several application domains, including graph theory, power systems, and signal processing.
Recently, diffusion processes in social networks have attracted increasing attention within computer science, marketing science, social sciences, and political science. Although the majority of existing works focus on maximizing the reach of desirable diffusion processes, we are interested in deploying a group of monitors to detect malicious diffusion processes such as the spread of computer worms. In this work, we introduce and study the (α,β)(𝛼,𝛽)-Monitoring Game} on networks. Our game is composed of two parties an attacker and a defender. The attacker can launch an attack by distributing a limited number of seeds (i.e., virus) to the network. Under our (α,β)-Monitoring Game, we say an attack is successful if and only if the following two conditions are satisfied: (1) the outbreak/propagation reaches at least α individuals without intervention, and (2) it has not been detected before reaching β individuals. Typically, we require that β is no larger than α in order to compensate the reaction delays after the outbreak has been detected. On the other end, the defender’s ultimate goal is to deploy a set of monitors in the network that can minimize attacker’s success ratio in the worst-case. (We also extend the basic model by considering a noisy diffusion model, where the propagation probabilities on each edge could vary within an interval.) Our work is built upon recent work in security games, our adversarial setting provides robust solutions in practice.Summary of Contribution: Although the diffusion processes in social networks have been extensively studied, most existing works aim at maximizing the reach of desirable diffusion processes. We are interested in deploying a group of monitors to detect malicious diffusion processes, such as the spread of computer worms. To capture the impact of model uncertainty, we consider a noisy diffusion model in which the propagation probabilities on each edge could vary within an interval. Our work is built upon recent work in security games; our adversarial setting leads to robust solutions in practice.
This study generalizes the r-interdiction median (RIM) problem with fortification to simultaneously consider two types of risks: probabilistic exogenous disruptions and endogenous disruptions caused by intentional attacks. We develop a bilevel programming model that includes a lower-level interdiction problem and a higher-level fortification problem to hedge against such risks. We then prove that the interdiction problem is supermodular and subsequently adopt the cuts associated with supermodularity to develop an efficient cutting-plane algorithm to achieve exact solutions. For the fortification problem, we adopt the logic-based Benders decomposition (LBBD) framework to take advantage of the two-level structure and the property that a facility should not be fortified if it is not attacked at the lower level. Numerical experiments show that the cutting-plane algorithm is more efficient than benchmark methods in the literature, especially when the problem size grows. Specifically, with regard to the solution quality, LBBD outperforms the greedy algorithm in the literature with an up-to 13.2% improvement in the total cost, and it is as good as or better than the tree-search implicit enumeration method.Summary of Contribution: This paper studies an r-interdiction median problem with fortification (RIMF) in a supply chain network that simultaneously considers two types of disruption risks: random disruptions that occur probabilistically and disruptions caused by intentional attacks. The problem is to determine the allocation of limited facility fortification resources to an existing network. It is modeled as a bilevel programming model combining a defender’s problem and an attacker’s problem, which generalizes the r-interdiction median problem with probabilistic fortification. This paper is suitable for IJOC in mainly two aspects: (1) The lower-level attacker’s interdiction problem is a challenging high-degree nonlinear model. In the literature, only a total enumeration method has been applied to solve a special case of this problem. By exploring the special structural property of the problem, namely, the supermodularity of the transportation cost function, we developed an exact cutting-plane method to solve the problem to its optimality. Extensive numerical studies were conducted. Hence, this paper fits in the intersection of operations research and computing. (2) We developed an efficient logic-based Benders decomposition algorithm to solve the higher-level defender’s fortification problem. Overall, this study generalizes several important problems in the literature, such as RIM, RIMF, and RIMF with probabilistic fortification (RIMF-p).
Consider optimizing a periodic schedule for an automated production plant as a last step of a more comprehensive design process. In our scenario, each robot’s cyclic sequence of operations and trajectories between potential waiting points have already been fully specified. Further given are those precedences that fix sequence requirements on operations between different robots. It remains to determine the starting time for each operation or movement of each robot within a common cyclic time period so as to avoid collisions of robots that operate in the same space simultaneously. So the task is to find a conflict-resolving schedule that minimizes this common periodic cycle time while observing all precedence relations and collision avoidance constraints. The proposed cycle time minimization problem for robot coordination has, to the best of our knowledge, not been studied before. We develop an approach for solving it by employing binary search for determining the smallest feasible period time of an iso-periodic event scheduling problem (IPESP). This is a variant of the periodic event scheduling problem in which the objects that have to be scheduled need to obey exactly the same period time. The possibility to wait arbitrarily long at waiting points turns out to be essential to justify the use of binary search for identifying the minimum cycle time, thereby avoiding bilinear mixed integer formulations. Special properties of the given scenario admit bounds on the periodic tension variables of an integer programming formulation. Although the IPESP subproblems remain NP-complete in general, these bounds allow solving real-world instances sufficiently fast for the approach to be applicable in practice. Numerical experiments on real-world and randomly generated data are supplied to illustrate the potential and limitations of this approach.Summary of Contribution: When designing automated production plants, a crucial step is to identify the smallest possible per unit period time for the production processes. Based on periodic event scheduling ideas, we develop and analyze mathematical methods for this purpose. We show that the algorithmic implementation of our approach provides an answer to current real-world designs in reasonable time.
In critical node problems, the task is to identify a small subset of so-called critical nodes whose deletion maximally degrades a network’s “connectivity” (however that is measured). Problems of this type have been widely studied, for example, for limiting the spread of infectious diseases. However, existing approaches for solving them have typically been limited to networks having fewer than 1,000 nodes. In this paper, we consider a variant of this problem in which the task is to delete b nodes so as to minimize the number of node pairs that remain connected by a path of length at most k. With the techniques developed in this paper, instances with up to 17,000 nodes can be solved exactly. We introduce two integer programming formulations for this problem (thin and path-like) and compare them with an existing recursive formulation. Although the thin formulation generally has an exponential number of constraints, it admits an efficient separation routine. Also helpful is a new, more general preprocessing procedure that, on average, fixes three times as many variables than before.Summary of Contribution: In this paper, we consider a distance-based variant of the critical node problem in which the task is to delete b nodes so as to minimize the number of node pairs that remain connected by a path of length at most k. This problem is motivated by applications in social networks, telecommunications, and transportation networks. In our paper, we aim to solve large-scale instances of this problem. Standard out-of-the-box approaches are unable to solve such instances, requiring new integer programming models, methodological contributions, and other computational insights. For example, we propose an algorithm for finding a maximum independent set of simplicial nodes that runs in time O(nm) that we use in a preprocessing procedure; we also prove that the separation problem associated with one of our integer programming models is NP-hard. We apply our branch-and-cut implementation to real-life networks from a variety of domains and observe speedups over previous approaches.
The inequity aversion pricing problem aims to maximize revenue while providing prices to people connected in a social network such that connected people receive prices that are not too different. This problem is known to be NP-hard even when the number of prices offered is three. This paper provides an extended graph formulation for the problem whose LP-relaxation is shown to be very strong. We show that the extended graph relaxation is integral on a network without any cycle. We develop extended cycle inequalities and show that the extended cycle inequalities cut off all the fractional extreme points of the extended graph relaxation on a cycle. We generalize cycle inequalities to zero half cuts performing a Chvátal–Gomory procedure on a cycle. Computational experiments show that the extended graph relaxation results in an integer solution for most problem instances with very small gaps (less than 3%) from optimality for the remaining instances. The addition of zero half cuts reduces the integrality gap significantly on the few difficult instances.Summary of Contribution: The inequity aversion pricing problem aims to maximize revenue while providing prices to people connected in a social network such that connected people receive prices that are not too different. This paper provides an extended graph formulation of this practical revenue management problem whose LP-relaxation is shown to be very strong. The authors show that the extended graph relaxation is integral on a network without any cycle. They develop extended cycle inequalities and generalize them to zero-half cuts. Computational experiments show that the extended graph formulation results in an integer solution or a very small integrality gap. For difficult instances, the addition of zero half cuts significantly reduces the integrality gap.
Motivated by applications arising on social networks, we study a generalization of the celebrated dominating set problem called the Positive Influence Dominating Set (PIDS). Given a graph G with a set V of nodes and a set E of edges, each node i in V has a weight bi, and a threshold requirement gi. We seek a minimum weight subset T of V, so that every node i not in T is adjacent to at least gi members of T. When gi is one for all nodes, we obtain the weighted dominating set problem. First, we propose a strong and compact extended formulation for the PIDS problem. We then project the extended formulation onto the space of the natural node-selection variables to obtain an equivalent formulation with an exponential number of valid inequalities. Restricting our attention to trees, we show that the extended formulation is the strongest possible formulation, and its projection (onto the space of the node variables) gives a complete description of the PIDS polytope on trees. We derive the necessary and sufficient facet-dening conditions for the valid inequalities in the projection and discuss their polynomial time separation. We embed this (exponential size) formulation in a branch-and-cut framework and conduct computational experiments using real-world graph instances, with up to approximately 2.5 million nodes and 8 million edges. On a test-bed of 100 real-world graph instances, our approach finds solutions that are on average 0.2% from optimality and solves 51 out of the 100 instances to optimality.Summary of Contribution: In influence maximization problems, a decision maker wants to target individuals strategically to cause a cascade at a minimum cost over a social network. These problems have attracted significant attention as their applications can be found in many different domains including epidemiology, healthcare, marketing, and politics. However, computationally solving large-scale influence maximization problems to near optimality remains a substantial challenge for the computing community, which thus represent significant opportunities for the development of operations-research based models, algorithms, and analysis in this interface. This paper studies the positive influence dominating set (PIDS) problem, an influence maximization problem on social networks that generalizes the celebrated dominating set problem. It focuses on developing exact methods for solving large instances to near optimality. In other words, the approach results in strong bounds, which then provide meaningful comparative benchmarks for heuristic approaches. The paper first shows that straightforward generalizations of well-known formulations for the dominating set problem do not yield strong (i.e., computationally viable) formulations for the PIDS problem. It then strengthens these formulations by proposing a compact extended formulation and derives its projection onto the space on the natural node-selection variables, resulting in two equivalent (stronger) formulations for the PIDS problem. The projected formulation on the natural node-variables contains a new class of valid inequalities that are shown to be facet-defining for the PIDS problem. These theoretical results are complemented by in-depth computational experiments using a branch-and-cut framework, on a testbed of 100 real-world graph instances, with up to approximately 2.5 million nodes and 8 million edges. They demonstrate the effectiveness of the proposed formulation in solving large scale problems finding solutions that are on average 0.2% from optimality and solving 51 of the 100 instances to optimality.
Fluence map optimization for intensity-modulated radiation therapy planning can be formulated as a large-scale inverse problem with competing objectives and constraints associated with the tumors and organs at risk. Unfortunately, clinically relevant dose–volume constraints are nonconvex, so standard algorithms for convex problems cannot be directly applied. Although prior work focuses on convex approximations for these constraints, we propose a novel relaxation approach to handle nonconvex dose–volume constraints. We develop efficient, provably convergent algorithms based on partial minimization, and show how to adapt them to handle maximum-dose constraints and infeasible problems. We demonstrate our approach using the CORT data set and show that it is easily adaptable to radiation treatment planning with dose–volume constraints for multiple tumors and organs at risk.Summary of Contribution: This paper proposes a novel approach to deal with dose–volume constraints in radiation treatment planning optimization, which is inherently nonconvex, mixed-integer programming. The authors tackle this NP-hard problem using auxiliary variables and continuous optimization while preserving the problem’s nonconvexity. Algorithms to efficiently solve the nonconvex optimization problem presented in this paper yield computation speeds suitable for a busy clinical setting.
In the resource allocation problem (RAP), the goal is to divide a given amount of a resource over a set of activities while minimizing the cost of this allocation and possibly satisfying constraints on allocations to subsets of the activities. Most solution approaches for the RAP and its extensions allow each activity to have its own cost function. However, in many applications, often the structure of the objective function is the same for each activity, and the difference between the cost functions lies in different parameter choices, such as, for example, the multiplicative factors. In this article, we introduce a new class of objective functions that captures a significant number of the objectives occurring in studied applications. These objectives are characterized by a shared structure of the cost function depending on two input parameters. We show that, given the two input parameters, there exists a solution to the RAP that is optimal for any choice of the shared structure. As a consequence, this problem reduces to the quadratic RAP, making available the vast amount of solution approaches and algorithms for the latter problem. We show the impact of our reduction result on several applications, and in particular, we improve the best-known worst-case complexity bound of two problems in vessel routing and processor scheduling from O(n2) to O(n log n).Summary of Contribution: The resource allocation problem (RAP) with submodular constraints and its special cases are classic problems in operations research. Because these problems are studied in many different scientific disciplines, many conceptual insights, structural properties, and solution approaches have been reinvented and rediscovered many times. The goal of this article is to reduce the amount of future reinventions and rediscoveries by bringing together these different perspectives on RAPs in a way that is accessible to researchers with different backgrounds. The article serves as an exposition on RAPs and on their wide applicability in many areas, including telecommunications, energy, and logistics. In particular, we provide tools and examples that can be used to formulate and solve problems in these areas as RAPs. To accomplish this, we make three concrete contributions. First, we provide a survey on algorithms and complexity results for RAPs and discuss several recent advances in these areas. Second, we show that many objectives for RAPs can be reduced to a (simpler) quadratic objective function, which makes available the extensive collection of fast and efficient algorithms for quadratic RAPs to solve these problems. Third, we discuss the impact that RAPs and the aforementioned reduction result can make in several application areas.
The subgame perfect equilibrium in stationary strategies (SSPE) is the most important solution concept in applications of stochastic games, making it imperative to develop efficient methods to compute an SSPE. For this purpose, this paper develops an interior-point differentiable path-following method (IPM), which establishes a connection between an artificial logarithmic barrier game and the stochastic game of interest by adding a homotopy variable. IPM brings several advantages over the existing methods for stochastic games. On the one hand, IPM provides a bridge between differentiable path-following methods and interior-point methods and remedies several issues of an existing homotopy method called the stochastic linear tracing procedure (SLTP). First, the starting stationary strategy profile can be arbitrarily chosen. Second, IPM does not need switching between different systems of equations. Third, the use of a perturbation term makes IPM applicable to all stochastic games rather than generic games only. Moreover, a well-chosen transformation of variables reduces the number of equations and variables by roughly one half. Numerical results show that the proposed method is more than three times as efficient as SLTP. On the other hand, the stochastic game can be reformulated as a mixed complementarity problem and solved by the PATH solver. We employ the proposed IPM and the PATH solver to compute SSPEs. Numerical results evince that for some stochastic games the PATH solver may fail to find an SSPE, whereas IPM is successful in doing so for all stochastic games, which confirms the reliability and stability of the proposed method.Summary of Contribution: This paper incorporates the interior-point methods into a differentiable path-following method for computing stationary equilibria for stochastic games. This novel method brings excellent computational advantages and remedies several issues with the existing methods for stochastic games. We prove the global convergence of the proposed method and employ this method to solve numerous randomly generated stochastic games with different scales. Numerical results further confirm the high efficiency, stability, and universality of this method for stochastic games.
A capacitated vehicle routing problem with two-dimensional loading constraints is addressed. Associated with each customer are a set of rectangular items, the total weight of the items, and a time window. Designing exact algorithms for the problem is very challenging because the problem is a combination of two NP-hard problems. An exact branch-and-price algorithm and an approximate counterpart are proposed to solve the problem. We introduce an exact dominance rule and an approximate dominance rule. To cope with the difficulty brought by the loading constraints, a new column generation mechanism boosted by a supervised learning model is proposed. Extensive experiments demonstrate the superiority of integrating the learning model in terms of CPU time and calls of the feasibility checker. Moreover, the branch-and-price algorithms are able to significantly improve the solutions of the existing instances from literature and solve instances with up to 50 customers and 103 items.Summary of Contribution: We wish to submit an original research article entitled “Learning-based branch-and-price algorithms for a vehicle routing problem with time windows and two-dimensional loading constraints” for consideration by IJOC. We confirm that this work is original and has not been published elsewhere, nor is it currently under for publication elsewhere. In this paper, we report a study in which we develop two branch-and-price algorithms with a machine learning model injected to solve a vehicle routing problem integrated the two-dimensional packing. Due to the complexity brought by the integration, studies on exact algorithms in this field are very limited. Our study is important to the field, because we develop an effective method to significantly mitigate computational burden brought by the packing problem so that exactness turns to be achievable within reasonable time budget. The approach can be generalized to the three-dimensional case by simply replacing the packing algorithm. It can also be adapted for other VRPs when high-dimensional loading constraints are concerned. Broadly speaking, the study is a typical example of adopting supervised learning to achieve acceleration for operations research algorithms, which expands the envelop of computing and operations research. Hence, we believe this manuscript is appropriate for publication by IJOC.
We consider a large family of problems in which an ordering (or, more precisely, a chain of subsets) of a finite set must be chosen to minimize some weighted sum of costs. This family includes variations of min sum set cover, several scheduling and search problems, and problems in Boolean function evaluation. We define a new problem, called the min sum ordering problem (MSOP), which generalizes all these problems using a cost and a weight function defined on subsets of a finite set. Assuming a polynomial time α-approximation algorithm for the problem of finding a subset whose ratio of weight to cost is maximal, we show that under very minimal assumptions, there is a polynomial time 4α4𝛼-approximation algorithm for MSOP. This approximation result generalizes a proof technique used for several distinct problems in the literature. We apply this to obtain a number of new approximation results.Summary of Contribution: This paper provides a general framework for min sum ordering problems. Within the realm of theoretical computer science, these problems include min sum set cover and its generalizations, as well as problems in Boolean function evaluation. On the operations research side, they include problems in search theory and scheduling. We present and analyze a very general algorithm for these problems, unifying several previous results on various min sum ordering problems and resulting in new constant factor guarantees for others.
This study introduces a branch-and-bound algorithm to solve mixed-integer bilinear maximum multiplicative programs (MIBL-MMPs). This class of optimization problems arises in many applications, such as finding a Nash bargaining solution (Nash social welfare optimization), capacity allocation markets, reliability optimization, etc. The proposed algorithm applies multiobjective optimization principles to solve MIBL-MMPs exploiting a special characteristic in these problems. That is, taking each multiplicative term in the objective function as a dummy objective function, the projection of an optimal solution of MIBL-MMPs is a nondominated point in the space of dummy objectives. Moreover, several enhancements are applied and adjusted to tighten the bounds and improve the performance of the algorithm. The performance of the algorithm is investigated by 400 randomly generated sample instances of MIBL-MMPs. The obtained result is compared against the outputs of the mixed-integer second order cone programming (SOCP) solver in CPLEX and a state-of-the-art algorithm in the literature for this problem. Our analysis on this comparison shows that the proposed algorithm outperforms the fastest existing method, that is, the SOCP solver, by a factor of 6.54 on average.Summary of Contribution: The scope of this paper is defined over a class of mixed-integer programs, the so-called mixed-integer bilinear maximum multiplicative programs (MIBL-MMPs). The importance of MIBL-MMPs is highlighted by the fact that they are encountered in applications, such as Nash bargaining, capacity allocation markets, reliability optimization, etc. The mission of the paper is to introduce a novel and effective criterion space branch-and-cut algorithm to solve MIBL-MMPs by solving a finite number of single-objective mixed-integer linear programs. Starting with an initial set of primal and dual bounds, our proposed approach explores the efficient set of the multiobjective problem counterpart of the MIBL-MMP through a criterion space–based branch-and-cut paradigm and iteratively improves the bounds using a branch-and-bound scheme. The bounds are obtained using novel operations developed based on Chebyshev distance and piecewise McCormick envelopes. An extensive computational study demonstrates the efficacy of the proposed algorithm.
Inverse optimization—determining parameters of an optimization problem that render a given solution optimal—has received increasing attention in recent years. Although significant inverse optimization literature exists for convex optimization problems, there have been few advances for discrete problems, despite the ubiquity of applications that fundamentally rely on discrete decision making. In this paper, we present a new set of theoretical insights and algorithms for the general class of inverse mixed integer linear optimization problems. Specifically, a general characterization of optimality conditions is established and leveraged to design new cutting plane solution algorithms. Through an extensive set of computational experiments, we show that our methods provide substantial improvements over existing methods in solving the largest and most difficult instances to date.
The sparse portfolio selection problem is one of the most famous and frequently studied problems in the optimization and financial economics literatures. In a universe of risky assets, the goal is to construct a portfolio with maximal expected return and minimum variance, subject to an upper bound on the number of positions, linear inequalities, and minimum investment constraints. Existing certifiably optimal approaches to this problem have not been shown to converge within a practical amount of time at real-world problem sizes with more than 400 securities. In this paper, we propose a more scalable approach. By imposing a ridge regularization term, we reformulate the problem as a convex binary optimization problem, which is solvable via an efficient outer-approximation procedure. We propose various techniques for improving the performance of the procedure, including a heuristic that supplies high-quality warm-starts, and a second heuristic for generating additional cuts that strengthens the root relaxation. We also study the problem’s continuous relaxation, establish that it is second-order cone representable, and supply a sufficient condition for its tightness. In numerical experiments, we establish that a conjunction of the imposition of ridge regularization and the use of the outer-approximation procedure gives rise to dramatic speedups for sparse portfolio selection problems.Summary of Contribution: This paper proposes a new decomposition scheme for tackling the problem of sparse portfolio selection: the problem of selecting a limited number of securities in a portfolio. This is a challenging problem to solve in high dimensions, as it belongs to the class of mixed-integer, nonseparable nonlinear optimization problems. We propose a new Benders-type cutting plane method and demonstrate its efficacy on a wide set of both synthetic and real-world problems, including problems with thousands of securities. Our approach also provides insights for other mixed-integer optimization problems with logical constraints.
This paper addresses a single machine total weighted tardiness (TWT) batch-scheduling problem in which jobs have release dates, nonidentical sizes, and are compatible between each other. We propose two integer linear programming models: the first one is a time-indexed formulation (TIF), and the second is an innovative time-size-indexed formulation (TSIF). Although TIF clearly outperforms the existing formulation for the problem, TSIF is capable of producing much stronger bounds in practice. The latter also enables us to develop an efficient column-generation (CG) algorithm. The pricing subproblem corresponds to a resource-constrained shortest path problem that is solved using a bucket graph–based labeling algorithm. The solutions of such a subproblem may contain cycles (reprocessing of jobs), and thus, a memory mechanism called dynamic arc-based ng-sets is employed in the labeling with a view toward avoiding some of them. Moreover, we also implement a preprocessing scheme based on Lagrangian relaxation to perform variable fixing. Extensive computational experiments were carried out in 810 benchmark instances. The proposed CG algorithm is capable of solving instances with up to 100 jobs to optimality. In addition, we believe that this is the first exact approach for a TWT batch-scheduling variant capable of systematically solving instances with up to 50 jobs. High-quality results are also reported for three special cases of the problem—more precisely, when (i) the penalty weights are unitary, (ii) there are no release dates, and (iii) all due dates are set to zero and, hence, the objective becomes equivalent to minimizing the weighted completion time.Summary of Contribution: This paper provides the first exact algorithm for a standard variant of a batch-scheduling total weighted tardiness problem that can solve instances with up to 100 jobs to optimality, a considerable leap with respect to previous works. In particular, we propose a time-indexed formulation that has the advantage of being relatively simple to implement, and yet we show that it is not theoretically dominated by the other innovative formulation proposed in the paper referred to as the time-size-indexed formulation (TSIF). Moreover, we present a Lagrangian approach to quickly fix variables and an iterative column-generation (CG) procedure over a Dantzig–Wolfe decomposition of TSIF that combines an efficient pricing algorithm with a dynamic scheme to adjust the subproblem constraints. The proposed CG approach is capable of producing very strong bounds for the problem as well as for some of its special cases.
Aggregation of heating, ventilation, and air conditioning (HVAC) loads can provide reserves to absorb volatile renewable energy, especially solar photo-voltaic (PV) generation. In this paper, we decide HVAC control schedules under uncertain PV generation, using a distributionally robust chance-constrained (DRCC) building load control model under two typical ambiguity sets: the moment-based and Wasserstein ambiguity sets. We derive mixed integer linear programming (MILP) reformulations for DRCC problems under both sets. Especially, for the Wasserstein ambiguity set, we use the right-hand side (RHS) uncertainty to derive a more compact MILP reformulation than the commonly known MILP reformulations with big-M constants. All the results also apply to general individual chance constraints with RHS uncertainty. Furthermore, we propose an adjustable chance-constrained variant to achieve tradeoff between the operational risk and costs. We derive MILP reformulations under the Wasserstein ambiguity set and second-order conic programming (SOCP) reformulations under the moment-based set. Using real-world data, we conduct computational studies to demonstrate the efficiency of the solution approaches and the effectiveness of the solutions.Summary of Contribution: The problem studied in this paper is motivated by a building load control problem that uses the aggregation of heating, ventilation, and air conditioning (HVAC) loads as flexible reserves to absorb uncertain solar photovoltaic (PV) generation. The problem is formulated as distributionally robust chance-constrained (DRCC) programs with right-hand side (RHS) uncertainty. In addition, we propose a risk-adjustable variant of the DRCC programs, where the risk level, instead of being predetermined, is treated as a decision variable. The paper aims to provide tractable reformulations and solution algorithms for both the (general) DRCC and the (general) adjustable DRCC models with RHS uncertainty.
In this paper, we propose a general algorithmic framework for rotating workforce scheduling. We develop a graph representation that allows to model a schedule as a Eulerian cycle of stints, which we then use to derive a problem formulation that is compact toward the number of employees. We develop a general branch-and-cut framework that solves rotating workforce scheduling in its basic variant, as well as several additional problem variants that are relevant in practice. These variants comprise, among others, objectives for the maximization of free weekends and the minimization of employees. Our computational studies show that the developed framework constitutes a new state of the art for rotating workforce scheduling. For the first time, we solve all 6,000 instances of the status quo benchmark for rotating workforce scheduling to optimality with an average computational time of 0.07 seconds and a maximum computational time of 2.53 seconds. These results reduce average computational times by more than 99% compared with existing methods. Our algorithmic framework shows consistent computational performance, which is robust across all studied problem variants.Summary of Contribution: This paper proposes a novel exact algorithmic framework for the well-known rotating workforce scheduling problem (RWSP). Although the RWSP has been extensively studied in different problem variants and for different exact and heuristic solution approaches, the presented algorithmic framework constitutes a new state-of-the-art for the RWSP that solves all known benchmark sets to optimality and improves on the current state-of-the-art by orders of magnitude with respect to computational times, especially for large-scale instances. The paper is both of methodological value for researchers and of high interest for practitioners. For researchers, the presented framework is amenable for various problem variants and provides a common ground for further studies and research. For practitioners and software developers, low computational times of a few seconds allows the framework to be to embedded into personnel scheduling software.
Learning human mobility behaviors from location-sensing data are crucial to mobility data mining because of its potential to address a range of analytical purposes in mobile context reasoning, including exploration, inference, and prediction. However, existing approaches suffer from two practical problems: temporal and spatial sparsity. To address these shortcomings, we present two unsupervised learning methods to model the mobility behaviors of multiple users (i.e., a population), considering efficiency and accuracy. These methods intelligently overcome the sparsity in individual data by seeking temporal commonality among users’ heterogeneous location behaviors. The advantages of our models are highlighted through experiments on several real-world mobility data sets, which also show how our methods can realize the three analytical purposes in a unified manner.
Various digital data sets that encode user-item relationships contain a multilevel overlapping cluster structure. The user-item relation can be encoded in a weighted bipartite graph and uncovering these overlapping coclusters of users and items at multiple levels in the bipartite graph can play an important role in analyzing user-item data in many applications. For example, for effective online marketing, such as placing online ads or deploying smart online marketing strategies, identifying co-occurring clusters of users and items can lead to accurately targeted advertisements and better marketing outcomes. In this paper, we propose fast algorithms inspired by algebraic multigrid methods for finding multilevel overlapping cocluster structures of feature matrices that encode user-item relations. Starting from the weighted bipartite graph structure of the feature matrix, the algorithms use agglomeration procedures to recursively coarsen the bipartite graphs that represent the relations between the coclusters on increasingly coarser levels. New fast coarsening routines are described that circumvent the bottleneck of all-to-all similarity computations by exploiting measures of direct connection strength between row and column variables in the feature matrix. Providing accurate coclusters at multiple levels in a manner that can scale to large data sets is a challenging task. In this paper, we propose heuristic algorithms that approximately and recursively minimize normalized cuts to obtain coclusters in the aggregated bipartite graphs on multiple levels of resolution. Whereas the main novelty and focus of the paper lies in algorithmic aspects of reducing computational complexity to obtain scalable methods specifically for large rectangular user-item matrices, the algorithmic variants also define several new models for determining multilevel coclusters that we justify intuitively by relating them to principles that underlie collaborative filtering methods for user-item relationships. Experimental results show that the proposed algorithms successfully uncover the multilevel overlapping cluster structure for artificial and real data sets.Summary of Contribution: This paper develops new and efficient computational methods for finding the multilevel overlapping cocluster structure of feature matrices that encode user-item relationships. We base our approach on the use of pairwise similarity measures between features, seeking clusters of points that are similar to each other and dissimilar from the points outside the cluster. We approximately solve the problem of finding optimal overlapping coclusters on multiple levels by employing a framework that is based on efficient multilevel methods that have been used previously to solve sparse linear systems and to cluster graphs. Our main contribution is that we extend these methods in efficient manners to find coclusters in the bipartite graphs that encode common and important user-item relationships or social network relations. The novel methods that we propose are inherently scalable to large problem sizes and are naturally able to uncover overlapping coclusters at multiple levels, whereas existing methods generally only find coclusters at the fine level. We illustrate the algorithm and its performance on some standard test problems from the literature and on a proof-of-concept real-world data set that relates LinkedIn users to their skills and expertise.
The randomutility maximization model is by far the most adopted framework to estimate consumer choice behavior. However, behavioral economics has provided strong empirical evidence of irrational choice behaviors, such as halo effects, that are incompatible with this framework. Models belonging to the random utility maximization family may therefore not accurately capture such irrational behavior. Hence, more general choice models, overcoming such limitations, have been proposed. However, the flexibility of such models comes at the price of increased risk of overfitting. As such, estimating such models remains a challenge. In this work, we propose an estimation method for the recently proposed generalized stochastic preference choice model, which subsumes the family of random utility maximization models and is capable of capturing halo effects. In particular, we propose a column-generation method to gradually refine the discrete choice model based on partially ranked preference sequences. Extensive computational experiments indicate that our model, explicitly accounting for irrational preferences, can significantly boost the predictive accuracy on both synthetic and real-world data instances.Summary of Contribution: In this work, we propose an estimation method for the recently proposed generalized stochastic preference choice model, which subsumes the family of random utility maximization models and is capable of capturing halo effects. Specifically, we show how to use partially ranked preferences to efficiently model rational and irrational customer types from transaction data. Our estimation procedure is based on column generation, where relevant customer types are efficiently extracted by expanding a treelike data structure containing the customer behaviors. Furthermore, we propose a new dominance rule among customer types whose effect is to prioritize low orders of interactions among products. An extensive set of experiments assesses the predictive accuracy of the proposed approach by comparing it against rank-based methods with only rational preferences and with more general benchmarks from the literature. Our results show that accounting for irrational preferences can boost predictive accuracy by 12.5% on average when tested on a real-world data set from a large chain of grocery and drug stores.
A key question in causal inference analyses is how to find subgroups with elevated treatment effects. This paper takes a machine learning approach and introduces a generative model, causal rule sets (CRS), for interpretable subgroup discovery. A CRS model uses a small set of short decision rules to capture a subgroup in which the average treatment effect is elevated. We present a Bayesian framework for learning a causal rule set. The Bayesian model consists of a prior that favors simple models for better interpretability as well as avoiding overfitting and a Bayesian logistic regression that captures the likelihood of data, characterizing the relation between outcomes, attributes, and subgroup membership. The Bayesian model has tunable parameters that can characterize subgroups with various sizes, providing users with more flexible choices of models from the treatment-efficient frontier. We find maximum a posteriori models using iterative discrete Monte Carlo steps in the joint solution space of rules sets and parameters. To improve search efficiency, we provide theoretically grounded heuristics and bounding strategies to prune and confine the search space. Experiments show that the search algorithm can efficiently recover true underlying subgroups. We apply CRS on public and real-world data sets from domains in which interpretability is indispensable. We compare CRS with state-of-the-art rule-based subgroup discovery models. Results show that CRS achieves consistently competitive performance on data sets from various domains, represented by high treatment-efficient frontiers.Summary of Contribution: This paper is motivated by the large heterogeneity of treatment effect in many applications and the need to accurately locate subgroups for enhanced treatment effect. Existing methods either rely on prior hypotheses to discover subgroups or greedy methods, such as tree-based recursive partitioning. Our method adopts a machine learning approach to find an optimal subgroup learned with a carefully global objective. Our model is more flexible in capturing subgroups by using a set of short decision rules compared with tree-based baselines. We evaluate our model using a novel metric, treatment-efficient frontier, that characterizes the trade-off between the subgroup size and achievable treatment effect, and our model demonstrates better performance than baseline models.
Because of the accessibility of big data collections from consumers, products, and stores, advanced sales forecasting capabilities have drawn great attention from many businesses, especially those in retail, because of the importance of forecasting in decision making. Improvement of forecasting accuracy, even by a small percentage, may have a substantial impact on companies’ production and financial planning, marketing strategies, inventory controls, and supply chain management. Specifically, our research goal is to forecast the sales of each product in each store in the near future. Motivated by tensor factorization methodologies for context-aware recommender systems, we propose a novel approach called the advanced temporal latent factor approach to sales forecasting, or ATLAS for short, which achieves accurate and individualized predictions for sales by building a single tensor factorization model across multiple stores and products. Our contribution is a combination of a tensor framework (to leverage information across stores and products), a new regularization function (to incorporate demand dynamics), and extrapolation of the tensor into future time periods using state-of-the-art statistical (seasonal autoregressive integrated moving-average models) and machine-learning (recurrent neural networks) models. The advantages of ATLAS are demonstrated on eight product category data sets collected by Information Resources, Inc., where we analyze a total of 165 million weekly sales transactions of over 15,560 products from more than 1,500 grocery stores.Summary of Contribution: Sales forecasting has been a task of long-standing importance. Accurate sales forecasting provides critical managerial implications for companies’ decision making and operations. Improvement of forecasting accuracy may have a substantial impact on companies’ production planning, marketing strategies, inventory controls, and supply chain management, among other things. This paper proposes a novel computational (machine-learning-based) approach to sales forecasting and thus is positioned directly at the intersection of computing and business/operations research.
We propose an approach for uncovering characteristic response paths of a population from an individual-level multivariate time series data set. The approach is based on a model that accommodates a set of arbitrary distributions for endogenous variables and interstep intervals and variables. The model enables reliable estimation of individual-level parameters by uncovering and statistically pooling clusters of similar individuals. We show that using such a model one can distribute the response of an outcome variable to an impulse over all possible preceding activity sequences. When a few such sequences explain most of the response, they describe the population’s characteristic response paths from the impulse to the outcome. We apply the proposed approach to a customer touchpoint data set from a large multichannel specialty retailer. This application uncovers six customer segments, each with unique characteristic paths to purchase. These paths provide insights into the behavior of customers and the optimal over-time communication strategy for different customer segments.Summary of Contribution: Uncovering users’ paths through physical and virtual spaces has been of considerable interest in the computing and operations research domain. The existing research suggests a demand for visualizing the primary paths of agents through geographic, online, and activity spaces. Thus far, most of the research have developed approaches that are unique to specific domains providing insight into the domain in the process. There is a need for a general statistically robust approach that can be applied to a broad range of domains to uncover variable sequences that lead to outcomes of interest. We propose a computational approach to uncover characteristic response paths of a population from an individual-level multivariate time series data set. The approach is based on a statistical model that accommodates arbitrary and mixed set of distributions for the endogenous variables, accommodates intersession intervals and variables, and reliably estimates individuals’ parameters through statistical pooling by uncovering clusters of similar members. These features make the proposed model suitable for a large variety of real-world datasets. We show that using such a model one can extract characteristic paths over possible activity sequences starting from an impulse leading up to a target variable of interest.
A notorious problem in queueing theory is to compute the worst possible performance of the GI/G/1 queue under mean-dispersion constraints for the interarrival- and service-time distributions. We address this extremal queue problem by measuring dispersion in terms of mean absolute deviation (MAD) instead of the more conventional variance, making available methods for distribution-free analysis. Combined with random walk theory, we obtain explicit expressions for the extremal interarrival- and service-time distributions and, hence, the best possible upper bounds for all moments of the waiting time. We also obtain tight lower bounds that, together with the upper bounds, provide robust performance intervals. We show that all bounds are computationally tractable and remain sharp also when the mean and MAD are not known precisely but are estimated based on available data instead.Summary of Contribution: Queueing theory is a classic OR topic with a central role for the GI/G/1 queue. Although this queueing system is conceptually simple, it is notoriously hard to determine the worst-case expected waiting time when only knowing the first two moments of the interarrival- and service-time distributions. In this setting, the exact form of the extremal distribution can only be determined numerically as the solution to a nonconvex nonlinear optimization problem. Our paper demonstrates that using mean absolute deviation (MAD) instead of variance alleviates the computational intractability of the extremal GI/G/1 queue problem, enabling us to state the worst-case distributions explicitly.
We consider a Markovian multiserver queueing system with two customer classes, preemptive priorities, and reneging. We formulate this system as an infinite level-dependent quasi-birth-death process (LDQBD). We introduce an algorithm that endogenously truncates the level and calculates lower and upper bounds on stationary probabilities of this LDQBD such that the gap between the bounds can be any desired amount. Our algorithm can be applied to any LDQBD for which the rate matrices become elementwise nonincreasing above some level. This appears to be the first algorithm that provides bounds on stationary probabilities for an infinite-level LDQBD. To obtain these bounds, the algorithm first obtains lower and upper bounds on the rate matrices of the LDQBD using a novel method, which can be applied to any LDQBD. We then extend this algorithm to approximate performance measures of the system of interest and calculate exact lower and upper bounds for those that can be expressed as probabilities, such as the probability that an incoming low-priority customer will wait. We generate a wide range of instances with up to 100 servers and compare the solution times and accuracy of our algorithm with two existing algorithms. These numerical experiments indicate that our algorithm is faster than the other two algorithms for a given accuracy requirement. We investigate the impact of changing service rates on the proportion of low-priority customers served and their wait time, and we demonstrate how ignoring one of these measures can possibly mislead decision makers.Summary of Contribution: We contribute to operations research by modeling a practically important queueing system and developing an algorithm to accurately compute performance measures for that system. We also contribute to computer science by providing error and complexity analysis for the algorithm to solve a broad class of two-dimensional Markov chains with infinite state space.
Sequential ranking-and-selection procedures deliver Bayesian guarantees by repeatedly computing a posterior quantity of interest—for example, the posterior probability of good selection or the posterior expected opportunity cost—and terminating when this quantity crosses some threshold. Computing these posterior quantities entails nontrivial numerical computation. Thus, rather than exactly check such posterior-based stopping rules, it is common practice to use cheaply computable bounds on the posterior quantity of interest, for example, those based on Bonferroni’s or Slepian’s inequalities. The result is a conservative procedure that samples more simulation replications than are necessary. We explore how the time spent simulating these additional replications might be better spent computing the posterior quantity of interest via numerical integration, with the potential for terminating the procedure sooner. To this end, we develop several methods for improving the computational efficiency of exactly checking the stopping rules. Simulation experiments demonstrate that the proposed methods can, in some instances, significantly reduce a procedure’s total sample size. We further show these savings can be attained with little added computational effort by making effective use of a Monte Carlo estimate of the posterior quantity of interest.Summary of Contribution: The widespread use of commercial simulation software in industry has made ranking-and-selection (R&S) algorithms an accessible simulation-optimization tool for operations research practitioners. This paper addresses computational aspects of R&S procedures delivering finite-time Bayesian statistical guarantees, primarily the decision of when to terminate sampling. Checking stopping rules entails computing or approximating posterior quantities of interest perceived as being computationally intensive to evaluate. The main results of this paper show that these quantities can be efficiently computed via numerical integration and can yield substantial savings in sampling relative to the prevailing approach of using conservative bounds. In addition to enhancing the performance of Bayesian R&S procedures, the results have the potential to advance other research in this space, including the development of more efficient allocation rules.
Estimating the unknown density from which a given independent sample originates is more difficult than estimating the mean in the sense that, for the best popular nonparametric density estimators, the mean integrated square error converges more slowly than at the canonical rate of 𝒪(1/𝑛)O(1/n). When the sample is generated from a simulation model and we have control over how this is done, we can do better. We examine an approach in which conditional Monte Carlo yields, under certain conditions, a random conditional density that is an unbiased estimator of the true density at any point. By averaging independent replications, we obtain a density estimator that converges at a faster rate than the usual ones. Moreover, combining this new type of estimator with randomized quasi–Monte Carlo to generate the samples typically brings a larger improvement on the error and convergence rate than for the usual estimators because the new estimator is smoother as a function of the underlying uniform random numbers.Summary of Contribution: Stochastic simulation is commonly used to estimate the mathematical expectation of some output random variable X together with a confidence interval for this expectation. But the simulations usually provide information to do much more, such as estimating the entire distribution (or density) of X. Histograms are routinely provided by standard simulation software, but they are very primitive density estimators. Kernel density estimators perform better, but they are trickier to use, have bias, and their mean square error converges more slowly than the canonical rate of O(1/n) with n independent samples. In this paper, we explain how to construct unbiased density estimators that converge at the canonical rate and even much faster when combined with randomized quasi–Monte Carlo. The key idea is to use conditional Monte Carlo to hide appropriate information and obtain a computable (random) conditional density, which acts (under certain conditions) as an unbiased density estimator. Moreover, this sample density is typically smoother than the classic density estimators as a function of the underlying uniform random numbers, so it can get along much better with randomized quasi–Monte Carlo methods. This offers an opportunity to further improve the O(1/n) rate. We observe rates near O(1/n2) on some examples, and we give conditions under which this type of rate provably holds. The proposed approach is simple, easy to implement, and extremely effective, so it provides a significant addition to the stochastic simulation toolbox.
Bilevel optimization formulates hierarchical decision-making processes that arise in many real-world applications, such as pricing, network design, and infrastructure defense planning. In this paper, we consider a class of bilevel optimization problems in which the upper level problem features some integer variables and the lower level problem enjoys strong duality. We propose a dedicated Benders decomposition method for solving this class of bilevel problems, which decomposes the Benders subproblem into two more tractable, sequentially solvable problems that can be interpreted as the upper and lower level problems. We show that the Benders subproblem decomposition carries over to an interesting extension of bilevel problems, which connects the upper level solution with the lower level dual solution, and discuss some special cases of bilevel problems that allow sequence-independent subproblem decomposition. Several novel schemes for generating numerically stable cuts, finding a good incumbent solution, and accelerating the search tree are discussed. A computational study demonstrates the computational benefits of the proposed method over a state-of-the-art, bilevel-tailored, branch-and-cut method; a commercial solver; and the standard Benders method on standard test cases and the motivating applications in sequential energy markets.
Distributionally robust optimization (DRO) is a modeling framework in decision making under uncertainty in which the probability distribution of a random parameter is unknown although its partial information (e.g., statistical properties) is available. In this framework, the unknown probability distribution is assumed to lie in an ambiguity set consisting of all distributions that are compatible with the available partial information. Although DRO bridges the gap between stochastic programming and robust optimization, one of its limitations is that its models for large-scale problems can be significantly difficult to solve, especially when the uncertainty is of high dimension. In this paper, we propose computationally efficient inner and outer approximations for DRO problems under a piecewise linear objective function and with a moment-based ambiguity set and a combined ambiguity set including Wasserstein distance and moment information. In these approximations, we split a random vector into smaller pieces, leading to smaller matrix constraints. In addition, we use principal component analysis to shrink uncertainty space dimensionality. We quantify the quality of the developed approximations by deriving theoretical bounds on their optimality gap. We display the practical applicability of the proposed approximations in a production–transportation problem and a multiproduct newsvendor problem. The results demonstrate that these approximations dramatically reduce the computational time while maintaining high solution quality. The approximations also help construct an interval that is tight for most cases and includes the (unknown) optimal value for a large-scale DRO problem, which usually cannot be solved to optimality (or even feasibility in most cases). Summary of Contribution: This paper studies an important type of optimization problem, that is, distributionally robust optimization problems, by developing computationally efficient inner and outer approximations via operations research tools. Specifically, we consider several variants of such problems that are practically important and that admit tractable yet large-scale reformulation. We accordingly utilize random vector partition and principal component analysis to derive efficient approximations with smaller sizes, which, more importantly, provide a theoretical performance guarantee with respect to low optimality gaps. We verify the significant efficiency (i.e., reducing computational time while maintaining high solution quality) of our proposed approximations in solving both production–transportation and multiproduct newsvendor problems via extensive computing experiments.
This paper compares risk-averse optimization methods to address the self-scheduling and market involvement of a virtual power plant (VPP). The decision-making problem of the VPP involves uncertainty in the wind speed and electricity price forecast. We focus on two methods: risk-averse two-stage stochastic programming (SP) and two-stage adaptive robust optimization (ARO). We investigate both methods concerning formulations, uncertainty and risk, decomposition algorithms, and their computational performance. To quantify the risk in SP, we use the conditional value at risk (CVaR) because it can resemble a worst-case measure, which naturally links to ARO. We use two efficient implementations of the decomposition algorithms for SP and ARO; we assess (1) the operational results regarding first-stage decision variables, estimate of expected profit, and estimate of the CVaR of the profit and (2) their performance taking into consideration different sample sizes and risk management parameters. The results show that similar first-stage solutions are obtained depending on the risk parameterizations used in each formulation. Computationally, we identified three cases: (1) SP with a sample of 500 elements is competitive with ARO; (2) SP performance degrades comparing to the first case and ARO fails to converge in four out of five risk parameters; (3) SP fails to converge, whereas ARO converges in three out of five risk parameters. Overall, these performance cases depend on the combined effect of deterministic and uncertain data and risk parameters.Summary of Contribution: The work presented in this manuscript is at the intersection of operations research and computer science, which are intrinsically related with the scope and mission of IJOC. From the operations research perspective, two methodologies for optimization under uncertainty are studied: risk-averse stochastic programming and adaptive robust optimization. These methodologies are illustrated using an energy scheduling problem. The study includes a comparison from the point of view of uncertainty modeling, formulations, decomposition methods, and analysis of solutions. From the computer science perspective, a careful implementation of decomposition methods using parallelization techniques and a sample average approximation methodology was done . A detailed comparison of the computational performance of both methods is performed. Finally, the conclusions allow establishing links between two alternative methodologies in operations research: stochastic programming and robust optimization.
The complexity of current electricity wholesale markets and the increased volatility of electricity prices because of the intermittent nature of renewable generation make independent power producers (IPPs) face significant challenges to submit offers. This challenge increases for those owning traditional coal-fired thermal generators and renewable generation. In this paper, an integrated stochastic optimal strategy is proposed for an IPP using the self-scheduling approach through its participation in both day-ahead and real-time markets (i.e., two-settlement electricity markets) as a price taker. In the proposed approach, the IPP submits an offer for all periods to the day-ahead market for which a multistage stochastic programming setting is explored for providing real-time market offers for each period as a recourse. This strategy has the advantage of achieving overall maximum profits for both markets in the given operational time horizon. Such a strategy is theoretically proved to be more profitable than alternative self-scheduling strategies as it takes advantage of the continuously realized scenario information of the renewable energy output and real-time prices over time. To improve computational efficiency, we explore polyhedral structures to derive strong valid inequalities, including convex hull descriptions for certain special cases, thus strengthening the formulation of our proposed model. Polynomial-time separation algorithms are then established for the derived exponential-sized inequalities to speed up the branch-and-cut process. Finally, both numerical and real case studies demonstrate the potential of the proposed strategy.Summary of Contribution: This paper develops innovative models and methods to study a family of practically important problems via the interactions of operations research and computing. Specifically, this paper provides in-depth analyses of innovative stochastic optimization modeling approaches and develops computationally efficient polyhedral results. The paper also verifies the effectiveness of proposed analyses via extensive computing experiments.
VeRoViz is an open-source vehicle routing visualization package consisting of both Python and web-based components. It was developed to streamline the workflow for vehicle routing researchers by simplifying and automating many of the tedious tasks associated with generating realistic test problems. VeRoViz also provides new functionality to produce customizable visualizations of complex vehicle routing problems. These visualization tools—including Gantt charts, static maps, and dynamic 3-D videos—assist researchers in validating models and communicating results. Additionally, a comprehensive collection of utility functions within VeRoViz provides researchers with useful tools to assess features of their problems. This paper provides an overview of VeRoViz and highlights the flexibility and ease-of-use of the toolkit.Summary of Contribution: This paper describes an open-source software package designed to assist vehicle routing researchers. The software package, named VeRoViz (vehicle routing visualization), streamlines the process of collecting and visualizing data relevant to vehicle routing research. This includes capturing road network travel times (and distances) between locations, displaying turn-by-turn vehicle routes, generating Gantt charts of vehicle assignments, and producing 3-D “movies” of vehicle routing solutions. The VeRoViz suite consists of three components. First, a web-based interface allows researchers (and instructors) to quickly “sketch” elements of a vehicle routing problem, providing visual representations of nodes and arcs on a map. Second, the VeRoViz Python package provides an extensive collection of functions that assist operations researchers in collecting data and displaying solutions. Finally, an HTML/JavaScript plugin is available for displaying time-dynamic 3-D visualization of vehicle routes. VeRoViz is not a vehicle routing solver; it is a software suite that complements and supports researchers who are developing solution approaches to increasingly realistic vehicle routing problems (e.g., problems related to drone delivery, electric vehicles, or dynamic delivery requests).
Operations researchers have long drawn insight from the structure of constraint coefficient matrices (CCMs) for mixed integer programs (MIPs). We propose a new question: Can pictorial representations of CCM structure be used to identify similar MIP models and instances? In this paper, CCM structure is visualized using digital images, and computer vision techniques are used to detect latent structural features therein. The resulting feature vectors are used to measure similarity between images and, consequently, MIPs. An introductory analysis examines a subset of the instances from strIPlib and MIPLIB 2017, two online repositories for MIP instances. Results indicate that structure-based comparisons may allow for relationships to be identified between MIPs from disparate application areas. Additionally, image-based comparisons reveal that ostensibly similar variations of an MIP model may yield instances with markedly different mathematical structures.Summary of Contribution: This paper presents a methodology for comparing mixed integer programs (MIPs) from any research domain based on the structure of the constraint coefficient matrices for one or more instances of a model. Specifically, computer vision and deep learning techniques are used to extract structural features and measure the similarity between these images. This process is agnostic to application area and instead focuses solely on mathematical structure. As a result, this methodology offers a fundamentally new way for operations researchers to view MIP similarity and highlights similarities between research problems that may have previously been viewed as unrelated.
Online bipartite matching (OBM) is a fundamental model underpinning many important applications, including search engine advertisement, website banner and pop-up ads, and ride hailing. We study the independent and identically distributed (i.i.d.) OBM problem, in which one side of the bipartition is fixed and known in advance, whereas nodes from the other side appear sequentially as i.i.d. realizations of an underlying distribution and must immediately be matched or discarded. We introduce dynamic relaxations of the set of achievable matching probabilities; show how they theoretically dominate lower dimensional, static relaxations from previous work; and perform a polyhedral study to theoretically examine the new relaxations’ strength. We also discuss how to derive heuristic policies from the relaxations’ dual prices in a similar fashion to dynamic resource prices used in network revenue management. We finally present a computational study to demonstrate the empirical quality of the new relaxations and policies.Summary of Contribution: Online bipartite matching (OBM) is one of the fundamental problems in the area of online decision analysis with a wide variety of applications in operations research and computer science, for example, online advertising, ride sharing, and general resource allocation. Over the last decades, both communities have been interested in the design and analysis of new approaches. Our main contribution is to provide a polyhedral study that considers the problem’s sequential nature. Specifically, we achieve this via dynamic relaxations. We also discuss how to derive heuristic policies from the relaxations’ dual prices. We support our theoretical findings with a detailed computational study.
We present StochasticPrograms.jl, a user-friendly and powerful open-source framework for stochastic programming written in the Julia language. The framework includes both modeling tools and structure-exploiting optimization algorithms. Stochastic programming models can be efficiently formulated using an expressive syntax, and models can be instantiated, inspected, and analyzed interactively. The framework scales seamlessly to distributed environments. Small instances of a model can be run locally to ensure correctness, whereas larger instances are automatically distributed in a memory-efficient way onto supercomputers or clouds and solved using parallel optimization algorithms. These structure-exploiting solvers are based on variations of the classical L-shaped, progressive-hedging, and quasi-gradient algorithms. We provide a concise mathematical background for the various tools and constructs available in the framework along with code listings exemplifying their usage. Both software innovations related to the implementation of the framework and algorithmic innovations related to the structured solvers are highlighted. We conclude by demonstrating strong scaling properties of the distributed algorithms on numerical benchmarks in a multinode setup.Summary of Contribution: This paper presents StochasticPrograms.jl, an open-source framework for stochastic programming implemented in the Julia programming language. The framework includes an expressive syntax for formulating stochastic programming models as well as versatile analysis tools and parallel optimization algorithms. The framework will prove useful to researchers, educators, and industrial users alike. Researchers will benefit from the readily extensible open-source framework, in which they can formulate complex stochastic models or quickly typeset and test novel optimization algorithms. Educators of stochastic programming will benefit from the clean and expressive syntax. Moreover, the framework supports analysis tools and stochastic programming constructs from classical theory and leading textbooks. We strongly believe that the StochasticPrograms.jl framework can reduce the barrier to entry for incoming practitioners of stochastic programming. Industrial practitioners can make use of StochasticPrograms.jl to rapidly formulate complex models, analyze small instances locally, and then run large-scale instances in production. In doing so, they get distributed capabilities for free without changing the code and access to well-tested state-of-the-art implementations of parallel structure-exploiting solvers. As the framework is open-source, anyone from these target audiences can contribute with new functionality to the framework. In conclusion, by providing both an intuitive interface for new users and an extensive development environment for expert users, StochasticPrograms.jl has strong potential to further the field of stochastic programming.
Scenario reduction is an effective method to ease the computational burden of stochastic programming problems, which aims at choosing a subset of scenarios that can better represent a large number of possible scenarios. Higher-order moments are critical in the scenario reduction process, especially for stochastic programming problems that are greatly affected by the moments. From this idea, we construct a mixed integer linear programming model to improve the reduction accuracy of traditional methods by minimizing the moments’ information loss between the original and reduced scenarios. An improved Benders decomposition algorithm is then designed to find an optimal solution for the model. Finally, the resulting scenarios are examined on an international portfolio selection problem. Empirical and comparative studies are also carried out to reveal the superiority of our proposed scenario reduction method over other existing approaches or models, together with the superior performance of the algorithm.Summary of Contribution: To effectively solve stochastic programming problems, the scenario reduction method has become an active research area to strike a balance between the fine representation of random variables and computational complexity. Thus, how to design a reasonable optimal scenario reduction model and effectively solve this complex model is very important and meaningful. On the other hand, for some stochastic programming problems, especially the portfolio selection problems, statistical properties of risky assets returns may play a more important role in the scenario reduction process. However, the traditional scenario reduction methods have ignored this point. Thus, in this paper, we propose a mixed integer linear programming model to improve the reduction accuracy by minimizing the higher-order moments’ information loss between the original and reduced scenarios. Furthermore, an accelerated Benders decomposition algorithm is also designed to solve the proposed model. Hence, the aim of this paper is to extend the existing scenario reduction method in substantial and meaningful ways.
Modern electric power systems have witnessed rapidly increasing penetration of renewable energy, storage, electrical vehicles, and various demand response resources. The electric infrastructure planning is thus facing more challenges as a result of the variability and uncertainties arising from the diverse new resources. This study aims to develop a multistage and multiscale stochastic mixed integer programming (MM-SMIP) model to capture both the coarse-temporal-scale uncertainties, such as investment cost and long-run demand stochasticity, and fine-temporal-scale uncertainties, such as hourly renewable energy output and electricity demand uncertainties, for the power system capacity expansion problem. To be applied to a real power system, the resulting model will lead to extremely large-scale mixed integer programming problems, which suffer not only the well-known curse of dimensionality but also computational difficulties with a vast number of integer variables at each stage. In addressing such challenges associated with the MM-SMIP model, we propose a nested cross decomposition algorithm that consists of two layers of decomposition—that is, the Dantzig–Wolfe decomposition and L-shaped decomposition. The algorithm exhibits promising computational performance under our numerical study and is especially amenable to parallel computing, which will also be demonstrated through the computational results.
Co-movement among individual firms’ stock prices can reflect complex interfirm relationships. This paper proposes a novel method to leverage such relationships for stock price predictions by adopting inductive graph representation learning on dynamic stock graphs constructed based on historical stock price co-movement. To learn node representations from such dynamic graphs for better stock predictions, we propose the hybrid-attention dynamic graph neural network, an inductive graph representation learning method. We also extended mini-batch gradient descent to inductive representation learning on dynamic stock graphs so that the model can update parameters over mini-batch stock graphs with higher training efficiency. Extensive experiments on stocks from different markets and trading simulations demonstrate that the proposed method significantly improves stock predictions. The proposed method can have important implications for the management of financial portfolios and investment risk.Summary of Contribution: Accurate predictions of stock prices have important implications for financial decisions. In today’s economy, individual firms are increasingly connected via different types of relationships. As a result, firms’ stock prices often feature synchronous co-movement patterns. This paper represents the first effort to leverage such phenomena to construct dynamic stock graphs for stock predictions. We develop hybrid-attention dynamic graph neural network (HAD-GNN), an inductive graph representation learning framework for dynamic stock graphs to incorporate temporal and graph attention mechanisms. To improve the learning efficiency of HAD-GNN, we also extend the mini-batch gradient descent to inductive representation learning on such dynamic graphs and adopt a t-batch training mechanism (t-BTM). We demonstrate the effectiveness of our new approach via experiments based on real-world data and simulations.
Peer influence and homophily are two entangled forces underlying social influences. However, distinguishing homophily from peer influence is difficult, particularly when there is latent homophily caused by unobservable features. This paper proposes a novel data-driven framework that combines the advantages of latent homophily identification and causal inference. Specifically, the approach first utilizes scalable network representation learning algorithms to obtain node embeddings, which are extracted from social network structures. Then, the embeddings are used to control latent homophily in a quasi-experimental design for causal inference. The simulation experiments show that the proposed approach can estimate peer influence more accurately than existing parameterized approaches and data-driven methods. We applied the proposed framework in an empirical study of players’ online gaming behaviors. First, our approach can achieve improved model fitness for estimating peer influence in online games. Second, we discover a heterogeneous effect of peer influence: players with higher tenure and playing levels receive stronger peer influence. Finally, our results suggest that the homophily effect has a stronger influence on players’ behavior than peer influence.Summary of Contribution: The study proposes a novel computational method to separate peer influence from homophily in an online network. Using network embeddings learned from data to control latent homophily, the approach effectively addresses the challenge of correctly identifying peer effects in the absence of randomized experimental conditions. While simplifying the computational process, the method achieves good computational performance, thus effectively helping researchers and practitioners extract useful network information in various online service contexts.
A formulation, a heuristic, and branch-and-cut algorithms are investigated for the chordless cycle problem. This is the problem of finding a largest simple cycle for a given graph so that no edge between nonimmediately subsequent cycle vertices is contained in the graph. Leaving aside procedures based on complete enumeration, no previous exact solution algorithm appears to exist for the problem, which is relevant both in theoretical and practical terms. Extensive computational results are reported here for randomly generated graphs and for graphs originating from the literature. Under acceptable CPU times, certified optimal solutions are presented for graphs with as many as 100 vertices.Summary of Contribution: Finding chordless cycles of a graph, also known as holes, is relevant, among others, to graph theory, to the design of polyhedral based exact solution algorithms to integer programming (IP) problems, and to the practical applications that benefit from these algorithms. For instance, perfect graphs do not contain odd holes. Additionally, odd hole inequalities are valid for strengthening the formulations to numerous problems that are directly defined over graphs. Furthermore, these inequalites, in association with applicable conflict graphs, are used by all modern IP solvers to preprocess and strengthen virtually any IP formulation submitted to them.
We tackle an optimization problem arising in the design of sensor networks: given a set of sensors, only one being connected to a backbone, to establish connection routes from each of them to the sink. Under a shortest path routing protocol, the set of connections form a spanning tree. Energy is required to transmit and receive data, and sensors have limited battery capacity: as soon as one sensor runs out of battery, a portion of the network is disconnected. We, therefore, search for the spanning tree maximizing the time elapsed before such a disconnection occurs, and therefore, maintenance is required. We propose new mathematical formulations for the problem, proving and exploiting theoretical results on its combinatorial structure. On that basis, we design algorithms offering a priori guarantees of global optimality. We undertake an extensive experimental campaign, showing our algorithms to outperform previous ones from the literature by orders of magnitude. We also identify which instance features have higher impact on network lifetime.
This paper presents the capacitated and economic districting problem (CEDP), which searches for the best edge partition defining connected, capacitated, and balanced districts in an undirected connected graph, weighing the economic value of each district. This problem provides a comprehensive description of the decision making on service networks districting, where the order by which the districts are serviced plays a role in the profit. This is observed in the arrangement of districts for meter reading, as the day in which each district is read impacts the revenue. Two integer linear programming formulations are proposed for CEDP, accompanied by a proof of NP-hardness. To tackle large instances, a greedy randomized adaptive search procedure (GRASP) metaheuristic, embedded with reactive parameter tuning, statistical filtering of solutions subjected to intensification, and a set of solution repairing procedures, is proposed. The GRASP is hybridized to each model to evaluate the outcomes of combining their individual merits. Computational experiments were performed on a new benchmark composed of 144 instances of different sizes, edge densities, network topologies, balance tolerances, and district capacities. The results show the effectiveness of the exact methodologies in solving the models and providing optimal solutions for the set of small instances. The GRASP was capable of tackling large networks, achieving feasible solutions to almost all instances. The results also show that the hybridized methodologies outperformed their standalone counterparts with respect to the attained primal and dual bounds.
Intensity-modulated radiation therapy (IMRT) allows for the design of customized, highly conformal treatments for cancer patients. Creating IMRT treatment plans, however, is a mathematically complex process, which is often tackled in multiple, simpler stages. This sequential approach typically separates radiation dose requirements from mechanical deliverability considerations, which may result in suboptimal treatment quality. For patient health to be considered paramount, holistic models must address these plan elements concurrently, eliminating quality loss between stages. This combined direct aperture optimization (DAO) approach is rarely paired with uncertainty mitigation techniques, such as robust optimization, because of the inherent complexity of both parts. This paper outlines a robust DAO (RDAO) model and discusses novel methodologies for efficiently integrating salient constraints. Because the highly complex RDAO model is difficult to solve, an original candidate plan generation (CPG) heuristic is proposed. The CPG produces rapid, high-quality, feasible plans, which are immediately clinically viable and can also be used to generate a feasible incumbent solution for warm-starting the RDAO model. Computational results obtained using clinical patient data sets with motion uncertainty show the benefit of incorporating the CPG, in terms of both the first incumbent solution and final output plan quality.Summary of Contribution: This paper describes the derivation, implementation, and solution of a large-scale robust direct aperture optimization model for the problem of intensity-modulated radiation therapy planning for cancer treatment. The contribution to operations research lies in the design of a novel mixed-integer programming model that describes all salient mechanical and clinical deliverability requirements for modern delivery equipment. Because of the large-scale nature of the resulting model, a novel tractable heuristic for generating high-quality, feasible treatment plans, as well as warm starts for the full model, is proposed and demonstrated on five clinical patient data sets.
Sepsis is a life-threatening condition, caused by the body’s extreme response to an infection. In the United States, 1.7 million cases of sepsis occur annually, resulting in 265,000 deaths. Delayed diagnosis and treatment are associated with higher mortality rates. An exponential rise in the availability of medical data has allowed for the development of sophisticated machine learning algorithms to predict sepsis earlier than the onset. However, these models often underperform, as the training data are retrospective and do not fully capture the uncertain future. In this study, we develop a novel framework, which we refer to as MLePOMDP, to leverage and combine the underlying, high-level knowledge about sepsis progression and machine learning (ML) for classification. Specifically, we use a hidden Markov model to describe sepsis development at a high level, where the ML model makes the higher-order “observations” from temporal data. Consequently, a partially observable Markov decision process (POMDP) model is developed to make classification decisions. We analytically establish that the optimal policy is of threshold-type, which we exploit to efficiently optimize MLePOMDP. MLePOMDP is calibrated and tested using high-frequency physiological data collected from bedside monitors. Different from past POMDP-based frameworks, MLePOMDP is developed for a prediction task using a very small state definition, produces highly interpretable results, and accounts for a novel and clinically meaningful action space. Our results show that MLePOMDP outperforms machine learning–based benchmarks by up to 8% in precision. Importantly, MLePOMDP is able to reduce false alarms by up to 28%. An additional experiment is conducted to show the generalizability of MLePOMDP to different patient cohorts.Summary of Contribution: This study develops a novel real-time decision support framework for early sepsis prediction by integrating well-known machine learning models (random forest and neural networks) with a well-established sequential decision-making model, namely, a partially observable Markov decision process (POMDP). The structural properties of the optimal policy are further explored and a threshold-type structure is established, which is then leveraged to develop a customized algorithm to solve the problem more efficiently. The resulting framework demonstrates the benefit of applying POMDPs to augment machine learning outputs. Specifically, the framework results in the reduction of false alarms in sepsis predictions where decisions are made in real time, hence improving the overall prediction precision.
In this paper, we propose a decision criterion that characterizes an enveloping bound on monetary risk measures and is computationally friendly. We start by extending the classical value at risk (VaR) measure. Whereas VaR evaluates the threshold loss value such that the loss from the risk position exceeding that threshold is at a given probability level, it fails to indicate a performance guarantee at other probability levels. We define the probabilistic enveloping measure (PEM) to establish the bound information for the tail probability of the loss at all levels. Using a set of normative properties, we then generalize the PEM to the risk enveloping measure (REM) such that the bound on the general monetary risk measures at all levels of risk aversion are captured. The coherent version of the REM (CREM) is also investigated. We demonstrate its applicability by showing how the coherent REM can be incorporated in distributionally robust optimization. Specifically, we apply the CREM criterion in surgery block allocation problems and provide a formulation that can be efficiently solved. Based on this application, we report favorable computational results from optimizing over the CREM criterion.Summary of Contribution: Our paper studies a fundamental problem in operations research: what criteria to optimize when uncertainties are involved. Extending from the classical chance constraint model, we propose a new decision criterion by an axiomatization approach. We then investigate the computing issue in the corresponding distributionally robust optimization problem. In particular, we provide solution methods for continuous and discrete optimization. After that, we apply it to a practical operations problem, surgery allocation decisions in healthcare management. The computational studies demonstrate the appealing performance of our proposed approach on this surgery allocation problem.
Multiperiod blending has a number of important applications in a range of industrial sectors. It is typically formulated as a nonconvex mixed integer nonlinear program (MINLP), which involves binary variables and bilinear terms. In this study, we first propose a reformulation of the constraints involving bilinear terms using lifting. We introduce a method for calculating tight bounds on the lifted variables calculated by aggregating multiple constraints. We propose valid constraints derived from the reformulation-linearization technique (RLT) that use the bounds on the lifted variables to further tighten the formulation. Computational results indicate our method can substantially reduce the solution time and optimality gap.Summary of Contribution: In this paper, we study the multiperiod blending problem, which has a number of important applications in a range of industrial sectors, such as refining, chemical production, mining, and wastewater management. Solving this problem efficiently leads to significant economic and environmental benefits. However, solving even medium-scale instances to global optimality remains challenging. To address this challenge, we propose a variable bound tightening algorithm and tightening constraints for multiperiod blending. Computational results show that our methods can substantially reduce the solution time and optimality gap.
Maximizing a convex function over convex constraints is an NP-hard problem in general. We prove that such a problem can be reformulated as an adjustable robust optimization (ARO) problem in which each adjustable variable corresponds to a unique constraint of the original problem. We use ARO techniques to obtain approximate solutions to the convex maximization problem. In order to demonstrate the complete approximation scheme, we distinguish the cases in which we have just one nonlinear constraint and multiple linear constraints. Concerning the first case, we give three examples in which one can analytically eliminate the adjustable variable and approximately solve the resulting static robust optimization problem efficiently. More specifically, we show that the norm constrained log-sum-exp (geometric) maximization problem can be approximated by (convex) exponential cone optimization techniques. Concerning the second case of multiple linear constraints, the equivalent ARO problem can be represented as an adjustable robust linear optimization problem. Using linear decision rules then returns a safe approximation of the constraints. The resulting problem is a convex optimization problem, and solving this problem gives an upper bound on the global optimum value of the original problem. By using the optimal linear decision rule, we obtain a lower bound solution as well. We derive the approximation problems explicitly for quadratic maximization, geometric maximization, and sum-of-max-linear-terms maximization problems with multiple linear constraints. Numerical experiments show that, contrary to the state-of-the-art solvers, we can approximate large-scale problems swiftly with tight bounds. In several cases, we have equal upper and lower bounds, which concludes that we have global optimality guarantees in these cases. Summary of Contribution: Maximizing a convex function over a convex set is a hard optimization problem. We reformulate this problem as an optimization problem under uncertainty, which allows us to transfer the hardness of this problem from its nonconvexity to the uncertainty of the new problem. The equivalent uncertain optimization problem can be relaxed tightly by using adjustable robust optimization techniques. In addition to building a new bridge between convex maximization and robust optimization, this approach also gives us strong algorithms that improve the state-of-the-art optimization solvers both in solution time and quality for various convex maximization problems.
Currently, there are few theoretical or practical approaches available for general nonlinear robust optimization. Moreover, the approaches that do exist impose restrictive assumptions on the problem structure. We present an adaptive bundle method for nonlinear and nonconvex robust optimization problems with a suitable notion of inexactness in function values and subgradients. As the worst-case evaluation requires a global solution to the adversarial problem, it is a main challenge in a general nonconvex nonlinear setting. Moreover, computing elements of an ε-perturbation of the Clarke subdifferential in the ℓ2ℓ2-norm sense is in general prohibitive for this class of problems. In this article, instead of developing an entirely new bundle concept, we demonstrate how existing approaches, such as Noll’s bundle method for nonconvex minimization with inexact information [Noll D (2013) Bundle method for non-convex minimization with inexact subgradients and function values. Computational and Analytical Mathematics, Springer Proceedings Mathematics, vol. 50 (Springer, New York), 555–592.] can be modified to be able to cope with this situation. Extending the nonconvex bundle concept to the case of robust optimization in this way, we prove convergence under two assumptions: first, that the objective function is lower C1 and, second, that approximately optimal solutions to the adversarial maximization problem are available. The proposed method is, hence, applicable to a rather general setting of nonlinear robust optimization problems. In particular, we do not rely on a specific structure of the adversary’s constraints. The considered class of robust optimization problems covers the case that the worst-case adversary only needs to be evaluated up to a certain precision. One possibility to evaluate the worst case with the desired degree of precision is the use of techniques from mixed-integer linear programming. We investigate the procedure on some analytic examples. As applications, we study the gas transport problem under uncertainties in demand and in physical parameters that affect pressure losses in the pipes. Computational results for examples in large realistic gas network instances demonstrate the applicability as well as the efficiency of the method.Summary of Contribution: Nonlinear robust optimization is a relevant field of research as real-world optimization problems usually suffer from not precisely known parameters, for example, physical parameters that cannot be measured exactly. Currently, there are few theoretical or practical approaches available for general nonlinear robust optimization. Moreover, the methods that do exist impose restrictive assumptions on the problem structure. Writing nonlinear robust optimization tasks in minimax form, in principle, bundle methods can be used to solve the resulting nonsmooth problem. However, there are a number of difficulties to overcome. First, the inner adversarial problem needs to be solved to global optimality, which is a major challenge in a general nonconvex nonlinear setting. In order to cope with this, an adaptive solution approach, which allows for inexactness, is required. A second challenge is then that the computation of elements from an ε-neighborhood of the Clarke subdifferential is, in general, prohibitive. We show how an existing bundle concept by D. Noll for nonconvex problems with inexactness in function values and subgradients can be adapted to this situation. The resulting method only requires availability of approximate worst-case evaluations, and in particular, it does not rely on a specific structure of the adversarial constraints. To evaluate the worst case with the desired degree of precision, one possibility is the use of techniques from mixed-integer linear programming. In the course of the paper, we discuss convergence properties of the resulting method and demonstrate its efficiency by means of robust gas transport problems.
Splitting methods in optimization arise when one can divide an optimization problem into two or more simpler subproblems. They have proven particularly successful for relaxations of problems involving discrete variables. We revisit and strengthen splitting methods for solving doubly nonnegative relaxations of the particularly difficult, NP-hard quadratic assignment problem. We use a modified restricted contractive splitting method approach. In particular, we show how to exploit redundant constraints in the subproblems. Our strengthened bounds exploit these new subproblems and new dual multiplier estimates to improve on the bounds and convergence results in the literature.Summary of Contribution: In our paper, we consider the quadratic assignment problem (QAP). It is one of the fundamental combinatorial optimization problems in the fields of optimization and operations research and includes many fundamental applications. We revisit and strengthen splitting methods for solving doubly nonnegative (DNN) relaxation of the QAP. We use a modified restricted contractive splitting method. We obtain strengthened bounds from improved lower and upper bounding techniques, and in fact, we solve many of these NP-hard problems to (provable) optimality, thus illustrating both the strength of the DNN relaxation and our new bounding techniques.
The minimum sum-of-squares clustering problem (MSSC) consists of partitioning n observations into k clusters in order to minimize the sum of squared distances from the points to the centroid of their cluster. In this paper, we propose an exact algorithm for the MSSC problem based on the branch-and-bound technique. The lower bound is computed by using a cutting-plane procedure in which valid inequalities are iteratively added to the Peng–Wei semidefinite programming (SDP) relaxation. The upper bound is computed with the constrained version of k-means in which the initial centroids are extracted from the solution of the SDP relaxation. In the branch-and-bound procedure, we incorporate instance-level must-link and cannot-link constraints to express knowledge about which data points should or should not be grouped together. We manage to reduce the size of the problem at each level, preserving the structure of the SDP problem itself. To the best of our knowledge, the obtained results show that the approach allows us to successfully solve, for the first time, real-world instances up to 4,000 data points.
A generating function technique for solving integer programs via the evaluation of complex path integrals is discussed from a theoretical and computational perspective. Applying the method to knapsack feasibility problems, it is demonstrated how the presented numerical integration algorithm benefits from a preoptimized path of integration. After discussing the algorithmic setup in detail, a numerical study is implemented to evaluate the computational performance of the preoptimized integration method, and the algorithmic parameters are tuned to a set of knapsack instances. The goal is to highlight the method’s computational advantage for hard knapsack instances.Summary of Contribution: A method for evaluating the feasibility of knapsack problems is discussed and connected to the existing theory on generating function techniques for computational integer optimization. Specifically, the number of solutions to knapsack instances is computed using numerical quadrature of complex path integrals. The choice of the path of integration is identified as an important degree of freedom, and it is shown that preoptimizing the path improves the computational performance for hard instances significantly. The scope of this work is to give a self-contained presentation of the mathematical theory, introduce and discuss path optimization as a presolving routine, and demonstrate the computational performance of the overall approach with the help of a detailed numerical study. The main goal is to highlight the computational advantage of the new algorithm for hard knapsack instances.
We propose a general logic-based Benders decomposition (LBBD) for production planning problems with process configuration decisions. This family of problems appears in contexts where the machines are set up according to specific patterns, templates, or, in general, process configurations that allow for simultaneously producing products of different types. The problem requires determining feasible configurations for the machines and their corresponding production levels to fulfill the demand at the minimum total cost. The structure of this problem contains nonlinear constraints that link the number of units produced of each product with the used configurations and their production levels. We decompose the original problem into a master problem, where the configurations are determined, and a subproblem, where the production amounts are determined. This allows us to apply the LBBD technique to solve the problem using a standard LBBD implementation and a branch-and-check algorithm. LBBD enhancements through logic-based inequalities generated for subsets of products with common characteristics are proposed. Such inequalities represent a form of the subproblem relaxation added to the master problem during its resolution. In our computational experiments, we apply the proposed LBBD approaches to two different applications from the literature: cutting stock problems in the steel industry and a printing problem. Results show that the LBBD methods find optimal solutions much faster than the solution approaches in the literature and have a superior performance with respect to the number of instances solved to optimality and the solution quality.Summary of Contribution: In this work, we introduce a unified exact solution algorithm based on logic-based Benders decomposition to solve a class of integrated production planning problems that include process configuration decisions. We propose a general mathematical representation of the original integrated planning problem and logic-based Benders reformulations that can be applied to solve several problems within the studied class. Our implementation frameworks provide guidelines to practitioners in the field. The solution approaches in this paper together with the proposed methodological enhancements can be adapted to solve other integrated planning problems in a similar context, including the case when the original problem has a complex combinatorial and nonlinear structure.
We propose and evaluate branch-and-price approaches for vehicle routing problems with picking, loading, and soft time windows. This general type of vehicle routing problem is of particular relevance in the same-day delivery context, in which fast routing algorithms are required because of the commitment to real-time delivery in the presence of high customer order frequencies. To boost the performance of the branch-and-price algorithms, we introduce the new method of tree-compatible labeling with nondominance trees. This method represents cost functions by a fixed number of breakpoints and uses a specialized tree-based data structure to store Pareto-optimal labels. We prove the theoretical soundness of the new method and evaluate its performance numerically with respect to pricing, column generation, and branch-and-price. Our numerical results show that the method yields substantial performance gains. In particular, we show that, with the new method, branch-and-price is able to reliably generate within a few minutes close to optimal solutions for problem instances with 50 customers. By additional experiments with classic vehicle routing problems with hard time windows, we show that the performance gains of our method result from its ability to handle cost functions in the pricing step. Our approach is the first branch-and-price approach for vehicle routing with picking, loading, and soft time windows. As such, it represents an exact routing algorithm that is able to reliably satisfy the runtime requirements of real-time delivery services.Summary of Contribution: In this paper, we propose the first branch-and-price approaches for a general class of vehicle routing problems with picking, loading, and soft time windows. This problem class is of particular importance for real-time delivery services, for which customers expect to be served within only a few hours after their order has been placed. We provide a problem formulation that takes into account that short runtimes of routing algorithms are required, that the upper bounds of the customers’ time windows may be violated at a certain cost, and that a significant part of the delivery time window is consumed by picking orders from a warehouse and loading orders into vehicles. Solving the problem with branch-and-price requires the use of cost functions as elements of the labels in the pricing step. We propose a method that approximates these cost functions and leverages the computational performance of nondominance tree data structures for solving the pricing problem. We prove the theoretical soundness of this new method for exact branch-and-price, and we show numerically that the method leads to a significant performance increase of branch-and-price. The paper lies at the intersection of computing and operations research, in particular because our algorithms are designed to leverage the computational performance of advanced data structures for solving a combinatorial optimization problem.
In this paper, we consider a variant of adaptive robust combinatorial optimization problems where the decision maker can prepare K solutions and choose the best among them upon knowledge of the true data realizations. We suppose that the uncertainty may affect the objective and the constraints through functions that are not necessarily linear. We propose a new exact algorithm for solving these problems when the feasible set of the nominal optimization problem does not contain too many good solutions. Our algorithm enumerates these good solutions, generates dynamically a set of scenarios from the uncertainty set, and assigns the solutions to the generated scenarios using a vertex p-center formulation, solved by a binary search algorithm. Our numerical results on adaptive shortest path and knapsack with conflicts problems show that our algorithm compares favorably with the methods proposed in the literature. We additionally propose a heuristic extension of our method to handle problems where it is prohibitive to enumerate all good solutions. This heuristic is shown to provide good solutions within a reasonable solution time limit on the adaptive knapsack with conflicts problem. Finally, we illustrate how our approach handles nonlinear functions on an all-or-nothing subset problem taken from the literature.Summary of Contribution: Our paper describes a new exact algorithm for solving adaptive robust combinatorial optimization problems when the feasible set of the nominal optimization problems does not contain too many good solutions. Its development relies on a progressive relaxation of the problem augmented with a row-and-column generation technique. Its efficient execution requires a reformulation of this progressive relaxation, coupled with dominance rules and a binary search algorithm. The proposed algorithm is amenable to exploiting the special structures of the problems considered as illustrated with various applications throughout the paper. A practical view is provided by the proposition of a heuristic variant. Our computational experiments show that our proposed exact solution method outperforms the existing methodologies and therefore pushes the computational envelope for the class of problems considered.
We propose a method to approximate the solution of online mixed-integer optimization (MIO) problems at very high speed using machine learning. By exploiting the repetitive nature of online optimization, we can greatly speed up the solution time. Our approach encodes the optimal solution into a small amount of information denoted as strategy using the voice of optimization framework. In this way, the core part of the optimization routine becomes a multiclass classification problem that can be solved very quickly. In this work, we extend that framework to real-time and high-speed applications focusing on parametric mixed-integer quadratic optimization. We propose an extremely fast online optimization method consisting of a feedforward neural network evaluation and a linear system solution where the matrix has already been factorized. Therefore, this online approach does not require any solver or iterative algorithm. We show the speed of the proposed method both in terms of total computations required and measured execution time. We estimate the number of floating point operations required to completely recover the optimal solution as a function of the problem dimensions. Compared with state-of-the-art MIO routines, the online running time of our method is very predictable and can be lower than a single matrix factorization time. We benchmark our method against the state-of-the-art solver Gurobi obtaining up to two to three orders of magnitude speedups on examples from fuel cell energy management, sparse portfolio optimization, and motion planning with obstacle avoidance.Summary of Contribution: We propose a technique to approximate the solution of online optimization problems at high speed using machine learning. By exploiting the repetitive nature of online optimization, we learn the mapping between the key problem parameters and an encoding of the optimal solution to greatly speed up the solution time. This allows us to significantly improve the computation time and resources needed to solve online mixed-integer optimization problems. We obtain a simple method with a very low computing time variance, which is crucial in online settings.
We introduce and motivate several variants of the bin packing problem where bins are assigned to time slots, and minimum and maximum lags are required between some pairs of items. We suggest two integer programming formulations for the general problem: a compact one and a stronger formulation with an exponential number of variables and constraints. We propose a branch-cut-and-price approach that exploits the latter formulation. For this purpose, we devise separation algorithms based on a mathematical characterization of feasible assignments for two important special cases of the problem: when the number of possible bins available at each period is infinite and when this number is limited to one and time lags are nonnegative. Computational experiments are reported for instances inspired from a real-case application of chemical treatment planning in vineyards, as well as for literature instances for special cases of the problem. The experimental results show the efficiency of our branch-cut-and-price approach, as it outperforms the compact formulation on newly proposed instances and is able to obtain improved lower and upper bounds for literature instances.Summary of Contribution: The paper considers a new variant of the bin packing problem, which is one of the most important problems in operations research. A motivation for introducing this variant is given, as well as a real-life application. We present a novel and original exact branch-cut-and-price algorithm for the problem. We implement this algorithm, and we present the results of extensive computational experiments. The results show a very good performance of our algorithm. We give several research directions that can be followed by subsequent researchers to extend our contribution to more complex and generic problems.
In the last decade, decision diagrams (DDs) have been the basis for a large array of novel approaches for modeling and solving optimization problems. Many techniques now use DDs as a key tool to achieve state-of-the-art performance within other optimization paradigms, such as integer programming and constraint programming. This paper provides a survey of the use of DDs in discrete optimization, particularly focusing on recent developments. We classify these works into two groups based on the type of diagram (i.e., exact or approximate) and present a thorough description of their use. We discuss the main advantages of DDs, point out major challenges, and provide directions for future work.
We consider a bilevel attacker–defender problem to find the worst-case attack on the relays that control transmission grid components. The attacker infiltrates some number of relays and renders all of the components connected to them inoperable with the goal of maximizing load shed. The defender responds by minimizing the resulting load shed, redispatching using a DC optimal power flow (DCOPF) problem on the remaining network. Though worst-case interdiction problems on the transmission grid have been studied for years, there remains a need for exact and scalable methods. Methods based on using duality on the inner problem rely on the bounds of the dual variables of the defender problem in order to reformulate the bilevel problem as a mixed integer linear problem. Valid dual bounds tend to be large, resulting in weak linear programming relaxations and, hence, making the problem more difficult to solve at scale. Often smaller heuristic bounds are used, resulting in a lower bound. In this work, we also consider a lower bound, but instead of bounding the dual variables, we drop the constraints corresponding to Ohm’s law, relaxing DCOPF to capacitated network flow. We present theoretical results showing that, for uncongested networks, approximating DCOPF with network flow yields the same set of injections and, thus, the same load shed, which suggests that this restriction likely gives a high-quality lower bound in the uncongested case. Furthermore, we show that, in the network flow relaxation of the defender problem, the duals are bounded by one, so we can solve our restriction exactly. Finally, because the big-M values in the linearization are equal to one and network flow has a well-known structure, we see empirically that this formulation scales well computationally with increased network size. Through empirical experiments on 16 networks with up to 6,468 buses, we find that this bound is almost always as tight as we can get from guessing the dual bounds even for congested networks in which the theoretical results do not hold. In addition, calculating the bound is approximately 150 times faster than achieving the same bound with the reformulation guessing the dual bounds.
The integral column generation algorithm (ICG) was recently introduced to solve set partitioning problems involving a very large number of variables. This primal algorithm generates a sequence of integer solutions with decreasing costs, leading to an optimal or near-optimal solution. ICG combines the well-known column generation algorithm and a primal algorithm called the integral simplex using decomposition algorithm (ISUD). In this paper, we develop a generalized version of ICG, denoted I2CG, that can solve efficiently large-scale set partitioning problems with side constraints. This new algorithm can handle the side constraints in the reduced problem of ISUD, in its complementary problem, or in both components. Computational experiments on instances of the airline crew pairing problem (CPP) and the multidepot vehicle routing problem with time windows show that the latter strategy is the most efficient one and I2CG significantly outperforms basic variants of two popular column generation heuristics, namely, a restricted master heuristic and a diving heuristic. For the largest tested CPP instance with 1,761 constraints, I2CG can produce in less than one hour of computational time more than 500 integer solutions leading to an optimal or near-optimal solution.Summary of Contribution: In this paper, we develop a new integral column generation algorithm that can solve efficiently large-scale set partitioning problems with side constraints. The latter alter the quasi-integrality property needed for primal integral algorithms. The paper adds a methodological contribution remedying this issue. This remedy should, in our opinion, boost the use of primal exact methods, especially in the column generation context. The paper also has a computational contribution. Effectively, computational experiments on instances of the airline crew pairing problem and the multidepot vehicle routing problem with time windows are extensively discussed. We compare the proposed algorithm to basic variants of two popular column generation heuristics.
We investigate new methods for generating Lagrangian cuts to solve two-stage stochastic integer programs. Lagrangian cuts can be added to a Benders reformulation and are derived from solving single scenario integer programming subproblems identical to those used in the nonanticipative Lagrangian dual of a stochastic integer program. Although Lagrangian cuts have the potential to significantly strengthen the Benders relaxation, generating Lagrangian cuts can be computationally demanding. We investigate new techniques for generating Lagrangian cuts with the goal of obtaining methods that provide significant improvements to the Benders relaxation quickly. Computational results demonstrate that our proposed method improves the Benders relaxation significantly faster than previous methods for generating Lagrangian cuts and, when used within a branch-and-cut algorithm, significantly reduces the size of the search tree for three classes of test problems.
To use simulation models to study the behaviors of stochastic systems, one needs to specify the distribution of the input random variables. However, specifying this distribution precisely is typically difficult and even impossible in practice. The issue is known as input uncertainty in the simulation literature, and it has been considered and studied extensively in recent years. In this paper, we model the uncertainty by an ambiguity set that is defined based on the likelihood ratio between the true (unknown) distribution and the nominal distribution (i.e., the best estimate), and develop a robust simulation (RS) approach that estimates the worst-case values of performance measures of the random simulation output when the true distribution varies in the ambiguity set. We show that the RS approach is computationally tractable, and the corresponding results reveal important information of the stochastic systems and help decision makers make better decisions.
Input model bias is the bias found in the output performance measures of a simulation model caused by estimating the input distributions/processes used to drive it. When the simulation response is a nonlinear function of its inputs, as is usually the case when simulating complex systems, input modelling bias is amongst the errors that arise. In this paper, we introduce a method that recalibrates the input parameters of parametric input models to reduce the bias in the simulation output. The proposed method is based on sequential quadratic programming with a closed form analytical solution at each step. An algorithm with guidance on how to practically implement the method is presented. The method is shown to be successful in reducing input modelling bias and the total mean squared error caused by input modelling error.Summary of Contribution: This paper furthers the understanding and treatment of input modelling error in computer simulation. We provide a novel method for reducing input model bias by recalibrating the input parameters used to drive a simulation model. A sequential quadratic programming approach with an explicit solution is provided to recalibrate the input parameters. The method is therefore computationally inexpensive. An algorithm outlining our proposed procedure is provided within the paper. An evaluation of the method shows the method successfully reduces input model bias and may also reduce the mean squared error caused by input modelling in the output of a simulation model.
We discuss two classes of methods of approximating gradients of noisy black box functions—the classical finite difference method and recently popular randomized finite difference methods. Despite of the popularity of the latter, we argue that it is unclear whether the randomized schemes have an advantage over the traditional methods when employed inside an optimization method. We point to theoretical and practical evidence that show that the opposite is true at least in a general optimization setting. We then pose the question of whether a particular setting exists when the advantage of the new method may be clearly shown, at least numerically. The larger underlying challenge is a development of black box optimization methods that scale well with the problem dimension.
We consider the problem of allocating multiple heterogeneous resources geographically and over time to meet demands that require some subset of the available resource types simultaneously at a specified time, location, and duration. The objective is to maximize the total reward accrued from meeting (a subset of) demands. We model this problem as an integer program, show that it is NP-hard, and analyze the complexity of various special cases. We introduce approximation algorithms and an extension to our problem that considers travel costs. Finally, we test the performance of the integer programming model in an extensive computational study.
We introduce a stochastic version of the cutting plane method for a large class of data-driven mixed-integer nonlinear optimization (MINLO) problems. We show that under very weak assumptions, the stochastic algorithm can converge to an ϵ-optimal solution with high probability. Numerical experiments on several problems show that stochastic cutting planes is able to deliver a multiple order-of-magnitude speedup compared with the standard cutting plane method. We further experimentally explore the lower limits of sampling for stochastic cutting planes and show that, for many problems, a sampling size of O(3√n)𝑂(𝑛√3) appears to be sufficient for high-quality solutions.
In this paper, we focus on a subclass of quadratic optimization problems, that is, disjoint bilinear optimization problems. We first show that disjoint bilinear optimization problems can be cast as two-stage robust linear optimization problems with fixed-recourse and right-hand-side uncertainty, which enables us to apply robust optimization techniques to solve the resulting problems. To this end, a solution scheme based on a blending of three popular robust optimization techniques is proposed. For disjoint bilinear optimization problems with a polyhedral feasible region and a general convex feasible region, we show that, under mild regularity conditions, the convex relaxations of the original bilinear formulation and its two-stage robust reformulation obtained from a reformulation-linearization-based technique and linear decision rules, respectively, are equivalent. For generic bilinear optimization problems, the convex relaxations from the reformulation-linearization-based technique are generally tighter than the one from linear decision rules. Numerical experiments on bimatrix games, synthetic disjoint bilinear problem instances, and convex maximization problems demonstrate the efficiency and effectiveness of the proposed solution scheme.Summary of Contribution: Computing solutions for disjoint bilinear optimization problems are of much interest in real-life applications, yet they are, in general, computationally intractable. This paper proposes a computationally tractable approximation as well as a convergent algorithm to the optimal values of such problems. Extensive computational experiments on (i) (constrained) bimatrix games, (ii) synthetic disjoint bilinear problems, and (iii) convex maximization problems are conducted to demonstrate the effectiveness and efficiency of the proposed approach.
We apply logic-based Benders decomposition (LBBD) to two-stage stochastic planning and scheduling problems in which the second stage is a scheduling task. We solve the master problem with mixed integer/linear programming and the subproblem with constraint programming. As Benders cuts, we use simple no-good cuts as well as analytic logic-based cuts we develop for this application. We find that LBBD is computationally superior to the integer L-shaped method. In particular, a branch-and-check variant of LBBD can be faster by several orders of magnitude, allowing significantly larger instances to be solved. This is due primarily to computational overhead incurred by the integer L-shaped method while generating classic Benders cuts from a continuous relaxation of an integer programming subproblem. To our knowledge, this is the first application of LBBD to two-stage stochastic optimization with a scheduling second-stage problem and the first comparison of LBBD with the integer L-shaped method. The results suggest that LBBD could be a promising approach to other stochastic and robust optimization problems with integer or combinatorial recourse.Summary of Contribution: We study an important class of optimization problems, namely, two-stage stochastic programs with integer recourse, which are known to be extremely difficult to solve in general. We focus on an application in which the second-stage problem is a scheduling problem, a first in the literature to the best of our knowledge. Our study exemplifies how one can exploit the combinatorial structure of the scheduling problem to derive novel analytic Benders cuts and use them within a branch-and-check algorithm. The proposed algorithm solves instances that are intractable for commercial solvers and state-of-the-art decomposition-based methods, such as the integer L-shaped method. We believe that our study will inspire further research in the use of hybrid logic-based optimization methods for solving stochastic combinatorial optimization problems.
As a key decision-making process in compensation and benefits (C&B) in human resource management, job salary benchmarking (JSB) plays an indispensable role in attracting, motivating, and retaining talent. Whereas the existing research mainly focuses on revealing the essential impacts of personal and organizational characteristics and economic factors on labor costs (e.g., C&B), few studies target optimizing JSB from a practical, data-driven perspective. Traditional approaches suffer from issues that result from using small and sparse data as well as from the limitations of linear statistical models in practice. Furthermore, there are also important technical issues that need to be addressed in the small number of machine learning–based JSB approaches, such as “cold start” issues when considering a brand-new type of company or job or model interpretability issues. To this end, we propose to address the JSB problem with data-driven techniques from a fine-grained perspective by modeling large-scale, real-world online recruitment data. Specifically, we develop a nonparametric Dirichlet process–based latent factor model (NDP-JSB) to jointly model the latent representations of both company and job position and then apply the model to predict salaries based on company and position information. Our model strengthens the usage of data-driven approaches in JSB optimization by addressing the aforementioned issues in existing models. For evaluation, extensive experiments are conducted on two large-scale, real-world data sets. Our results validate the effectiveness of the NDP-JSB and demonstrate its strength in providing interpretable salary benchmarking to benefit complex decision-making processes in talent management.Summary of Contribution: This paper bridges the cutting-edge machine learning techniques to their implementation in a practical operation research problem in human resources. We focus on optimizing the salary-matching work to help the companies to seek reasonable salaries for their positions by proposing a data-driven approach to capture hidden patterns from user and company profiles. The contributions of this work reside in both operation research and computing. We (1) formulate the JSB optimization problem and (2) solve it by developing a data-driven method along with an effective algorithm optimization. Moreover, the proposed methodology has strengths in addressing the issues of data sparseness and result interpretability.
Ribonucleic acid (RNA) is a fundamental biological molecule that is essential to all living organisms, performing a versatile array of cellular tasks. The function of many RNA molecules is strongly related to the structure it adopts. As a result, great effort is being dedicated to the design of efficient algorithms that solve the “folding problem”—given a sequence of nucleotides, return a probable list of base pairs, referred to as the secondary structure prediction. Early algorithms largely rely on finding the structure with minimum free energy. However, the predictions rely on effective simplified free energy models that may not correctly identify the correct structure as the one with the lowest free energy. In light of this, new, data-driven approaches that not only consider free energy, but also use machine learning techniques to learn motifs are also investigated and recently been shown to outperform free energy–based algorithms on several experimental data sets. In this work, we introduce the new ExpertRNA algorithm that provides a modular framework that can easily incorporate an arbitrary number of rewards (free energy or nonparametric/data driven) and secondary structure prediction algorithms. We argue that this capability of ExpertRNA has the potential to balance out different strengths and weaknesses of state-of-the-art folding tools. We test ExpertRNA on several RNA sequence-structure data sets, and we compare the performance of ExpertRNA against a state-of-the-art folding algorithm. We find that ExpertRNA produces, on average, more accurate predictions of nonpseudoknotted secondary structures than the structure prediction algorithm used, thus validating the promise of the approach.Summary of Contribution: ExpertRNA is a new algorithm inspired by a biological problem. It is applied to solve the problem of secondary structure prediction for RNA molecules given an input sequence. The computational contribution is given by the design of a multibranch, multiexpert rollout algorithm that enables the use of several state-of-the-art approaches as base heuristics and allowing several experts to evaluate partial candidate solutions generated, thus avoiding assuming the reward being optimized by an RNA molecule when folding. Our implementation allows for the effective use of parallel computational resources as well as to control the size of the rollout tree as the algorithm progresses. The problem of RNA secondary structure prediction is of primary importance within the biology field because the molecule structure is strongly related to its functionality. Whereas the contribution of the paper is in the algorithm, the importance of the application makes ExpertRNA a showcase of the relevance of computationally efficient algorithms in supporting scientific discovery.
Feature selection is a fundamental problem in online advertising, as features usually need to be purchased from third parties, and they are costly. Although many feature selection techniques can be used in online advertising and the general information systems (IS) domain, their performance is often context specific. Therefore, the literature of IS is suffering from a lack of adequate and generic methods. In this study, we address this issue by proposing a novel approach that employs ideas from the field of cooperative game theory. We derive a (continuous) second-order cone program that any convex programming solver can solve for determining the best subset of features. We show the efficacy of our proposed method on a real-life online advertising case study. We demonstrate that our proposed approach performs better in accuracy, precision, recall, and F-1 score than the best of the other approaches with much fewer features. Also, to illustrate that our method’s benefits are not limited to the context of online advertising, we perform an extensive set of simulations and consider a well-established real-life data set drawn from the UCI Machine Learning Repository at the University of California, Irvine.Summary of Contribution: Selecting the best subset of features is an important problem in the context of online advertising and, more broadly, in the field of information systems because firms usually need to buy costly data to model and forecast economic outcomes. In this study, we propose a novel methodology for addressing this problem. The proposed method employs the concept of the Nash bargaining solution in cooperative game theory to create a good balance between maximizing the fit while minimizing the noise when selecting the best subset of features. We apply the method to a real-life online advertising case study, providing superior performance in predicting and interpreting the features. Moreover, we show that the proposed method applies to a broader range of feature selection problems. We conduct a comprehensive computational study on simulated regression data sets and other real-life classification data sets widely available in the machine learning domain. The result of these efforts indicates that our method is robust in terms of prediction accuracy by outperforming several state-of-the-art techniques.
Makespan minimization in permutation flow-shop scheduling is a well-known hard combinatorial optimization problem. Among the 120 standard benchmark instances proposed by E. Taillard in 1993, 23 have remained unsolved for almost three decades. In this paper, we present our attempts to solve these instances to optimality using parallel Branch-and-Bound (BB) on the GPU-accelerated Jean Zay supercomputer. We report the exact solution of 11 previously unsolved problem instances and improved upper bounds for eight instances. The solution of these problems requires both algorithmic improvements and leveraging the computing power of peta-scale high-performance computing platforms. The challenge consists in efficiently performing parallel depth-first traversal of a highly irregular and fine-grained search tree on distributed systems composed of hundreds of massively parallel accelerator devices and multicore processors. We present and discuss the design and implementation of our permutation-based BB and experimentally evaluate its parallel performance on up to 384 V100 GPUs (2 million CUDA cores) and 3840 CPU cores. The optimality proof for the largest solved instance requires about 64 CPU-years of computation—using 256 GPUs and over 4 million parallel search agents, the traversal of the search tree is completed in 13 hours, exploring 339×1012339×1012 nodes.
We consider binary integer programming problems with the min-max regret objective function under interval objective coefficients. We propose a heuristic framework, the iterated dual substitution (iDS) algorithm, which iteratively invokes a dual substitution heuristic and excludes from the search space any solution already checked in previous iterations. In iDS, we use a best scenario–based lemma to improve performance. We apply iDS to four typical combinatorial optimization problems: the knapsack problem, the multidimensional knapsack problem, the generalized assignment problem, and the set covering problem. For the multidimensional knapsack problem, we compare the iDS approach with two algorithms widely used for problems with the min-max regret criterion: a fixed-scenario approach, and a branch-and-cut approach. The results of computational experiments on a broad set of benchmark instances show that the proposed iDS approach performs best on most tested instances. For the knapsack problem, the generalized assignment problem, and the set covering problem, we compare iDS with state-of-the-art results. The iDS algorithm successfully updates best-known records for a number of benchmark instances.Summary of Contribution: This paper proposes a heuristic framework for binary integer programming (BIP) problems with the min-max regret objective function under interval objective coefficients. We selected four representative NP-hard combinatorial optimization problems: the knapsack problem, the multidimensional knapsack problem, the set covering problem, and the generalized assignment problem. We show the effectiveness and efficiency of the approach by comparing with state-of-the-art results.
The game of darts has enjoyed great growth over the past decade with the perception of darts moving from that of a pub game to a game that is regularly scheduled on prime-time television in many countries such as the United Kingdom, Germany, the Netherlands, and Australia, among others. It involves strategic interactions between two players, but to date, the literature has ignored these interactions. In this paper, we formulate and solve the game of darts as a dynamic zero-sum game (ZSG), and to the best of our knowledge, we are the first to do so. We also estimate individual skill models using a novel data set based on darts matches that were played by the top 16 professional players in the world during the 2019 season. Using the fitted skill models and our ZSG problem formulation, we quantify the importance of playing strategically—that is, taking into account the score and strategy of one’s opponent—when computing an optimal strategy. For top professionals, we find that playing strategically results in an increase in win probability of just 0.2%–0.6% over a single leg but as much as 2.2% over a best-of-31-legs match.Summary of Contribution: Dynamic zero-sum games (ZSGs) are of considerable interest, as they arise in many applications including sports, the management of communication networks, interdiction games, and heads-up poker—an important topic in modern artificial intelligence. In this study we consider the game of darts, which is growing increasingly popular around the world today. We formulate the game of darts as a ZSG and solve it iteratively by formulating each player’s best-response problem as a stochastic shortest-path (SSP) problem. We then solve these SSPs using standard dynamic programming methods. In solving the ZSG, we are able to accurately quantify the importance of top professionals playing strategically.
Prescriptive analytics provides organizations with scalable solutions for large-scale, automated decision making. At the core of prescriptive analytics methodology is optimization, a field devoted to the study of algorithms that solve complex decision-making problems. Optimization algorithms rely heavily on generic methods for identifying tight bounds, which provide both solutions to problems and optimality guarantees. In the last decade, decision diagrams (DDs) have demonstrated significant advantages in obtaining bounds compared with the standard linear relaxation commonly used by commercial solvers. However, the quality of the bounds computed by DDs depends heavily on the variable ordering chosen for the construction. Besides, the problem of finding an ordering that optimizes a given metric is generally NP-hard. This paper studies how machine learning, specifically deep reinforcement learning (DRL), can be used to improve bounds provided by DDs, in particular through learning a good variable ordering. The introduced DRL models improve primal and dual bounds, even over standard linear programming relaxations, and are integrated in a full-fledged branch-and-bound algorithm. This paper, therefore, provides a novel mechanism for utilizing machine learning to tighten bounds, adding to recent research on using machine learning to obtain high-quality heuristic solutions and, for the first time, using machine learning to improve relaxation bounds through a generic bounding method. We apply the methods on a classic optimization problem, the maximum independent set, and demonstrate through computational testing that optimization bounds can be significantly improved through DRL. We provide the code to replicate the results obtained on the maximum independent set. Summary of Contribution: This paper studies the use of reinforcement learning to compute a variable ordering of decision diagram-based approximations for discrete optimization problems. This is among the first works to propose the use of machine learning to improve upon generic bounding methods for discrete optimization problems, thereby establishing a critical bridge between optimization and learning.
Network design, a cornerstone of mathematical optimization, is about defining the main characteristics of a network satisfying requirements on connectivity, capacity, and level-of-service. It finds applications in logistics and transportation, telecommunications, data sharing, energy distribution, and distributed computing. In multicommodity network design, one is required to design a network minimizing the installation cost of its arcs and the operational cost to serve a set of point-to-point connections. The definition of this prototypical problem was recently enriched by additional constraints imposing that each origin-destination of a connection is served by a single path satisfying one or more level-of-service requirements, thus defining the Network Design with Service Requirements. These constraints are crucial, for example, in telecommunications and computer networks to ensure reliable and low-latency communication. In this paper we provide a new formulation for the problem, where variables are associated with paths satisfying the end-to-end service requirements. We present a fast algorithm for enumerating all the exponentially many feasible paths, and when this is not viable, we provide a column generation scheme that is embedded into a branch-and-cut-and-price algorithm. Extensive computational experiments on a large set of instances show that our approach can move a step further in the solution of the network design with service requirements compared with the current state-of-the-art.
Many network/graph structures are continuously monitored by various sensors that are placed at a subset of nodes and edges. The multidimensional data collected from these sensors over time create large-scale graph data in which the data points are highly dependent. Monitoring large-scale attributed networks with thousands of nodes and heterogeneous sensor data to detect anomalies and unusual events is a complex and computationally expensive process. This paper introduces a new generic approach inspired by state-space models for network anomaly detection that can utilize the information from the network topology, the node attributes (sensor data), and the anomaly propagation sets in an integrated manner to analyze the entire network all at once. This article presents how heterogeneous network sensor data can be analyzed to locate the sources of anomalies as well as the anomalous regions in a network, which can be impacted by one or multiple anomalies at any time instance. Experimental results demonstrate the superior performance of our proposed framework in detecting anomalies in attributed graphs.Summary of Contribution: With the increasing availability of large-scale network sensors and rapid advances in artificial intelligence methods, fundamentally new analytical tools are needed that can integrate data collected from sensors across the networks for decision making while taking into account the stochastic and topological dependencies between nodes, sensors, and anomalies. This paper develops a framework to intelligently and efficiently analyze complex and highly dependent data collected from disparate sensors across large-scale network/graph structures to detect anomalies and abnormal behavior in real time. Unlike general purpose (often black-box) machine learning models, this paper proposes a unique framework for network/graph structures that incorporates the complexities of networks and interdependencies between network entities and sensors. Because of the multidisciplinary nature of the paper that involves optimization, machine learning, and system monitoring and control, it can help researchers in both operations research and computer science domains to develop new network-specific computing tools and machine learning frameworks to efficiently manage large-scale network data.
We present FrankWolfe.jl, an open-source implementation of several popular Frank–Wolfe and conditional gradients variants for first-order constrained optimization. The package is designed with flexibility and high performance in mind, allowing for easy extension and relying on few assumptions regarding the user-provided functions. It supports Julia’s unique multiple dispatch feature, and it interfaces smoothly with generic linear optimization formulations using MathOptInterface.jl.
Mixed-integer second-order cone programs (MISOCPs) form a novel class of mixed-integer convex programs, which can be solved very efficiently as a result of the recent advances in optimization solvers. This paper shows how various performance metrics of M/G/1 queues can be modeled by different MISOCPs. To motivate the reformulation method, it is first applied to a challenging stochastic location problem with congestion, which is broadly used to design socially optimal service systems. Three different MISOCPs are developed and compared on different sets of benchmark test problems. The new formulations efficiently solve very large-size test problems that cannot be solved by the two existing methods developed based on linear programming within reasonable time. The superiority of the conic reformulation method is next shown over a state-space decomposition method recently used to solve an assignment problem in queueing systems. Finally, the general applicability of the method is shown for similar optimization problems that use queue-theoretic performance measures to address customer satisfaction and service quality.
Many relevant applications from diverse areas such as marketing, wildlife conservation, and defending critical infrastructure can be modeled as interdiction games. In this work, we introduce interdiction games whose objective is a monotone and submodular set function. Given a ground set of items, the leader interdicts the usage of some of the items of the follower in order to minimize the objective value achievable by the follower, who seeks to maximize a submodular set function over the uninterdicted items subject to knapsack constraints. We propose an exact branch-and-cut algorithm for this kind of interdiction game. The algorithm is based on interdiction cuts, which allow the leader to capture the follower’s objective function value for a given interdiction decision of the leader and exploit the submodularity of the objective function. We also present extensions and liftings of these cuts and discuss additional preprocessing procedures. We test our solution framework on the weighted maximal covering interdiction game and the bipartite inference interdiction game. For both applications, the improved variants of our interdiction cut perform significantly better than the basic version. For the weighted maximal covering interdiction game for which a mixed-integer bilevel linear programming (MIBLP) formulation is available, we compare the results with those of a state-of-the-art MIBLP solver. Whereas the MIBLP solver yields a minimum of 54% optimality gap within one hour, our best branch-and-cut setting solves all but four of 108 instances to optimality with a maximum of 3% gap among unsolved ones.
We study the network pricing problem where the leader maximizes revenue by determining the optimal amounts of tolls to charge on a set of arcs, under the assumption that the followers will react rationally and choose the shortest paths to travel. Many distinct single-level reformulations of this bilevel optimization program have been proposed; however, their relationship has not been established. In this paper, we aim to build a connection between those reformulations and explore the combination of the path representation with various modeling options, allowing us to generate 12 different reformulations of the problem. Moreover, we propose a new path enumeration scheme, path-based preprocessing, and hybrid framework to further improve performance and robustness when solving the final model. We provide numerical results, comparing all the derived reformulations and confirming the efficiency of the novel dimensionality reduction procedures.
It has been an open question for some time whether there exists a polynomial-time constant approximation for the lifetime scheduling problem of p-percent coverage. In this paper, we give a positive answer to this question.
Many convex optimization problems can be represented through conic extended formulations (EFs) using only the small number of standard cones recognized by advanced conic solvers such as MOSEK 9. However, EFs are often significantly larger and more complex than equivalent conic natural formulations (NFs) represented using the much broader class of exotic cones. We define an exotic cone as a proper cone for which we can implement easily computable logarithmically homogeneous self-concordant barrier oracles for either the cone or its dual cone. Our goal is to establish whether a generic conic interior point solver supporting NFs can outperform an advanced conic solver specialized for EFs across a variety of applied problems. We introduce Hypatia, a highly configurable open-source conic primal-dual interior point solver written in Julia and accessible through JuMP. Hypatia has a generic interface for exotic cones, some of which we define here. For seven applied problems, we introduce NFs using these cones and construct EFs that are necessarily larger and more complex. Our computational experiments demonstrate the advantages, especially in terms of solve time and memory usage, of solving the NFs with Hypatia compared with solving the EFs with either Hypatia or MOSEK 9.
In a telecommunication network, routing and wavelength assignment (RWA) is the problem of finding lightpaths for incoming connection requests. When facing a dynamic traffic, greedy assignment of lightpaths to incoming requests based on predefined deterministic policies leads to a fragmented network that cannot make use of its full capacity because of stranded bandwidth. At this point, service providers try to recover the capacity via a defragmentation process. We study this setting from two perspectives: (i) while granting the connection requests via the RWA problem and (ii) during the defragmentation process by lightpath rerouting. For both problems, we present the first two-stage stochastic integer programming model incorporating incoming request uncertainty to maximize the expected grade of service. We develop a decomposition-based solution approach, which uses various relaxations of the problem and a newly developed problem-specific cut family. Simulation of two-stage policies for a variety of instances in a rolling-horizon framework of 52 stages shows that our stochastic models provide high-quality solutions when compared with traditionally used deterministic ones. Specifically, the proposed provisioning policies yield improvements of up to 19% in overall grade of service and 20% in spectrum saving, while the stochastic lightpath rerouting policies grant up to 36% more requests, using up to just 4% more bandwidth spectrum.Summary of Contribution: For handling the intrinsic uncertainty of demand in the telecommunications industry, this paper proposes novel stochastic models and solution methodology for two fundamental problems in telecommunications at operational level: (i) routing and wavelength assignment (RWA) and (ii) lightpath rerouting problem. Despite the vast literature on the RWA problem, stochastic optimization has not been considered as a viable solution for resource allocation in optical networks. We propose two-stage stochastic programming models for both problems and design efficient decomposition-based solution methods that use various relaxations of the models and a new family of cutting planes. Our extensive and rigorous numerical experiments show the significant merit of incorporating uncertainty into decision making, as well as the effectiveness of the decomposition framework and our newly designed family of cuts in enhancing the solvability of both models. This work opens new avenues to explore where the powerful stochastic programming literature can be leveraged to make operational decisions in telecommunications problems, a field that currently relies mostly on deterministic and heuristic solution methods.
This work addresses inverse linear optimization, where the goal is to infer the unknown cost vector of a linear program. Specifically, we consider the data-driven setting in which the available data are noisy observations of optimal solutions that correspond to different instances of the linear program. We introduce a new formulation of the problem that, compared with other existing methods, allows the recovery of a less restrictive and generally more appropriate admissible set of cost estimates. It can be shown that this inverse optimization problem yields a finite number of solutions, and we develop an exact two-phase algorithm to determine all such solutions. Moreover, we propose an efficient decomposition algorithm to solve large instances of the problem. The algorithm extends naturally to an online learning environment where it can be used to provide quick updates of the cost estimate as new data become available over time. For the online setting, we further develop an effective adaptive sampling strategy that guides the selection of the next samples. The efficacy of the proposed methods is demonstrated in computational experiments involving two applications: customer preference learning and cost estimation for production planning. The results show significant reductions in computation and sampling efforts.Summary of Contribution: Using optimization to facilitate decision making is at the core of operations research. This work addresses the inverse problem (i.e., inverse optimization), which aims to infer unknown optimization models from decision data. It is, conceptually and computationally, a challenging problem. Here, we propose a new formulation of the data-driven inverse linear optimization problem and develop an efficient decomposition algorithm that can solve problem instances up to a scale that has not been addressed previously. The computational performance is further improved by an online adaptive sampling strategy that substantially reduces the number of required data points.
We explore the Projective Cutting-Planes algorithm proposed in Porumbel (2020) from new angles by applying it to two new problems, that is, to robust linear programming and to a cutting-stock problem with multiple lengths. Projective Cutting-Planes is a generalization of the widely used Cutting-Planes, and it aims at optimizing a linear function over a polytope P𝒫 with prohibitively many constraints. The main new idea is to replace the well-known separation subproblem with the following projection subproblem: given an interior point x∈P and a direction d, find the maximum steplength t such that x+td∈P. This enables one to generate a feasible solution at each iteration, a feature that does not exist built-in in a standard Cutting-Planes algorithm. The practical success of this new algorithm does not mainly come from the higher level ideas already presented in Porumbel (2020). Its success is significantly more dependent on the computation time needed to solve the projection subproblem in practice. Thus, the main challenge addressed by the current paper is the design of new techniques for solving this subproblem very efficiently for different polytopes P. We first address a well-known robust linear programming problem in which P is defined as a primal polytope. We then solve a multiple-length cutting stock problem in which P is a dual polytope defined in a column generation model. Numerical experiments on both these new problems confirm the potential of the proposed ideas. This enables us to draw conclusions supported by numerical results from both the current paper and Porumbel (2020) while also gaining more insight into the dynamics of the algorithm.Summary of Contribution: The well-known Cutting-Planes algorithm relies on the separation subproblem to cut off the current optimal solution. This paper belongs to a line of work whose goal is to “upgrade” the widely used separation subproblem to the following projection subproblem: given a feasible solution x inside some polytope P and a direction d, what is the maximum t value such that x + td ∈ P. In simple terms, one has to “shoot” from x along direction d up to the point where the boundary of the polytope is hit. The greatest challenge is to solve this projection subproblem rapidly enough to compete well in terms of computational speed with the separation subproblem algorithm. This paper shows how to achieve this on two new problems: robust linear programming and multiple length cutting stock. The numerical results confirm one important advantage offered by the projection logic: it enables one to generate a new feasible interior solution (i.e., x + td) at each iteration. The interior points thus generated actually guide the evolution of the overall algorithm, which is a feature that does not exist (built-in) in a traditional Cutting-Planes algorithm.
During the COVID-19 pandemic, Get Us PPE provided a platform aimed at connecting prospective donors of personal protective equipment (PPE) to prospective recipients of PPE. Requests by donors and recipients were collected over time, and periodically, the PPE matching problem was solved in order to instruct each donor to ship a certain quantity of PPE to a given recipient. The objectives of the PPE matching problem include maximizing the recipients’ fill rate, minimizing the total shipping distance, minimizing the holding time of PPE, and minimizing the number of shipments of each donor. This paper presents a software framework to facilitate the development of methodologies to solve the PPE matching problem and their testing on a real-world data set collected by Get Us PPE during the COVID-19 pandemic. Both software and data set are available on GitHub.
Sparse learning in high-dimensional survival analysis is of great practical importance, as exemplified by modern applications in credit risk analysis and high-throughput genomic data analysis. In this article, we consider the L0-regularized learning for simultaneous variable selection and estimation under the framework of additive hazards models and utilize the idea of primal dual active sets to develop an algorithm targeted at solving the traditionally nonpolynomial time optimization problem. Under interpretable conditions, comprehensive statistical properties, including model selection consistency, oracle inequalities under various estimation losses, and the oracle property, are established for the global optimizer of the proposed approach. Moreover, our theoretical analysis for the algorithmic solution reveals that the proposed L0-regularized learning can be more efficient than other regularization methods in that it requests a smaller sample size as well as a lower minimum signal strength to identify the significant features. The effectiveness of the proposed method is evidenced by simulation studies and real-data analysis.Summary of Contribution: Feature selection is a fundamental statistical learning technique under high dimensions and routinely encountered in various areas, including operations research and computing. This paper focuses on the L0-regularized learning for feature selection in high-dimensional additive hazards regression. The matching algorithm for solving the nonconvex L0-constrained problem is scalable and enjoys comprehensive theoretical properties.
This work focuses on a broad class of facility location problems in the context of adaptive robust stochastic optimization under the state-dependent demand uncertainty. The demand is assumed to be significantly affected by related state information, such as the seasonal or socio-economic information. In particular, a state-wise ambiguity set is adopted for modeling the distributional uncertainty associated with the demand in different states. The conditional distributional characteristics in each state are described by a support, as well as by mean and dispersion measures, which are assumed to be conic representable. A robust sensitivity analysis is performed, in which, on the one hand, we analyze the impact of the change in ambiguity-set parameters (e.g., state probabilities, mean value abounds, and dispersion bounds in different states) onto the optimal worst-case expected total cost using the ambiguity dual variables. On the other hand, we analyze the impact of the change in location design onto the worst-case expected second-stage cost and show that the sensitivity bounds are fully described as the worst-case expected shadow-capacity cost. As for the solution approach, we propose a nested Benders decomposition algorithm for solving the model exactly, which leverages the subgradients of the worst-case expected second-stage cost at the location decisions formed insightfully by the associated worst-case distributions. The nested Benders decomposition approach ensures a finite-step convergence, which can also be regarded as an extension of the classic L-shaped algorithm for two-stage stochastic programming to our state-wise, robust stochastic facility location problem with conic representable ambiguity. Finally, the results of a series of numerical experiments are presented that justify the value of the state-wise distributional information incorporated in our robust stochastic facility location model, the robustness of the model, and the performance of the exact solution approach.
The Benders decomposition algorithm often shows poor convergence. To improve the convergence of the Benders decomposition algorithm. Recently, it was proposed the use of feasibility cuts closest to a solution in the set defined by all feasibility cuts. We extend this feasibility cut selection scheme to a new cut selection scheme for optimality cuts and propose a new Benders separation framework that a single linear programming problem can solve. We show that optimality cuts generated by this scheme are Pareto optimal when some conditions are satisfied. Theoretical connections to the existing Benders cut generation methods are also identified. Extensive computational experiments on the multiple classes of benchmark problems demonstrate that the proposed algorithm improves the convergence speed and computational time.Summary of Contribution: The Benders decomposition algorithm is one of the most widely used algorithms in operations research. However, the Benders decomposition algorithm often shows poor convergence for some optimization problems. In this paper, to improve the convergence of the Benders decomposition algorithm, we propose a unified closest Benders cut generation scheme. We give theoretical properties of the proposed Benders cuts, including Pareto optimality and facet-defining conditions. Also, we conducted extensive computational tests on various instances, such as network design and expansion problems. The results show the effectiveness of the closest Benders cut compared with existing algorithms and Cplex.
We model the hierarchical and decentralized nature of product transitions using a mixed-integer bilevel program with two followers, a manufacturing unit and an engineering unit. The leader, corporate management, seeks to maximize revenue over a finite planning horizon. The manufacturing unit uses factory capacity to satisfy the demand for current products. The demand for new products, however, cannot be fulfilled until the engineering unit completes their development, which, in turn, requires factory capacity for prototype fabrication. We model this interdependency between the engineering and manufacturing units as a generalized Nash equilibrium game at the lower level of the proposed bilevel model. We present a reformulation where the interdependency between the followers is resolved through the leader’s coordination, and we derive a solution method based on constraint and column generation. Our computational experiments show that the proposed approach can solve realistic instances to optimality in a reasonable time. We provide managerial insights into how the allocation of decision authority between corporate leadership and functional units affects the objective function performance. This paper presents the first exact solution algorithm to mixed-integer bilevel programs with interdependent followers, providing a flexible framework to study decentralized, hierarchical decision-making problems.
The appointment scheduling problem (ASP) studies how to manage patient arrivals to a healthcare system to improve system performance. An important challenge occurs when some patients may not show up for an appointment. Although the ASP is well studied in the literature, the vast majority of the existing work does not consider the well-observed phenomenon that patient no-show is influenced by the appointment time, the usual decision variable in the ASP. This paper studies the ASP with random service time (exogenous uncertainty) with known distribution and patient decision-dependent no-show behavior (endogenous uncertainty). This problem belongs to the class of stochastic optimization with decision-dependent uncertainties. Such problems are notoriously difficult as they are typically nonconvex. We propose a stochastic projected gradient path (SPGP) method to solve the problem, which requires the development of a gradient estimator of the objective function—a nontrivial task, as the literature on gradient-based optimization algorithms for problems with decision-dependent uncertainty is very scarce and unsuitable for our model. Our method can solve the ASP problem under arbitrarily smooth show-up probability functions. We present solutions under different patterns of no-show behavior and demonstrate that breaking the assumption of constant show-up probability substantially changes the scheduling solutions. We conduct numerical experiments in a variety of settings to compare our results with those obtained with a distributionally robust optimization method developed in the literature. The cost reduction obtained with our method, which we call the value of distribution information, can be interpreted as how much the system performance can be improved by knowing the distribution of the service times, compared to not knowing it. We observe that the value of distribution information is up to 31% of the baseline cost, and that such value is higher when the system is crowded or/and the waiting time cost is relatively high.Summary of Contribution: This paper studies an appointment scheduling problem under time-dependent patient no-show behavior, a situation commonly observed in practice. The problem belongs to the class of stochastic optimization problems with decision-dependent uncertainties in the operations research literature. Such problems are notoriously difficult to solve as a result of the lack of convexity, and the present case requires different techniques because of the presence of continuous distributions for the service times. A stochastic projected gradient path method, which includes the development of specialized techniques to estimate the gradient of the objective function, is proposed to solve the problem. For problems with a similar structure, the algorithm can be applied once the gradient estimator of the objective function is obtained. Extensive numerical studies are presented to demonstrate the quality of the solutions, the importance of modeling time-dependent no-shows in appointment scheduling, and the value of using distribution information about the service times.
Analysts often rely on methods that presume constant stochastic variance, even though its degree can differ markedly across experimental and field settings. This reliance can lead to misestimation of effect sizes or unjustified theoretical or behavioral inferences. Classic utility-based discrete-choice theory makes sharp, testable predictions about how observed choice patterns should change when stochastic variance differs across items, brands, or conditions. We derive and examine the implications of assuming constant stochastic variance for choices made under different conditions or at different times, in particular, whether substantive effects can arise purely as artifacts. These implications are tested via an experiment designed to isolate the effects of stochastic variation in choice behavior. Results strongly suggest that the stochastic component should be carefully modeled to differ across both available brands and temporal conditions, and that its variance may be relatively greater for choices made for the future. The experimental design controls for several alternative mechanisms (e.g., flexibility seeking), and a series of related models suggest that several econometrically detectable explanations like correlated error, state dependence, and variety seeking add no explanatory power. A series of simulations argues for appropriate flexibility in discrete-choice specification when attempting to detect temporal stochastic inflation effects.
We discuss the Salisbury and Feinberg paper [Salisbury, L. C., F. M. Feinberg. 2010. Alleviating the constant stochastic variance assumption in decision research: Theory, measurement, and experimental test. Marketing Sci.29(1) 1–17], setting their contribution in the historical context of the wider literature on the role of error variability in discrete choice models. We discuss the seminal nature of their contribution and suggest that the paper should be required reading for current and future Ph.D. students.
The implications of Salisbury and Feinberg's (2010) paper [Salisbury, L. C., F. M. Feinberg. 2010. Alleviating the constant stochastic variance assumption in decision research: Theory, measurement, and experimental test. Marketing Sci.29(1) 1–17] for the process of model development and testing in the field of intertemporal choice analysis is explored. Although supporting the overall thrust of Salisbury and Feinberg's critique of previous empirical work in the area, we also see their paper as illustrating the dangers of drawing strong inferences about the behavioral interpretation of statistical model parameters without seeking convergent empirical evidence. In particular, we are skeptical about the extent to which the reported effects of temporal distance on the estimated scale parameter, σc, are uniquely, or even primarily, due to unobserved error inflation that reflects consumer's uncertainty about future utility. This interpretation is brought into question by several lines of reasoning. Conceptually, we note that “uncertainty” is different from “error” and that, for choice data, the error inflation model is mathematically identical to a model in which the scale parameter is a deterministic function of the temporal discount rate. Empirically, a reanalysis of data from previously published experiments does not consistently support temporal error inflation, temporal convergence of choice shares, or the scale parameter as an explanation of variety seeking in choice sequences. In our opinion, the cumulative results of research on intertemporal choice require models in which the attributes of choice alternatives are differentially discounted over time. Despite these findings, we advocate that choice researchers should indeed follow Salisbury and Feinberg's advice to not assume that error variances will be unaffected by experimental manipulations, and such effects should be explicitly modeled. We also agree that uncovering effects on error variance is just the first step, and the ultimate goal should be to rigorously explain the reasons for such effects.
We examine the specification and interpretation of discrete-choice models used in behavioral theory testing, with a focus on separating “coefficient scale” from “error scale,” particularly over time. Numerous issues raised in the thoughtful commentaries of Louviere and Swait [Louviere, J., J. Swait. 2010. Discussion of “Alleviating the constant stochastic variance assumption in decision research: Theory, measurement, and experimental test.” Marketing Sci.29(1) 18–22] and Hutchinson, Zauberman, and Meyer (HZM) [Hutchinson, J. W., G. Zauberman, R. Meyer. 2010. On the interpretation of temporal inflation parameters in stochastic models of judgment and choice. Marketing Sci.29(1) 23–31] are addressed, specifically the roles of response scaling, preference covariates, actual versus hypothetical consumption, “immediacy,” and heterogeneity, as well as key differences between the experimental setup in Salisbury and Feinberg [Salisbury, L. C., F. M. Feinberg. 2010. Alleviating the constant stochastic variance assumption in decision research: Theory, measurement, and experimental test. Marketing Sci.29(1) 1–17] and those typifying intertemporal choice and construal level theory. We strongly concur with most of the general conclusions put forth by the commentary authors, but we also emphasize a central point made in our research that may have been lost: that the temporal inflation effects observed in our empirical analysis could be attributed to stochastic effects, deterministic influences, or an amalgam; appropriate inferences depend on the nature of one's data and stimuli. We also report on further analyses of our data, as well as a meta-analysis of HZM's Table 1 that is consistent with our original findings. Implications for, and dimensions relevant to, future research on temporal stochastic inflation and its role in choice-based research are discussed.
A common theme in marketing literature is the acquisition and retention of customers as they trade up from inexpensive introductory offerings to those of higher quality. We develop a nonhomothetic choice model to accommodate effects of advertising, professional recommendation, and other factors that facilitate the description and management of trade-up. Our model allows advertising to affect the relative superiority or inferiority of products. This allows for a wide variety of trade-up patterns beyond those obtained from a standard random utility formulation of the logit model. Our nonhomothetic model allows for advertising to affect more than just brand intercepts (perceived quality), but also the rate at which consumers are willing to trade up to higher-quality brands. Advertising effects are measured using a randomized treatment and evaluated by considering their direct implications for firm pricing and profits.
This research aims to provide insights into the determinants of channel profitability and the relative power in the channel by considering consumer demand and the interactions between manufacturers and retailers in an equilibrium model.We use the Nash bargaining solution to determine wholesale prices and thus how margins are split in the channel. Equilibrium margins are a function of demand primitives and of retailer and manufacturer bargaining power. Bargaining power is itself a function of exogenous retail and manufacturer characteristics. The parties' bargaining positions are determined endogenously from the estimated substitution patterns on the demand side. The more they have to lose in a negotiation relative to an outside option, the weaker the bargaining position. We use the proposed bargaining model to investigate the role of the three main factors that have been blamed for the power shift from manufacturers to retailers in recent years (firm-size increases, store-brand introductions, and service-level differentiation). In our empirical analysis of the German market for coffee, we find that bargaining power varies among the different manufacturer-retailer pairs. This result suggests that bargaining power is not an inherent characteristic of a firm but rather depends on the negotiation partner. We are able to confirm empirically previous theoretical findings that there can be cases where the slice of the pie that goes to one of the channel members may decrease, but the overall pie increases and compensates for the smaller share of profits.
We examine a previously unstudied category of exchange items in which the true value is unknown to both the buyer and seller at the time of exchange but becomes known to both at a future time after the exchange. Real-world examples of such exchange items as in our study include forward contracts and fixed-fee turnkey contracts. We demonstrate that the discrepancy between the seller's willingness to accept (WTA) and buyer's willingness to pay (WTP) increases with (1) the level of uncertainty about the exchange item's value and (2) the exchange parties' level of risk aversion. In a series of studies, we manipulate and measure the level of uncertainty of the exchange item, measure the level of risk aversion of the exchange parties, and study the respective effects on decreasing the WTP while increasing the WTA.
The past few years have seen increasing interest in taking the notion of customer lifetime value (CLV) and extending it to value a customer base (with subsequent links to corporate valuation). The application of standard textbook discussions of CLV leads to calculations based on a single retention rate. However, at the cohort level, retention rates typically increase over time. It has been suggested that these observed dynamics are due, in large part, to a sorting effect in a heterogeneous population. We show that failing to recognize these dynamics yields a downward-biased estimate of the residual value of the customer base (compared to an aggregate analysis that ignores these dynamics). We also explore the implications of failing to account for retention dynamics when computing retention elasticities and find that the frequently reported values underestimate the true effect of increases in underlying retention rates in a heterogeneous world.
We study the effects of retailer in-store media on distribution channel relationships. Retailers open in-store media (ISM) and allow manufacturers to advertise to shoppers. Our results suggest that ISM has an important role in coordinating a distribution channel on advertising volume and product sales, and on mitigating supplier competition. Improved channel coordination is achieved through the internalization of advertising decisions from commercial forms of media (e.g., radio, TV, newspaper). A retailer may strategically subsidize manufacturers for their advertising on ISM. This subsidy is optimal even if ISM is more effective than commercial media. With manufacturer competition, a retailer can strategically use a “competitive premium” to ration excessive advertising between competing suppliers in a category. When manufacturers are asymmetric with preadvertising brand awareness, a retailer has an incentive to price discriminate by charging lower prices to manufacturers whose brand awareness is higher.
Referral bonuses, in which an existing customer gets an in-kind or cash reward for referring a new customer, are a popular way to stimulate word of mouth. In this paper, we examine key firm decisions about such bonuses. Others have studied referral bonus programs; a key difference is that we study the role of recommendations not just in spreading awareness (as they do) but also in providing assessments. We start with the idea that people have a variety of reasons for making product recommendations, including placing a value on a friend's outcome with a product they recommend. We apply that idea in a context of asymmetric information: A customer combines his knowledge about the product and his familiarity with friends' tastes, making him more informed than the friends. Thus, the recommendation is a signal about the value of the product to the friend. In this setting, we consistently find that the greater the concern for others' outcomes, the higher the referral bonus should be, as long as the firm cannot more efficiently motivate recommendations with a lower price. Moreover, if price is the more efficient lever, the optimal bonus is zero, and the optimal price is low. We also show that greater concern tends to reduce firm profit and, in some cases, actually reduces consumer welfare as well.
This paper examines the interaction of information provision, product quality, and pricing decisions by competitive firms to explore the following question: in a competitive market where consumers face uncertainty about product quality and/or their preference for quality, which firms—those that sell higher- or lower-quality products—have the higher incentive to provide what type of information? We find that while the higher-quality firm should always provide information resolving consumer uncertainty on product quality, the lower-quality firm under certain conditions will have the higher incentive to and will be the one to provide information resolving consumer uncertainty about their quality preferences. In the analysis, we trace the latter result to competition and to free-riding on the information provision. Specifically, in a monopoly market or when consumer free-riding is restricted by the costliness of store visits, the lower-quality firm would have a lower incentive to provide information resolving consumer preference uncertainty than otherwise. The model is also adapted to examine product returns as a possible strategy of information provision.
Choice decisions in the marketplace are often made by a collection of individuals or a group. Examples include purchase decisions involving families and organizations. A particularly unique aspect of a joint choice is that the group's preference is very likely to diverge from preferences of the individuals that constitute the group. For a marketing researcher, the biggest hurdle in measuring group preference is that it is often infeasible or cost prohibitive to collect data at the group level. Our objective in this research is to propose a novel methodology to estimate joint preference without the need to collect joint data from the group members. Our methodology makes use of both stated and inferred preference measures, and merges experimental design, statistical modeling, and utility aggregation theories to capture the psychological processes of preference revision and concession that lead to the joint preference. Results based on a study involving a cell phone purchase for 214 parent-teen dyads demonstrate predictive validity of our proposed method.
Many consumer durable retailers often do not advertise their prices and instead ask consumers to call them for prices. It is easy to see that this practice increases the consumers' cost of learning the prices of products they are considering, yet firms commonly use such practices. Not advertising prices may reduce the firm's advertising costs, but the strategic effects of doing so are not clear. Our objective is to examine the strategic effects of this practice. In particular, how does making price discovery more difficult for consumers affect competing retailers' price, service decisions, and profits?We develop a model in which a manufacturer sells its product through a high-service retailer and a low-service retailer. Consumers can purchase the retail service at the high-end retailer and purchase the product at the competing low-end retailer. Therefore, the high-end retailer faces a free-riding problem. A retailer first chooses its optimal service levels. Then, it chooses its optimal price levels. Finally, a retailer decides whether to advertise its prices. The model results in four structures: (1) both retailers advertise prices, (2) only the low-service retailer advertises price, (3) only the high-service retailer advertises price, and (4) neither retailer advertises price.We find that when a retailer does not advertise its price and makes price discovery more difficult for consumers, the competition between the retailers is less intense. However, the retailer is forced to charge a lower price. In addition, if the competing retailer does advertise its prices, then the competing retailer enjoys higher profit margins. We identify conditions under which each of the above four structures is an equilibrium and show that a low-service retailer not advertising its price is a more likely outcome than a high-service retailer doing so. We then solve the manufacturer's problem and find that there are several instances when a retailer's advertising decisions are different from what the manufacturer would want. We describe the nature of this channel coordination problem and identify some solutions.
We examine two questions: Does the roundness or precision of prices bias magnitude judgments? If so, do these biased judgments affect buyer behavior? Results from five studies suggest that buyers underestimate the magnitudes of precise prices. We term this the precision effect. The first three studies are laboratory experiments designed to test the existence of the precision effect and examine the underlying psychological processes. In Study 1, we find that precise prices are judged to be smaller than round prices of similar magnitudes. For example, participants in this experiment incorrectly judged $395,425 to be smaller than $395,000. In Study 2, we show that precision is more likely to affect magnitude judgments under conditions of uncertainty. Study 3 demonstrates that manipulating prior experience with the pattern of roundness and precision in numbers can moderate the precision effect. Studies 4 and 5 examine whether the precision effect influences buyers' willingness to pay for negotiated purchases (e.g., houses). In Study 4, we conduct an experiment on a nationally representative sample of homeowners to demonstrate that participants are willing to pay more for houses when the sellers use precise (e.g., $364,578) instead of comparable round (e.g., $365,000) prices. In Study 5, we analyze data from residential real estate transactions in two separate markets and find that buyers pay higher sale prices when list prices are more precise. These empirical results enrich our understanding of the psychological processes that underlie price magnitude judgments and have substantive implications for buyer and seller behavior.
Paid placements on search engines reached sales of nearly $11 billion in the United States last year and represent the most rapidly growing form of online advertising today. In its classic form, a search engine sets up an auction for each search word in which competing websites bid for their sponsored links to be displayed next to the search results. We model this advertising market, focusing on two of its key characteristics: (1) the interaction between the list of search results and the list of sponsored links on the search page and (2) the inherent differences in attractiveness between sites. We find that both of these special aspects of search advertising have a significant effect on sites' bidding behavior and the equilibrium prices of sponsored links. Often, sites that are not among the most popular ones obtain the sponsored links, especially if the marginal return of sites on clicks is quickly decreasing and if consumers do not trust sponsored links. In three extensions, we also explore (1) heterogeneous valuations across bidding sites, (2) the endogenous choice of the number of sponsored links that the search engine sells, and (3) a dynamic model where websites' bidding behavior is a function of their previous positions on the sponsored list. Our results shed light on the seemingly random order of sites on search engines' list of sponsored links and their variation over time. They also provide normative insights for both buyers and sellers of search advertising.
This paper develops a framework for measuring “tipping”—the increase in a firm's market share dominance caused by indirect network effects. Our measure compares the expected concentration in a market to the hypothetical expected concentration that would arise in the absence of indirect network effects. In practice, this measure requires a model that can predict the counterfactual market concentration under different parameter values capturing the strength of indirect network effects. We build such a model for the case of dynamic standards competition in a market characterized by the classic hardware/software paradigm. To demonstrate its applicability, we calibrate it using demand estimates and other data from the 32/64-bit generation of video game consoles, a canonical example of standards competition with indirect network effects. In our example, we find that indirect network effects can lead to a strong, economically significant increase in market concentration. We also find important roles for beliefs on both the demand side, as consumers tend to pick the product they expect to win the standards war, and on the supply side, as firms engage in penetration pricing to invest in growing their networks.
Packaged goods manufacturers spend in excess of $75 billion annually on trade promotions, even though their effectiveness has been hotly debated by academics and practitioners for decades. One reason for this ongoing debate is that empirical research has been limited mostly to case studies, managerial surveys, and data from one or two supermarket chains in a single market. In this paper, we assemble a unique data set containing information on prices, quantities, and promotions throughout the entire channel in a category. Our study extends the empirical literature on pass-through in three important ways. First, we investigate how pass-through varies across more than 1,000 retailers in over 30 states. Second, we study pass-through at multiple levels of the distribution channel. Third, we show how the use of accounting metrics, such as average acquisition cost, rather than transaction cost, yields biased estimates of pass-through and therefore overstates the effectiveness of trade promotions.We find that mean pass-through elasticities are 0.71, 0.59, and 0.41, for the wholesaler, retailer, and total channel, respectively. More importantly, at each level of the channel we observe large variances in pass-through estimates that we explain using various measures of cost and competition. Surprisingly, we find that market structure and competition have a relatively small impact on pass-through.We conclude by showing how the profitability of manufacturer and wholesaler deals can be improved by utilizing detailed effectiveness estimates. For example, a manufacturer using an inclusive trade promotion strategy might offer a 10% off invoice deal to all retailers on every product. This strategy would decrease manufacturer and wholesaler profits for 56% of product/store combinations, whereas retailers experience a profit boost in 96% of cases. Manufacturers and wholesalers can avoid unprofitable trade deals for specific products and retailers by utilizing estimates of pass-through, consumer price elasticity, and margins. Compared to the inclusive strategy, such a selective trade promotion strategy would improve deal profitability by 80% and reduce costs by 40%.
During the summer of 2005, the three domestic U.S. automobile manufacturers offered a customer promotion that allowed customers to buy new cars using discount programs formerly offered only to employees. The initial months of the promotion were record sales months for each of the three firms, suggesting that customers thought that the prices offered during the promotion were particularly attractive. In reality, however, many customers paid higher prices under the employee discount pricing promotion. We propose that the promotion changed customers' beliefs about current versus future prices, convincing them to purchase during the promotion rather than delay in anticipation of future discounts. We investigate several alternative explanations for the simultaneous increase in prices and sales, including advertising, decreased financing costs, industry trends, disutility of bargaining, consumer differences, and changes in trade-in values. None of these explanations fully explains the concomitant increase in prices and sales.
We present a framework of durable goods purchasing behavior in related technology product categories that incorporates the following aspects unique to technology product purchases. First, it accounts for consumers' anticipation of declining prices (or increasing quality) over time. Second, the durable nature of technology products implies that even if two categories are related as complements, consumers may stagger their purchases over several periods. Third, the forward-looking consumer decision process, as well as the durable nature of technology products, implies that a consumer's purchase in one category will depend on the anticipated price and quality trajectories of all categories. As a potential aid to future researchers, we also lay out the data requirements for empirically estimating the parameters of our model and the consequences of not having various elements of these data on our ability to estimate the parameters. The data available for our empirical analysis are household-level information on category-level first-time adoption decisions in three categories—personal computers, digital cameras, and printers. Our results reveal a strong complementary relationship between the three categories. Policy simulations based on a temporary price decrease in any one category provide interesting insights into how consumers would modify their adoption behavior over time and also across categories as a consequence of the price decrease.
Mere observation of others' choices can be informative about product quality. This paper develops an individual-level dynamic model of observational learning and applies it to a novel data set from the U.S. kidney market, where transplant candidates on a waiting list sequentially decide whether to accept a kidney offer. We find strong evidence of observational learning: patients draw negative quality inferences from earlier refusals in the queue, thus becoming more inclined towards refusal themselves. This self-reinforcing chain of inferences leads to poor kidney utilization despite the continual shortage in kidney supply. Counterfactual policy simulations show that patients would have made more efficient use of kidneys had the concerns behind earlier refusals been shared. This study yields a set of marketing implications. In particular, we show that observational learning and information sharing shape consumer choices in markedly different ways. Optimal marketing strategies should take into account how consumers learn from others.
In this study we develop a method that optimally selects online media vehicles and determines the number of advertising impressions that should be purchased and then served from each chosen website. As a starting point, we apply Danaher's [Danaher, P. J. 2007. Modeling page views across multiple websites with an application to Internet reach and frequency prediction. Marketing Sci.26(3) 422–437] multivariate negative binomial distribution (MNBD) for predicting online media exposure distributions. The MNBD is used as a component in the broader task of media selection. Rather than simply adapting previous selection methods used in traditional media, we show that the Internet poses some unique challenges. Specifically, online banner ads and other forms of online advertising are sold by methods that differ substantially from the way other media advertising is sold. We use a nonlinear optimization algorithm to solve the optimization problem and derive the optimum online media schedule. Data from an online audience measurement firm and an advertising agency are used to illustrate the speed and accuracy of our method, which is substantially quicker than using complete enumeration.
In a viral marketing campaign, an organization develops a marketing message and encourages customers to forward this message to their contacts. Despite its increasing popularity, there are no models yet that help marketers to predict how many customers a viral marketing campaign will reach and how marketers can influence this process through marketing activities. This paper develops such a model using the theory of branching processes. The proposed viral branching model allows customers to participate in a viral marketing campaign by (1) opening a seeding e-mail from the organization, (2) opening a viral e-mail from a friend, and (3) responding to other marketing activities such as banners and offline advertising. The model parameters are estimated using individual-level data that become available in large quantities in the early stages of viral marketing campaigns. The viral branching model is applied to an actual viral marketing campaign in which over 200,000 customers participated during a six-week period. The results show that the model quickly predicts the actual reach of the campaign. In addition, the model proves to be a valuable tool to evaluate alternative what-if scenarios.
We present a demand system for tied goods incorporating dynamics arising from the tied nature of the products and the stockpiling induced by storability and durability. We accommodate competition across tied good systems and competing downstream retail formats by endogenizing the retail format at which consumers choose to stockpile inventory. This facilitates measurement of long-run retail substitution effects and yields estimates of complementarities within, and substitution across, competing systems of tied goods. We present an empirical application to an archetypal tied goods category: razors and blades. We discuss the implications of measured effects for manufacturer pricing when selling the tied products through an oligopolistic downstream retail channel and assess the extent to which retail substitution reduces channel conflict.
No Abstract available.
The mixed or heterogeneous multinomial logit (MIXL) model has become popular in a number of fields, especially marketing, health economics, and industrial organization. In most applications of the model, the vector of consumer utility weights on product attributes is assumed to have a multivariate normal (MVN) distribution in the population. Thus, some consumers care more about some attributes than others, and the IIA property of multinomial logit (MNL) is avoided (i.e., segments of consumers will tend to switch among the subset of brands that possess their most valued attributes). The MIXL model is also appealing because it is relatively easy to estimate. Recently, however, some researchers have argued that the MVN is a poor choice for modelling taste heterogeneity. They argue that much of the heterogeneity in attribute weights is accounted for by a pure scale effect (i.e., across consumers, all attribute weights are scaled up or down in tandem). This implies that choice behaviour is simply more random for some consumers than others (i.e., holding attribute coefficients fixed, the scale of their error term is greater). This leads to a “scale heterogeneity” MNL model (S-MNL). Here, we develop a generalized multinomial logit model (G-MNL) that nests S-MNL and MIXL. By estimating the S-MNL, MIXL, and G-MNL models on 10 data sets, we provide evidence on their relative performance. We find that models that account for scale heterogeneity (i.e., G-MNL or S-MNL) are preferred to MIXL by the Bayes and consistent Akaike information criteria in all 10 data sets. Accounting for scale heterogeneity enables one to account for “extreme” consumers who exhibit nearly lexicographic preferences, as well as consumers who exhibit very “random” behaviour (in a sense we formalize below).
This paper examines demand elasticities using an integrated framework proposed by Hanemann [Hanemann, M. W. 1984. Discrete/continuous models of consumer demand. Econometrica52(3) 541–561], which models the incidence, brand choice, and quantity decisions of a consumer as an outcome of her utility maximization subject to budget constraints. Although the Hanemann framework has been the mainstay of earlier efforts to examine these decisions jointly, empirical researchers who have used the it to study purchase behavior have often found that the quantity elasticities are around −1, regardless of the brand or category. We attempt to uncover the underlying reasons for this finding and propose approaches to get as close to the “true” quantity elasticities as possible. We do this by (i) analytically demonstrating how assumptions on the distribution of the brand-specific econometrician's errors imply certain restrictions that in turn force quantity elasticities to −1, (ii) discussing how these restrictions can be alleviated by considering a suitable specification of unobserved parameter heterogeneity, and (iii) using scanner data to empirically illustrate the impact of the restrictions on quantity elasticities and the relative efficacy of multiple specifications of unobserved heterogeneity in easing those restrictions. We find that the specification of unobserved heterogeneity crucially influences estimates of quantity elasticities and that the mixture normal specification outperforms the alternatives.
We introduce and test a behavioral model of consumer product search that extends a baseline normative model of sequential search by incorporating nonnormative influences that are local in the sense that they reflect consumers' undue sensitivity to recently encountered alternatives. We propose two types of such local behavioral influences that, at each stage of a search process, can manifest themselves both in which of the products inspected up to that point is deemed to be the most preferred one (the product comparison decision) and whether to terminate the search at that stage (the stopping decision). The first of these influences is that consumers respond excessively to the attractiveness of the currently inspected product, at the expense of all others (“focalism”). The second proposed behavioral influence is that consumers overreact to the difference in attractiveness between the current product and the one encountered just prior to it (“local contrast”). Converging evidence from two experiments, which combine to guarantee both high internal and high external validity, provides support for the proposed behavioral influences. Our findings demonstrate that consumers' product comparison and stopping decisions in sequential product search are jointly governed by normative principles and by the proposed local behavioral influences.
The price-aggressive discount format, popularized by chains such as Aldi and Lidl, is very successful in most Western economies. Its success is a major source of concern for traditional supermarkets. Discounters not only have a direct effect on supermarkets' market shares, but they also exert considerable pressure to improve operational efficiency and/or to decrease prices.We use an empirical entry model to study the degree of intra- and interformat competition between discounters and supermarkets. Information on the competitive impact of new entrants is derived from the observed entry decisions of supermarkets and discounters in a large cross section of local markets, after controlling for a number of local market characteristics. In our modeling framework, we endogenize the number of retailers and allow for asymmetric intra- and interformat competitive effects in a flexible way.We apply our modeling approach to the German grocery industry, where the discount format has stabilized after two decades of continued growth. We find evidence of intense competition within both the supermarket and discounter format, although competition between supermarkets is found to be more severe. Most importantly, discounters only start to affect the profitability of conventional supermarkets from the third entrant onwards. This may explain why many retailers rush to add a discount chain to their portfolio: early entrants may benefit from the growth of the discount-prone segment without cannibalizing the profits of their more conventional supermarket stores.
This paper studies a manufacturer's optimal decisions on extending its product line when the manufacturer sells through either a centralized channel or a decentralized channel. We show that a manufacturer may provide a longer product line for consumers in a decentralized channel than in a centralized channel if the market is fully covered. In addition, a manufacturer's decisions on the length of its product line may not always be optimal from a social welfare perspective in either a centralized or a decentralized channel. Under certain conditions, a decentralized channel can provide the product line length that is socially optimal, whereas a centralized channel cannot.
Manufacturers can acquire consumer information in a sequential manner and influence downstream retail behavior through sharing the acquired information. This paper examines the interaction between a manufacturer's optimal information acquisition and sharing strategies in a vertical relationship, capturing the impacts of both the flexibility to sequentially control information collection and the flexibility in ex post voluntary sharing. We show that when information acquisition is sequential, the manufacturer may not acquire perfect information even if it is costless to do so. This self-restriction in information acquisition follows from the manufacturer's motivation to strategically influence retail behavior. When information acquisition is inflexible and constrained to be either zero or perfect information, the manufacturer acquires less (more) information under mandatory (voluntary) sharing. Moreover, voluntary sharing unambiguously leads to more information being generated, because the manufacturer has the option to strategically withhold the acquired information that turns out to be unfavorable. Finally, the conditions under which the manufacturer ex ante prefers a particular sharing format are examined.
This paper shows that when the alternatives offered to consumers span the preference space (as it would be chosen by a firm), search or evaluation costs may lead consumers not to search and not to choose if too many or too few alternatives are offered. If too many alternatives are offered, then the consumer may have to engage in many searches or evaluations to find a satisfactory fit. This may be too costly and result in the consumer avoiding making a choice altogether. If too few alternatives are offered, then the consumer may not search or choose, fearing that an acceptable choice is unlikely. These two forces result in the existence of a finite optimal number of alternatives that maximizes the probability of choice.
Panel survey data have been gaining importance in marketing. However, one challenge of estimating econometric models based on panel survey data is how to account for underreporting; that is, respondents do not report behavioral incidences that actually occur. Underreporting is especially likely to occur in a panel survey because the data-recording mechanism is often tedious, complex, and effortful. The probability of underreporting is likely to vary across respondents and also over the duration of the survey period. In this paper, we propose a model to simultaneously study reported behavioral incidences and partially observed actual behavioral incidences. We propose a Bayesian approach for estimating the proposed model. We treat those unobserved actual behavioral incidences as latent variables, and the Gibbs sampler makes it convenient to impute the nonreported consumption incidences along with making inferences on other model parameters. Our proposed method has two advantages. First, it offers a model-based approach to remove the underreporting bias in panel survey data and therefore allows marketing researchers to make accurate inferences about consumers' actual behavior. Second, the method also offers a natural way to study factors that influence respondents' propensity to underreport. Because we treat those underreported behavioral incidences as nonmissing at random, this underreporting propensity varies across respondents and over time. This understanding can help marketing researchers design the right strategy to intervene and incentivize respondents to authentically report and hence improve the quality of survey data. The proposed model and estimation approach are tested on both synthetic data and actual panel survey data on consumer-reported beverage-drinking behavior. Our analysis suggests that underreporting can significantly mask respondents' true behavior.
Used goods markets are currently important transaction channels for durable products. For some durable products, such markets first appeared when retailers started buying back used products from “old” customers and selling them to new ones for a profit (retail used goods market). The growth of electronic peer-to-peer (P2P) markets opened up a second, frictionless used goods channel where new customers can buy used products directly from old customers (P2P used goods market). Both these markets compete with the original primary market where retailers sell unused products procured from the manufacturer. This paper focuses on understanding the role that the sequential emergence of the above two used goods markets plays in shaping the product upgrade strategy of the manufacturer and the pricing strategy of the primary market retailer in the context of a decentralized, dyadic channel dealing with a renewable set of consumers. Our analysis establishes that frequent product upgrades and rising retail prices in durable product sectors of our interest are due to the emergence of the P2P used goods market and how the market interacts with the retail used goods source in altering the relative powers of the channel partners. Moreover, contrary to popular belief, we show that the initial introduction of the retail used goods channel actually discourages introduction of new versions and restrains the rise in retail prices. We also comment on how the two used goods markets affect the profits of the channel partners. We then provide empirical support for our theoretical result regarding product upgrades using data from the college textbook industry, a durable product that fits our model setup.
The interest in social networks among marketing scholars and practitioners has sharply increased in the last decade. One social network of which network scholars increasingly recognize the unique value is the academic collaboration (coauthor) network.We offer a comprehensive database of the collaboration network among marketing scholars over the last 40 years (available at http://mktsci.pubs.informs.org). Based on the ProQuest database, it documents the social collaboration among researchers in dozens of the leading marketing journals, enabling us to create networks of active marketing researchers. Unlike most of the published academic collaboration research, our database is dynamic and follows the evolution of the field over many years.In this paper, we describe the database and point to some basic network descriptives that lead to interesting research questions. We believe this database can be of much value to researchers interested in the evolution of social networks over time, as well as the specific evolution of the marketing discipline.The data set described in this paper is maintained by the authors and available through http://mktsci.pubs.informs.org
Prerelease demand forecasting is one of the most crucial yet difficult tasks facing marketers in the $60 billion motion picture industry. We propose functional shape analysis (FSA) of virtual stock markets (VSMs) to address this long-standing challenge. In VSMs, prices of a movie's stock reflect the dynamic demand expectations prior to the movie's release. Using FSA, we identify a small number of distinguishing shapes, e.g., the last-moment velocity spurt, that carry information about a movie's future demand and produce early and accurate prerelease forecasts.We find that although forecasting errors from the existing methods, e.g., those that rely on movie features, can be as high as 90.87%, our approach results in an error of only 4.73%. Because demand forecasting is especially useful for managerial decision making when provided longbefore a movie's release, we further demonstrate how our method can be used for early forecasting and compare its power against alternative approaches. We also discuss the theoretical implications of the discovered shapes that may help managers identify indicators of a potentially successful movie early and dynamically.
This paper develops a model for the estimation and analysis of demand in the context of social interactions. Decisions made by a group of customers are modeled to be an equilibrium outcome of an empirical discrete game, such that all group members must be satisfied with chosen outcomes. The game-theoretic approach assists estimation by allowing us to account for the endogeneity of group members' decisions while also serving as a managerial tool that can simulate equilibrium outcomes for the group when the firm alters the marketing mix to the group. The model builds upon the existing literature on empirical models of discrete games by introducing a random coefficients heterogeneity distribution. Monte Carlo simulations reveal that including the heterogeneity resolves the endogenous group formation bias commonly noted in the social interactions literature. By estimating the heterogeneous equilibrium model using Bayesian hierarchical Markov chain Monte Carlo, we can also recover some parameters at the individual level to evaluate group-specific characteristics and targeted marketing strategies. To validate the model and illustrate its implications, we apply it to a data set of groups of golfers. We find significant social interaction effects, such that 65% of the median customer value is attributable to the customer and the other 35% is attributable to the customer's affect on members of his group. We also consider targeted marketing strategies and show that group-level targeting increases profit by 1%, whereas targeting within groups can increase profitability by 20%. We recognize that customer backlashes to targeting could be greater when group members receive different offers, so we suggest some alternatives that could retain some of the profitability of within group targeting while avoiding customer backlashes.
The phenomenon of paid search advertising has now become the most predominant form of online advertising in the marketing world. However, we have little understanding of the impact of search engine advertising on consumers' responses in the presence of organic listings of the same firms. In this paper, we model and estimate the interrelationship between organic search listings and paid search advertisements. We use a unique panel data set based on aggregate consumer response to several hundred keywords over a three-month period collected from a major nationwide retailer store chain that advertises on Google. In particular, we focus on understanding whether the presence of organic listings on a search engine is associated with a positive, a negative, or no effect on the click-through rates of paid search advertisements, and vice versa for a given firm. We first build an integrated model to estimate the relationship between different metrics such as search volume, click-through rates, conversion rates, cost per click, and keyword ranks. A hierarchical Bayesian modeling framework is used and the model is estimated using Markov chain Monte Carlo methods. Our empirical findings suggest that click-throughs on organic listings have a positive interdependence with click-throughs on paid listings, and vice versa. We also find that this positive interdependence is asymmetric such that the impact of organic clicks on increases in utility from paid clicks is 3.5 times stronger than the impact of paid clicks on increases in utility from organic clicks. Using counterfactual experiments, we show that on an average this positive interdependence leads to an increase in expected profits for the firm ranging from 4.2% to 6.15% when compared to profits in the absence of this interdependence. To further validate our empirical results, we also conduct and present the results from a controlled field experiment. This experiment shows that total click-through rates, conversions rates, and revenues in the presence of both paid and organic search listings are significantly higher than those in the absence of paid search advertisements. The results predicted by the econometric model are also corroborated in this field experiment, which suggests a causal interpretation to the positive interdependence between paid and organic search listings. Given the increased spending on search engine-based advertising, our analysis provides critical insights to managers in both traditional and Internet firms.
Price promotions and bundling have been two of the most widely used marketing tools in industry practice. Past literature has assumed that firms respond to price promotions by promoting a product in the same category. In this paper, we extend this literature as well as the bundling literature by considering the possibility that a firm may respond to a competitor's price promotions by also offering a cross-buying or bundling discount. Using a game-theoretic model, we show that bundle discounts can help increase profits in a competitive market by creating endogenous loyalty, thereby reducing the intensity of promotional competition. We also find that bundle discounts can be used as an effective defensive marketing tool to prevent customer defection to the competition.
With increasing fragmentation of media markets and recent advances in technology, loss of advertising effectiveness has been a great concern for marketers. For consumers, the digital video recorder (DVR) offers the possibility to fast-forward through live programming. Whereas the DVR thus benefits consumers by reducing nuisance from commercials, industry observers believe that it may diminish advertisers' profits by rendering commercials ineffective. We use a model of informative advertising to study the effect of DVR penetration on competing advertisers' strategies and profits. We find that the overall effect of DVRs depends on the trade off between loss of advertising effectiveness and reduction in competition between firms. The latter effect arises because DVR penetration may increase the ratio of partially informed to fully informed consumers. We identify conditions under which an increase in DVR penetration counterintuitively leads firms to increase advertising levels and enjoy higher profits. Interestingly, we find that greater DVR penetration is beneficial for firms when the share of DVR owners in the population is above—rather than below—a threshold level. We also study the impact of different fast-forwarding (“zipping”) behaviors on product market competition.
This paper investigates the effects of a limited consumer memory on the price competition between firms. It studies a specific aspect of memory—namely, the categorization of available price information that the consumers may need to recall for decision making. This paper analyzes competition between firms in a market with uninformed consumers who do not compare prices, informed consumers who compare prices but with limited memory, and informed consumers who have perfect memory. Consumers, aware of their memory limitations, choose how to encode the prices into categories, whereas firms take the limitations of consumers into account in choosing their pricing strategies. Two distinct types of categorization processes are investigated: (1) a symmetric one in which consumers compare only the labels of price categories from the competing firms and (2) an asymmetric one in which consumers compare the recalled price of one firm with the actual price of the other. We find that the equilibrium partition for the consumers calls for finer categorization toward the bottom of the price distribution. Thus consumers have a motivation to invest in greater memory resources in encoding lower prices to induce firms to charge more favorable prices. The interaction between the categorization strategies of the consumers and the price competition between the firms is such that small initial improvements in recall move the market outcomes quickly toward the case of perfect recall. Even with few memory categories, the expected price consumers pay and their surplus is close to the case of perfect recall. There is thus a suggestion in this model that market competition adjusts to the memory limitations of consumers.
This study attempts to answer a basic customer management dilemma facing firms: when should the firm use behavior-based pricing (BBP) to discriminate between its own and competitors' customers in a competitive market? If BBP is profitable, when should the firm offer a lower price to its own customers rather than to the competitor's customers? This analysis considers two features of customer behavior up to now ignored in BBP literature: heterogeneity in customer value and changing preference (i.e., customer preferences are correlated but not fixed over time). In a model where both consumers and competing firms are forward-looking, we identify conditions when it is optimal to reward the firm's own or competitor's customers and when BBP increases or decreases profits. To the best of our knowledge, we are the first to identify conditions in which (1) it is optimal to reward one's own customers under symmetric competition and (2) BBP can increase profits with fully strategic and forward-looking consumers.
This paper documents the existence and magnitude of contiguous word-of-mouth effects of signal quality of a video-on-demand (VOD) service on customer acquisition. We operationalize contiguous word-of-mouth effect based on geographic proximity and use behavioral data to quantify the effect.The signal quality for this VOD service is exogenously determined, objectively measured, and spatially uncorrelated. Furthermore, it is unobserved to the potential subscriber and is revealed postadoption. For a subscriber, the signal quality translates directly into the number of movies available for viewing, thus representing a part of the overall service quality. The combination of signal quality along with location and neighborhood information for each subscriber and potential subscriber allows us to resolve the typical challenges in measuring causal social network effects.We find that contiguous word of mouth affects about 8% of the subscribers with respect to their adoption behavior. However, this effect acts as a double-edged sword because it is asymmetric. We find that the effect of negative word of mouth arising from poor signal quality is more than twice as large as the effect of positive word of mouth arising from excellent signal quality. Besides contiguous word of mouth, we find that advertising and the retail environment also play a role in adoption.
Before the deregulation of digital subscriber line (DSL) services by the Federal Communications Commission (FCC) in 2005, phone companies were required to share their DSL bandwidth with independent DSL providers. Despite the large number of independent providers that entered the market, phone companies accounted for 95.3% of all DSL subscribers in 2005. A common explanation for this is based on supply-side factors such as the costs faced by these providers to lease phone lines from phone companies, as well as the price discounts offered by phone companies. In this paper, we look for a demand-side explanation for this market outcome. Analyzing consumer choices in the broadband category alone would lead us to the conclusion that consumers have a much higher preference for their local phone providers—a finding at odds with service awards received by independent DSL providers. Thus we look for a demand-side explanation that is based on the demand not just for broadband services but also for related services such as cable TV and local phone. We find evidence of strong complementarities between the consumption of broadband and of those related categories. The main source of such complementarities, in our data, is the benefits to consumers from having a single provider for multiple services. We then carry out counterfactual experiments assuming that there are no changes in the regular prices of the various services. Our results indicate that the share of phone companies in the broadband market would have been 43% smaller without complementarities stemming from such a single-provider effect, whereas shutting off the state dependence effects would have reduced their share by 30%, and shutting off the effects of price discounts on the DSL+local phone bundle would have resulted in their share declining by 21%.
Despite the economic significance of the theme park industry and the huge investments needed to set up new attractions, no marketing models exist to guide these investment decisions. This study addresses this gap in the literature by estimating a response model for theme park attendance. The model not only determines the contribution of each attraction to attendance, but also how this contribution is distributed within and across years. The model accommodates saturation effects, which imply that the impact of a new attraction is smaller if similar attractions are already present. It also captures reinforcement effects, meaning that a new attraction may reinforce the drawing power of similar extant attractions, especially when these were introduced recently. The model is calibrated on 25 years of weekly attendance data from the Efteling, a leading European theme park. Our return on investment calculations show that it is more profitable to invest in multiple smaller attractions than in one big one. This finding is in remarkable contrast with the current “arms race” in the industry. Furthermore, even though thrill rides tend to be more effective than theme rides, there are conditions under which one should consider to switch to the latter.
Private labels (PLs) are ubiquitous in several categories, including groceries, apparel, and appliances. However, existing empirical work has not examined the differential impact of various upstream supply arrangements for PL products or the strategic motives for PL supply. To do so requires one to model the interaction between private and national label (NL) products both upstream and downstream while accounting for strategic behavior on the part of manufacturers and retailers and retaining essential differences between NL and PL products. We build a model that satisfies these requirements and lets us answer our two research questions: First, can an NL firm profit from being an outsourced PL supplier? Second, what are the upstream and downstream impacts of different PL supply arrangements?We answer these questions by modeling private labels as homogenous products at wholesale, but as differentiated products at retail. In contrast, national label products are differentiated at both wholesale and retail levels. Using structural model estimates for fluid milk in a major metropolitan area, we conduct three counterfactual experiments. We find that both NL producers and retailers profit from adding private labels. We also find that a vertically integrated supply of PL leads to lower prices for end consumers.
Many firms increasingly offer community venues to their customers to facilitate social interactions amongst them. Prior studies have shown that community participants have high engagement and loyalty toward the firm and provide useful feedback and referrals. However, it is not clear whether community participants are the firm's “fans” to begin with and self-select themselves into the community, or whether community participation leads to increased relational customer behaviors. In the current research, we employ data from a field experiment to help answer this question. The data come from a year-long study conducted by eBay Germany, and they reveal that a simple e-mail invitation significantly increased customer participation in the firm's community. Results also show that community participation had mixed effects on customers' likelihoods of participating in buying and selling behaviors. Community participation did not translate into increased behaviors, as would be commonly expected. Although there is no impact of participation on the number of bids placed or the revenue earned, there is a negative impact of participation on the number of listings and the amount spent. Together, these results suggest that the community participants become more selective and efficient sellers, and they also become more conservative in their spending on the items for which they bid. The results also show that customer community marketing programs may be targeted to a broader set of the firm's customers than just the fans.
Health-care experts believe that increases in portion sizes served by food vendors contribute to the obesity epidemic. This paper shows that food vendors can profit handsomely by using supersizing strategies where regular portion sizes are priced sufficiently high to discourage price-conscious consumers from selecting them, and the prices for enlarging food portions are set so low that these customers are tempted to order the larger portion sizes and overeat. Setting aside the impact of obesity on health-care costs, we show that using supersizing to steer customers toward consuming excessive amounts of food can destroy value from a social perspective; thus this social value destruction trap adds another justification for pressuring food vendors to reduce supersizing for unhealthy food. As a public policy response, we consider how “moderating policies” may counter these effects through measures designed specifically to encourage eating in moderation by applying supersizing bans, taxes, and warnings.
René Algesheimer (“The Impact of Customer Community Participation on Customer Behaviors: An Empirical Investigation”) is a professor of marketing and market research at the University of Zurich. He has a diploma in mathematics from the University of Mainz, Germany, and a doctorate in marketing from the University of St. Gallen, Switzerland. His research focus lies in studying contextual effects in consumer preferences formation and decision making. He has published a number of articles about virtual communities and contextual multilevel effects in marketing and management journals, and he frequently cooperates with Internet companies.Subramanian Balachander (“Why Bundle Discounts Can Be a Profitable Alternative to Competing on Price Promotions”) is an associate professor of management at Purdue University's Krannert School of Management. He has a Ph.D. in industrial administration from Carnegie Mellon University. His research interests are in competitive marketing strategy, pricing, sales promotions, and market signaling. He was recognized as a Purdue University Faculty Scholar in 2009. His research has been published in the Journal of Marketing, Management Science, Marketing Science, the Journal of Consumer Research, Review of Marketing Science, and Marketing Letters.Sharad Borle (“The Impact of Customer Community Participation on Customer Behaviors: An Empirical Investigation”) is an associate professor of marketing at the Jesse H. Jones Graduate School of Business, Rice University, Houston. He holds an MBA degree from the XLRI Institute of Management, Jamshedpur, India, and a Ph.D. from the Carnegie Mellon University. His research interests include Bayesian econometrics and the application of quantitative methods to study consumer behavior.Pradeep K. Chintagunta (“The Effect of Signal Quality and Contiguous Word of Mouth on Customer Acquisition for a Video-on-Demand Service”; “Complementarities and the Demand for Home Broadband Internet Services”) is the Robert Law Professor of Marketing at the Booth School of Business, University of Chicago. His research interests include understanding pharmaceutical, media, technology, and entertainment markets as well as studying consumer packaged goods markets.Jack (Xinlei) Chen (“An Empirical Investigation of Private Label Supply by National Label Producers”) is an assistant professor in marketing at the Sauder School of Business, University of British Columbia. He received his Ph.D. in marketing from the University of Minnesota and his B.Eng. from Tsinghua University, China. His research interests center around firms' strategic actions and irrationality/externality in consumers' choices. In his spare time, he enjoys sports, reading, and spending time with his kids.Yuxin Chen (“Limited Memory, Categorization, and Competition”) is the Polk Brothers Professor in Retailing and a professor of marketing at the Kellogg School of Management, Northwestern University. He holds a Ph.D. in marketing from Washington University in St. Louis. His primary research areas include competitive strategy, database and Internet marketing, Bayesian econometric methods, and behavior economics. He is an area editor for Marketing Science and serves as an associate editor for the Journal of Marketing Research, Management Science, and Quantitative Marketing and Economics. His research has appeared in journals such as Marketing Science, the Journal of Marketing Research, Management Science, and Quantitative Marketing and Economics.Marnik G. Dekimpe (“Return on Roller Coasters: A Model to Guide Investments in Theme Park Attractions”) is a research professor at Tilburg University, The Netherlands, and a professor of marketing at the Catholic University Leuven, Belgium. He received his Ph.D. from the University of California, Los Angeles. He has won best paper awards for Marketing Science (1995, 2001), the Journal of Marketing Research (1999), the International Journal of Research in Marketing (1997, 2001, 2002), and Technological Forecasting and Social Change (2000). He has also won the 2010 Louis W. Stern Award for his work on the valuation of Internet channels. He is an academic trustee with both the Marketing Science Institute and AiMark. He serves as editor of the International Journal of Research in Marketing and also serves on the editorial boards of Marketing Science, the Journal of Marketing, the Journal of Marketing Research, Marketing Letters, Review of Marketing Science, and the Journal of Interactive Marketing.Tirtha Dhar (“An Empirical Investigation of Private Label Supply by National Label Producers”) is an assistant professor of marketing at the Sauder School of Business, University of British Columbia. He received his Ph.D. in agricultural and resource economics from the University of Connecticut and M.A. in economics from the Delhi School of Economics. His current research interests deal with marketing and public policy, channel bargaining, and the role of information in online markets.Utpal M. Dholakia (“The Impact of Customer Community Participation on Customer Behaviors: An Empirical Investigation”) is the William S. Mackey and Verne F. Simons associate professor of management at the Jesse H. Jones Graduate School of Management, Rice University. He has a master's degree in psychology and a Ph.D. in marketing from the University of Michigan, as wells as a master's degree in operations research from The Ohio State University. His research interests lie in studying motivational psychology of consumers and online marketing issues such as virtual communities and online auctions. He has published in a number of marketing and management journals and consults with firms in financial services, energy, health care, and high-tech industries.Paul W. Dobson (“For a Few Cents More: Why Supersize Unhealthy Food?”) is the head of Norwich Business School at the University of East Anglia in the United Kingdom. He was previously the Storaid Professor of Retailing and Professor of Competition Economics at Loughborough University and has also held posts at the University of Nottingham and the University of St Andrews. His research interests span marketing, economics, strategy, and public policy, and he is currently focusing on the dynamics of price competition in grocery retailing and the impact of retail pricing on overeating and food waste.Eitan Gerstner (“For a Few Cents More: Why Supersize Unhealthy Food?”) is a professor of marketing at the Faculty of Industrial Engineering and Management, Technion–Israel Institute of Technology. He received a B.A. in statistics and economics (1976) from Haifa University Israel, and he received his M.A. (1980) and Ph.D. in economics (1983) from the University of California, San Diego. He works in the areas of pricing, service marketing, and marketing strategies, and his research has appeared in leading academic journals. His most recent research focuses on pricing under uncertain demand, customer acquisition and referrals, customer service and satisfaction, and public policy measures to fight obesity and food waste.Anindya Ghose (“Analyzing the Relationship Between Organic and Sponsored Search Advertising: Positive, Negative, or Zero Interdependence?”) is an assistant professor of information, operations, and management sciences at New York University's Stern School of Business. His primary research seeks to analyze two related issues: the economic impact of the Internet on industries transformed by its technology infrastructure, and the economic value of user-generated content in social media as well as the means for monetization such content through online advertising. In 2007, he received the prestigious National Science Foundation CAREER Award for his work on quantifying the economic value of user-generated content on the Internet. He is also a winner of a 2005 ACM SIGMIS Doctoral Dissertation Award, a 2006 Microsoft Live Labs Award, and a 2007 Microsoft Virtual Earth Award. He serves as an associate editor of Management Science and Information Systems Research. His work has been published or is forthcoming in leading journals such as Information Systems Research, the Journal of Management Information Systems, the Journal of Economics and Management Strategy, MIS Quarterly, Management Science, Marketing Letters, Marketing Science, and Statistical Science.Bikram Ghosh (“Why Bundle Discounts Can Be a Profitable Alternative to Competing on Price Promotions”; “Advertising Effectiveness, Digital Video Recorders, and Product Market Competition”) is an assistant professor of marketing at the Moore School of Business, University of South Carolina. He holds an M.S. in economics and a Ph.D. in management with specialization in marketing from Purdue University. His research focuses on product bundling, pricing, and competitive strategy. He is also interested on the effect of advertising avoidance technologies such as digital video recorders on product market competition. His research has appeared in Marketing Science and Management Science.Wesley R. Hartmann (“Demand Estimation with Social Interactions and the Implications for Targeted Marketing”) is an associate professor of marketing at the Stanford Graduate School of Business. He holds a Ph.D. in economics from the University of California, Los Angeles. He is interested in applying and developing econometric techniques to analyze questions relevant to marketing and economics. His current research focuses on dynamic choice contexts, pricing, social interactions, and targeted marketing.Ganesh Iyer (“Limited Memory, Categorization, and Competition”) is the Edgar F. Kaiser Professor of Business Administration at the Haas School of Business, University of California, Berkeley. He received his Ph.D. in management from the University of Toronto. His research interests include coordination in distribution channels, competitive strategy, Internet strategy, customer information markets, and the effects of bounded rationality on competitive markets. He is an area editor at Marketing Science and an associate editor at Quantitative Marketing and Economics. His research has appeared in several journals including Marketing Science, Management Science, the Journal of Marketing Research, and Quantitative Marketing and Economics.George John (“An Empirical Investigation of Private Label Supply by National Label Producers”) is the General Mills-Gerot Chair in Marketing and Chair of the Marketing Department at the Carlson School of Management, where he also serves as the academic director of the Carlson Brand Enterprise, a consultancy that partners top MBA students with insightful faculty on real-world brand projects. His work centers on the governance of interfirm links. He is one of the nation's leading experts in marketing channels, industrial marketing, and high technology. His current research includes work on component branding and consumer guarantees.Hongju Liu (“Complementarities and the Demand for Home Broadband Internet Services”) is an assistant professor of marketing at the University of Connecticut's School of Business. His research interests include dynamic structural models, technology markets, network efforts, pricing, and empirical industrial organization. Before receiving his Ph.D. and MBA from the University of Chicago, he earned an MA in mathematics and an MS in computer science at the University of Wisconsin–Madison.Puneet Manchanda (“The Effect of Signal Quality and Contiguous Word of Mouth on Customer Acquisition for a Video-on-Demand Service”) is a professor of marketing at the University of Michigan's Ross School of Business. He received his M.Phil. and Ph.D. in marketing from Columbia University. His research interests span models of social interactions, micromarketing and targeting, advertising, new media, multicategory choice, and learning models. He uses data from various domains such as online and offline social networks, pharmaceuticals, packaged goods, high technology, and gaming in his research. His methodological interests are empirical industrial organization methods and Bayesian econometrics. A recent study identified him as one of the most productive researchers in marketing from 1982 to 2006. He is an associate editor for Management Science and Quantitative Marketing and Economics and is on the editorial boards of Marketing Science, the Journal of Marketing Research, and the International Journal of Research in Marketing. His research has appeared in Marketing Science, the Journal of Marketing Research, Quantitative Marketing and Economics, the Journal of Consumer Psychology, and Marketing Letters.Sungjoon Nam (“The Effect of Signal Quality and Contiguous Word of Mouth on Customer Acquisition for a Video-on-Demand Service”) is an assistant professor at the Rutgers Business School at Rutgers, The State University of New Jersey. His research interests include customer relationship management, social interactions, and new product launch strategy in technology, pharmaceutical, and financial markets.Om Narasimhan (“An Empirical Investigation of Private Label Supply by National Label Producers”) is an associate professor in marketing at the Carlson School of Management, University of Minnesota.Amit Pazgal (“Limited Memory, Categorization, and Competition”) is the Jones Distinguished Associate Professor of Management and an associate professor of marketing at the Jesse H. Jones Graduate School of Business, Rice University. He received his Ph.D. from the Kellogg Graduate School of Management, Northwestern University. His research focuses on the analysis and characterization of marketing to strategic, sophisticated consumers on the one hand and boundedly rational ones on the other. He is an associate editor at Quantitative Marketing and Economics. He has published several articles in leading marketing, management, and economic journals, exploring dynamic pricing, advertising, and personalization mechanisms.Jiwoong Shin (“A Customer Management Dilemma: When Is It Profitable to Reward One's Own Customers?”) is an assistant professor of marketing at the Yale School of Management, Yale University. He holds an M.S. and B.S. from Seoul National University, as well as a Ph.D. from the Massachusetts Institute of Technology. His current research focuses on analytical modeling of strategic interactions between firms and consumers—in particular, consumer search theory, advertising, pricing strategies, and customer relationship management. His previous work has appeared in journals such as the Journal of Marketing Research, Management Science, and Marketing Science.Siddharth S. Singh (“The Impact of Customer Community Participation on Customer Behaviors: An Empirical Investigation”) is an assistant professor of marketing at the Jesse H. Jones Graduate School of Business, Rice University. He has a Ph.D. in marketing from the J. L. Kellogg School of Management, Northwestern University, an MBA in marketing and finance from the University of Illinois at Urbana-Champaign, and a bachelor's degree in electronics engineering from the Institute of Technology, Banaras Hindu University (India). Prior to receiving his Ph.D., he worked for several years with Johnson & Johnson in sales and product management. He is an applied econometrician, and his research interests include database marketing, customer lifetime value, loyalty programs, customer purchase and return behavior, value co-creation, customer communities, and online marketing issues.Axel Stock (“Why Bundle Discounts Can Be a Profitable Alternative to Competing on Price Promotions”; “Advertising Effectiveness, Digital Video Recorders, and Product Market Competition”) is an assistant professor of marketing at the College of Business Administration, University of Central Florida. He holds an M.S. in economics and a Ph.D. in management with specialization in marketing from Purdue University. His research focuses on product line management, pricing, and competitive strategy. Studying the effectiveness of product scarcity strategies is of particular interest to him. His research has appeared in Marketing Science and Management Science.K. Sudhir (“A Customer Management Dilemma: When Is It Profitable to Reward One's Own Customers?”) is a professor of marketing and director of the China India Consumer Insights Program at the Yale School of Management. He received his Ph.D. from Cornell University and was an assistant professor at New York University's Stern School (1998–2001), a lecturer at Cornell's Johnson School (1995) and Cornell's School of Hotel Administration (1996). His papers have been winners, honorable mentions, or finalists of the Bass, Lehmann, Little, Green and Wittink awards. Two of his papers were among the 10 finalists for the Inaugural INFORMS Long Term Impact Award in 2009. He serves as an area editor for Marketing Science, as an associate editor for Management Science and Quantitative Marketing and Economics, and on the editorial board of the Journal of Marketing Research.Harald J. van Heerde (“Return on Roller Coasters: A Model to Guide Investments in Theme Park Attractions”) is a professor of marketing at the University of Waikato, Hamilton, New Zealand. He holds a Ph.D. (cum laude) from the University of Groningen, The Netherlands. His work has appeared in journals such as Marketing Science, the Journal of Marketing Research (JMR), the International Journal of Research in Marketing (IJRM), and Quantitative Marketing and Economics. His research focuses on assessing the effectiveness of the marketing mix using econometric models. It covers various substantive domains such as sales promotions and advertising, pricing and price wars, and loyalty programs. His work has been awarded with the Paul E. Green Award and William F. O'Dell Award for JMR and with the IJRM Best Article Award. He serves on the editorial board of JMR and is an area editor of IJRM.Rutger D. van Oest (“Return on Roller Coasters: A Model to Guide Investments in Theme Park Attractions”) is an assistant professor of marketing at Tilburg University, The Netherlands. He holds a Ph.D. from the Erasmus University Rotterdam. His work has appeared in journals such as the Journal of Econometrics and Quantitative Marketing and Economics.Sha Yang (“Analyzing the Relationship Between Organic and Sponsored Search Advertising: Positive, Negative, or Zero Interdependence?”) is an associate professor of marketing at New York University's Stern School of Business. Her primary research focuses on consumer preference/behavioral interaction, social network, search engine advertising, and hierarchical Bayesian analysis. Her research has been published in Marketing Science, Management Science, the Journal of Marketing Research, the Journal of Marketing, Quantitative Marketing and Economics, Marketing Letters, the Journal of Economic Psychology, and the International Journal of Forecasting.Ting Zhu (“Complementarities and the Demand for Home Broadband Internet Services”) is an assistant professor of marketing at the Booth School of Business, University of Chicago. Her current research focuses on retail competition, technology markets, pricing, and firms' entry decisions. Before receiving her Ph.D. from Carnegie Mellon University, she earned an M.S. and B.Eng. in management from Tsinghua University.
We develop a conceptual framework about the impact that branding activity (the audiovisual representation of brands) and consumers' focused versus dispersed attention have on consumer moment-to-moment avoidance decisions during television advertising. We formalize this framework in a dynamic probit model and estimate it with Markov chain Monte Carlo methods. Data on avoidance through zapping, along with eye tracking on 31 commercials for nearly 2,000 participants, are used to calibrate the model. New, simple metrics of attention dispersion are shown to strongly predict avoidance. Independent of this, central on-screen brand positions, but not brand size, further promote commercial avoidance. Based on the model estimation, we optimize the branding activity that is under marketing control for ads in the sample to reduce commercial avoidance. This reveals that brand pulsing—while keeping total brand exposure constant—decreases commercial avoidance significantly. Both numerical simulations and a controlled experiment using regular and edited commercials, respectively, provide evidence of the benefits of brand pulsing to ward off commercial avoidance. Implications for advertising management and theory are addressed.
Two-sided exchange networks (such as eBay.com) often advertise their number of users, presumably to encourage further participation. However, these networks differ markedly on how they advertise their user base. Some highlight the number of sellers, some emphasize the number of buyers, and others disclose both. We use field experiment data from a business-to-business website to examine the efficacy of these different display formats. Before each potential seller posted a listing, the website randomized whether to display the number of buyers and/or sellers, and if so, how many buyers and/or sellers to claim. We find that when information about both buyers and sellers is displayed, a large number of sellers deters further seller listings. However, this deterrence effect disappears when only the number of sellers is presented. Similarly, a large number of buyers is more likely to attract new listings when it is displayed together with the number of sellers. These results suggest the presence of indirect network externalities, whereby a seller prefers markets with many other sellers because they help attract more buyers.
Can negative information about a product increase sales, and if so, when? Although popular wisdom suggests that “any publicity is good publicity,” prior research has demonstrated only downsides to negative press. Negative reviews or word of mouth, for example, have been found to hurt product evaluation and sales. Using a combination of econometric analysis and experimental methods, we unify these perspectives to delineate contexts under which negative publicity about a product will have positive versus negative effects. Specifically, we argue that negative publicity can increase purchase likelihood and sales by increasing product awareness. Consequently, negative publicity should have differential effects on established versus unknown products. Three studies support this perspective. Whereas a negative review in the New York Times hurt sales of books by well-known authors, for example, it increased sales of books that had lower prior awareness. The studies further underscore the importance of a gap between publicity and purchase occasion and the mediating role of increased awareness in these effects.
Whereas a growing body of research has examined the consumer-related implications of deceptive advertising, the stock market consequences stemming from the regulatory exposure of such infractions remain largely unexplored. In a step to address this gap, the current research examines the effect of regulatory reports of misleading ads on firm stock prices. Results from an event study, focusing on the pharmaceutical industry as the empirical context, show an average abnormal return of −0.91% associated with regulatory reports of deceptive advertising. Analysis of the abnormal returns, however, reveals that the stock market response to these reports is shaped by omission bias, in that investors penalize commission violations more than omission violations. Furthermore, firm reputation is found to moderate the penalty for commission violations. In addition, two experiments examine the effect of such violations on investor beliefs. The first helps elucidate the process mechanism underlying the observed stock market effects and the second provides insights regarding the reputation-omission bias interaction for firms committing repeat violations. Overall, our findings provide important theoretical, managerial, and public policy implications regarding the role of financial markets in regulating deceptive ad practices.
The pioneering Pasternack returns-policy model analyzed channel coordination with a single supplier catering to a retailer facing stochastic demand for a perishable product with a fixed price, and the model showed that giving partial returns of unsold stock to the retailer is the optimal policy for the entire supply chain. The result thus begs the question as to why manufacturers of perishable commodities widely accept full returns of unsold stock as the norm. We model the environment as one where two capacity-constrained manufacturers compete for shelf space with the same retailer, and we show that a complete-credit returns policy is in fact the only possible equilibrium of the game. Our results obviate the need for knowing the exact functional form of the demand distribution in order to compute the returns credit, as Pasternack's results would require. From a retailer's standpoint, we establish a simple procurement strategy and show that it is optimal. The same game with price-only contracting has a pure-strategy equilibrium when the supplier capacities are below a threshold value and a mixed-strategy equilibrium when the supplier capacities cross this threshold but are still so limited that no single supplier can with certainty supply all the quantity demanded.
This paper examines pairwise assortment similarities at U.S. supermarkets to understand how assortment composition and size are related to underlying factors that describe local store clientele, local competitive structure, and the retail outlets' characteristics. The top-selling items, which cumulatively make up 50% of sales, are sold at nearly every store, but other items are viewed as optional. We find that, within states, supermarkets owned by the same chain carry similar assortments and that the composition of their clientele and the presence of competing stores have effects on assortment similarity that are an order of magnitude smaller than ownership structure. In contrast, we find that, across states, supermarkets owned by the same chain do version their assortment. We explain this difference using extant work on the minimal efficient scale of supermarkets and on local demand effects. Furthermore, we investigate the distribution and role of regional brands. We find that regional brands are primarily distributed by small regional chains or independent stores. “Value” regional brands are primarily distributed by supermarket firms without store brands, whereas the distribution of “premium” regional brands is unrelated to the presence of store brands. We discuss our findings in the context of modeling assortment decisions and manufacturers designing distribution policies.
Market segmentation is inherently a multicriterion problem even though it has often been modeled as a single-criterion problem in the traditional marketing literature and in practice. This paper discusses the multicriterion nature of market segmentation and develops a new mathematical model that addresses this issue. A new method for market segmentation based on multiobjective evolutionary algorithms, called MMSEA, is developed. It complements existing segmentation methods by optimizing multiple objectives simultaneously, searching for globally optimal solutions, and approximating a set of Pareto-optimal solutions. We have applied and evaluated this method in two empirical studies for two firms from distinct industries: descriptive segmentation of the cell phone service market from a dual-value creation perspective and predictive segmentation of retail customers based on profit and customer sociodemographic attributes. The results provide decision makers with compelling alternatives and enhanced flexibility currently missing in existing market segmentation methods.
The ability to demonstrate the impact of marketing action on firm financial performance is crucial for evaluating, justifying, and optimizing the expenditure of a firm's marketing resources. This presents itself as a formidable task when one considers both the variety and potential influence of marketing activity. We propose a hierarchical Bayesian model of simultaneous supply and demand that allows us to formally study the financial impact of a variety of marketing activities, including those that operate on different timescales. The supply-side model provides insight into how the firm allocates resources across its various subunits. We illustrate our approach in a services context by integrating data from three independent studies conducted by a large national bank. Our model allows customer and employee satisfaction to influence firm profitability by moderating the conditional relationship between the bank's operational inputs and its proclivity to produce revenue.
The U.S. pharmaceutical industry spent upwards of $18 billion on marketing drugs in 2005; detailing and drug sampling activities accounted for the bulk of this spending. To stay competitive, pharmaceutical managers need to maximize the return on these marketing investments by determining which physicians to target as well as when and how to target them.In this paper, we present a two-stage approach for dynamically allocating detailing and sampling activities across physicians to maximize long-run profitability. In the first stage, we estimate a hierarchical Bayesian, nonhomogeneous hidden Markov model to assess the short- and long-term effects of pharmaceutical marketing activities. The model captures physicians' heterogeneity and dynamics in prescription behavior. In the second stage, we formulate a partially observable Markov decision process that integrates over the posterior distribution of the hidden Markov model parameters to derive a dynamic marketing resource allocation policy across physicians.We apply the proposed approach in the context of a new drug introduction by a major pharmaceutical firm. We identify three prescription-behavior states, a high degree of physicians' dynamics, and substantial long-term effects for detailing and sampling. We find that detailing is most effective as an acquisition tool, whereas sampling is most effective as a retention tool. The optimization results suggest that the firm could increase its profits substantially while decreasing its marketing spending. Our suggested framework provides important implications for dynamically managing customers and maximizing long-run profitability.
This paper explores whether and how a firm should adapt its strategy in view of consumer use of prior customer ratings. Specifically, we consider optimal pricing and whether the firm should offer an unexpected frill to early customers to enhance their product experiences. We show that if price history is unobserved by consumers, a forward-looking firm should always modify its strategy from single-period optimal one, but it may be optimal to do so by lowering price, by lowering price and offering frills, or by raising price and offering frills, depending on the market growth rate. Specifically, the last strategy becomes optimal when market growth rate is high enough. The results are similar when the price history is observed by consumers, except that no deviation from single-period profit maximization choices is optimal when market growth is low enough. We also analyze whether the firm should prefer that the price information be stated in or left out of consumer reviews. In addition, in considering the effects of consumer heterogeneity, we conclude that the optimal firm's effort to affect ratings is higher when the idiosyncratic part of consumer uncertainty is larger.
Our objective in this paper is to measure the impact (valence, volume, and variance) of national online user reviews on designated market area (DMA)-level local geographic box office performance of movies. We account for three complications with analyses that use national-level aggregate box office data: (i) aggregation across heterogeneous markets (spatial aggregation), (ii) serial correlation as a result of sequential release of movies (endogenous rollout), and (iii) serial correlation as a result of other unobserved components that could affect inferences regarding the impact of user reviews. We use daily box office ticket sales data for 148 movies released in the United States during a 16-month period (out of the 874 movies released) along with user review data from the Yahoo! Movies website. The analysis also controls for other possible box office drivers. Our identification strategy rests on our ability to identify plausible instruments for user ratings by exploiting the sequential release of movies across markets—because user reviews can only come from markets where the movie has previously been released, exogenous variables from previous markets would be appropriate instruments in subsequent markets.In contrast with previous studies that have found that the main driver of box office performance is the volume of reviews, we find that it is the valence that seems to matter and not the volume. Furthermore, ignoring the endogenous rollout decision does not seem to have a big impact on the results from our DMA-level analysis. When we carry out our analysis with aggregated national data, we obtain the same results as those from previous studies, i.e., that volume matters but not the valence. Using various market-level controls in the national data model, we attempt to identify the source of this difference.By conducting our empirical analysis at the DMA level and accounting for prerelease advertising, we can classify DMAs based on their responsiveness to firm-initiated marketing effort (advertising) and consumer-generated marketing (online word of mouth). A unique feature of our study is that it allows marketing managers to assess a DMA's responsiveness along these two dimensions. The substantive insights can help studios and distributors evaluate their future product rollout strategies. Although our empirical analysis is conducted using motion picture industry data, our approach to addressing the endogeneity of reviews is generalizable to other industry settings where products are sequentially rolled out.
Greg M. Allenby (“Investigating the Strategic Influence of Customer and Employee Satisfaction on Firm Financial Performance”) is the Helen C. Kurtz Chair in Marketing at Ohio State University. He specializes in the study of economic and statistical issues in marketing. His research deals with developing new insights about consumer behavior from customer data routinely collected by most organizations; these insights are used to develop and improve product development, pricing, promotion, market segmentation, and target marketing activities. He is a fellow of the American Statistical Association, coauthor of Bayesian Statistics and Marketing (Wiley, 2005), and coeditor of Quantitative Marketing and Economics.Subhajyoti Bandyopadhyay (“Equilibrium Returns Policies in the Presence of Supplier Competition”) is an assistant professor in the Department of Information Systems and Operations Management at the University of Florida, Gainesville. He received his Ph.D. in management information systems from Purdue University in 2002. His current research interests include economics of information systems and information systems policy issues, especially in the area of net neutrality, national broadband policy, and health informatics.Jonah Berger (“Positive Effects of Negative Publicity: When Negative Reviews Increase Sales”) is an assistant professor of marketing at The Wharton School of the University of Pennsylvania. He received a Ph.D. in marketing from the Stanford Graduate School of Business in 2007 and a B.A. in human judgment and decision making from Stanford University in 2002. His research examines how individual decision making and social dynamics (e.g., social influence) between people generate collective outcomes such as social contagion and trends.Bart J. Bronnenberg (“An Empirical Analysis of Assortment Similarities Across U.S. Supermarkets”) is a professor of marketing and a CentER fellow at the Tilburg School of Economics and Management of Tilburg University, The Netherlands. He holds Ph.D. and M.S. degrees in management from INSEAD, Fontainebleau, France, and an M.S. in industrial engineering from Twente University, The Netherlands. He is interested marketing strategy, the persistence of branding effects, and the structure of CPG markets, and he continues to work on empirical analyses of new product growth and consumer choice behavior. He was named the recipient of the 2003 and 2008 Paul Green Award, the 2003 IJRM Best Paper Award, and the 2004 John D. C. Little Best Paper Award.Michael Brusco (“Multicriterion Market Segmentation: A New Model, Implementation, and Evaluation”) is a professor in the marketing department at Florida State University, where he teaches courses in operations research, operations management, and marketing research. He received a Ph.D. in management science from Florida State University in 1990. His research focuses on the development of exact and approximate algorithms for combinatorial optimization problems in the areas of scheduling, layout, and computational statistics.Pradeep K. Chintagunta (“The Effects of Online User Reviews on Movie Box Office Performance: Accounting for Sequential Rollout and Aggregation Across Local Markets”) is the Robert Law Professor of Marketing at the Booth School of Business, University of Chicago. He is interested in studying strategic interactions among firms in vertical and horizontal relationships, in measuring the effectiveness of marketing activities in pharmaceutical markets, in investigating aspects of technology product markets, and in the analysis of household purchase behavior in online and off-line markets.Jeffrey P. Dotson (“Investigating the Strategic Influence of Customer and Employee Satisfaction on Firm Financial Performance”) is an assistant professor of marketing in the Owen Graduate School of Management at Vanderbilt University. He holds a Ph.D. in marketing from Ohio State University and master's degrees in statistics and business administration from the University of Utah. His research focuses on the application of Bayesian statistics to a variety of marketing problems, including linking customers and employee satisfaction to firm performance and developing more accurate models of consumer decision making. His work has appeared in Quantitative Marketing and Economics.Shyam Gopinath (“The Effects of Online User Reviews on Movie Box Office Performance: Accounting for Sequential Rollout and Aggregation Across Local Markets”) is a doctoral student in marketing at the Kellogg School of Management, Northwestern University. He has master's degrees in industrial management and statistics from IIT Madras and the University of Virginia, respectively. He also has a bachelor's degree in industrial engineering from the University of Kerala. His research focuses on two areas: studying the impact of online word of mouth on firm performance and understanding customer purchasing behavior in noncontractual settings using probability models.Minha Hwang (“An Empirical Analysis of Assortment Similarities Across U.S. Supermarkets”) is a Ph.D. candidate in marketing at the UCLA Anderson School of Management. He holds a Ph.D. in materials science and engineering from the Massachusetts Institute of Technology. His research focuses on empirical analyses of assortment and store brands in U.S. supermarkets.Shailendra P. Jain (“Stock Market Response to Regulatory Reports of Deceptive Advertising: The Moderating Effect of Omission Bias and Firm Reputation”) is an associate professor of marketing at the University of Washington's Michael G. Foster School of Business and has previously held faculty positions at Indiana University's Kelley School, Cornell University's Johnson School, and University of Rochester's Simon School. He received his Ph.D. and M.Phil. from New York University's Stern School, an M.B.A. from the Indian Institute of Management, Ahmedabad, India, and a chemical engineering degree from Birla Institute of Technology and Science, Pilani, India. He conducts research in branding, comparative advertising, consumer behavior and marketing strategy, cross-cultural and other individual differences in thought and consumption, and economics of information. He is on the editorial board of Journal of Consumer Psychology and has co-chaired the 2009 Society for Consumer Psychology Conference and the 2005 American Psychological Association's Division 23 Conference. He has received several teaching excellence awards, including the PACCAR Award at University of Washington's Foster School of Business.Kamel Jedidi (“Dynamic Allocation of Pharmaceutical Detailing and Sampling for Long-Term Profitability”) is the John A. Howard Professor of Marketing at Columbia Business School, Columbia University, New York. He holds a bachelor's degree in economics from the Faculté des Sciences Economiques de Tunis, Tunisia, and master's and Ph.D. degrees in marketing from The Wharton School of the University of Pennsylvania. His substantive research interests include pricing, product design and positioning, diffusion of innovations, market segmentation, and the long-term impact of advertising and promotions. His methodological interests lie in multidimensional scaling, classification, structural equation modeling, and Bayesian and finite-mixture models. He was awarded the 1998 International Journal of Research in Marketing Best Article Award and the Marketing Science Institute Best Paper Award in 2000. He was also a finalist for the 2009 Paul Green Award for the Journal of Marketing Research and the 2009 Long-Term Impact Award for Marketing Science and Management Science.Dmitri Kuksov (“Pricing, Frills, and Customer Ratings”) is an associate professor of marketing at Olin Business School, Washington University in St. Louis, and he has a Ph.D. in marketing from the Haas School of Business, University of California, Berkeley. His research interests include studying competitive strategy, consumer and firm behavior in markets with incomplete information, consumer communication and networks, branding and product line strategy, and customer satisfaction. He is currently an associate editor at Management Science and Quantitative Marketing and Economics, and he is on the editorial board of Marketing Science. He received the 2005 Frank M. Bass Dissertation Award, and two of his papers were finalists for the 2007 John D. C. Little Award.Charles Lindsey (“Stock Market Response to Regulatory Reports of Deceptive Advertising: The Moderating Effect of Omission Bias and Firm Reputation”) is an assistant professor of marketing in the School of Management at the State University of New York, Buffalo. He received his Ph.D. and master's in business from Indiana University's Kelley School of Business. He also holds an M.B.A. and a B.S. in accounting (magna cum laude) from St. Louis University's Cook School of Business. He conducts research in branding, marketing strategy, consumer behavior, group and cross-functional team effectiveness, and information recognition and memory. He serves as an ad hoc reviewer for the Journal of Consumer Psychology, Journal of Consumer Research, and Journal of Experimental Psychology. He is a member of the conference committee for the 2010 Society for Consumer Psychology Conference. He has received several teaching/research excellence awards, including the inaugural Dean's Fellowship for Research Excellence Award at the State University of New York, Buffalo.Ying Liu (“Multicriterion Market Segmentation: A New Model, Implementation, and Evaluation”) is an assistant professor in the Department of Information Systems of the College of Business Administration at the California State University, Long Beach. He received a Ph.D. in management information systems with a minor in marketing from University of Arizona. His research interests include marketing data mining algorithms and applications.Robert F. Lusch (“Multicriterion Market Segmentation: A New Model, Implementation, and Evaluation”) is the Executive Director of the McGuire Center for Entrepreneurship and holder of the James and Pamela Muzzy Chair in Entrepreneurship of the Eller College of Management at the University of Arizona. His research interests are broad and include marketing strategy, innovation, agent-based modeling, and the service-dominant logic of marketing. He is past editor of the Journal of Marketing and chairperson of the American Marketing Association.Saurabh Mishra (“Stock Market Response to Regulatory Reports of Deceptive Advertising: The Moderating Effect of Omission Bias and Firm Reputation”) is an assistant professor of marketing at the Desautels Faculty of Management, McGill University. He received his Ph.D. from the Kelley School of Business at Indiana University, an M.A. in economics from the Delhi School of Economics, India, and a B.A. (Honors) in economics from University of Delhi, India. His research investigates the role of marketing resources, capabilities, and strategies in firm performance and innovations. He has received funding from the Marketing Science Institute and the Social Sciences and Humanities Research Council of Canada.Ricardo Montoya (“Dynamic Allocation of Pharmaceutical Detailing and Sampling for Long-Term Profitability”) is an assistant professor at the Industrial Engineering Department, University of Chile, Santiago. He received a Ph.D. in marketing from the Columbia Business School, Columbia University. He holds a bachelor's degree in engineering and a M.S. in operations management from the University of Chile. His research interests include dynamic choice models, optimizing marketing decisions, and the long-term impact of marketing actions. His methodological interests lie in Bayesian econometrics, hidden Markov models, and stochastic dynamic programming.Oded Netzer (“Dynamic Allocation of Pharmaceutical Detailing and Sampling for Long-Term Profitability”) is an associate professor of business at Columbia University. He received an M.S. in statistics and a Ph.D. in business from Stanford University and also holds a B.S. in industrial engineering and management from Technion (Israel Institute of Technology). His research interests focus on modeling customer relationships, preference measurement methods, and modeling various aspects of choice behavior, including how choices change over time, contexts, and customers. He is the recipient of the John D. C. Little and the Frank M. Bass Awards.Anand A. Paul (“Equilibrium Returns Policies in the Presence of Supplier Competition”) is an associate professor in the Department of Information Systems and Operations Management at the University of Florida, Gainesville. He completed his bachelor's degree in electrical engineering, followed by an M.B.A., and then worked for three years as a business consultant before returning to academia to complete his Ph.D. in operations management at the University of Texas at Austin. His research interests are in supply chain management, project management, and applied probability theory.Rik Pieters (“Moment-to-Moment Optimal Branding in TV Commercials: Preventing Avoidance by Pulsing”) is a professor of marketing at Tilburg University and visiting research fellow at the Robert H. Smith School of Business, University of Maryland. He developed latent cake analysis (LCA) to categorize apple pie families. In addition, he researches consumer behavior to improve return on marketing.Sudha Ram (“Multicriterion Market Segmentation: A New Model, Implementation, and Evaluation”) is the McClelland Professor of Management Information Systems in the Eller College of Management at the University of Arizona. Her main research interests are in the areas of enterprise data management and business analytics. She is currently a senior editor for Information Systems Research and serves on several other journal editorial boards.Scott J. Rasmussen (“Positive Effects of Negative Publicity: When Negative Reviews Increase Sales”) currently works as an actuary in Seattle. He received a B.A. in economics and a B.S. in mathematical and computational science from Stanford University in 2003.Alan T. Sorensen (“Positive Effects of Negative Publicity: When Negative Reviews Increase Sales”) is an associate professor of economics and strategic management at the Stanford Graduate School of Business and a research fellow of the National Bureau of Economic Research. He received a Ph.D. in economics from the Massachusetts Institute of Technology in 1999 and a B.S. from Brigham Young University in 1995. He works in the area of applied microeconomics, primarily in the field of industrial organization, with a research focus on the role of information in markets.Thales S. Teixeira (“Moment-to-Moment Optimal Branding in TV Commercials: Preventing Avoidance by Pulsing”) is an assistant professor in the Marketing Unit at Harvard Business School. He holds a Ph.D. in marketing from the University of Michigan. His research focuses on modeling consumer behavior in the domains of advertising and branding.Raphael Thomadsen (“An Empirical Analysis of Assortment Similarities Across U.S. Supermarkets”) is an assistant professor in marketing at the UCLA Anderson School of Management. He holds Ph.D. and A.M. degrees in economics from Stanford University. Before joining the UCLA Anderson faculty, he taught at the Columbia Business School. His research focuses primarily on pricing and especially how product differentiation affects firms' pricing and product line strategies. He was named a finalist of the 2007 John D. C. Little Best Paper Award.Catherine Tucker (“Growing Two-Sided Networks by Advertising the User Base: A Field Experiment”) is the Douglas Drane Career Development Professor in IT and Management and an assistant professor of marketing at the MIT Sloan School of Management. She received an undergraduate degree in politics, philosophy, and economics from Oxford University and a Ph.D. in economics from Stanford University. Her areas of research include network externalities, online advertising, customer privacy, and Internet regulation.Sriram Venkataraman (“The Effects of Online User Reviews on Movie Box Office Performance: Accounting for Sequential Rollout and Aggregation Across Local Markets”) is an assistant professor of marketing at Goizueta Business School, Emory University. His research looks at predicting consumer demand, modeling competitive interactions between firms, and understanding the ROI from social media (user reviews, blogs, etc.). Industries that his research has examined include entertainment (movies, music, and video games), pharmaceutical marketing, automobiles, and U.S. lodging.Michel Wedel (“Moment-to-Moment Optimal Branding in TV Commercials: Preventing Avoidance by Pulsing”) is the PepsiCo Professor of Consumer Science at the Smith School of Business, University of Maryland, and honorary professor of marketing at the University of Groningen, The Netherlands. His main research interests are in the application of statistical and econometric methods to marketing problems, in particular to the analysis of eye-tracking data. He is area editor for Journal of Marketing Research and Marketing Science, and he serves on the editorial boards of Journal of Marketing, Journal of Classification, and Quantitative Marketing and Economics. He has received the O'Dell Best Paper Award from the Journal of Marketing Research and the Hendrik Muller Award from The Netherlands Royal Academy of the Sciences for outstanding lifetime contributions to the social sciences. Ranked the most productive academic in business and economics in The Netherlands, he has published several monographs on market segmentation and visual marketing.Michael A. Wiles (“Stock Market Response to Regulatory Reports of Deceptive Advertising: The Moderating Effect of Omission Bias and Firm Reputation”) is an assistant professor of marketing at the W. P. Carey School of Business at Arizona State University. He received his Ph.D. from the Kelley School of Business at Indiana University and his B.A. from Dartmouth College. His research investigates how financial markets evaluate firm marketing actions and resource deployments and issues pertaining to new product development in the consumer packaged goods industry.Ying Xie (“Pricing, Frills, and Customer Ratings”) is an assistant professor of marketing at Olin Business School, Washington University in St. Louis. She received her doctorate degree in marketing from the Kellogg School of Management, Northwestern University. Her research interests include pharmaceutical marketing, word of mouth, customer ratings and social contagion, search advertising, multichannel shopping, and consumer decision making in stock investment.Juanjuan Zhang (“Growing Two-Sided Networks by Advertising the User Base: A Field Experiment”) is an assistant professor of marketing at the MIT Sloan School of Management. She holds a Ph.D. from the University of California, Berkeley, and a B.Econ. from Tsinghua University, China. Her areas of research include observational learning, social influence, inference, and firm interference.
This paper presents five empirical tests of the popular modeling abstraction that assumes bids from online auctions with proxy bidding can be analyzed “as if” they were bids from a second-price sealed-bid auction. The tests rely on observations of the magnitudes and timings of the top two proxy bids, with the different tests stemming from different regularity assumptions about the underlying distribution of valuation signals. We apply the tests to data from three eBay markets—MP3 players, DVDs, and used cars—and we reject the sealed-bid abstraction in all three data sets. A closer examination of these rejections suggests that they are driven by less experienced bidders. This consistent rejection casts doubt on several existing theories of online auction behavior and suggests some demand estimates based on the abstraction can be biased. To assess the direction and magnitude of this bias, we propose and estimate a new model in which some bidders conform to the abstraction while other bidders bid in a reactive fashion. Because reactive bidding can be at least partially detected from the data, we are able to estimate the underlying distribution of demand and compare it to what the sealed-bid abstraction implies. We find that our proposed model fits the data better, and our demand estimates reveal a large potential downward bias were we to assume the second-price sealed-bid model instead.
This study explores the implications of rejecting the sealed-bid abstraction proposed by Zeithammer and Adams [Zeithammer, R., C. Adams. 2010. The sealed-bid abstraction in online auctions. Marketing Sci.29(6) 964–987]. Using a conditional order statistic model that relies on the joint distribution of the top two proxy bids of an auction, Zeithammer and Adams show that inexperienced bidders' reactive bidding is the main cause of the rejection of the sealed-bid abstraction. Their empirical study suggests that a large percentage of bidders reactively bid, and there is weak evolutionary pressure for bidders to converge to sealed bidding. We discuss theoretical implications of this rejection and the role of bidder experience, as well as inferences about bidder learning. Tracking an inexperienced bidder's bidding behavior over time, we show that bidders learn and their bidding strategy gravitates toward rational bidding. Potential biases in bidder experience measurement and bidder learning can be assessed using a cross-sectional, time-series data set that tracks a random sample of new eBay bidders. Learning speed is faster with their complete bidding history rather than feedback ratings or winning observations only. We highlight the importance of proper measures of bidder experience and its effect on bidding strategy evolutions, both of which play important roles in clarifying bidding behavior in online auctions.
We argue that the Zeithammer and Adams paper [Zeithammer, R., C. Adams. 2010. The sealed-bid abstraction in online auctions. Marketing Sci.29(6) 964–987] successfully documents consistent patterns in eBay bidding data that cast doubt on the common assumption that bidders in such auctions follow a “bid = value” strategy. These anomalies lend support to the authors' alternative model in which some bidders bid reactively and consequently bid below their valuation most of the time. The consistency of the authors' findings as well as the ability of their alternative explanation to account for all of their test results lends great support to their thesis. However, we think that several of their empirical tests examine ancillary assumptions about bidder behavior and do not test the bid = value assumption directly. Furthermore, although their reduced-form model incorporating “reactive” bidders is a good first attempt at expanding the canonical framework, we worry that their counterfactual pricing analysis using the reactive model is suspect because the parameters they estimate are not structural. Overall, the Zeithammer and Adams paper is a carefully argued critique of empirical methods used to study online auctions and provides valuable ideas to improve on these methods.
This paper presents the authors' rejoinder to Zeithammer and Adams [Zeithammer, R., C. Adams. 2010. The sealed-bid abstraction in online auctions. Marketing Sci.29(6) 964–987]. This rejoinder clarifies and qualifies conclusions of the original paper and makes suggestions for fruitful areas of future research. In particular, the original paper shows that bidding style can make a big difference in managerial decisions, but a structural model would be necessary to make confident predictions under different reserve prices. The rejoinder also clarifies the interpretation of feedback as a measure of bidder experience, and the relationship between bidder experience and bidding style.
Using aggregate product search data from Amazon.com, we jointly estimate consumer information search and online demand for consumer durable goods. To estimate the demand and search primitives, we introduce an optimal sequential search process into a model of choice and treat the observed market-level product search data as aggregations of individual-level optimal search sequences. The model builds on the dynamic programming framework by Weitzman [Weitzman, M. L. 1979. Optimal search for the best alternative. Econometrica47(3) 641–654] and combines it with a choice model. It can accommodate highly complex demand patterns at the market level. At the individual level, the model has a number of attractive properties in estimation, including closed-form expressions for the probability distribution of alternative sets of searched goods and breaking the curse of dimensionality. Using numerical experiments, we verify the model's ability to identify the heterogeneous consumer tastes and search costs from product search data. Empirically, the model is applied to the online market for camcorders and is used to answer manufacturer questions about market structure and competition and to address policy-maker issues about the effect of selectively lowered search costs on consumer surplus outcomes. We demonstrate that the demand estimates from our search model predict the actual product sales ranks. We find that consumer search for camcorders at Amazon.com is typically limited to 10–15 choice options and that this affects estimates of own and cross elasticities. In a policy simulation, we also find that the vast majority of the households benefit from Amazon.com's product recommendations via lower search costs.
To evaluate the success of a new product, managers need to determine how much of its new demand is due to cannibalizing the firm's other products, rather than drawing from competition or generating primary demand. We introduce a time-varying vector error-correction model to decompose the base sales of a new product into its constituent sources. The model allows managers to estimate cannibalization effects and calculate the new product's net demand, which may be considerably less than its total demand. We apply our methodology to the introduction of the Lexus RX300 using detailed car transaction data. This case is especially interesting because the Lexus RX300 was the first crossover sport utility vehicle (SUV), implying that its demand could come from both the luxury SUV and the luxury sedan categories. Because Lexus was active in both categories, there was a double cannibalization potential. We show how the contribution of the different demand sources varies over time and discuss the managerial implications for both the focal brand and its competitors.
Although advance selling and probabilistic selling differ in both motivation and implementation, we argue that they share a common characteristic—both offer consumers a choice involving buyer uncertainty. We develop a formal model to examine the general economics of purchase options that involve buyer uncertainty, explore the differences in buyer uncertainty created via these two strategies, and derive conditions under which one dominates the other. We show that the seller can address unobservable buyer heterogeneity by inducing sales involving buyer uncertainty via two different mechanisms: (1) homogenizing heterogeneous consumers and (2) separating heterogeneous consumers. Offering advance sales encourages customers to purchase while they are uncertain about their consumption states (more homogeneous), but offering probabilistic goods encourages customers to reveal their heterogeneity via self-selecting whether or not to purchase the uncertain product. The relative attractiveness of these two selling strategies depends on the degree of two types of buyer heterogeneity: (1) Max_Value-Heterogeneity, which is the variation in consumers' valuations for their preferred good, and (2) Strength-Heterogeneity, which is the variation in the strength of consumers' preferences. Neither strategy is advantageous unless the market exhibits sufficient Max_Value-Heterogeneity. However, whereas Strength-Heterogeneity can destroy the profit advantage of advance selling, a mid-range of Strength-Heterogeneity is necessary for probabilistic selling to be advantageous.
Reverse pricing is a market mechanism under which a consumer's bid for a product leads to a sale if the bid exceeds a hidden acceptance threshold the seller has set in advance. The seller faces two key decisions in designing such a mechanism. First, he must decide where in the process to collect the revenue—that is, whether to commit to a minimum markup above cost (and thus define the bid-acceptance threshold given cost) and whether to set a fee for the consumer's right to bid. Second, the seller must decide whether to facilitate or hinder consumer learning about the current bid-acceptance threshold. We analyze these decisions for a profit-maximizing small intermediary retailer selling to consumers who can also purchase the product in an outside posted-price market. The optimal revenue model is to charge a fee for the right to bid and then accept all bids above cost, rather than to set a positive minimum markup above cost. Avoiding minimum markups in favor of a bidding fee is more profitable because of increased efficiency arising from more entry by consumers and higher bids by the entrants. When consumers learn about the bid-acceptance threshold before they enter the market, efficiency increases further, and generating revenue through a bidding fee can compensate the seller for his loss of information rent when the competition from the outside posted-price firm is relatively weak.
Consumers often return a product to a retailer because they learn after purchase that the product does not match as well with preferences as had been expected. This is a costly issue for retailers and manufacturers—in fact, it is estimated that the U.S. electronics industry alone spent $13.8 billion dollars in 2007 to restock returned products [Lawton, C. 2008. The war on returns. Wall Street Journal (May 8) D1]. The bulk of these returns were nondefective items that simply were not what the consumer wanted. To eliminate returns and to recoup the cost of handling returns, many retailers are adopting the practice of charging restocking fees to consumers as a penalty for making returns. This paper employs an analytical model of a bilateral monopoly to examine the impact of reverse channel structure on the equilibrium return policy and profit. More specifically, we examine how the return penalty is affected by whether returns are salvaged by the manufacturer or by the retailer. Interestingly, we find that the return penalty may be more severe when returns are salvaged by a channel member who derives greater value from a returned unit. Also, the manufacturer may earn greater profit by accepting returns even if the retailer has a more efficient outlet for salvaging units.
Many businesses track repeat transactions on a discrete-time basis. These include (1) companies for whom transactions can only occur at fixed regular intervals, (2) firms that frequently associate transactions with specific events (e.g., a charity that records whether supporters respond to a particular appeal), and (3) organizations that choose to utilize discrete reporting periods even though the transactions can occur at any time. Furthermore, many of these businesses operate in a noncontractual setting, so they have a difficult time differentiating between those customers who have ended their relationship with the firm versus those who are in the midst of a long hiatus between transactions. We develop a model to predict future purchasing patterns for a customer base that can be described by these structural characteristics. Our beta-geometric/beta-Bernoulli (BG/BB) model captures both of the underlying behavioral processes (i.e., customers' purchasing while “alive” and time until each customer permanently “dies”). The model is easy to implement in a standard spreadsheet environment and yields relatively simple closed-form expressions for the expected number of future transactions conditional on past observed behavior (and other quantities of managerial interest). We apply this discrete-time analog of the well-known Pareto/NBD model to a data set on donations made by the supporters of a nonprofit organization located in the midwestern United States. Our analysis demonstrates the excellent ability of the BG/BB model to describe and predict the future behavior of a customer base.
In this research, we examine a novel mechanism of interorganizational relationship dissolution: incoherence in a partner's behavior. We propose that the discrepancy between an exchange partner's opportunistic behavior and the focal firm's expectations may create a state of incoherence and uncertainty and that this effect can be damaging to the exchange even when the partner's behavior is better than expected. Using nearly 500 longitudinal, confidential reports of industrial buyers and sellers, we find supportive evidence that (1) the net effect of the discrepancy is initially positive when behavior is better than expected but becomes rapidly negative thereafter, and (2) the net effect of the discrepancy is always negative when behavior is worse than expected. Thus, these effects will generally damage the exchange even as the partner tries to improve the relationship. This gives insight into why exchange relationships that hit a downward spiral can be difficult, if not impossible, to salvage. We also show that the dysfunctional consequences of discrepancy are mitigated through exchange structures such as the magnitude of dependence on an organizational partner, the development phase of the relationship, and the presence of bilateral idiosyncratic investments. Implications for theory and the management of interorganizational relationships are developed.
This paper looks into the effects of information transparency on market participants in an online trading environment. We study these effects in business-to-business electronic markets with firms competing in both upstream and downstream industries. The prior literature generally assumes that either the downstream firm (buyer) or the upstream firm (seller) is a monopoly. It is not clear whether information transparency would still create value if both buyers and sellers face oligopolistic competition, where the benefits of information transparency could be competed away. To answer this question, we first develop a simple two-echelon e-market model and then extend the model to more general settings. We find that information transparency can create value for the overall e-market, yet it affects buyers and sellers very differently: one side will be hurt, depending on the competition mode (Cournot or Bertrand) in the downstream. This suggests that a manufacturer-owned, a supplier-owned, and a neutral e-market will have different preferences for information transparency. Finally, we find that information transparency can hurt consumers when the downstream industry engages in Bertrand competition. This is a surprising result given the expectation that online markets create substantial value for consumers.
The availability of digital channels for media distribution has raised many important questions for marketers, notably, whether digital distribution channels will cannibalize physical sales and whether legitimate digital distribution channels will dissuade consumers from using (illegitimate) digital piracy channels. We address these two questions using the removal of NBC content from Apple's iTunes store in December 2007, and its restoration in September 2008, as natural shocks to the supply of legitimate digital content, and we analyze the impact of this shock on demand through BitTorrent piracy channels and the Amazon.com DVD store.To do this we collected two large data sets from Mininova.com and Amazon.com, documenting levels of piracy and DVD sales for both NBC and other major networks' content around these events. We analyze these data in a difference-in-difference model and find that NBC's decision to remove its content from iTunes in December 2007 is causally associated with an 11.4% increase in the demand for NBC's pirated content. This is roughly equivalent to an increase of 48,000 downloads a day for NBC's content and is approximately twice as large as the total legal purchases on iTunes for the same content in the period preceding the removal. We also find evidence of a smaller, and statistically insignificant, decrease in piracy for the same content when it was restored to the iTunes store in September 2008. Finally, we see no change in demand for NBC's DVD content at Amazon.com associated with NBC's closing or reopening of its digital distribution channel on iTunes.
Price dispersion in simultaneous online auctions is a puzzle in light of the relatively low search costs required to find the lower price. Much of this price dispersion appears to be due to a lack of switching by bidders between auctions, which in turn could be due to inertia related to search costs. We identify some of the influencing factors through a controlled field experiment involving pairs of simultaneous auctions. Keeping the sellers and the goods sold identical between two auctions, we vary auction design features between and within pairs including shipping cost, open reserve, secret reserve price, and duration, and we provide bidders with incentives to search. We use a choice model that examines individual choice between pairs of simultaneous auctions. We find that within-pair price dispersion is substantial and that prices and auction choice by bidders are indeed related to search costs. We find strong inertia in auction choice and find that this effect significantly interacts with time left in the auction. Although individuals do not always choose a lower-priced auction, they are more likely to do so when search costs are low or search incentives are high.
In the original version of the paper “New Perspectives on Customer ‘Death’ Using a Generalization of the Pareto/NBD Model” by Kinshuk Jerath, Peter S. Fader, and Bruce G. S. Hardie (Marketing Science, Articles in Advance, May 27, 2010, DOI: 10.1287/mksc.1100.0568), some issues were brought to the attention of the journal by the authors after online publication in Articles in Advance. The accepting editor-in-chief, Steven M. Shugan, has chosen to retract the original published paper, allowing the authors the opportunity to resubmit a new paper that fully resolves those issues to the satisfaction of the authors and the journal.
Christopher Adams (“The Sealed-Bid Abstraction in Online Auctions”; “Rejoinder—Causes and Implications to Some Bidders Not Conforming to the Sealed-Bid Abstraction”) is a staff economist with the Federal Trade Commission (FTC). He has a Ph.D. in economics from the University of Wisconsin and a B.Comm (Hons) from the University of Melbourne. At the FTC, he has worked on mergers and antitrust cases in a number of industries including pharmaceuticals, real estate, software, and retail. Before joining the FTC, he taught at the University of Vermont. In 2007, he was awarded the FTC's Paul Rand Dixon Award for outstanding contributions to the Commission.Paulo Albuquerque (“Online Demand Under Limited Consumer Search”) is an assistant professor of marketing at the Simon Graduate School of Business, University of Rochester. He holds a Ph.D. in management from the UCLA Anderson School of Management. He is currently interested in competition and consumer behavior in online markets, new product diffusion across markets, and spatial competition models.Bart J. Bronnenberg (“Online Demand Under Limited Consumer Search”) is a professor of marketing and CentER research fellow at Tilburg University. He holds Ph.D. and M.Sc. degrees in management from INSEAD, Fontainebleau, France, and an M.S. in industrial engineering from Twente University, The Netherlands. He is currently interested in marketing strategy and multimarket competition in consumer goods and medical industries. He has previously worked, and continues to work, on empirical analyses of new product growth and consumer choice behavior. He was named the recipient of the 2003 and 2008 Paul Green Award, the 2003 International Journal of Research in Marketing (IJRM) Best Paper Award, and the 2004 John D. C. Little Best Paper Award.Anne T. Coughlan (“Optimal Reverse Channel Structure for Consumer Product Returns”) is the John L. and Helen Kellogg Professor of Marketing at the Kellogg School of Management at Northwestern University. Her research on channel design and compensation problems, sales force management, sales force compensation, and pricing has been published in the top journals for marketing and operation and decision technologies. She is an area editor for Marketing Science and an author of the Marketing Channels textbook. Her favorite leisure activity is cultivating cacti and succulents in her greenhouse.Brett Danaher (“Converting Pirates Without Cannibalizing Purchasers: The Impact of Digital Distribution on Physical Sales and Internet Piracy”) is an assistant professor of economics at Wellesley College. He received a bachelor's of science in economics from Haverford College and a Ph.D. in managerial science and applied economics from The Wharton School of the University of Pennsylvania. His research interests include digital media, intellectual property, and the economics of information goods.Marnik G. Dekimpe (“Estimating Cannibalization Rates for Pioneering Innovations”) is a research professor at Tilburg University (The Netherlands) and a professor of marketing at the Catholic University Leuven (Belgium), and he is currently an academic trustee with both MSI and AiMark. He received his Ph.D. from the University of California, Los Angeles. He has advised several key players in the consumer packaged goods industry, especially on private-label and marketing-mix effectiveness issues. He has won best paper awards at Marketing Science, the Journal of Marketing Research, the International Journal of Research in Marketing, and Technological Forecasting and Social Change. He has also won the 2010 Louis W. Stern Award for his work on the valuation of Internet channels. He serves as editor for the International Journal of Research in Marketing and serves on the editorial boards of Marketing Science, the Journal of Marketing, the Journal of Marketing Research, Marketing Letters, the Review of Marketing Science, and the Journal of Interactive Marketing.Samita Dhanasobhon (“Converting Pirates Without Cannibalizing Purchasers: The Impact of Digital Distribution on Physical Sales and Internet Piracy”) is a Ph.D. candidate in public policy and management at the Heinz College, Carnegie Mellon University. Her research interests include digital piracy, digital media, and e-commerce marketing.Peter S. Fader (“Customer-Base Analysis in a Discrete-Time Noncontractual Setting”) is the Frances and Pei-Yuan Chia Professor of Marketing at The Wharton School of the University of Pennsylvania, and codirector of the Wharton Interactive Media Initiative.Scott Fay (“The Economics of Buyer Uncertainty: Advance Selling vs. Probabilistic Selling”) is an assistant professor of marketing at the Whitman School of Management at Syracuse University. He received a Ph.D. in economics from the University of Michigan and previously taught at the Warrington College of Business of the University of Florida. In his research, he employs analytical modeling to study a variety of topics, many of which are related to e-commerce, including reverse auctions, opaque products, the personalization process, the bundling of information goods, shipping fee schedules, retail price endings, and consumer bankruptcy. He was elected to and served two terms as the newsletter editor for the INFORMS Society for Marketing Science (2002–2006). He serves on the editorial board of Marketing Science, served as a guest area editor for Marketing Science, and recently received the Meritorious Service Award from Management Science.Bruce G. S. Hardie (“Customer-Base Analysis in a Discrete-Time Noncontractual Setting”) is a professor of marketing at the London Business School. His primary research interest lies in the development of data-based models to support marketing analysts and decision makers, with a particular interest in models that are easy to implement. Most of his current projects focus on the development of probability models for customer-base analysis.Ernan Haruvy (“Search and Choice in Online Consumer Auctions”) is an associate professor of marketing at the University of Texas at Dallas. He received his Ph.D. in economics in 1999 from the University of Texas at Austin. His research focuses primarily on market design, with a special interest in auctions, procurement, learning, and bounded rationality.Gerald Häubl (“Optimal Reverse-Pricing Mechanisms”) is the Canada Research Chair in Behavioral Science and an associate professor of marketing at the University of Alberta's School of Business. He is the founding director of the Institute for Online Consumer Studies (IOCS). He received M.S. and Ph.D. degrees in business administration and marketing from the Vienna University of Economics and Business Administration (Wirtschaftsuniversität Wien) in his native Austria. His primary research interests are consumer decision making, the construction of preference and value, human–information interaction, decision assistance for consumers, and bidding behavior in interactive-pricing markets.Ali Hortaçsu (“Commentary—Do Bids Equal Values on eBay?”) is a professor of economics at the University of Chicago. He received his Ph.D. from Stanford University in 2001, and his main research area is industrial organization. He has developed novel econometric methods to study auction and matchmaking markets, and he has applied these methods to answer market design questions in central bank operations, government bond auctions, electricity markets, online auctions, and online matchmaking. He has also developed empirical methods to study markets with search frictions, with applications to e-commerce and the mutual fund industry. He has been awarded an Alfred P. Sloan Fellowship and an NSF CAREER grant and is a research associate of the National Bureau of Economic Research. He has served as the coeditor for the International Journal of Industrial Organization and as associate editor for the Journal of Business and Economic Statistics and the Journal of Industrial Economics.Sandy Jap (“The Seeds of Dissolution: Discrepancy and Incoherence in Buyer–Supplier Exchange”) is the Dean's Term Chair Professor of Marketing at the Goizueta Business School at Emory University. She is a graduate of the University of Florida (Go Gators!) and has served on the faculties of the Sloan School of Management at the Massachusetts Institute of Technology and The Wharton School of the University of Pennsylvania. Her research interests lie in interorganizational exchange management and the design and management of business-to-business markets with auction mechanisms. She is an area editor for the International Journal of Research in Marketing and an editorial board member of the Journal of Marketing Research and Marketing Letters.Ujwal Kayande (“The Seeds of Dissolution: Discrepancy and Incoherence in Buyer–Supplier Exchange”) is a professor of marketing in the Research School of Business at the Australian National University. He was previously on the faculty at the Smeal College of Business (Pennsylvania State University) and the Australian Graduate School of Management (University of New South Wales, Sydney). He obtained his Ph.D. from the University of Alberta. His current research focuses on developing quantitative models to understand marketplace behavior and the effect of marketing activity upon that behavior; additionally, he is interested in understanding the pathways by which quantitative models impact business practice. He is a recipient of the 1998 Don Lehmann Award from the American Marketing Association.Jun B. Kim (“Online Demand Under Limited Consumer Search”) is an assistant professor at the College of Management, Georgia Institute of Technology. He holds a Ph.D. in management from the UCLA Anderson School of Management and a Ph.D. in mechanical engineering from the Massachusetts Institute of Technology. His current research interests include information economics, choice models, and durable goods markets.Eric R. Nielsen (“Commentary—Do Bids Equal Values on eBay?”) is a University of Chicago economics graduate student specializing in labor economics, industrial organization, and econometrics. He received his A.B. from Harvard University in 2007. His current research focuses on matching markets and educational investment decisions.Peter T. L. Popkowski Leszczyc (“Search and Choice in Online Consumer Auctions”) is an associate professor of marketing at the School of Business, University of Alberta, and director of CampusAuctionMarket.com. He received is Ph.D. in marketing in 1992 from the University of Texas at Dallas. His research focuses on empirical and theoretical issues related to (Internet) auctions, influence of information on price formation, and charitable giving.R. Canan Savaskan (“Optimal Reverse Channel Structure for Consumer Product Returns”) is currently an associate professor at the Cox School of Business at Southern Methodist University. She received her Ph.D. in operations management from INSEAD, France. Her research is at the interface of operations and marketing, with a special focus on product returns management and reverse logistics.Jen Shang (“Customer-Base Analysis in a Discrete-Time Noncontractual Setting”) is an assistant professor at the School of Public and Environmental Affairs at Indiana University, Bloomington. She is a philanthropic psychologist who studies the psychological determinants for giving. She is the author of Fundraising: Principles and Practice.Jeffrey D. Shulman (“Optimal Reverse Channel Structure for Consumer Product Returns”) is an assistant professor of marketing at the Michael G. Foster School of Business at the University of Washington. His research focusing on strategic pricing issues has also appeared in Quantitative Marketing and Economics, Manufacturing and Service Operations Management, and the second edition of Kellogg on Marketing. He met his amazing wife Stephanie, mother of his beautiful daughter Olivia, while getting his Ph.D. in marketing at the Kellogg School of Management.Michael D. Smith (“Converting Pirates Without Cannibalizing Purchasers: The Impact of Digital Distribution on Physical Sales and Internet Piracy”) is an associate professor of information systems and marketing and the codirector of the Center for Digital Media Research at Carnegie Mellon University. He holds academic appointments at the School of Information Systems and Management and the Tepper School of Business. He received a bachelor's of science in electrical engineering (summa cum laude) and a master's of science in telecommunications science from the University of Maryland, and a Ph.D. in management science from the Sloan School of Management at MIT.Martin Spann (“Optimal Reverse-Pricing Mechanisms”) is a professor of electronic commerce at the School of Management of the Ludwig-Maximilians-University (LMU) in Munich, Germany. He received his Ph.D. from Goethe University in Frankfurt and was a professor of marketing and innovation at the University of Passau, Germany. He has visited the University of California at Los Angeles, the University of Southern California, and Bocconi University in Milan, Italy. His current research interests are electronic commerce, pricing, auctions, innovation management, prediction markets, and social network analysis.Kannan Srinivasan (“Commentary—Bidders' Experience and Learning in Online Auctions: Issues and Implications”) is the Rohet Tolani Distinguished Professor of International Business and the H. J. Heinz II Professor of Management, Marketing and Information Systems at the Tepper School of Business, Carnegie Mellon University. He is an associate editor for Management Science and Quantitative Marketing and Economics, and an area editor for Marketing Science. He has chaired 15 doctoral dissertations, and his students serve as faculty in various leading universities around the world. He is the President Elect of the INFORMS Society of Marketing Science.Shuba Srinivasan (“Estimating Cannibalization Rates for Pioneering Innovations”) is an associate professor of marketing and Dean's Research Fellow at Boston University's School of Management. Her research focuses on strategic marketing problems—in particular, linking marketing to financial performance, to which she applies her expertise in time-series analysis and econometrics. Her current research focuses on metrics for gauging marketing performance, and she has consulting experience with a wide spectrum of companies. Her research won the 2001 European Marketing Academy (EMAC) Best Paper Award. She serves on editorial boards of the Journal of Marketing Research and the International Journal of Research in Marketing, among others.Rahul Telang (“Converting Pirates Without Cannibalizing Purchasers: The Impact of Digital Distribution on Physical Sales and Internet Piracy”) is an associate professor of information systems and management and the codirector of the Center for Digital Media Research at the School of Information Systems and Management at the Heinz College, Carnegie Mellon University. He received his Ph.D. in information systems from the Tepper School of Business, Carnegie Mellon University.Harald J. van Heerde (“Estimating Cannibalization Rates for Pioneering Innovations”) is a professor of marketing at the University of Waikato, Hamilton, New Zealand. He holds a Ph.D. (cum laude) from the University of Groningen, The Netherlands. His research focuses on assessing the effectiveness of the marketing mix using econometric models and covers various substantive domains such as sales promotions and advertising, pricing and price wars, and loyalty programs. His work has been awarded with the Paul E. Green and William F. O'Dell Awards (Journal of Marketing Research (JMR)), and with the International Journal of Research in Marketing (IJRM) Best Paper Award. He serves on the editorial board of JMR and is an area editor for IJRM.Qiong Wang (“The Seeds of Dissolution: Discrepancy and Incoherence in Buyer–Supplier Exchange”) is an assistant professor of marketing at the Smeal College of Business, Pennsylvania State University. She joined Smeal in the fall of 2006 after receiving her Ph.D. from the University of Florida in August 2006. Her primary research focuses on interorganizational exchange behavior, relationship development, and governance mechanisms.Xin Wang (“Commentary—Bidders' Experience and Learning in Online Auctions: Issues and Implications”) is an assistant professor of marketing at Brandeis International Business School, Brandeis University. She received her doctoral degree from the Tepper School of Business, Carnegie Mellon University. Her research interests include online auctions, service quality, and consumer learning. She also taught at the Krannert School of Management, Purdue University.Jinhong Xie (“The Economics of Buyer Uncertainty: Advance Selling vs. Probabilistic Selling”) is the Etheridge Professor of International Business and a professor of marketing at the Warrington College of Business Administration, University of Florida. She holds a Ph.D. in engineering and public policy from Carnegie Mellon University, an M.S. in optimal control from the Second Academy of the Ministry of Astronautics (China), and a B.S. in electrical engineering from Tsinghua University. Her research interests include pricing, network effects and standards competition, consumer social interactions, innovation strategies, and cross-culture effects. She is a recipient of INFORMS' John D. C. Little Best Paper Award, the Marketing Science Institute's Research Competition Award, the Product Development and Management Association's Research Competition Award, and the University of Florida's Best Teaching Award. She has served as an associate editor for Management Science and an area editor for Marketing Science.Robert Zeithammer (“The Sealed-Bid Abstraction in Online Auctions”; “Rejoinder—Causes and Implications of Some Bidders Not Conforming to the Sealed-Bid Abstraction”; “Optimal Reverse-Pricing Mechanisms”) is an assistant professor of marketing at the UCLA Anderson School of Management. His research focuses on auction-driven marketplaces, such as eBay, in which the burden of pricing is on the interplay between buyers' bidding strategies and the seller's selling strategies. He is working on models that capture the essence of such interplay and help us understand the nature of these emerging marketplaces. In addition, he is interested in choice-based conjoint analysis and the analysis of choices from subsets of brands.Kevin Xiaoguo Zhu (“The Effects of Information Transparency on Suppliers, Manufacturers, and Consumers in Online Markets”) received his Ph.D. from Stanford University and is currently on the faculty of the Rady School of Management, University of California, San Diego. His research focuses on technology-enabled innovations, electronic markets, economic impacts of IT on firms/industries, and IT-enabled supply chains. His work has been published in top academic journals, as well as in a book, Global E-Commerce (Cambridge University Press, 2006). His research has been recognized by several best paper awards in the field and the prestigious CAREER Award from the U.S. National Science Foundation.Zach Zhizhong Zhou (“The Effects of Information Transparency on Suppliers, Manufacturers, and Consumers in Online Markets”) is currently a postdoctoral research fellow at the Rady School of Management, University of California, San Diego. He received his Ph.D. from the University of California, Irvine. His research focuses on electronic markets, competitive marketing strategies of software vendors, and economics of IT security.
In this research we introduce a new class of multivariate probability models to the marketing literature. Known as “copula models,” they have a number of attractive features. First, they permit the combination of any univariate marginal distributions that need not come from the same distributional family. Second, a particular class of copula models, called “elliptical copula,” has the property that they increase in complexity at a much slower rate than existing multivariate probability models as the number of dimensions increase. Third, they are very general, encompassing a number of existing multivariate models and providing a framework for generating many more. These advantages give copula models a greater potential for use in empirical analysis than existing probability models used in marketing. We exploit and extend recent developments in Bayesian estimation to propose an approach that allows reliable estimation of elliptical copula models in high dimensions. Rather than focusing on a single marketing problem, we demonstrate the versatility and accuracy of copula models with four examples to show the flexibility of the method. In every case, the copula model either handles a situation that could not be modeled previously or gives improved accuracy compared with prior models.
The likelihood for copula modeling appears when both the data and the copula representations are seen as being driven by common uniform latent variables. This perspective facilitates Bayesian inference for prediction and copula selection.
Estimating copula models using Bayesian methods presents some subtle challenges, ranging from specification of the prior to computational tractability. There is also some debate about what is the most appropriate copula to employ from those available. We address these issues here and conclude by discussing further applications of copula models in marketing.
By analyzing various alternative mixed channel structures composed of a monopoly manufacturer and online and offline outlets, we investigate how the specific channel structure and varying market conditions moderate the impact of Internet channel entry on the channel members and consumers. As an extension of Balasubramanian's model [Balasubramanian, S. 1998. Mail versus mall: A strategic analysis of competition between direct marketers and conventional retailers. Marketing Sci.17(3) 181–195], our game-theoretic model captures the fundamental difference between two different channel types and consumer heterogeneity in preference for the Internet channel use. The equilibrium solutions indicate that Internet channel entry does not always lead to lower retail prices and enhanced consumer welfare. We also find that an independent retailer might become worse off after adding its own Internet outlet under certain market conditions. We find that the impact of the Internet channel introduction substantially varies across channel structures and market environments. We explain these varied results by proposing a framework of five key strategic forces that shape the overall impact of the Internet channel introduction.
The Internet has increased the flexibility of retailers, allowing them to operate an online arm in addition to their physical stores. The online channel offers potential benefits in selling to customer segments that value the convenience of online shopping, but it also raises new challenges. These include the higher likelihood of costly product returns when customers' ability to “touch and feel” products is important in determining fit. We study competing retailers that can operate dual channels (“bricks and clicks”) and examine how pricing strategies and physical store assistance levels change as a result of the additional Internet outlet. A central result we obtain is that when differentiation among competing retailers is not too high, having an online channel can actually increase investment in store assistance levels (e.g., greater shelf display, more-qualified sales staff, floor samples) and decrease profits. Consequently, when the decision to open an Internet channel is endogenized, there can exist an asymmetric equilibrium where only one retailer elects to operate an online arm but earns lower profits than its bricks-only rival. We also characterize equilibria where firms open an online channel, even though consumers only use it for research and learning purposes but buy in stores. A number of extensions are discussed, including retail settings where firms carry multiple product categories, shipping and handling costs, and the role of store assistance in impacting consumer perceived benefits.
Brand preferences and marketplace demand are a reflection of the importance of underlying needs of consumers and the efficacy of product attributes for delivering value. Dog owners, for example, may look to dog foods to provide specific benefits for their pets (e.g., shiny coats) that may not be available from current offerings. An analysis of consumer wants for these consumers would reveal weak demand for product attributes resulting from low efficacy, despite the presence of strong latent interest. The challenge in identifying such unmet demand is in distinguishing it from other reasons for weak preference, such as general noninterest in the category and heterogeneous tastes. We propose a model for separating out these effects within the context of conjoint analysis, and we demonstrate its value with data from a national survey of toothpaste preferences. Implications for product development and reformulation are explored.
The nature of the effect of media advertising on brand choice is investigated in two product categories in analyses that combine household scanner panel data with media exposure information. Alternative model specifications are tested in which advertising is assumed to directly affect brand utility, model error variance, and brand consideration. We find strong support for advertising effects on choice through an indirect route of consideration set formation that does not directly affect brand utility. Implications for media buying and advertising effects are explored.
Are brands the “new religion”? Practitioners and scholars have been intrigued by the possibility, but strong theory and empirical evidence supporting the existence of a relationship between brands and religion is scarce. In what follows, we argue and demonstrate that religiosity is indeed related to “brand reliance,” i.e., the degree to which consumers prefer branded goods over unbranded goods or goods without a well-known national brand.We theorize that brands and religiosity may serve as substitutes for one another because both allow individuals to express their feelings of self-worth. We provide support for this substitution hypothesis with U.S. state-level data (field study) as well as individual-level data where religiosity is experimentally primed (study 1) or measured as a chronic individual difference (study 2). Importantly, studies 1 and 2 demonstrate that the relationship between religiosity and brand reliance only exists in product categories in which brands enable consumers to express themselves (e.g., clothes). Moreover, studies 3 and 4 demonstrate that the expression of self-worth is an important factor underlying the negative relationship.
The use of a durable good is limited by both its physical life and usable life. For example, an electric-car battery can last for five years (physical life) or 100,000 miles (usable life), whichever comes first. We propose a framework for examining how a profit-maximizing firm might choose the usable life, physical life, and selling price of a durable good. The proposed framework considers differences in usage rates and product valuations by consumers and allows for the effects of technological constraints and product obsolescence on a product's usable and physical lives. Our main result characterizes a relationship between optimal price, cost elasticities, and opportunity costs associated with relaxing upper bounds on usable and physical lives. We describe conditions under which either usable life or physical life, or both, obtains its maximum possible values; examine why a firm might devote effort to relaxing nonbinding constraints on usable life or physical life; consider when price cuts might be accompanied with product improvements; and examine how a firm might be able to cross-subsidize product improvements.
The idea of hierarchical, sequential, or intermediate effects has long been posited in textbooks and academic literature. Hierarchical effects occur when relationships among variables are mediated through other variables. Challenges in studying hierarchical effects in marketing include the large number of items present in most commercial studies and the presence of heterogeneous relationships among the variables. Existing approaches have dealt with the large number of variables by employing a factor structure representation of the data and have used standard mixture distributions for representing different response segments. In this paper, we propose a Bayesian model for the analysis of hierarchical data using the actual response items and incorporating heterogeneity that better reflects consumer stages in a decision process. Cross-sectional data from a national brand-tracking study are used to illustrate our model, where we find empirical support for a hierarchical relationship among media recall, brand beliefs, and intended actions. We find these effects to be insignificant when measured with standard models and aggregate analyses. The proposed model is useful for understanding the influence of variables that lead to intermediate as opposed to direct effects on brand choice.
Firms in several markets attract consumers by offering discounts in other unrelated markets. This promotion strategy, which we call “cross-market discounts,” has been successfully adopted in the last few years by many grocery retailers in partnership with gasoline retailers across North America, Europe, and Australia. In this paper, we use an analytical model to investigate the major forces driving the profitability of this novel promotion strategy. We consider a generalized scenario in which purchases in a source market lead to price discounts redeemable in a target market. Our analysis shows that this strategy can be a revenue driver by simultaneously increasing prices as well as sales in the source market, even though we assume the demand curve to be downward sloping in price. Moreover, it distributes additional consumption (motivated by the discount) in two markets, and under diminishing marginal returns from consumption, this can simultaneously increase firm profits and consumer welfare more effectively than traditional nonlinear pricing strategies. Our study provides many other interesting insights as well, and our key results are in accordance with anecdotal evidence obtained from managers and industry publications.
In certain categories, an important element of competition is the use of previews to signal information to potential consumers about product attributes. For example, the front page of a newspaper provides a preview to potential newspaper buyers before they purchase the product. In this context, a news provider can provide previews that are highly informative about the content of the news product. Conversely, a news provider can utilize a preview that is relatively uninformative. We examine the incentives that firms have to adopt different preview strategies in a context where they do not have complete control of product positioning. Our analysis shows that preview strategy can be a useful source of differentiation. However, when a firm adopts a strategy of providing informative previews, it confers a positive externality on a competitor that utilizes uninformative previews. This reinforces the incentive of the competitor to use uninformative previews and explains why the market landscape in news provision is often characterized by asymmetric competition.
“Behavior-based personalization” has gained popularity in recent years, whereby businesses offer personalized products based on consumers' purchase histories. This paper highlights two perils of behavior-based personalization in competitive markets. First, although purchase histories reveal consumer preferences, competitive exploitation of such information damages differentiation, similar to the classic finding that behavior-based price discrimination intensifies price competition. With endogenous product design, there is yet a second peril. It emerges when forward-looking firms try to avoid the first peril by suppressing the information value of purchase histories. Ideally, if a market leader serves all consumers on day 1, purchase histories contain no information about consumer preferences. However, knowing that their rivals are willing to accommodate a market leader, firms are more likely to offer a mainstream design at day 1, which jeopardizes differentiation. Based on this understanding, I investigate how the perils of behavior-based personalization change under alternative market conditions, such as firms' better knowledge about their own customers, consumer loyalty and inertia, consumer self-selection, and the need for classic designs.
Greg M. Allenby (“Identifying Unmet Demand”; “The Effect of Media Advertising on Brand Consideration and Choice”; “Bayesian Analysis of Hierarchical Effects”) is the Helen C. Kurtz Chair in Marketing at The Ohio State University. He specializes in the study of economic and statistical issues in marketing. His research deals with developing new insights about consumer behavior from customer data routinely collected by most organizations. These insights are used to develop and improve product development, pricing, promotion, market segmentation, and target marketing activities. He is a fellow of the American Statistical Association, coauthor of Bayesian Statistics and Marketing (Wiley, 2005), and coeditor of Quantitative Marketing and Economics.Masataka Ban (“The Effect of Media Advertising on Brand Consideration and Choice”) is a lecturer at the Faculty of Business Administration, Mejiro University (Japan). His research interests involve measuring advertising effectiveness and optimizing media scheduling.Jeff D. Brazell (“Bayesian Analysis of Hierarchical Effects”) is the CEO of The Modellers, LLC, based in Salt Lake City. Over the past 20 years, he has held various high-level management positions at several national and international firms, has taught marketing at two universities, and has been extensively involved in researching new quantitative methods. He has a passion for developing new theory and methods that make a real impact in industry—methods that will be used and that drive important decisions. He loves designing and delivering research projects to help clients with their most difficult challenges. Those research interests include projects from such clients as General Motors, American Express, P&G, Citibank, Disney, GE, eBay, IBM, Lucas Arts, Paramount, Purina, Schick, Toyota, and Verizon. He has been blissfully married for nearly 30 years and has three wonderful children.Sandeep R. Chandukala (“Identifying Unmet Demand”; “Bayesian Analysis of Hierarchical Effects”) joined the Kelley School of Business, Indiana University, as an assistant professor of marketing in 2008. Prior to joining Kelley, he received a Ph.D. in marketing from The Ohio State University. He also has an MBA and M.S. from the University of Texas at Dallas and an M.S. in computer engineering from the University of Minnesota, Minneapolis. His research interests are related to developing quantitative models of consumer behavior using industrial data. In particular, his modeling interests are in understanding and measuring the impact of advertising and proposing new approaches for market segmentation using Bayesian and Markov chain Monte Carlo methods.Keisha M. Cutright (“Brands: The Opiate of the Nonreligious Masses?”) is a doctoral candidate at the Fuqua School of Business at Duke University. She studies consumer behavior, with a particular emphasis on consumers' needs for order and structure. She currently has research forthcoming in the Journal of Marketing Research and Marketing Science.Peter J. Danaher (“Modeling Multivariate Distributions Using Copulas: Applications in Marketing”; “Rejoinder—Estimation Issues for Copulas Applied to Marketing Data”) is the Coles Myer Chair of Marketing and Retailing at the Melbourne Business School in Australia. He was previously at the University of Auckland and has had visiting positions at London Business School, The Wharton School of the University of Pennsylvania, and MIT. His primary research interests are media exposure distributions, advertising effectiveness, television audience measurement and behavior, Internet usage behavior, customer satisfaction measurement, forecasting, and sample surveys. He serves on the editorial boards of the Journal of Marketing, the Journal of Marketing Research, Marketing Science, and the Journal of Service Research. He is also an area editor for the International Journal of Research in Marketing.Jeffrey P. Dotson (“Bayesian Analysis of Hierarchical Effects”) is an assistant professor of marketing in the Owen Graduate School of Management at Vanderbilt University. He holds a Ph.D. in marketing from The Ohio State University and master's degrees in statistics and business administration from the University of Utah. His research focuses on the application of Bayesian statistics to a variety of marketing problems, including linking customer and employee satisfaction to firm performance and developing more accurate models of consumer decision making.Yancy D. Edwards (“Identifying Unmet Demand”) is an associate professor of marketing at the School of Business at Saint Leo University. He holds a Ph.D. in business administration (marketing) from The Ohio State University. His research involves building models that are more insightful and predictive of consumer behavior, quantifying aspects of consumer behavior using data routinely collected by most organizations, and developing methodologies to aid in estimating marketing models.Tülin Erdem (“Brands: The Opiate of the Nonreligious Masses?”) is the Leonard N. Stern Professor of Business and a professor of marketing at the Stern School of Business, New York University. Before joining the Stern School, she was the E. T. Grether Professor of Business Administration at the Haas School of Business, University of California, Berkeley, where she served also as the Marketing Group Chair, Haas Ph.D. Program Director, and the Associate Dean for Research. Her research interests include advertising, branding, choice modeling, consumer decision making under uncertainty, econometric modeling, and pricing. She has published several articles in major field journals and won the Little and Bass awards. She has served as an AE for Marketing Science, the Journal of Consumer Research, and Quantitative Marketing and Economics. She has also served as the ISMS President. Currently, she is serving as the editor for the Journal of Marketing Research.Gavan J. Fitzsimons (“Brands: The Opiate of the Nonreligious Masses?”) is the R. David Thomas professor of marketing and psychology at Duke University's Fuqua School of Business. His research focuses on understanding the ways in which consumers may be influenced without their conscious knowledge or awareness by marketers and marketing researchers, often without any intent on the part of the marketer. His ideas have been featured in many popular press outlets such as NPR; CNN; MSNBC; the New York Times; the Financial Times; the Wall Street Journal; Psychology Today; O, The Oprah Magazine; and Time, among many others. He serves as an associate editor for the Journal of Consumer Research.Edward I. George (“Commentary—A Latent Variable Perspective of Copula Modeling”) is the Chair of the Department of Statistics and the Universal Furniture Professor at The Wharton School of the University of Pennsylvania. He is a fellow of the American Statistical Association, a fellow of the Institute for Mathematical Statistics, and a member of International Statistical Institute. He has served as President of the International Society for Bayesian Analysis, as executive editor of Statistical Science, and as associate editor of Bayesian Analysis, Biometrika, Journal of the American Statistical Association, and Statistics Surveys. An ISI Highly Cited Researcher, his current research interests include Bayesian analysis, classification and regression tree modeling, model uncertainty, predictive inference, statistical decision theory, and variable selection.Marcel Goić (“Cross-Market Discounts”) is a doctoral candidate at the Tepper School of Business, Carnegie Mellon University and an assistant professor at the Department of Industrial Engineering, University of Chile. He previously received a B.S. degree in industrial engineering and a M.S. degree in operations management from the University of Chile. His research interest includes database marketing, decision support systems, and retail management, where he focuses on pricing, assortment, and promotion decisions.Shane T. Jensen (“Commentary—A Latent Variable Perspective of Copula Modeling”) is an associate professor of statistics at The Wharton School at the University of Pennsylvania, where he has been teaching since completing his Ph.D. at Harvard University in 2004. He has published over 30 academic papers in statistical and machine learning methodology for a variety of applied areas, including molecular biology, economics, and sports.Kinshuk Jerath (“Cross-Market Discounts”) is an assistant professor of marketing at the Tepper School of Business at Carnegie Mellon University. He received a B.Tech. degree in computer science and engineering from the Indian Institute of Technology Bombay and a Ph.D. degree in marketing from The Wharton School of the University of Pennsylvania. His research interests are two-fold—theoretical models that help to obtain deeper understanding of marketing phenomena, especially phenomena related to retailing, and applied statistical models that support marketing analysts and decision makers. His research has appeared in top-tier marketing journals.Zsolt Katona (“‘Bricks and Clicks’: The Impact of Product Returns on the Strategies of Multichannel Retailers”) is an assistant professor of marketing at the Haas School of Business, University of California, Berkeley. He has a Ph.D. in management from INSEAD and earned a Ph.D. in computer science from Eotvos University, Budapest. His current research focuses on understanding the interaction between websites' online advertising strategies. He also studies the role that link structure of social networks plays in word-of-mouth effects and community formation. Previously, he had analyzed characteristics of different random networks and published his work in such journals as the Journal of Applied Probability, Statistics and Probability Letters, and Random Structures and Algorithms.Oded Koenigsberg (“The Design of Durable Goods”) is the Barbara and Meyer Feldberg Associate of Business at Columbia Business School, Columbia University. He received a Ph.D. in operations management from Fuqua School of Business, Duke University. His research interests include manufacturing/marketing interface, management of distribution channels, marketing of durable goods, product line, and product design.Rajeev Kohli (“The Design of Durable Goods”) is the Ira Leon Rennert Professor of Business, and Chairman of the Marketing Division, at the Graduate School of Business, Columbia University. His research interests include product design and development, pricing, mathematical models of consumer preference structures, design and analysis of algorithms, and marketing and policy issues in emerging markets. His papers on these topics have appeared in the leading industry journals.Eunkyu Lee (“Internet Channel Entry: A Strategic Analysis of Mixed Channel Structures”) is an associate professor of marketing at the Whitman School of Management, Syracuse University. He holds a Ph.D. in marketing from Duke University and previously taught at Seattle University and the University of British Columbia. His research interests include marketing channel strategy, store brand management, market competition strategy, and consumer survey methodology.Ricardo Montoya (“The Design of Durable Goods”) is an assistant professor at the Industrial Engineering Department, University of Chile, Santiago. He received a Ph.D. in marketing from the Columbia Business School, Columbia University. He holds a bachelor's degree in engineering and an M.S. in operations management from the University of Chile. His research interests include dynamic choice models, optimal product design, and the long-term impact of marketing actions. His methodological interests lie in Bayesian econometrics, hidden Markov models, and stochastic dynamic programming.Elie Ofek (“‘Bricks and Clicks’: The Impact of Product Returns on the Strategies of Multichannel Retailers”) is the T.J. Dermot Dunphy Professor of Business Administration at the Harvard Business School. He received his Ph.D. in business and M.A. in economics from Stanford University. His research focuses on how marketing input can impact innovation strategy and on how firms can leverage novel technologies or major trends to deliver value to customers. He is an associate editor for Management Science and serves on the editorial boards of Marketing Science, the Journal of Marketing Research, and the International Journal of Research in Marketing.Miklos Sarvary (“‘Bricks and Clicks’: The Impact of Product Returns on the Strategies of Multichannel Retailers”) is a professor of marketing at INSEAD. Prior to his current position, he was a faculty member at the Harvard Business School and the Graduate School of Business at Stanford University. He has a Ph.D. in management from INSEAD. His current research focuses on social networks and new media (metaverses) and how these technologies transform marketing. His recent papers study media competition, online advertising, the structure of the Internet, and techniques related to “community management.” Previously, he did work on information marketing, the worldwide pricing of cellular telephone services, and the global diffusion of telecommunications products. He is an associate editor for Marketing Science and Quantitative Marketing and Economics and a member of the editorial boards of the International Journal of Research in Marketing and the Journal of Interactive Marketing.Ron Shachar (“Brands: The Opiate of the Nonreligious Masses?”) is a professor of marketing at Tel Aviv University, where he also serves as the chairperson of the marketing group. He also has a visiting position at Duke University's Fuqua School of Business since 2005. His research interests include advertising, branding, choice modeling, the entertainment industries, identity marketing, and political marketing. He serves as an AE for the Journal of Marketing Research and Quantitative Marketing and Economics and on the editorial boards of Marketing Science and the International Journal of Research in Marketing.Michael S. Smith (“Modeling Multivariate Distributions Using Copulas: Applications in Marketing”; “Rejoinder—Estimation Issues for Copulas Applied to Marketing Data”) is currently the Chair of Management (Econometrics) at the Melbourne Business School. His research is focused on the development of Bayesian methodology and its application to problems in economics, business, and the physical sciences. Specifically, he has worked on high-dimensional model averaging and its use in semiparametric regression, covariance matrix estimation, time-series estimation, and spatial modeling. He has held visiting appointments at the Universities of Munich, Pennsylvania, and Texas.David A. Soberman (“Preview Provision Under Competition”) is the Canadian National Chair in Strategic Management and Professor of Marketing at the Rotman School of Management at the University of Toronto. He holds a Ph.D. (management) from the University of Toronto and an MBA and a B.S. in chemical engineering from Queen's University in Kingston. He has received awards for his research including the International Journal of Research in Marketing 2006 Best Paper Award and the INFORMS 2000 John D. C. Little Best Paper Award. He is an area editor for the International Journal of Research in Marketing and a member of the Marketing Science editorial board. Prior to academia, he held a number of positions in marketing management, sales, and engineering with Molson Breweries, Nabisco Brands Ltd., and Imperial Oil Ltd.Kannan Srinivasan (“Cross-Market Discounts”) is the Rohet Tolani Distinguished Professor of International Business and H.J. Heinz II Professor of Management, Marketing and Information Systems at the Tepper School of Business at Carnegie Mellon University. He is currently an area editor for Marketing Science and Quantitative Marketing and Economics and an associate editor for Management Science. He has published over 50 papers in leading journals.Nobuhiko Terui (“The Effect of Media Advertising on Brand Consideration and Choice”) is a professor at the Graduate School of Economics and Management, Tohoku University (Japan). His current research interests are in the modeling of nonlinear responses of heterogeneous consumers, dynamic marketing models, and related decision problems.Yi Xiang (“Preview Provision Under Competition”) is an assistant professor of marketing at The Hong Kong University of Science and Technology. He received his Ph.D. and M.S. in management from INSEAD (Fontainebleau, France) and a B.Eng. from Tsinghua University (Beijing, China). His research focuses on media competition, media strategy, information efficiency, consumer processing, and advertising. Prior to his academic career, he held a number of marketing and managerial positions at BaoSteel and Bekaert in Shanghai.Weon Sang Yoo (“Internet Channel Entry: A Strategic Analysis of Mixed Channel Structures”) is an assistant professor of marketing at Korea University. He received his B.A. in economics from Korea University, an MBA from the George Washington University, and Ph.D. in business administration from the University of British Columbia. He previously taught at Singapore Management University and Hanyang University. His current research interests include distribution channel management, emerging marketing channels, and competitive marketing strategies.Juanjuan Zhang (“The Perils of Behavior-Based Personalization”) is the Class of 1948 Career Development Professor and an assistant professor of marketing at the MIT Sloan School of Management. She holds a B.Econ. from Tsinghua University and a Ph.D. in business administration from the University of California, Berkeley. She studies observational learning and its implication for marketers. She is also interested in how market information interacts with firms' product strategies. Her recent research explores why firms would continue bad products in spite of negative market feedback, why product personalization may damage profits, and how companies should manage consumers' self-discovery of their preferences.
Marketing Science greatly benefited from the admirable and fastidious efforts of more than 200 different individuals who provided manuscript reviews last year. Beyond those individuals already recognized on the editorial board, the editor-in-chief and guest editors of Marketing Science are indebted to the many guest area editors and ad hoc reviewers who provided expert counsel and guidance on a voluntary basis. The following list acknowledges the contribution of guest area editors and ad hoc reviewers who served from January to December 2010. Finally, let us not forget to thank the authors. Marketing Science requires and receives outstanding submissions from many leading researchers and prestigious organizations.
We study how opinion leadership and social contagion within social networks affect the adoption of a new product. In contrast to earlier studies, we find evidence of contagion operating over network ties, even after controlling for marketing effort and arbitrary systemwide changes. More importantly, we also find that the amount of contagion is moderated by both the recipients' perception of their opinion leadership and the sources' volume of product usage. The other key finding is that sociometric and self-reported measures of leadership are weakly correlated and associated with different kinds of adoption-related behaviors, which suggests that they probably capture different constructs. We discuss the implications of these novel findings for diffusion theory and research and for marketing practice.
Isuggest five broad directions for future research on social influence and opinion leadership that could, if appropriately addressed, dramatically improve how we conceptualize and manage social contagions in a variety of domains.
Building on the commentaries on our work, we make additional suggestions for future research on social contagion and new product diffusion. In particular, we note that social contagion may occur for many reasons and that investigating how various personal or group characteristics moderate the amount of influence some customers exert or the extent to which others are sensitive to potential influence can provide insights into the social mechanism(s) at work.
We show both analytically and through Monte Carlo simulations that applying standard hazard models to right-truncated data, i.e., data from which all right-censored observations are omitted, induces spurious positive duration dependence and hence can trick researchers into believing to have found evidence of social contagion when there is none. Truncation also tends to deflate the effect of time-invariant covariates. These results imply that not accounting for right truncation can lead managers to rely too much on word of mouth in generating new product adoption and to poorly identify the customers most likely to adopt early. Not accounting for right truncation can also lead to suboptimal pricing decisions and to erroneous assessments of variations in customer lifetime value. We assess the effectiveness of four possible solutions to the problem and find that only using an analytically corrected likelihood function protects one against truncation artifacts inflating coefficients of contagion and attenuating coefficients of time-invariant covariates.
Facebook and Google offer hybrid advertising auctions that allow advertisers to bid on a per-impression or a per-click basis for the same advertising space. This paper studies the properties of equilibrium and considers how to increase efficiency in this new auction format. Rational expectations require the publisher to consider past bid types to prevent revenue losses to strategic advertiser behavior. The equilibrium results contradict publisher statements and suggest that, conditional on setting rational expectations, publishers should consider offering multiple bid types to advertisers. For a special case of the model, we provide a payment scheme that achieves the socially optimal allocation of advertisers to slots and maximizes publisher revenues within the class of socially optimal payment schemes. When this special case does not hold, no payment scheme will always achieve the social optimum.
We present a dynamic factor-analytic choice model to capture evolution of brand positions in latent attribute space. Our dynamic model allows researchers to investigate brand positioning in new categories or mature categories affected by structural change such as entry. We argue that even for mature categories not affected by structural change, the assumption of stable attributes may be untenable. We allow for evolution in attributes by modeling individual-level time-specific attributes as arising from dynamic means. The dynamic attribute means are modeled as a Bayesian dynamic linear model (DLM). The DLM is nested within a factor-analytic choice model. Our approach makes efficient use of the data by leveraging estimates from previous and future periods to estimate current period attributes. We demonstrate the robustness of our model with data that simulate a variety of dynamic scenarios, including stationary behavior. We show that misspecified attribute dynamics induce temporal heteroskedasticty and correlation between the preference weights and the error term. Applying the model to a panel data set on household purchases in the malt beverage category, we find considerable evidence for dynamics in the latent brand attributes. From a managerial perspective, we find advertising expenditures help explain variation in the dynamic attribute means.
In this paper, I study profitability of the name-your-own-price channel (NYOP) in the presence of risk-averse buyers. First, I provide conditions that guarantee that for the monopolistic seller the NYOP is more profitable than the posted price. Second, I consider a more competitive framework where buyers with rejected bids have access to an alternative option. I show that if under the posted-price scenario there are unserved customers with low valuations, then NYOP is more profitable than the posted price. Finally, I study whether adding the posted-price option to the NYOP will further increase the seller's profit and show that for the decreasing absolute risk-aversion utility and a monopolistic seller it does not. In the presence of an alternative option, the answer depends on whether buyers consider the posted-price option and the alternative option to be close substitutes or not. Adding the posted-price option will increase the profit in the former case and will not in the latter.
Experimental and survey-based research suggests that consumers often rely on their intuition and cognitive shortcuts to make decisions. Intuition and cognitive shortcuts can lead to suboptimal decisions and, especially in high-stakes decisions, to legitimate welfare concerns. In this paper, we propose an extension of a Bayesian learning model that allows us to quantify the impact of salience—the fact that some pieces of information are easier to retrieve from memory than others—on physician learning. We show, using data on actual prescriptions for real patients, that physicians' belief formation is strongly influenced by salience effects. Feedback from switching patients—the ones the physician decided to switch to a clinically equivalent treatment—receives considerably more weight than feedback from other patients. In the category we study, salience effects slowed down physicians' speed of learning and the adoption of a new treatment, which raises welfare concerns. For managers, our findings suggest that firms that are able to eliminate, or at least reduce, salience effects to a greater extent than their competitors can speed up the adoption of new treatments. We explore the implications of these results and suggest alternative applications of our model that are relevant for policy makers and managers.
Existing research on choice designs focuses exclusively on compensatory models that assume that all available alternatives are considered in the choice process. In this paper, we develop a method to construct efficient designs for a two-stage, consider-then-choose model that involves a noncompensatory screening process at the first stage and a compensatory choice process at the second stage. The method applies to both conjunctive and disjunctive screening rules. Under certain conditions, the method also applies to the subset conjunctive and disjunctions of conjunctions screening rules. Based on the local design criterion, we conduct a comparative study of compensatory and conjunctive designs—the former are optimized for a compensatory model and the latter for a two-stage model that uses conjunctive screening in its first stage. We find that conjunctive designs have higher level overlap than compensatory designs. This occurs because level overlap helps pinpoint screening behavior. Higher overlap of conjunctive designs is also accompanied by lower orthogonality, less level balance, and more utility balance. We find that compensatory designs have a significant loss of design efficiency when the true model involves conjunctive screening at the consideration stage. These designs also have much less power than conjunctive designs in identifying a true consider-then-choose process with conjunctive screening. In contrast, when the true model is compensatory, the efficiency loss from using a conjunctive design is lower. Also, conjunctive designs have about the same power as compensatory designs in identifying a true compensatory choice process. Our findings make a strong case for the use of conjunctive designs when there is prior evidence to support respondent screening.
The failure of firms in the face of technological change has been a topic of intense research and debate, spawning the theory (among others) of disruptive technologies. However, the theory suffers from circular definitions, inadequate empirical evidence, and lack of a predictive model. We develop a new schema to address these limitations. The schema generates seven hypotheses and a testable model relating to platform technologies. We test this model and hypotheses with data on 36 technologies from seven markets. Contrary to extant theory, technologies that adopt a lower attack (“potentially disruptive technologies”) (1) are introduced as frequently by incumbents as by entrants, (2) are not cheaper than older technologies, and (3) rarely disrupt firms; and (4) both entrants and lower attacks significantly reduce the hazard of disruption. Moreover, technology disruption is not permanent because of multiple crossings in technology performance and numerous rival technologies coexisting without one disrupting the other. The proposed predictive model of disruption shows good out-of-sample predictive accuracy. We discuss the implications of these findings.
Many firms have introduced Internet-based customer self-service applications such as online payments or brokerage services. Despite high initial sign-up rates, not all customers actually shift their dealings online. We investigate whether the multistage nature of the adoption process (an “adoption funnel”) for such technologies can explain this low take-up. We use exogenous variation in events that possibly interrupt adoption, in the form of vacations and public holidays in different German states, to identify the effect on regular usage of being interrupted earlier in the adoption process. We find that interruptions in the early stages of the adoption process reduce a customer's probability of using the technology regularly. Our results suggest significant cost-saving opportunities from eliminating interruptions in the adoption funnel.
We study how multiattribute product choices are affected by peer influence. We propose a two-stage conjoint-based approach to examine three behavioral mechanisms of peer influence. We find that when faced with information on peer choices, consumers update their attribute preferences in a Bayesian manner. This suggests that greater uncertainty in the attribute preferences of a focal consumer and lesser uncertainty in preferences of peers both lead to greater preference revision. Greater number of peers is associated with greater preference revision, although the extent of preference revision diminishes with increasing number of peers. Furthermore, to address the significant time and costs associated with collecting sociometric data, we estimate the accuracy of predicted consumer choices when peer influence data are unavailable. Online social network membership and frequency of peer interactions provide better proxies than more common demographic similarity measures. These findings have key implications, especially for word-of-mouth marketing.
Sinan Aral (“Commentary—Identifying Social Influence: A Comment on Opinion Leadership and Social Contagion in New Product Diffusion”) is a faculty member in the Information, Operations and Management Sciences Department of the New York University Stern School of Business and affiliated faculty at the Massachusetts Institute of Technology (MIT). He is a Phi Beta Kappa graduate of Northwestern University and holds master's degrees from the London School of Economics and Harvard University as well as a Ph.D. from MIT. He studies how behavioral contagions spread through social networks—from products to productivity to public health—by analyzing how the distribution and movement of information inside firms impacts information worker productivity; how information diffusion in massive online social networks influences demand patterns, consumer e-commerce behaviors, and word-of-mouth marketing; and how investments in IT capital and complementary intangible assets combine to create productivity and business value benefits for firms. His research has won numerous awards.Neeraj Arora (“Efficient Choice Designs for a Consider-Then-Choose Model”) is the John P. Morgridge Chair in Business Administration at the University of Wisconsin–Madison, where he also serves as the Executive Director of the A.C. Nielsen Center for Marketing Research. He has an undergraduate degree in engineering from Delhi University, and an MBA and Ph.D. from The Ohio State University. He serves on the editorial boards of Journal of Marketing Research and Marketing Science. His papers have appeared in the Journal of Marketing Research, Marketing Science, Journal of Consumer Research, Journal of Marketing, International Journal of Research in Marketing, and Marketing Letters.Nuno Camacho (“Predictably Non-Bayesian: Quantifying Salience Effects in Physician Learning About Drug Quality”) is a doctoral student in marketing at the Erasmus School of Economics, Erasmus University Rotterdam (The Netherlands). His research interests include behavioral modeling (i.e., building econometric models to study individual and joint consumer decision processes) and behavioral economics applied to marketing. In terms of substantive focus, he is working on topics in the life sciences industry and is interested in new product adoption, cross-cultural differences, and social influences in decision making.Nicholas A. Christakis (“Commentary—Contagion in Prescribing Behavior Among Networks of Doctors”) is an internist and social scientist who conducts research on social factors that affect health, health care, and longevity. He is a professor of medical sociology in the Department of Health Care Policy at Harvard Medical School, a professor of medicine in the Department of Medicine at Harvard Medical School, and a professor of sociology in the Department of Sociology in the Harvard Faculty of Arts and Sciences. In 2009, he was named one of the 100 most influential people in the world by Time magazine.Bas Donkers (“Predictably Non-Bayesian: Quantifying Salience Effects in Physician Learning About Drug Quality”) is an associate professor of marketing at the Erasmus School of Economics, Erasmus University Rotterdam (The Netherlands). His research interests are in behavioral modeling with a specific focus on individual decision making. Applications include, among others, charitable giving, search behavior, and patient preferences. His work has been published in Marketing Science, the Journal of Marketing Research, and the International Journal of Research in Marketing.James H. Fowler (“Commentary—Contagion in Prescribing Behavior Among Networks of Doctors”) is a social scientist whose work lies at the intersection of the natural and social sciences. His primary areas of research are social networks, behavioral economics, evolutionary game theory, political participation, cooperation, and genopolitics (the study of the genetic basis of political behavior). He is a professor in the School of Medicine and the Division of Social Sciences at the University of California, San Diego. For 2010–2011, he has been named a fellow of the John Simon Guggenheim Foundation.David Godes (“Commentary—Invited Comment on ‘Opinion Leadership and Social Contagion in New Product Diffusion’”) is an associate professor in the Marketing Department at the Robert H. Smith School of Business, University of Maryland. Prior to joining the University of Maryland, he taught at Harvard Business School. He received a B.S. in economics at the University of Pennsylvania and an S.M. and Ph.D. in management science from the Massachusetts Institute of Technology. His research interests include word-of-mouth communication, social networks, media competition, and sales management.Raghuram Iyengar (“Opinion Leadership and Social Contagion in New Product Diffusion”; “Rejoinder—Further Reflections on Studying Social Influence in New Product Diffusion”; “Tricked by Truncation: Spurious Duration Dependence and Social Contagion in Hazard Models”) is an assistant professor of marketing at The Wharton School of the University of Pennsylvania. He earned his Ph.D. from Columbia University and his B.Tech. from IIT Kanpur, India. His research focuses on social networks, new product diffusion, and pricing.Anja Lambrecht (“Stuck in the Adoption Funnel: The Effect of Interruptions in the Adoption Process on Usage”) is an assistant professor of marketing at the London Business School. She received her Ph.D. from Goethe-University, Frankfurt, Germany. Her research interests lie in firms' nonlinear pricing strategies, how consumers choose and use under nonlinear pricing plans, and how consumers adopt new service technologies.Qing Liu (“Efficient Choice Designs for a Consider-Then-Choose Model”) is an assistant professor of marketing at the University of Wisconsin–Madison. She received her B.S. degree from the University of Science and Technology of China, and her M.S. and Ph.D. in statistics from The Ohio State University. Her research focuses on the application and development of statistical theories and methodology to help solve problems in marketing and marketing research; areas of interest include conjoint analysis, consumer choice, experimental design, and Bayesian methods. Her papers have appeared in Marketing Science, Quantitative Marketing and Economics, and Statistica Sinica.Vishal Narayan (“How Peer Influence Affects Attribute Preferences: A Bayesian Updating Mechanism”) is an assistant professor of marketing at the Johnson School at Cornell University. He holds a Ph.D. in marketing from the Stern School of Business, New York University, and an MBA degree from the Indian Institute of Management, Lucknow, India. His research interests include understanding how social interactions affect market outcomes. He applies Bayesian econometric methods to study consumer and firm behavior.Vithala R. Rao (“How Peer Influence Affects Attribute Preferences: A Bayesian Updating Mechanism”) is the Deane Malott Professor of Management and Professor of Marketing and Quantitative Methods, Johnson Graduate School of Management, Cornell University. He holds a Ph.D. in applied economics/marketing from The Wharton School of the University of Pennsylvania. He has published more than 100 papers on topics including conjoint analysis and multidimensional scaling, pricing, bundle design, brand equity, market structure, corporate acquisition, and linking branding strategies to financial performance; his current work includes peer influence, competitive bundling, dynamic attribute trade-offs, and trade promotions. He received several awards, including the 2008 Charles Coolidge Parlin Marketing Research Award, presented by the American Marketing Association and the American Marketing Association Foundation, recognizing his “outstanding leadership and sustained impact on advancing the evolving profession of marketing research over an extended period of time.”Oliver J. Rutz (“The Evolution of Internal Market Structure”) is an assistant professor of marketing at the Yale School of Management, New Haven, where he researches and lectures on online marketing with an emphasis on paid search management. He received his Ph.D. in marketing from UCLA Anderson in 2007. He won the 2007 EMAC best dissertation paper award and honorable mention in the 2007 Alden G. Clayton Doctoral Dissertation Proposal Competition. He is a member of the Handelsblatt-Management-Forum, a bimonthly international academic panel in Germany's leading business and financial newspaper.Carolyne Saunders (“How Peer Influence Affects Attribute Preferences: A Bayesian Updating Mechanism”) is a Ph.D. student in marketing at the Johnson Graduate School of Management at Cornell University. She holds an M.Sc. in economics and business research from the University of Groningen, The Netherlands, and a B.A. (Hons) in economics from the University of Cambridge, United Kingdom. Her research interests include quantitative analysis of consumer behavior and theoretical and empirical modeling of new product development.Katja Seim (“Stuck in the Adoption Funnel: The Effect of Interruptions in the Adoption Process on Usage”) is an assistant professor of business and public policy at The Wharton School of the University of Pennsylvania. She received her Ph.D. in economics from Yale University. Her research interests lie in the empirical analysis of competitive behavior, including firms' product introduction and entry decisions, nonlinear pricing, and the adoption of new technologies.Dmitry Shapiro (“Profitability of the Name-Your-Own-Price Channel in the Case of Risk-Averse Buyers”) is an assistant professor of economics at the University of North Carolina Charlotte. He received a Ph.D. degree in economics from Yale University in 2006. He works on experimental economics and applied micro theory.Garrett P. Sonnier (“The Evolution of Internal Market Structure”) is an assistant professor of marketing at the McCombs School of Business at the University of Texas at Austin. He received his Ph.D. in marketing from the UCLA Anderson School of Management. His current research interests include product management, product and brand perceptions, pricing, Bayesian econometrics, and multivariate Bayesian statistics. His research has been published or accepted for publication in Quantitative Marketing and Economics, the Journal of Marketing Research, and Marketing Science.Ashish Sood (“Demystifying Disruption: A New Model for Understanding and Predicting Disruptive Technologies”) is an assistant professor of marketing at Goizueta School of Business, Emory University. He has a Ph.D. in marketing from the University of Southern California, a bachelor's degree in electrical engineering, and an MBA in marketing from Nanyang Technological University, Singapore. Having worked for 12 years in the industry before joining academia, he is an expert in the areas of marketing, innovation, technology management, and financial analysis. His research has been published and featured in many reputed management publications (http://www.bus.emory.edu/individuals/asood).Stefan Stremersch (“Predictably Non-Bayesian: Quantifying Salience Effects in Physician Learning About Drug Quality”) holds a chair in marketing and is the Desiderius Erasmus Distinguished Chair of Economics at the Erasmus School of Economics, Erasmus University Rotterdam (The Netherlands) and a professor of marketing at the IESE Business School, Universidad de Navarra (Spain). His current research interests are in innovation acceptance/diffusion, marketing of technology and science, and international marketing. He has won several awards, such as the Harold H. Maynard Best Paper Award of the Journal of Marketing (2002), the J. C. Ruigrok Prize (2005) for the most productive young researcher in the social sciences in The Netherlands (only once in four years awarded to an economist), and the AMA Early Career Award in Marketing Strategy (2008). He also received the 2004 Research Prize at Erasmus University Rotterdam for outstanding research performance, selected among all Erasmus faculty across all disciplines and schools.Gerard J. Tellis (“Demystifying Disruption: A New Model for Understanding and Predicting Disruptive Technologies”) is Professor, Jerry and Nancy Neely Chair of American Enterprise, and Director of the Center for Global Innovation, at the University of Southern California Marshall School of Business. He is an expert in advertising, innovation, global market entry, new product growth, global diffusion, quality, and pricing. He has published four books and over 100 papers that have won over 15 awards, including the Frank M. Bass Award, the William F. Odell Award, the Harold D. Maynard Award (twice), and the Vijay Mahajan Award for Lifetime Contributions to Marketing Strategy. He is a trustee of the Marketing Science Institute; a distinguished professor of Marketing Research, Erasmus University, Rotterdam; a senior research associate at the Judge Business School, Cambridge University, United Kingdom; and a fellow of Sidney Sussex College (http://www.gtellis.net).Catherine Tucker (“Stuck in the Adoption Funnel: The Effect of Interruptions in the Adoption Process on Usage”) is the Douglas Drane Career Development Professor in IT and Management and an assistant professor of marketing at the MIT Sloan School of Management. She received an undergraduate degree in politics, philosophy and economics from Oxford University and a Ph.D. in economics from Stanford University. She is interested in understanding how networks, privacy concerns, and regulation affect marketing outcomes.Thomas W. Valente (“Opinion Leadership and Social Contagion in New Product Diffusion”; “Rejoinder—Further Reflections on Studying Social Influence in New Product Diffusion”) is a professor and Director of the Master of Public Health Program in the Department of Preventive Medicine, Keck School of Medicine, University of Southern California. He uses network analysis, health communication, and mathematical models to implement and evaluate health promotion programs designed to prevent tobacco and substance abuse, unintended fertility, and STD/HIV infections. He is also engaged in mapping community coalitions and collaborations to improve health-care delivery and reduce health-care disparities. He is author of Social Networks and Health: Models, Methods, and Applications (2010, Oxford University Press), Evaluating Health Promotion Programs (2002, Oxford University Press), Network Models of the Diffusion of Innovations (1995, Hampton Press), and more than 100 articles and chapters on social networks, behavior change, and program evaluation.Christophe Van den Bulte (“Opinion Leadership and Social Contagion in New Product Diffusion”; “Rejoinder—Further Reflections on Studying Social Influence in New Product Diffusion”; “Tricked by Truncation: Spurious Duration Dependence and Social Contagion in Hazard Models”) is an associate professor of marketing at The Wharton School of the University of Pennsylvania. He earned a Ph.D. in business administration at The Pennsylvania State University. His research focuses on social networks, new product diffusion, and business marketing. He is an associate or area editor at Marketing Science and the Journal of Marketing Research, and also serves on the editorial boards of the International Journal of Research in Marketing, the Journal of Business-to-Business Marketing, and the Journal of Marketing.Kenneth C. Wilbur (“Hybrid Advertising Auctions”) is a marketing professor at the Duke University Fuqua School of Business. He has previously published papers on click fraud and television advertising in Marketing Science. This latter paper was a finalist for the John D. C. Little Award and won the Frank M. Bass Dissertation Award. His current research focuses on how technology changes advertising and media markets, and econometric models of advertising effectiveness.Yi Zhu (“Hybrid Advertising Auctions”) is a Ph.D. student studying quantitative marketing at the University of Southern California Marshall School of Business. He has conducted research in the areas of online auctions and search advertising. He worked as a consultant at Shanghai Investment Consulting Corporation before he came to Vancouver, where he received his M.A. in economics from the University of British Columbia. His research interests focus on the application of industrial organization models in marketing.
We use data from a large-scale field experiment to explore what influences the effectiveness of online advertising. We find that matching an ad to website content and increasing an ad's obtrusiveness independently increase purchase intent. However, in combination, these two strategies are ineffective. Ads that match both website content and are obtrusive do worse at increasing purchase intent than ads that do only one or the other. This failure appears to be related to privacy concerns: the negative effect of combining targeting with obtrusiveness is strongest for people who refuse to give their income and for categories where privacy matters most. Our results suggest a possible explanation for the growing bifurcation in Internet advertising between highly targeted plain text ads and more visually striking but less targeted ads.
In a very intriguing and groundbreaking study, Goldfarb and Tucker [Goldfarb, A., C. Tucker. 2011. Online display advertising: Targeting and obtrusiveness. Marketing Sci.30(3) 389–404] show that online advertising targeting and obtrusiveness boost purchase intent independently, but not jointly. The authors rule out recall as an explanatory mechanism and provide preliminary evidence that the effect may be driven by privacy concerns. We comment on the substantive importance of this finding by discussing the psychological and economic implications of the effect.
In “Online Display Advertising: Targeting and Obtrusiveness,” Avi Goldfarb and Catherine Tucker present an empirical investigation and discussion of consumers' reactions to obtrusive and targeted Internet advertisements. They find, among other things, that obtrusive advertisements when combined with targeting tend to generate no greater effectiveness in consumer response than either method used alone. Perhaps one of the most interesting findings in their results involves the authors' focus on privacy salience and its connection to advertisement presentation. This brief discussion of their work (1) highlights the importance of further research in line with the well-crafted privacy preference inquiries of Goldfarb and Tucker's study and (2) complements the authors' discussion regarding possible public policy implications with a legal discussion elaborating on existing legal grounds for liability and restrictions on advertising consumers perceive as privacy invasive.
The commentaries on our work suggest several broader implications of our findings as well as a concern that we understate the size of the effect. In this rejoinder, we discuss our views on the regulatory implications, the implications for firm strategies, and the implications for our understanding of the underlying behavioral processes. We also acknowledge that our original calculation of $464 million in cost savings for industry is conservative. We conclude with a call for “privacy engineering” research that combines computer science tools with an understanding of consumer behavior and economics to improve marketing and economic outcomes while safeguarding consumer privacy.
Automotive sales forecasts traditionally focus on predictors such as advertising, brand preference, life cycle position, retail price, and technological sophistication. The quality of the cars' design is, however, an often-neglected variable in such models. We show that incorporating objective measures of design prototypicality and design complexity in sales forecasting models improves their prediction by up to 19%. To this end, we professionally photographed the frontal designs of 28 popular models, morphed the images, and created objective prototypicality (car-to-morph Euclidian proximity) and complexity (size of a compressed image file) scores for each car. Results show that prototypical but complex car designs feel surprisingly fluent to process, and that this form of surprising fluency evokes positive gut reactions that become associated with the design and positively impact car sales. It is important to note that the effect holds for both economy (functionality oriented) and premium (identity oriented) cars, as well as when the above-mentioned traditional forecasting variables are considered. These findings are counter to a common intuition that consumers like unusual–complex designs that reflect their individuality or prototypical–simple designs that are functional.
This paper studies the tendency to use negative ads. For this purpose, we focus on an interesting industry (political campaigns) and an intriguing empirical regularity (the tendency to “go negative” is higher in close races). We present a model of electoral competition in which ads inform voters either of the good traits of the candidate or of the bad traits of his opponent. We find that in equilibrium, the proportion of negative ads depends on both voters' knowledge and the candidate's budget. Furthermore, for an interesting subset of the parameter space, negativity increases in both knowledge and budget. Using data on the elections for the U.S. House of Representative in 2000, 2002, and 2004, we examine the model and its implications. Using nonstructural estimation, we find that negativity indeed increases in both voters' knowledge and the candidate's budget. Furthermore, we also find that knowledge and budget mediate the effect of closeness on negativity. Using structural estimation, we reinforce these findings. Specifically, we find that the model's parameters are within the subset of the parameter space discussed above. Thus, the evidence implies that the model is not only helpful in identifying variables that were ignored by previous studies (i.e., knowledge and budget) but also in explaining an intriguing empirical regularity.
Sponsored search advertising is ascendant—Forrester Research reports expenditures rose 28% in 2007 to $8.1 billion and will continue to rise at a 26% compound annual growth rate [VanBoskirk, S. 2007. U.S. interactive marketing forecast, 2007 to 2012. Forrester Research (October 10)], approaching half the level of television advertising and making sponsored search one of the major advertising trends to affect the marketing landscape. Yet little empirical research exists to explore how the interaction of various agents (searchers, advertisers, and the search engine) in keyword markets affects consumer welfare and firm profits. The dynamic structural model we propose serves as a foundation to explore these outcomes. We fit this model to a proprietary data set provided by an anonymous search engine. These data include consumer search and clicking behavior, advertiser bidding behavior, and search engine information such as keyword pricing and website design.With respect to advertisers, we find evidence of dynamic bidding behavior. Advertiser value for clicks on their links averages about 26 cents. Given the typical $22 retail price of the software products advertised on the considered search engine, this implies a conversion rate (sales per click) of about 1.2%, well within common estimates of 1%–2% [Narcisse, E. 2007. Magid: Casual free to pay conversion rate too low. GameDaily.com (September 20)]. With respect to consumers, we find that frequent clickers place a greater emphasis on the position of the sponsored advertising link. We further find that about 10% of consumers do 90% of the clicks.We then conduct several policy simulations to illustrate the effects of changes in search engine policy. First, we find the search engine obtains revenue gains of 1% by sharing individual-level information with advertisers and enabling them to vary their bids by consumer segment. This also improves advertiser revenue by 6% and consumer welfare by 1.6%. Second, we find that a switch from a first- to second-price auction results in truth telling (advertiser bids rise to advertiser valuations). However, the second-price auction has little impact on search engine profits. Third, consumer search tools lead to a platform revenue increase of 2.9% and an increase of consumer welfare by 3.8%. However, these tools, by reducing advertising exposures, lower advertiser profits by 2.1%.
In many categories consumers display cyclical buying: they repeatedly purchase in the category for several periods, followed by several periods of not buying. We believe that the cyclicality is a manifestation of cross-category substitution by the consumer, caused by “variety-seeking” tendencies as well as by the firm's marketing activities in all relevant categories. We propose a Markov regime-switching random coefficient logit model to represent these behaviors as stochastic switching between high and low category purchase tendencies. The main feature of the proposed model is that it divides the stream of purchase decisions of a consumer into distinct regimes with different parameter values that characterize high versus low purchase tendencies. In an empirical application of the model to purchases of yogurt-buying households, we find that as many as 38.3% households display cyclicality between high and low yogurt-purchasing tendencies. Predictions from our proposed model track observed yogurt purchases of households over time closely, and the model also fits better than two benchmark models. Alternating between high and low purchase tendencies may correspond with changing levels of consumer inventory in a substitute category. If one ignores this phenomenon, a correlation between yogurt inventory and the error term in utility arises, leading to biased estimates. Also, we show that cyclicality in buying has a key implication for a firm's price promotion strategies: a price reduction that is offered to a household during its high purchasing tendency period will result in greater increases in sales than one that is offered during its low purchasing period. This opens up a new dimension for enhancing the effectiveness of promotions—customized timing of price reductions.
A choice model based on direct utility maximization subject to an arbitrary number of constraints is developed and applied to conjoint data. The model can accommodate both corner and interior solutions, and it provides insights into the proportion of respondents bound by each constraint. Application to volumetric choice data reveals that the majority of respondents make choices consistent with price and quantity restrictions. Estimates based on a single monetary-constraint choice model are shown to lead to biased estimates of the monetary value of attribute levels.
The dramatic impact of the current crisis on performance of businesses across sectors and economies has been headlining the business press for the past several months. Extant reconciliations of these patterns in the popular press rely on ad hoc reasoning. Using historical data on currency crisis episodes across the world, we show that the impact of the crisis on a firm's business is best understood by focusing on the impact of the crisis on the behavior of consumers. Our analyses show that consumer behavior in a crisis is characterized by consumption smoothing at various levels—intertemporal, intercategory, and intracategory. These behavioral adjustments result in significant reallocation of consumption expenditures. More importantly, the smoothing decisions because of a crisis are distinct and independent of the impact of changes in income and prices that accompany a crisis. Interestingly, there is marked variation in the patterns of consumption smoothing across different types of economies. Taken together, these results have important and interesting implications for managers, policy makers, and academics.
Under the sociological theory of homophily, people who are similar to one another are more likely to interact with one another. Marketers often have access to data on interactions among customers from which, with homophily as a guiding principle, inferences could be made about the underlying similarities. However, larger networks face a quadratic explosion in the number of potential interactions that need to be modeled. This scalability problem renders probability models of social interactions computationally infeasible for all but the smallest networks. In this paper, we develop a probabilistic framework for modeling customer interactions that is both grounded in the theory of homophily and is flexible enough to account for random variation in who interacts with whom. In particular, we present a novel Bayesian nonparametric approach, using Dirichlet processes, to moderate the scalability problems that marketing researchers encounter when working with networked data. We find that this framework is a powerful way to draw insights into latent similarities of customers, and we discuss how marketers can apply these insights to segmentation and targeting activities.
The World Wide Web contains a vast corpus of consumer-generated content that holds invaluable insights for improving the product and service offerings of firms. Yet the typical method for extracting diagnostic information from online content—text mining—has limitations. As a starting point, we propose analyzing a sample of comments before initiating text mining. Using a combination of real data and simulations, we demonstrate that a sampling procedure that selects respondents whose comments contain a large amount of information is superior to the two most popular sampling methods—simple random sampling and stratified random sampling—-in gaining insights from the data. In addition, we derive a method that determines the probability of observing diagnostic information repeated a specific number of times in the population, which will enable managers to base sample size decisions on the trade-off between obtaining additional diagnostic information and the added expense of a larger sample. We provide an illustration of one of the methods using a real data set from a website containing qualitative comments about staying at a hotel and demonstrate how sampling qualitative comments can be a useful first step in text mining.
Using the Bayes factor estimated by harmonic mean [Newton, M. A., A. E. Raftery. 1994. Approximate Bayesian inference by the weighted likelihood bootstrap. J. Roy. Statist. Soc. Ser. B.56(1) 3–48] to compare models with and without cross-brand pass-through, Dubé and Gupta [Dubé, J.-P., S. Gupta. 2008. Cross-brand pass-through in supermarket pricing. Marketing Sci.27(3) 324–333] found that, in the refrigerated orange juice category, a model with cross-brand pass-through was selected 68% of the time. However, Lenk [Lenk, P. J. 2009. Simulation pseudo-bias correction to the harmonic mean estimator of integrated likelihoods. J. Comput. Graph. Statist.18(1) 941–960] has demonstrated that the infinite variance harmonic mean estimator often exhibits simulation pseudo-bias in favor of more complex models. We replicate the results of Dubé and Gupta in the refrigerated orange juice category and then show that any of three more stable finite variance estimators select the model with cross-brand pass-through less than 1% of the time. Relaxing the assumption that model errors are distributed normally eliminates all instances in which the cross-brand pass-through model is selected. In 10 additional categories, the harmonic-mean-estimated Bayes factor selects the model with cross-brand pass-through 69% of the time, whereas a finite variance estimator of the Bayes factor selects the model with cross-brand pass-through only 5% of the time. Applying arguments in McAlister [McAlister, L. 2007. Cross-brand pass-through: Fact or artifact? Marketing Sci.26(6) 876–898], these 5% of cases can be attributed to capitalization on chance. We conclude that Dubé and Gupta should not be interpreted as providing evidence of cross-brand pass-through.
Greg M. Allenby (“Multiple-Constraint Choice Models with Corner and Interior Solutions”) is the Helen C. Kurtz Chair in Marketing at Ohio State University. He specializes in the study of economic and statistical issues in marketing. His research deals with developing insights about consumer behavior from customer data routinely collected by most organizations. These insights are used to develop and improve product development, pricing, promotion, market segmentation, and target marketing activities. He is a fellow of the American Statistical Association, coauthor of Bayesian Statistics and Marketing (Wiley, 2005), and coeditor of Quantitative Marketing and Economics.André Bonfrer (“Scalable Inference of Customer Similarities from Interactions Data Using Dirichlet Processes”) is a professor of marketing at the School of Management, Marketing and International Business, Australian National University. He holds a Ph.D. degree in business and an MBA from the Graduate School of Business, University of Chicago. His research interests are in developing and applying econometric models in a variety of marketing science applications, focusing predominately on areas such as market response modeling, database marketing, retailing, telecommunications, advertising, and pricing.Michael Braun (“Scalable Inference of Customer Similarities from Interactions Data Using Dirichlet Processes”) is the Homer A. Burnell (1928) Career Development Professor and assistant professor of management science at the MIT Sloan School of Management. The core of his research program is in developing probability models to uncover patterns of customer behavior from complex data structures in business and marketing contexts, and in using those models to address practical marketing and management issues. He has written on, spoken on, and taught about applications of probability models to marketing problems as diverse as forecasting, customer retention, marketing returns on investment, social networking models, segmentation and targeting strategies, and real-time customization of website design. He also involved in developing efficient Bayesian statistical methods for the analysis of large data sets to meet the needs of managers in an increasingly data-driven marketplace.Jason A. Duan (“Commentary—Reexamining Bayesian Model-Comparison Evidence of Cross-Brand Pass-Through”) is an assistant professor of marketing at the McCombs School of Business, University of Texas at Austin. He received his Ph.D. in statistics and master's in economics from Duke University, and he was a postdoctoral research associate at the School of Management, Yale University before joining the University of Texas at Austin. His research interests are in the quantitative marketing areas, including structural and Bayesian statistical models. His research has appeared in academic journals such as Biometrika and the Journal of Marketing Research.Pushan Dutt (“Crisis and Consumption Smoothing”) is an associate professor of economics and political science at INSEAD, Singapore. He holds a Ph.D. in economics from New York University, a master's in economics from the Delhi School of Economics, and a bachelor's in economics from Presidency College, Kolkata. His research interests include economic development, international trade, and international finance. His work lies at the intersection of politics, institutions, and international economics.Avi Goldfarb (“Online Display Advertising: Targeting and Obtrusiveness”; “Rejoinder—Implications of ‘Online Display Advertising: Targeting and Obtrusiveness’”) is an associate professor of marketing at the Rotman School of Management, University of Toronto. He received his Ph.D. from Northwestern University. His research primarily explores the impact of information technology on marketing, on universities, and on the economy. Other research examines brand value estimation and behavioral modeling in industrial organization.Sachin Gupta (“A Regime-Switching Model of Cyclical Category Buying”) is a professor of marketing and the Henrietta Johnson Louis Professor of Management at the Johnson Graduate School of Management at Cornell University. He received his Ph.D. from Cornell as well. His earlier papers have been honored with the O'Dell Award and the Paul Green Award of the American Marketing Association, and he is also the recipient of several teaching awards.Andreas Herrmann (“Gut Liking for the Ordinary: Incorporating Design Fluency Improves Automobile Sales Forecasts”) is a professor of marketing and director of the Center for Customer Insight at the University of St. Gallen, Switzerland.Steve Hillmer (“Efficient Methods for Sampling Responses from Large-Scale Qualitative Data”) is a professor in decision sciences at the School of Business, University of Kansas. He received his Ph.D. from the University of Wisconsin–Madison. His research has appeared in scholarly journals such as the Journal of the American Statistical Association, Journal of Financial Economics, Journal of Business and Economic Statistics, Applied Statistics, and Survey Methodology.Jaehwan Kim (“Multiple-Constraint Choice Models with Corner and Interior Solutions”) is an associate professor of marketing at the Korea University Business School in Seoul. He holds BBA (business) and MBA degrees from Korea University, an M.S. in statistics from the University of Iowa, and a Ph.D. in marketing from Ohio State University. His research is basically in model building for understanding market demand. He loves to talk about modeling research issues with his students and colleagues, especially at the “Modeling Lunch” seminar every Thursday.Aparna A. Labroo (“Gut Liking for the Ordinary: Incorporating Design Fluency Improves Automobile Sales Forecasts”) is an associate professor of marketing and the Robert King Steel Faculty Fellow at the Booth School of Business, University of Chicago.Jan R. Landwehr (“Gut Liking for the Ordinary: Incorporating Design Fluency Improves Automobile Sales Forecasts”) is an assistant professor of marketing at the University of St. Gallen, Switzerland. He holds a diploma degree in psychology from the University of Wuerzburg, Germany, and a Ph.D. in marketing from the University of St. Gallen, Switzerland.Leonard M. Lodish (“Commentary—When Is Less More, and How Much More? Thoughts on the Psychological and Economic Implications of Online Targeting and Obtrusiveness”) is the Samuel R. Harrell Professor; a professor of marketing; vice dean, Program for Social Impact; and leader and cofounder of the Global Consulting Practicum (GCP) at the Wharton School of the University of Pennsylvania.Mitchell J. Lovett (“The Seeds of Negativity: Knowledge and Money”) is an assistant professor of marketing at the University of Rochester, Simon Graduate School of Business. He received a Ph.D. in business administration from Duke University and an MBA from Boise State University. His research interests include advertising, targeted marketing, consumer learning, consumer decisions under uncertainty, and political marketing.Andrea M. Matwyshyn (“Commentary—Discussion of ‘Online Display Advertising: Targeting and Obtrusiveness’ by Avi Goldfarb and Catherine Tucker”) is an assistant professor in the Legal Studies and Business Ethics Department at the Wharton School of the University of Pennsylvania. She holds a Ph.D. and J.D. with honors from Northwestern University. She is a leading legal scholar in the fields of corporate data security, consumer information privacy, and the technology implications of contract law.Leigh McAlister (“Commentary—Reexamining Bayesian Model-Comparison Evidence of Cross-Brand Pass-Through”) is the Ed and Molly Smith Chair in Business Administration at the McCombs School of Business, University of Texas at Austin. She received her Ph.D. from Stanford University and served on the faculties of the University of Washington and the Massachusetts Institute of Technology before joining the University of Texas at Austin. Long associated with the Marketing Science Institute, she served there most recently as executive director. Her work with packaged goods manufacturers and grocery retailers influences her research and also motivated her collaboration with Barbara Kahn on the book Grocery Revolution: The New Focus on the Consumer. She strongly favors rigorous research that has managerial relevance.Carl F. Mela (“A Dynamic Model of Sponsored Search Advertising”) is the T. Austin Finch Foundation Professor of Marketing at Duke University, where he teaches brand management and the marketing core. His research focuses on the long-term effects of marketing activity, customer management, the Internet, and new media. His articles have appeared in the Journal of Marketing Research, Marketing Science, Journal of Marketing, Harvard Business Review, and Journal of Consumer Research, and they have received or been nominated for more than 20 best paper awards. His home page is located at http://www.duke.edu/∼mela.V. Padmanabhan (“Crisis and Consumption Smoothing”) is the John H. Loudon Professor of International Management at INSEAD, Singapore. His research has generated numerous honors, including recognition as among the top 10 most influential papers published in the 50 years of publication of Management Science (1954–2004). His current research interests include the implications of economic crises, business opportunities and challenges in developing economies, and social networks.Sungho Park (“A Regime-Switching Model of Cyclical Category Buying”) is an assistant professor of marketing at the W. P. Carey School of Business, Arizona State University. He received his Ph.D. in marketing from the Johnson School of Management, Cornell University. His research interests include dynamics in consumer choice and applications of structural econometric models to marketing.Americus Reed II (“Commentary—When Is Less More, and How Much More? Thoughts on the Psychological and Economic Implications of Online Targeting and Obtrusiveness”) is the Whitney M. Young, Jr. Associate Professor and associate professor of marketing at the Wharton School of the University of Pennsylvania.Takuya Satomura (“Multiple-Constraint Choice Models with Corner and Interior Solutions”) is a professor of marketing at Keio University's Faculty of Business and Commerce in Tokyo, Japan. He holds a B.S. in geography from the University of Tokyo, an MBA from the University of Tsukuba, and a Ph.D. in economics from Osaka University. His research interest includes consumer judgment and decision making, and modeling of consumer behavior.Ron Shachar (“The Seeds of Negativity: Knowledge and Money”) is a professor of marketing at the Arison School of Business, the Interdisciplinary Center Herzliya, Israel. He also has a visiting position in the Duke University's Fuqua School of Business since 2005. His research interests include advertising, branding, choice modeling, the entertainment industries, identity marketing, and political marketing. His work has been published in numerous academic journals. He serves as an AE at the Journal of Marketing Research and the Quantitative Marketing and Economics and is on the editorial boards of Marketing Science and the International Journal of Research in Marketing.Surendra N. Singh (“Efficient Methods for Sampling Responses from Large-Scale Qualitative Data”) is a professor of marketing and the Southwestern Bell Chair in Business, School of Business, University of Kansas, and a professor of health policy and management, School of Medicine, University of Kansas. He received his Ph.D. from the University of Wisconsin–Madison. His research has appeared in numerous scholarly journals, such as the Journal of Marketing Research, the Journal of Consumer Research, and the Journal of Marketing.Shameek Sinha (“Commentary—Reexamining Bayesian Model-Comparison Evidence of Cross-Brand Pass-Through”) is a Ph.D. candidate in the Department of Marketing at McCombs School of Business, University of Texas at Austin. He received his bachelor's in economics from Presidency College, Kolkata and his master's in economics from the Indian Statistical Institute, New Delhi as well as from the Department of Economics, University of Texas at Austin. His research interests include Bayesian econometric modeling of individual and firm behavior, optimal Bayesian decision theory, as well as game-theoretic models of contracts and bargaining. His dissertation work involves Bayesian modeling of donation behavior in nonprofit fundraising and optimal sequential solicitation decisions involving learning.Catherine Tucker (“Online Display Advertising: Targeting and Obtrusiveness”; “Rejoinder—Implications of ‘Online Display Advertising: Targeting and Obtrusiveness’”) is the Douglas Drane Career Development Professor in IT and Management and an assistant professor of marketing at the MIT Sloan School of Management. She received an undergraduate degree in politics, philosophy and economics from Oxford University and a Ph.D. in economics from Stanford University. Her areas of research include network externalities, online advertising, customer privacy, and Internet regulation.Ze Wang (“Efficient Methods for Sampling Responses from Large-Scale Qualitative Data”) is an assistant professor of marketing at the College of Business Administration, University of Central Florida, and received a Ph.D. from the University of Kansas.Song Yao (“A Dynamic Model of Sponsored Search Advertising”) is an assistant professor of marketing at the Kellogg School of Management at Northwestern University. He received his Ph.D. in business administration from Duke University and an M.A. in economics from the University of California, Los Angeles. His research focuses on Internet marketing, empirical industrial organization, auctions, competitive strategy, and customer management. His home page is located at http://www.songyao.org.
I introduce the work of the finalists in the 2009–2010 ISMS-MSI Practice Prize Competition, representing once again the best combinations of rigor and relevance produced by marketing scientists. The winning paper is by a team from several German universities and represents a collaboration with Bayer AG on an important problem—allocating the marketing budget dynamically across countries, products, and the marketing mix. The other three finalists described projects involving a branding project for an Australian airline, Jetstar, based on a dynamic choice model; a new tool for differentiating Prudential's variable annuities from the competition; and a marketing communications model for a small, family-run European office furniture supplier, Inofec.
Previous research on marketing budget decisions has shown that profit improvement from better allocation across products or regions is much higher than from improving the overall budget. However, despite its high managerial relevance, contributions by marketing scholars are rare.In this paper, we introduce an innovative and feasible solution to the dynamic marketing budget allocation problem for multiproduct, multicountry firms. Specifically, our decision support model allows determining near-optimal marketing budgets at the country–product–marketing–activity level in an Excel-supported environment each year. The model accounts for marketing dynamics and a product's growth potential as well as for trade-offs with respect to marketing effectiveness and profit contribution. The model has been successfully implemented at Bayer, one the world's largest pharmaceutical and chemical firms. The profit improvement potential is more than 50% and worth nearly €500 million in incremental discounted cash flows.
This paper describes the use of a marketing science model by Jetstar, a subsidiary of Australia's leading airline, Qantas, to effectively and profitably compete in the low-cost carrier marketplace. We trace the evolution of the Jetstar strategy from a baseline calibration of its initial position, to its efforts to attain price competitiveness and service parity, followed by its highly focused, cost-effective service delivery strategy. We develop a hierarchical model with parameters estimated at the individual level. This allows us to study not only how service design and pricing initiatives shift the perceived performance of Jetstar relative to its competitors but also how the airline can move market preferences toward areas in which it has competitive advantage. The contribution of the research is substantial. The Jetstar market share went from 14.0% to 18.1% during the first five quarterly waves of the research, and profits went from $79 million in 2006–2007, before the study was commissioned, to $124 million in 2008–2009.
A variable annuity is a popular product for investing retirement income. However, thousands of similar-looking variable annuity products are being offered by hundreds of financial service companies. In such a scenario, how can Prudential achieve meaningful product differentiation to increase the sales of its variable annuities? The solution led to the development and implementation of the “Emotion Quotient” (EQ) Tool. The EQ Tool enabled Prudential to redefine its marketing and sales approach along a proactive (as opposed to responsive) market orientation paradigm. This was accomplished by first using the EQ Tool to uncover and quantify the prevalence of certain emotions (such as fear and regret) in the prospective consumer and then pitching relevant variable annuity product(s) that could mitigate the specific behavioral risk corresponding to the prevalent emotion(s). This approach, which was backed by extensive research (as described in this study), enabled Prudential to gain over $450 million lift in variable annuity sales and contributed to consumer welfare by promoting awareness of behavioral risk to investors who are within five years of their retirement. This research study illustrates how industry can collaborate with academia to successfully apply marketing science to solve real-world business problems.
Inofec, a small- to medium-sized enterprise in the business-to-business sector, desired a more analytic approach to allocate marketing resources across communication activities and channels. We developed a conceptual framework and econometric model to empirically investigate (1) the marketing communication effects on off-line and online purchase funnel metrics and (2) the magnitude and timing of the profit impact of firm-initiated and customer-initiated contacts. We find evidence of many cross-channel effects, in particular, off-line marketing effects on online funnel metrics and online funnel metrics on off-line purchases. Moreover, marketing communication activities directly affect both early and later purchase funnel stages (website visits, online and off-line information, and quote requests). Finally, we find that online customer-initiated contacts have substantially higher profit impact than off-line firm-initiated contacts. Shifting marketing budgets toward these activities in a field experiment yielded net profit increases 14 times larger than those for the status quo allocation.
We study the bidding strategies of vertically differentiated firms that bid for sponsored search advertisement positions for a keyword at a search engine. We explicitly model how consumers navigate and click on sponsored links based on their knowledge and beliefs about firm qualities. Our model yields several interesting insights; a main counterintuitive result we focus on is the “position paradox.” The paradox is that a superior firm may bid lower than an inferior firm and obtain a position below it, yet it still obtains more clicks than the inferior firm. Under a pay-per-impression mechanism, the inferior firm wants to be at the top where more consumers click on its link, whereas the superior firm is better off by placing its link at a lower position because it pays a smaller advertising fee, but some consumers will still reach it in search of the higher-quality firm. Under a pay-per-click mechanism, the inferior firm has an even stronger incentive to be at the top because now it only has to pay for the consumers who do not know the firms' reputations and, therefore, can bid more aggressively. Interestingly, as the quality premium for the superior firm increases, and/or if more consumers know the identity of the superior firm, the incentive for the inferior firm to be at the top may increase. Contrary to conventional belief, we find that the search engine may have the incentive to overweight the inferior firm's bid and strategically create the position paradox to increase overall clicks by consumers. To validate our model, we analyze a data set from a popular Korean search engine firm and find that (i) a large proportion of auction outcomes in the data show the position paradox, and (ii) sharp predictions from our model are validated in the data.
The critical role of research and development (R&D) and advertising in the marketing strategy of the firm is well established. This paper conceptually and empirically examines why and how much the effectiveness of these two marketing instruments differs between times of economic expansions versus periods of economic contractions—and whether these results depend on the cyclicality of the industry in question. We consider a key marketing metric (market share) and a key financial metric (firm profit). Our empirical setting is 1,175 U.S. firms across a time period spanning over three decades. We find that R&D and advertising contribute to firm performance but that their effectiveness is not constant across the business cycle. Increasing advertising share in contractions has a stronger effect on profit and market share than increasing advertising share in expansions. Likewise, investments in R&D in contractions lead to higher gains in market share and profit than R&D investments in expansions, albeit only in subsequent years. If in contractions the firm faces tight budget constraints and has to choose between either maintaining R&D or advertising, our simulation results show that maintaining R&D is associated with better company performance. We find that advertising effectiveness, in general, and in contractions, in particular, is systematically moderated by the degree of cyclicality of the industry in which the firm operates. In relatively stable industries, advertising effects are small or even nonsignificant, and they do not go beyond the year the firm advertises. However, in highly cyclical industries, advertising effects are long-lasting, its total effect being 50% larger (market share) and 200% larger (profits) than in industries of average cyclicality. The effect of industry cyclicality on advertising effectiveness is especially pronounced in contractions. Collectively, these findings provide valuable and actionable insights into how firms should respond to contractions in order to grow profits and market share.
Many online shoppers initially acquired through paid search advertising later return to the same website directly. These so-called “direct type-in” visits can be an important indirect effect of paid search. Because visitors come to sites via different keywords and can vary in their propensity to make return visits, traffic at the keyword level is likely to be heterogeneous with respect to how much direct type-in visitation is generated.Estimating this indirect effect, especially at the keyword level, is difficult. First, standard paid search data are aggregated across consumers. Second, there are typically far more keywords than available observations. Third, data across keywords may be highly correlated. To address these issues, the authors propose a hierarchical Bayesian elastic net model that allows the textual attributes of keywords to be incorporated.The authors apply the model to a keyword-level data set from a major commercial website in the automotive industry. The results show a significant indirect effect of paid search that clearly differs across keywords. The estimated indirect effect is large enough that it could recover a substantial part of the cost of the paid search advertising. Results from textual attribute analysis suggest that branded and broader search terms are associated with higher levels of subsequent direct type-in visitation.
What the firm should say in an advertising message, the choice of content, is a critical managerial decision. Here, we focus on a particular aspect of the advertising content choice: an attribute-focused appeal versus an appeal with no direct information on product attributes. We make two assumptions that capture the reality of the advertising context. First, we assume that the bandwidth of advertising is limited: a firm can only communicate about a limited number of attributes. Second, we assume that consumers are active: they can choose to engage in a costly search to obtain additional product-related information. In this setting, we show that there exists an equilibrium where the high-quality firm chooses to produce messages devoid of any attribute information in order to invite the consumer to engage in search, which is likely to uncover positive information about the product. Whereas most of the previous literature has focused on the decision to advertise as a signal of quality, we show that message content, coupled with consumer search, can also serve as a credible signal of quality. In an extension, we show that our results are robust to endogenizing the firm's decision on the amount of advertising spending.
Marketing expenditures in the form of pricing, product development, promotion, and channel development are made to maximize profits. A challenge in evaluating the effectiveness of these expenditures is that decisions such as whether to lower prices or run promotions are made based on managers' knowledge of how sensitive consumers are to these marketing activities. Although marketing control variables are explanatory of sales, they are often set in anticipation of a market response, which reflects strategic behavior on the part of a firm. A challenge in developing a model of strategic behavior is that the process by which marketing expenditures are made is often not directly observable. We propose tests for comparing supply-side model formulations in which input variables are strategically determined. In these models, the joint likelihood of demand (y) and supply (x) can be factored into a conditional factor of demand given supply and into a marginal factor of supply. We illustrate our approach using data from a services company that operates in multiple geographic regions.
Interpersonal communications have long been recognized as an influential source of information for consumers. Internet-based media have facilitated information exchange among firms and consumers, as well as observability and measurement of such exchanges. However, much of the research addressing online communication focuses on ratings collected from online forums. In this paper, we look beyond ratings to a more comprehensive view of online communications. We consider the sales effect of the volume of positive, negative, and neutral online communications captured by Web crawler technology and classified by automated sentiment analysis. Our modeling approach captures two key features of our data, dynamics and endogeneity. In terms of dynamics, we model daily measures of online communications about a firm and its products as contributing to a latent demand-generating stock variable. To account for the endogeneity, we extend the latent instrumental variable technique to account for dynamic endogenous regressors. Our results demonstrate a significant effect of positive, negative, and neutral online communications on daily sales performance. Failure to account for endogeneity results in a severe attenuation of the estimated effects. From a managerial perspective, we demonstrate the importance of accounting for communication valence as well as the impact of shocks to positive, negative, and neutral online communications.
In marketing applications, it is common that some key covariates in a regression model, such as marketing mix variables or consumer profiles, are subject to missingness. The convenient method that excludes the consumers with missingness in any covariate can result in a substantial loss of efficiency and may lead to strong selection bias in the estimation of consumer preferences and sensitivities. To solve these problems, we propose a new Bayesian distribution-free approach, which can ensure that no customer is left behind in the analysis as a result of missing covariates. In this way, all customers are being considered in devising managerial policies. The proposed approach allows for flexible modeling of a joint distribution of multidimensional interrelated covariates that can contain both continuous and discrete variables. At the same time, it minimizes the impact of distributional assumptions involved in covariate modeling because the method does not require researchers to specify parametric distributions for covariates and can automatically generate suitable distributions for missing covariates. We have developed an efficient Markov chain Monte Carlo algorithm for inference. Besides robustness and flexibility, the proposed approach reduces modeling and computational efforts associated with missing covariates and therefore makes the missing covariate problems easier to handle. We evaluate the performance of the proposed method using extensive simulation studies. We then illustrate the method in two real data examples in which missing covariates occur: a mixed multinomial logit discrete-choice model in a ketchup data set and a hierarchical probit purchase incidence model in a retail store data set. These analyses demonstrate that the proposed method overcomes several important limitations of existing approaches for solving missing covariate problems and offers opportunities to make better managerial decisions with the current available marketing databases. Although our applications focus on consumer-level data, the proposed method is general and can be applied to other marketing applications where other types of marketing players are the units of analysis.
Online retailing provides an opportunity for new pricing options that are not feasible in traditional retail settings. This paper proposes an interactive, dynamic pricing strategy from the perspective of customized bundling to derive savings for customers while maximizing profits for electronic retailers (“e-tailers”). Given product costs, posted prices, shipping fees, and customers' reservation prices, we propose a nonlinear mixed-integer programming model to increase e-tailers' profits by sequentially pricing customized bundles. The model is flexible in terms of the number and variety of products customers may choose to incorporate during the various stages of their online shopping. Our computational study suggests that the proposed model not only attracts more customers to purchase the discounted bundle but also noticeably increases profits for e-tailers. This online dynamic bundle pricing model is robust under various bundle sizes and scenarios. It improves e-tailer profit and customer savings the most when facing divergent views about product values, lower budgets, and higher cost ratios.
Sönke Albers (“Dynamic Marketing Budget Allocation Across Countries, Products, and Marketing Activities”) is a newly appointed professor of marketing and innovation and Dean of Research at Kühne Logistics University in Hamburg, Germany. He holds a Ph.D. in operations research from the University of Hamburg, and his research interests lie in the areas of marketing planning and sales management. He is a fellow of the European Marketing Academy and served as president of the German Academic Association for Business Research, which comprises nearly all 1,800 business professors in Germany, Austria, and Switzerland.Greg M. Allenby (“Testing Models of Strategic Behavior Characterized by Conditional Likelihoods”) is the Helen C. Kurtz Chair in Marketing at the Ohio State University. His research deals with developing new insights about consumer behavior from customer data routinely collected by most organizations; these insights are used to develop and improve product development, pricing, promotion, market segmentation, and target marketing activities. He is a fellow of the American Statistical Association, coauthor of Bayesian Statistics and Marketing (Wiley 2005), and coeditor of Quantitative Marketing and Economics.Joep Arts (“Marketing's Profit Impact: Quantifying Online and Off-line Funnel Progression”) is an assistant professor in the marketing department, VU University Amsterdam, specializing in research and teaching on innovation diffusion, return on marketing, and word of mouth. He joined the VU University Amsterdam to obtain a Ph.D. in marketing under the supervision of Professor R. T. Frambach; during his Ph.D. work, he was as a visiting scholar at the University of Southern California, where he worked as a managing director for the Center for Global Innovation. In 2007, he cofounded his own company, Oxyme.Randolph E. Bucklin (“Modeling Indirect Effects of Paid Search Advertising: Which Keywords Lead to More Future Visits?”) is the Peter W. Mullin Professor at the UCLA Anderson School, where he has been on the faculty since 1988. He holds a Ph.D. in business (marketing), an M.S. in statistics from Stanford University, and an A.B. in economics from Harvard University. His research interests are in the quantitative analysis of customer behavior, and he specializes in models using historical records of customer transactions from scanner and Internet data.Peter J. Danaher (“Applying a Dynamic Model of Consumer Choice to Guide Brand Development at Jetstar Airways”) is a professor of marketing and econometrics of the Department of Marketing at Monash University; he was previously the Coles Myer Chair of Marketing and Retailing at the Melbourne Business School in Australia. His primary research interests are media exposure distributions, advertising effectiveness, television audience measurement and behavior, Internet usage behavior, customer satisfaction measurement, forecasting, and sample surveys. He serves on the editorial boards for the Journal of Marketing, the Journal of Marketing Research, Marketing Science, and the Journal of Service Research and is also an area editor for the International Journal of Research in Marketing.Eric (Er) Fang (“The Impact of Economic Contractions on the Effectiveness of R&D and Advertising: Evidence from U.S. Companies Spanning Three Decades”) is an assistant professor of marketing and the James F. Towey Faculty Fellow of the College of Business at the University of Illinois at Urbana–Champaign. His research focuses on marketing strategy, innovation, and relationship marketing and has appeared in Marketing Science, the Journal of Marketing, the Journal of Marketing Research, Organization Science, and the Journal of International Business Studies, among others. He received the MSI Young Scholar Award in 2010 and AMA SERVSIG best paper award in 2009.Marc Fischer (“Dynamic Marketing Budget Allocation Across Countries, Products, and Marketing Activities”) holds the Chair of Marketing and Market Research at the University of Cologne, Germany. His expertise includes the measurement and management of marketing performance, brand management, and the optimization of the marketing mix. His research appears in journals such as Marketing Science, the Journal of Marketing Research, Quantitative Marketing and Economics, and Interfaces.Monika Frie (“Dynamic Marketing Budget Allocation Across Countries, Products, and Marketing Activities”) is the head of Global Business Support at Bayer Schering Pharma. She serves internal clients with expertise in marketing and sales excellence, market research, competitive intelligence, and digital marketing. She obtained her Ph.D. in chemistry at the Leibniz University of Hannover and worked in various national and international positions in research, sales, and marketing.Timothy J. Gilbride (“Testing Models of Strategic Behavior Characterized by Conditional Likelihoods”) is an associate professor of marketing in the Mendoza College of Business at the University of Notre Dame. He has an undergraduate degree in economics from the University of Dayton and master's and Ph.D. degrees in marketing from the Ohio State University. His research interests focus on the application of Bayesian statistical methods to investigate marketing problems, particularly in the areas of consumer choice, modeling heterogeneity, and managerial decision models.Kinshuk Jerath (“A ‘Position Paradox’ in Sponsored Search Auctions”) is an assistant professor of marketing at the Tepper School of Business at Carnegie Mellon University. He received a B.Tech. degree in computer science and engineering from the Indian Institute of Technology Bombay and a Ph.D. degree in marketing from The Wharton School of the University of Pennsylvania. His research interests are twofold—theoretical models that help to obtain deeper understanding of marketing phenomena, especially phenomena related to retailing, and applied statistical models that support marketing analysts and decision makers.Yuanchun Jiang (“Optimizing E-tailer Profits and Customer Savings: Pricing Multistage Customized Online Bundles”) is a Ph.D. student at the Institute of Electronic Commerce, School of Management, Hefei University of Technology. He is currently a visiting Ph.D. student in the Joseph M. Katz Graduate School of Business at the University of Pittsburgh. He has published papers in journals such as Decision Support Systems, Expert Systems with Applications, and Knowledge-Based Systems.Chris F. Kemerer (“Optimizing E-tailer Profits and Customer Savings: Pricing Multistage Customized Online Bundles”) is the David M. Roderick Professor of Information Systems and a professor of business administration at the Joseph M. Katz Graduate School of Business, University of Pittsburgh. He received his Ph.D. in systems sciences (information systems) from Carnegie Mellon University. He has published more than 60 papers in journals such as Management Science, Information Systems Research, MIS Quarterly, IEEE Transactions on Software Engineering, Communications of the ACM, and Sloan Management Review.V. Kumar (“Uncovering Implicit Consumer Needs for Determining Explicit Product Positioning: Growing Prudential Annuities' Variable Annuity Sales”) is the Richard and Susan Lenny Distinguished Chair Professor of Marketing, executive director of the Center for Excellence in Brand & Customer Management, and director, Ph.D. Program in Marketing, at the J. Mack Robinson College of Business, Georgia State University. He has been recognized with over 25 teaching and research excellence awards, including seven lifetime achievement awards. He has published over 150 articles and books, including Managing Customers for Profit, Customer Lifetime Value: The Path to Profitability, and Customer Relationship Management: A Databased Approach.Yezheng Liu (“Optimizing E-tailer Profits and Customer Savings: Pricing Multistage Customized Online Bundles”) is a professor of electronic commerce, School of Management, Hefei University of Technology. He received his Ph.D. in management science and engineering from Hefei University of Technology. He has published papers in journals such as Decision Support Systems, Expert Systems with Applications, Knowledge-Based Systems, and the International Journal of Knowledge and Systems Sciences.Liye Ma (“A ‘Position Paradox’ in Sponsored Search Auctions”) is a doctoral candidate at the Tepper School of Business at Carnegie Mellon University. He received a B.Eng. degree in computer science from Tsinghua University and an Sc.M. degree in computer science from Brown University. His research focuses on technology-enabled dynamic marketing interactions, primarily within the context of the Internet and social media, using both theoretical and empirical models.Dina Mayzlin (“Uninformative Advertising as an Invitation to Search”) is an associate professor of marketing at the Yale School of Management. Her research focuses on social interactions (such as word of mouth)—in particular, on what roles the firm can play to manage these interactions—and her other interests include issues relating to advertising and sales compensation. Her papers have appeared in Marketing Science and the Journal of Marketing Research and have won the Frank M. Bass Award and the William F. O'Dell Award.Leigh McAlister (“A Dynamic Model of the Effect of Online Communications on Firm Sales”) is the Ed and Molly Smith Chair in Business Administration at the McCombs School of Business, University of Texas at Austin. She received her Ph.D. from Stanford University and served on the faculties of University of Washington and the Massachusetts Institute of Technology before joining University of Texas at Austin. Long associated with the Marketing Science Institute, she served there most recently as executive director.Thomas Otter (“Testing Models of Strategic Behavior Characterized by Conditional Likelihoods”) is a professor of marketing in the Faculty of Economics and Business Administration at Goethe University, Frankfurt. He received his Ph.D. from the Vienna University of Economics and Business Administration (WU-Wien). His research interests are in the development and application of Bayesian techniques to help conceptualize and solve problems in marketing and marketing research.Young-Hoon Park (“A ‘Position Paradox’ in Sponsored Search Auctions”) is the AmorePacific Professor of Management and an associate professor of marketing at the Johnson Graduate School of Management, Cornell University. He received his Ph.D. from the University of Pennsylvania. His research emphasizes the development of methods for improving marketing decisions and has appeared in leading marketing and statistics journals, such as the Journal of Marketing Research, Management Science, Marketing Letters, Marketing Science, and Journal of the Royal Statistical Society: Series A.Koen Pauwels (“Marketing's Profit Impact: Quantifying Online and Off-line Funnel Progression”) is an associate professor at Özyeğin University, İstanbul, and at the Tuck School of Business at Dartmouth, where he teaches and researches marketing, statistics, and return on marketing investment. He received his Ph.D. in management from the University of California at Los Angeles, won the European Marketing Academy's 2001 Best Paper Award, and won the 2007 O'Dell Award for the most influential paper in the Journal of Marketing Research. He serves on the editorial boards of the International Journal of Research in Marketing, Journal of Marketing, Journal of Marketing Research, and Marketing Science.Yi Qian (“No Customer Left Behind: A Distribution-Free Bayesian Approach to Accounting for Missing Xs in Marketing Models”) is an assistant professor of marketing and the Kraft Research Professor at Kellogg School of Management, Northwestern University. She holds an A.B. in economics, an A.M. in statistics, and a Ph.D. in economics, all from Harvard University. Her current research interests focus on issues relating to brand management against counterfeits, intellectual property rights, causal inference methodology, entrepreneurship in emerging markets, and China.John H. Roberts (“Applying a Dynamic Model of Consumer Choice to Guide Brand Development at Jetstar Airways”) is a professor of marketing at the Australian National University and London Business School, as well as an Emeritus Scientia Professor at the University of New South Wales. He has won the American Marketing Association's John Howard, William O'Dell, and ART Forum Best Paper Awards, and he has been a finalist in the John D. C. Little Award three times and ISMS Practice Award twice. He was an intelligence officer in the Royal Australian Air Force, on the executive committee of the Second Australian University Arts Council, Telstra's market planning director, and founder and chairman of Marketing Insights.Ken Roberts (“Applying a Dynamic Model of Consumer Choice to Guide Brand Development at Jetstar Airways”) is the managing partner of the Australian research firm Forethought; he is an honorary fellow at Monash and Melbourne Universities and is a former associate professor in marketing research at the Melbourne Business School. He has a B.Bus in marketing and an MBA from the Melbourne Business School. His research focus is on brand measurement and neuromarketing methods for revealing the blend and extent of consumers' nonconscious emotional response to brands and advertising.Oliver J. Rutz (“Modeling Indirect Effects of Paid Search Advertising: Which Keywords Lead to More Future Visits?”; “A Dynamic Model of the Effect of Online Communications on Firm Sales”) is an assistant professor of marketing at the Foster School of Business, University of Washington, Seattle; he was previously on the faculty of the Yale School of Management from 2007 to 2011. He received his Ph.D. in marketing from the UCLA Anderson School of Management in 2007. He won the 2007 EMAC best dissertation paper award and an honorable mention in the 2007 Alden G. Clayton Doctoral Dissertation Proposal Competition. He is a member of the Handelsblatt-Management-Forum, a bimonthly international academic panel in Germany's leading business and financial newspaper.Denish Shah (“Uncovering Implicit Consumer Needs for Determining Explicit Product Positioning: Growing Prudential Annuities' Variable Annuity Sales”) is an assistant professor of marketing and the assistant director of the Center for Excellence in Brand and Customer Management at the J. Mack Robinson College of Business of Georgia State University in Atlanta. His research focuses on issues pertaining to the impact of marketing on firm performance and has been published in journals such as the Journal of Marketing, Marketing Science, the Journal of Retailing, and the Journal of Service Research. Besides academic research, he has executed several research-based consulting projects for Fortune 500 firms.Jennifer Shang (“Optimizing E-tailer Profits and Customer Savings: Pricing Multistage Customized Online Bundles”) is an associate professor at the Joseph M. Katz Graduate School of Business, University of Pittsburgh. She received her Ph.D. in operations management from the University of Texas at Austin. She has published nearly 40 papers in journals such as Management Science, the Journal of Marketing, Information Systems Research, the European Journal of Operational Research, and IEEE Transactions on Engineering Management, as well as a popular management science book in China.Jiwoong Shin (“Uninformative Advertising as an Invitation to Search”) is an associate professor of marketing at the School of Management, Yale University. He holds an M.S. and B.S. from Seoul National University, as well as a Ph.D. from the Massachusetts Institute of Technology. His current research focuses on analytical modeling of strategic interactions between firms and consumers—in particular, consumer search theory, advertising, pricing strategies, and customer relationship management.Alan Simpson (“Applying a Dynamic Model of Consumer Choice to Guide Brand Development at Jetstar Airways”) is the principal consultant and banking and finance portfolio manager within the marketing science team at Forethought, where he has had an integral role in the development of new research methodologies and processes. He holds an honors degree in mathematics from the University of Western Australia and is currently enrolled in Ph.D. studies in statistics at the University of Melbourne. His particular expertise is in the area of market segmentation, with interests in multivariate analysis, linear and nonlinear modeling, and multivariate methods including cluster analysis and principal components analysis, as well as parametric and nonparametric time-series methods.Garrett P. Sonnier (“A Dynamic Model of the Effect of Online Communications on Firm Sales”) is an assistant professor of marketing at the McCombs School of Business at the University of Texas at Austin. He received his Ph.D. in marketing from the UCLA Anderson School of Management. His current research interests are in the area of product management, product and brand perceptions, and pricing; his research has been accepted for publication in Quantitative Marketing and Economics, Marketing Science, and the Journal of Marketing Research.Kannan Srinivasan (“A ‘Position Paradox’ in Sponsored Search Auctions”) is the Rohet Tolani Distinguished Professor of International Business and H.J. Heinz II Professor of Management, Marketing and Information Systems at the Tepper School of Business at Carnegie Mellon University. He is currently on the advisory board of Marketing Science and is an area editor of Quantitative Marketing and Economics and an associate editor of Management Science. He has published over 50 papers in leading journals such as Management Science, Marketing Science, the Journal of Marketing Research, the Journal of Marketing, the Journal of the American Statistics Association, and the Journal of Business.Jan-Benedict E. M. Steenkamp (“The Impact of Economic Contractions on the Effectiveness of R&D and Advertising: Evidence from U.S. Companies Spanning Three Decades”) is the C. Knox Massey Distinguished Professor of Marketing and Marketing Area Chair, Kenan-Flagler Business School, University of North Carolina at Chapel Hill, and executive director of AiMark, which brings together academics, market research companies (GfK and Kantar), and consumer packaged goods (CPG) companies with the mission to be the leader in CPG consumer and market knowledge. His most recent book, Private Label Strategy: How to Meet the Store Brand Challenge (with Nirmalya Kumar; published by Harvard Business School Press), has been translated in simple and complex Chinese, Portuguese, and Polish, with translation rights sold for Spanish and Russian editions as well as an Indian edition by Macmillan. He was awarded the Dr. Hendrik Muller lifetime prize for the behavioral and social sciences, given by the Royal Netherlands Academy of Sciences, and received the Doctor Mercaturae Honoris Causa from Aarhus University.Michael Trusov (“Modeling Indirect Effects of Paid Search Advertising: Which Keywords Lead to More Future Visits?”) is an assistant professor of marketing at the Robert H. Smith School of Business at the University of Maryland. He received his Ph.D. degree from the UCLA Anderson School of Management and also holds a master's degree in computer science and a master's degree in business administration. He is a winner of MSI's Alden Clayton Award, a finalist for the Paul Green Award, a winner of the Emerald Management Reviews Citation of Excellence Award, a runner-up for the Paul Root Award, and a finalist for the Harold H. Maynard Award.Nils Wagner (“Dynamic Marketing Budget Allocation Across Countries, Products, and Marketing Activities”) is a Ph.D. candidate and research assistant at the Chair of Business Administration with a specialization in marketing and services at the University of Passau, Germany. His research focuses on marketing budget allocation processes. He has studied business administration as well as economics at the University of Kiel, Germany, and the University of Warwick, United Kingdom.Thorsten Wiesel (“Marketing's Profit Impact: Quantifying Online and Off-line Funnel Progression”) is an assistant professor in the marketing department, University of Groningen. He finished his Ph.D. at the Chair of Electronic Commerce and the E Finance Lab at the Johann Wolfgang Goethe-University in Frankfurt/Main. During his Ph.D. work, he was a visiting scholar at IESE Business School, Barcelona (Spain), Penn State University, and Goizueta Business School, Emory University. In 2004, he was named an ISBM Business Marketing Doctoral Fellow, and at EMAC 2006, the award for the best paper based on a doctoral dissertation was awarded to him and his coauthors.Hui Xie (“No Customer Left Behind: A Distribution-Free Bayesian Approach to Accounting for Missing Xs in Marketing Models”) is an assistant professor of biostatistics at the University of Illinois at Chicago. He holds a B.S. from Peking University, an M.S. from Purdue University, and a Ph.D. from Columbia University. His current research interests include multivariate statistics, missing data methods, and Bayesian methods.
While millions of products are sold on its retail platform, Amazon.com itself stocks and sells only a very small fraction of them. Most of these products are sold by third-party sellers who pay Amazon a fee for each unit sold. Empirical evidence clearly suggests that Amazon tends to sell high-demand products and leave long-tail products for independent sellers to offer. We investigate how a platform owner such as Amazon, facing ex ante demand uncertainty, may strategically learn from these sellers' early sales which of the “mid-tail” products are worthwhile for its direct selling and which are best left for others to sell. The platform owner's “cherry-picking” of the successful products, however, gives an independent seller the incentive to mask any high demand by lowering his sales with a reduced service level (unobserved by the platform owner).We analyze this strategic interaction between a platform owner and an independent seller using a game-theoretic model with two types of sellers—one with high demand and one with low demand. We show that it may not always be optimal for the platform owner to identify the seller's demand. Interestingly, the platform owner may be worse off by retaining its option to sell the independent seller's product, whereas both types of sellers may benefit from the platform owner's threat of entry. The platform owner's entry option may reduce consumer surplus in the early period, although it increases consumer surplus in the later period. We also investigate how consumer reviews influence the market outcome.
Recent business research points to the fortune awaiting to be tapped in low-end markets. In this paper, we investigate how the size of the low-end market influences a firm's profits and the pioneering firm's quality choice. As low-valuation consumers increase in a market, on average, consumers' willingness to pay decreases. This may lead us to expect firms' profits to decrease as the size of the low-end market increases. Our analysis shows that, if the size of the low-end market is below a threshold, an increase in the size of the low-end market may actually dampen price competition and improve profits, as firms can then strategically choose their quality levels such that their products are more differentiated. Conventional wisdom also suggests that the pioneering firm will offer a higher-quality product and earn more profits compared with the later entrant. In contrast to this notion of quality advantage, our analysis identifies circumstances in which a pioneer can offer a lower-quality product and yet earn more profits. An experimental test lends support for some of our model's predictions. We further extend the model to consider markets with multiple firms, firms with multiple products, and consumers with limited purchasing power.
We develop a two-stage consumer-level model of paid search advertising response based on standard aggregated data provided to advertisers by major search engines such as Google or Bing. The proposed model uses behavioral primitives in accord with utility maximization and allows recovering parameters of the heterogeneity distribution in consumer preferences. The model is estimated on a novel paid search data set that includes information on the ad copy. To that end, we develop an original framework to analyze composition and design attributes of paid search ads. Our results allow us to correctly evaluate the effects of specific ad properties on ad performance, taking consumer heterogeneity into account. Another benefit of our approach is allowing recovery of preference correlation across the click-through and conversion stage. Based on the estimated correlation between price- and position-sensitivity, we propose a novel contextual targeting scheme in which a coupon is offered to a consumer depending on the position in which the paid search ad was displayed. Our analysis shows that total revenues from conversion can be increased using this targeting scheme while keeping cost constant.
We develop and test an active-machine-learning method to select questions adaptively when consumers use heuristic decision rules. The method tailors priors to each consumer based on a “configurator.” Subsequent questions maximize information about the decision heuristics (minimize expected posterior entropy). To update posteriors after each question, we approximate the posterior with a variational distribution and use belief propagation (iterative loops of Bayes updating). The method runs sufficiently fast to select new queries in under a second and provides significantly and substantially more information per question than existing methods based on random, market-based, or orthogonal-design questions.Synthetic data experiments demonstrate that adaptive questions provide close-to-optimal information and outperform existing methods even when there are response errors or “bad” priors. The basic algorithm focuses on conjunctive or disjunctive rules, but we demonstrate generalizations to more complex heuristics and to the use of previous-respondent data to improve consumer-specific priors. We illustrate the algorithm empirically in a Web-based survey conducted by an American automotive manufacturer to study vehicle consideration (872 respondents, 53 feature levels). Adaptive questions outperform market-based questions when estimating heuristic decision rules. Heuristic decision rules predict validation decisions better than compensatory rules.
Past research in marketing and psychology suggests that pricing structure may influence consumers' perception of value. In the context of two commonly used pricing schemes, pay-per-use and two-part tariff, we evaluate the impact of pricing structure on consumer preferences for access services. To this end, we develop a utility-based model of consumer retention and usage of a new service. A notable feature of the model is its ability to capture the pricing structure effect and measure its impact on consumer retention, usage, and pricing policy.Using data from a pricing field experiment for a new telecommunication service, we find that consumers derive lower utility from consumption under a two-part tariff than pay-per-use pricing, resulting in lower retention of customers and lower usage of the service. Specifically, our demand analysis shows that a two-part tariff structure leads to an average decline of 10.5% in the annual retention rate and an average decrease of 38.7% in yearly usage relative to pay-per-use pricing after controlling for income effects. Despite the higher customer churn and lower usage, we find that the two-part tariff is still the profit-maximizing pricing structure. However, our results show that if firms ignore the pricing structure (or access fee) effect, then they would overcharge customers for the access fee and undercharge them for the per-minute price. Translated in terms of profitability, the failure to account for the access fee effect leads to a reduction of 11% in firm profit.
Our main objective in this paper is to measure the value of customers acquired from Google search advertising accounting for two factors that have been overlooked in the conventional method widely adopted in the industry: (1) the spillover effect of search advertising on customer acquisition and sales in off-line channels and (2) the lifetime value of acquired customers. By merging Web traffic and sales data from a small-sized U.S. firm, we create an individual customer-level panel that tracks all repeated purchases, both online and off-line, and tracks whether or not these purchases were referred from Google search advertising.To estimate the customer lifetime value, we apply the methodology in the customer relationship management literature by developing an integrated model of customer lifetime, transaction rate, and gross profit margin, allowing for individual heterogeneity and a full correlation of the three processes. Results show that customers acquired through Google search advertising in our data have a higher transaction rate than customers acquired from other channels. After accounting for future purchases and spillover to off-line channels, the calculated value of new customers using our approach is much higher than the value obtained using conventional method. The approach used in our study provides a practical framework for firms to evaluate the long-term profit impact of their search advertising investment in a multichannel setting.
We analyze the impacts of social learning (SL) on the dynamic pricing and consumer adoption of durable goods in a two-period monopoly. Consumers can make either early, uninformed purchases or late but potentially informed purchases as a result of social learning. Several results are derived. First, we identify the market conditions under which ex ante homogeneous consumers may choose to purchase at different times. Second, equilibrium adoption may demonstrate inertia (where all adopt late) or frenzy (where all adopt early). In particular, adoption inertia appears when SL intensity is reasonably high but may vanish when SL intensity exceeds a certain threshold. Third, firm profits and social welfare first weakly decrease in SL intensity and may then jump up by a lump-sum amount at the threshold SL intensity level mentioned above. Last, we show that the firm potentially benefits from informative advertising or investing to cultivate more social learning.
Several researchers have proposed models of buyer behavior in noncontractual settings that assume that customers are “alive” for some period of time and then become permanently inactive. The best-known such model is the Pareto/NBD, which assumes that customer attrition (dropout or “death”) can occur at any point in calendar time. A recent alternative model, the BG/NBD, assumes that customer attrition follows a Bernoulli “coin-flipping” process that occurs in “transaction time” (i.e., after every purchase occasion). Although the modification results in a model that is much easier to implement, it means that heavy buyers have more opportunities to “die.”In this paper, we develop a model with a discrete-time dropout process tied to calendar time. Specifically, we assume that every customer periodically “flips a coin” to determine whether she “drops out” or continues as a customer. For the component of purchasing while alive, we maintain the assumptions of the Pareto/NBD and BG/NBD models. This periodic death opportunity (PDO) model allows us to take a closer look at how assumptions about customer death influence model fit and various metrics typically used by managers to characterize a cohort of customers. When the time period after which each customer makes her dropout decision (which we call period length) is very small, we show analytically that the PDO model reduces to the Pareto/NBD. When the period length is longer than the calibration period, the dropout process is “shut off,” and the PDO model collapses to the negative binomial distribution (NBD) model. By systematically varying the period length between these limits, we can explore the full spectrum of models between the “continuous-time-death” Pareto/NBD and the naïve “no-death” NBD.In covering this spectrum, the PDO model performs at least as well as either of these models; our empirical analysis demonstrates the superior performance of the PDO model on two data sets. We also show that the different models provide significantly different estimates of both purchasing-related and death-related metrics for both data sets, and these differences can be quite dramatic for the death-related metrics. As more researchers and managers make managerial judgments that directly relate to the death process, we assert that the model employed to generate these metrics should be chosen carefully.
Customer retention and customer churn are key metrics of interest to marketers, but little attention has been placed on linking the different reasons for which customers churn to their value to a contractual service provider. In this paper, we put forth a hierarchical competing-risk model to jointly model when customers choose to terminate their service and why. Some of these reasons for churn can be influenced by the firm (e.g., service problems or price–value trade-offs), but others are uncontrollable (e.g., customer relocation and death). Using this framework, we demonstrate that the impact of a firm's efforts to reduce customer churn for controllable reasons is mitigated by the prevalence of uncontrollable ones, resulting in a “damper effect” on the return from a firm's retention marketing efforts. We use data from a provider of land-based telecommunication services to demonstrate how the competing-risk model can be used to derive a measure of the incremental customer value that a firm can expect to accrue through its efforts to delay churn, taking this damper effect into account. In addition to varying across customers based on geodemographic information, the magnitude of the damper effect depends on a customer's tenure to date. We discuss how our framework can be used to tailor the firm's retention strategy to individual customers, both in terms of which customers to target and when retention efforts should be deployed.
We study a duopoly model where consumers are heterogeneous with respect to their willingness to pay for two product characteristics and marginal costs are increasing with the quality level chosen on each attribute. We show that although firms seek to manage competition through product positioning, their differentiation strategies critically depend on how costly it is to provide higher quality. When the cost of providing quality is not too high, firms use only one attribute to differentiate their products: they maximally differentiate on one dimension and minimally differentiate on the other (a Max-Min equilibrium). Furthermore, they always differentiate along the dimension with the greater attribute range. As for the dimension with the smaller range and along which they agglomerate, firms either choose the highest quality level or the lowest quality level possible, depending on whether the marginal costs of quality provision are low or intermediate, respectively. However, for larger quality provision costs, firms exploit both dimensions to differentiate their products. In particular, we characterize a maximal differentiation equilibrium in which one firm chooses the highest quality level on both attributes while its rival offers the lowest quality level on both attributes (a Max-Max equilibrium). We discuss the managerial implications of our findings and explain how they enrich and qualify previous results reported in the literature on two-dimensional differentiation models.
Recent research has empirically characterized the buyer–seller relationship as dynamically evolving from one discrete state to another. Conventional wisdom would suggest that a customer in a higher relationship state that has a higher transaction value would also have greater lifetime value to the firm. However, recent evidence suggests that higher relationship states can be ephemeral. Hence, the link between transaction value and lifetime value is not obvious. In this study, we seek to understand, within a specific empirical context, (i) the relationship between a customer's transaction value and that customer's lifetime value and (ii) the relationship between the lifetime value of a customer and the optimal level of marketing activity that needs to be directed at that customer. To this end, we develop a trivariate Tobit hidden Markov model that allows for (a) transitions among relationship states, (b) possible synergies between the various products that the supplier firm offers, (c) endogeneity in marketing activity, (d) heterogeneity in model parameters, and (e) the presence of the no-purchase option. Our results reinforce recent findings by Schweidel et al. [Schweidel, D. A., E. T. Bradlow, P. S. Fader. 2011. Portfolio dynamics for customers of a multiservice provider. Management Sci.57(3) 471–486] that higher relationship states can be short-lived. Importantly for the supplier firm, a customer in the highest relationship state in a given period does not yield the highest lifetime value to the firm. Hence, the relationship between transaction value (i.e., relationship state) and lifetime value can be nonmonotonic. At the same time, we also find a nonmonotonic relationship between the optimal expenditures that should be directed at a customer and that customer's lifetime value; i.e., the optimal level of marketing contacts is not the highest for customers with the highest lifetime value. Furthermore, we find that the optimal marketing expenditures for myopic agents are 14%–33% lower than the corresponding values for forward-looking agents. Therefore, not accounting for the long-term effects of marketing contacts would lead to suboptimal marketing budgets. Moreover, a comparison with the current marketing expenditures suggests that the current practice is closer to the myopic policy than to the forward-looking one.
Wilfred Amaldoss (“Competing for Low-End Markets”) is a professor of marketing at the Fuqua School of Business of Duke University. He holds an M.B.A. from the Indian Institute of Management (Ahmedabad), and an M.A. (applied economics) and a Ph.D. from the Wharton School of the University of Pennsylvania. His research interests include experimental economics, advertising, pricing, new product development, and social effects in consumption. His recent publications have appeared in Marketing Science, Management Science, the Journal of Marketing Research, the Journal of Economic Behavior and Organization, and the Journal of Mathematical Psychology.Michael Braun (“Modeling Customer Lifetimes with Multiple Causes of Churn”) is an associate professor of management science in the marketing group of the MIT Sloan School of Management. The core of his research program is in developing probability models to uncover patterns of customer behavior from complex data structures in business and marketing contexts, and in using those models to address practical marketing and management issues. He has written on, spoken on, and taught about applications of probability models to marketing problems as diverse as forecasting, customer retention, marketing returns on investment, social networking models, segmentation and targeting strategies, and real-time customization of website design. He is also involved in developing efficient Bayesian statistical methods for the analysis of large data sets to meet the needs of managers in an increasingly data-driven marketplace.Tat Y. Chan (“Measuring the Lifetime Value of Customers Acquired from Google Search Advertising”) is an associate professor of marketing at the Olin Business School, Washington University in St. Louis. He received his doctoral degree in economics from Yale University. His recent research focuses on the empirical studies of consumer choices and firm competition. His research has appeared in journals such as the RAND Journal of Economics, the Journal of Political Economy, Marketing Science, and the Journal of Marketing Research.Pradeep K. Chintagunta (“Assessing the Effect of Marketing Investments in a Business Marketing Context”) is the Robert Law Professor of marketing at the Booth School of Business at the University of Chicago. He graduated from Northwestern University and has also served on the faculty of the Johnson School, Cornell University. He is interested in studying the effectiveness of marketing activities in pharmaceutical markets, investigating aspects of technology product markets, studying online and off-line purchase behavior, and studying the analysis of household purchase behavior using scanner data.Peter J. Danaher (“The Impact of Tariff Structure on Customer Retention, Usage, and Profitability of Access Services”) is a professor of marketing and econometrics at Monash University in Melbourne, Australia. He has had visiting positions at London Business School, the Wharton School, New York University, and the Massachusetts Institute of Technology. He serves on the editorial boards for the Journal of Marketing, the Journal of Marketing Research, Marketing Science, and the Journal of Service Research, and he is also an area editor for the International Journal of Research in Marketing. His primary research interests are media exposure distributions, advertising effectiveness, television audience measurement and behavior, Internet usage behavior, customer satisfaction measurement, forecasting, and sample surveys, resulting in many publications in journals such as the Journal of Marketing Research, Marketing Science, the Journal of Marketing, the Journal of Advertising Research, the Journal of the American Statistical Association, the Journal of Retailing, the Journal of Business and Economic Statistics, and the American Statistician.Daria Dzyabura (“Active Machine Learning for Consideration Heuristics”) is a Ph.D. student at the MIT Sloan School of Management. She received an S.B. in mathematics from the Massachusetts Institute of Technology. Her research interests are machine learning, adaptive experimental design, heuristic decision processes, recommendation systems, selling strategies in the presence of self-reflection learning, and consumer response to recalls (analyzed from online textual data). Her earlier papers on cognitive simplicity and unstructured direct elicitation appeared in the Journal of Marketing Research.Skander Essegaier (“The Impact of Tariff Structure on Customer Retention, Usage, and Profitability of Access Services”) is an associate professor of marketing at Koç University in Istanbul, Turkey. He earned a Ph.D. from Columbia University in New York, an M.S. from the London School of Economics, and a B.A. from ENSAE in Paris. His research has focused on pricing, channels and retailing, and personalization. His work has been published in Marketing Science, the Journal of Marketing Research, Management Science, the Journal of Applied Probabilities, and the SIAM Journal on Control and Optimization.Peter S. Fader (“New Perspectives on Customer ‘Death’ Using a Generalization of the Pareto/NBD Model”) is the Frances and Pei-Yuan Chia Professor of Marketing and codirector of the Wharton Interactive Media Initiative at the Wharton School of the University of Pennsylvania. He loves messing around with “buy-till-you-die” models, such as the one developed in this issue. He is delighted that this research area is alive and well 25 years after it was first conceptualized by Schmittlein, Morrison, and Colombo; and he hopes that current and future researchers will continue to buy into it for years to come.Bruce G. S. Hardie (“New Perspectives on Customer ‘Death’ Using a Generalization of the Pareto/NBD Model”) is a professor of marketing at the London Business School. His primary research interest lies in the development of data-based models to support marketing analysts and decision makers, with a particular interest in models that are easy to implement. Most of his current projects focus on the development of probability models for customer-base analysis.John R. Hauser (“Active Machine Learning for Consideration Heuristics”) is the Kirin Professor of Marketing at the MIT Sloan School of Management, where he teaches new product development, marketing management, competitive marketing strategy, and research methodology. His awards include the Converse Award for contributions to the science of marketing and the Parlin Award for contributions to marketing research. He has consulted for a variety of corporations on product development, sales forecasting, marketing research, voice of the customer, defensive strategy, and R&D management. He is a founder and principal at Applied Marketing Science, Inc., is a former trustee of the Marketing Science Institute, is a fellow of INFORMS and of the INFORMS Society of Marketing Science, and serves on many editorial boards.Raghuram Iyengar (“The Impact of Tariff Structure on Customer Retention, Usage, and Profitability of Access Services”) is an assistant professor of marketing at the Wharton School of the University of Pennsylvania. He earned his Ph.D. from Columbia University and his B.Tech. from Indian Institute of Technology Kanpur, India. His research focuses on pricing and social networks. His work has been published in Marketing Science, the Journal of Marketing Research, Quantitative Marketing and Economics, and Psychometrika.Kamel Jedidi (“The Impact of Tariff Structure on Customer Retention, Usage, and Profitability of Access Services”) is the John A. Howard Professor of Marketing at Columbia Business School, Columbia University, New York. He holds a bachelor's degree in economics from the Faculté des Sciences Economiques de Tunis, Tunisia, and master's and Ph.D. degrees in marketing from the Wharton School of the University of Pennsylvania. His substantive research interests include pricing, product design and positioning, diffusion of innovations, market segmentation, and the long-term impact of advertising and promotions; his methodological interests lie in multidimensional scaling, classification, structural equation modeling, and Bayesian and finite-mixture models. He has published extensively in the leading marketing, statistics, and psychometric journals, the most recent of which include the Journal of Marketing Research, Marketing Science, Management Science, and Psychometrika.Kinshuk Jerath (“Firm Strategies in the ‘Mid Tail’ of Platform-Based Retailing”; “New Perspectives on Customer ‘Death’ Using a Generalization of the Pareto/NBD Model”) is an assistant professor of marketing at the Tepper School of Business at Carnegie Mellon University. He received a B.Tech. degree in computer science and engineering from the Indian Institute of Technology Bombay and a Ph.D. degree in marketing from the Wharton School of the University of Pennsylvania. His research interests are twofold: theoretical models that help to obtain deeper understanding of marketing phenomena, especially phenomena related to retailing, and applied statistical models that support marketing analysts and decision makers. His research has appeared in top-tier marketing journals such as Marketing Science, Management Science, the Journal of Marketing Research, and the Journal of Interactive Marketing.Baojun Jiang (“Firm Strategies in the ‘Mid Tail’ of Platform-Based Retailing”) is an assistant professor of marketing at the Olin Business School at the Washington University in St. Louis. He received a B.A. in economics and physics from Grinnell College, an M.S. in physics and an M.S. in electrical engineering from Stanford University, an M.B.A. from the University of Texas at Austin, and an M.S. and Ph.D. in industrial administration from Carnegie Mellon University. His research interests include new technology-enabled markets and business models, online word of mouth, platforms, software licensing innovations, and competitive strategies. His doctoral dissertation won the 2010 ISMS Doctoral Dissertation Award.Bing Jing (“Social Learning and Dynamic Pricing of Durable Goods”) has been an assistant professor of marketing at the Cheung Kong Graduate School of Business, Beijing, China since 2007. He received his Ph.D. from the William E. Simon Graduate School of Business Administration at University of Rochester in 2001. Between 2001 and 2007, he served as an assistant professor of information systems at the Stern School of Business at New York University.V. Kumar (“Assessing the Effect of Marketing Investments in a Business Marketing Context”) is the Lenny Distinguished Chair Professor of Marketing, and Executive Director, Center for Excellence in Brand and Customer Management, J. Mack Robinson College of Business, Georgia State University. He has been recognized with seven lifetime achievement awards in marketing strategy, interorganizational issues, retailing, and marketing research from the AMA and other professional organizations. His books include Managing Customers for Profit, Customer Relationship Management, Customer Lifetime Value,Marketing Research, Statistical Methods in CRM, and International Marketing Research. Recently, he has been included in the Legends in Marketing series, where his papers will be published as a 10-volume collection with commentaries from marketing scholars worldwide.Dominique Olié Lauga (“Product Positioning in a Two-Dimensional Vertical Differentiation Model: The Role of Quality Costs”) is an assistant professor of management and strategy at the Rady School of Management, University of California, San Diego. She received her Ph.D. in economics from the Massachusetts Institute of Technology. Her research interests include new product development, advertising, and behavioral economics. Her work has appeared in Marketing Science.Anita Luo (“Assessing the Effect of Marketing Investments in a Business Marketing Context”) is a visiting assistant professor at Georgia State University. She received her Ph.D. from the University of Connecticut. She was the winner of the Mary Kay Doctoral Dissertation Competition in 2010 and the Shankar-Spiegel Award in 2008. She is mainly interested in business-to-business marketing and dynamic customer relationship management.Elie Ofek (“Product Positioning in a Two-Dimensional Vertical Differentiation Model: The Role of Quality Costs”) is the T. J. Dermot Dunphy Professor of Business Administration at the Harvard Business School. He received his Ph.D. in business and an M.A. in economics from Stanford University. His research focuses on how marketing input can impact innovation and product strategy and on how firms can leverage novel technologies or major trends to create and communicate value to customers. His research has appeared in Marketing Science, Management Science, the Journal of Marketing Research, and the Journal of Consumer Research.Oliver J. Rutz (“Zooming In on Paid Search Ads—A Consumer-Level Model Calibrated on Aggregated Data”) is an assistant professor of marketing at the Foster School of Business, University of Washington, Seattle; he was previously on the faculty of the Yale School of Management from 2007 to 2011. His research focuses on online marketing with an emphasis on paid search management. He won the 2007 EMAC best dissertation paper award and honorable mention in the 2007 Alden G. Clayton Doctoral Dissertation Proposal Competition. He is a member of the Handelsblatt-Management-Forum, a bimonthly international academic panel in Germany's leading business and financial newspaper.David A. Schweidel (“Modeling Customer Lifetimes with Multiple Causes of Churn”) is an assistant professor of marketing at the University of Wisconsin–Madison School of Business. He earned a B.A. in mathematics in 2001 from the University of Pennsylvania, and an M.A. in statistics in 2004 and a Ph.D. in 2006 from the Wharton School of the University of Pennsylvania. His research interests are in the development of stochastic models for media and customer relationship management applications; his current research projects include examining dynamics in social media.Woochoel Shin (“Competing for Low-End Markets”) is an assistant professor of marketing at the Warrington College of Business Administration, University of Florida. He received a Ph.D. from the Fuqua School of Business, Duke University. His research interests include competitive product policy, online advertising, and two-sided markets.Kannan Srinivasan (“Firm Strategies in the ‘Mid Tail’ of Platform-Based Retailing”) is the Rohet Tolani Distinguished Professor of International Business and H. J. Heinz II Professor of Management, Marketing and Information Systems at the Tepper School of Business at Carnegie Mellon University. He has published over 50 papers in leading journals such as Management Science, Marketing Science, the Journal of Marketing Research, the Journal of Marketing, the Journal of the American Statistics Association, and the Journal of Business. He is currently on the advisory board of Marketing Science, and he is an area editor for Quantitative Marketing and Economics and an associate editor for Management Science.S. Sriram (“Assessing the Effect of Marketing Investments in a Business Marketing Context”) is an assistant professor of marketing at the Ross School of Business, University of Michigan. He holds a B.Tech. from the Indian Institute of Technology and a Ph.D. in marketing from Purdue University. His research focuses on using econometric models to understand various marketing phenomena; substantively, his primary interests are in the areas of brand equity, long-term implications of marketing actions, consumer adoption of technology products, retail proliferation and cannibalization issues, and competitive interactions between firms. His research has been published in several marketing journals, including Marketing Science, Management Science, the Journal of Marketing, and the Journal of Consumer Research.Michael Trusov (“Zooming In on Paid Search Ads—A Consumer-Level Model Calibrated on Aggregated Data”) is an assistant professor of marketing at the Robert H. Smith School of Business at the University of Maryland. He received his Ph.D. from the Anderson School of Management at the University of California, Los Angeles, and also holds a master's in computer science and a master's in business administration. He is a winner of the Paul E. Green Award, the Emerald Management Reviews Citation of Excellence Award, and the Alden G. Clayton Doctoral Dissertation Competition Award; a runner-up for the Paul Root Award; and a finalist for the Harold H. Maynard Award. His research interests include Internet marketing (social media marketing, search engine marketing, social networks, clickstream analysis, electronic word-of-mouth marketing, e-commerce, recommendation systems, consumer-generated content), text analysis, eye tracking, and data mining.Chunhua Wu (“Measuring the Lifetime Value of Customers Acquired from Google Search Advertising”) is a Ph.D. candidate of marketing at the Olin Business School, Washington University in St. Louis. He received his bachelor's of science in statistics from Fudan University in China. His current research focuses on the topics of search advertising, contextual advertising, two-sided markets, word of mouth, and user-generated contents.Ying Xie (“Measuring the Lifetime Value of Customers Acquired from Google Search Advertising”) is an assistant professor of marketing at the Olin Business School, Washington University in St. Louis. She received her doctoral degree from the Kellogg School of Management at Northwestern University in Evanston, IL. Her recent research focuses on pharmaceutical marketing, search advertising, word of mouth, customer ratings, and social contagion. Her research has appeared in journals such as Marketing Science and the Journal of Marketing Research.
This foreword and the subsequent four invited articles were commissioned by Eric T. Bradlow while Editor-in-Chief of Marketing Science. The foreword was written in four parts; each part covers a different aspect of the Workshop on Quantitative Marketing and Structural Econometrics. The workshop was cosponsored by Columbia Business School, Duke University, the University of California at Los Angeles, and the INFORMS Society for Marketing Science and was held at the Fuqua School of Business at Duke University in August 2010. The introductory section, written by Bradlow, covers why he commissioned these articles in the first place. In his section, Jean-Pierre Dubé discusses “going from good to great” in the structural econometrics area as applied to marketing problems. A section jointly written by Brett R. Gordon and Raphael Thomadsen (both co-organizers of the workshop) discusses the workshop itself and some important thoughts for those people doing “structural econometrics in the trenches.” Finally, co-workshop organizer Richard Staelin's section provides some perspective on both the workshop and structural econometrics as they relate to analytical models and empirical work for quantitative marketing researchers.
What can be learned about marketing phenomena from descriptive, structural, and experimental empirical models? Is structure implicit in a descriptive empirical model? What is a “reduced-form model?” What is a natural experiment, and what can one infer from a study that uses experimental data? Having clear answers to these questions can improve empirical dialog. This paper defines descriptive, structural, and experimental empirical work, provides examples, discusses their similarities and differences, and comments on their strengths and weaknesses. An important theme is that the marketing question and the data available should determine the methods used, and not the other way around. Most of the examples discussed reference linear models that are widely employed in the marketing literature. Many of the points, however, extend to the development and interpretation of cutting-edge nonlinear, dynamic, or nonparametric models used in marketing.
In this note I overview the data selection and procurement process in the context of structural models. Data selection for structural models presents unique challenges because data and structure often substitute and because it is imperative to consider what information identifies causal effects of interest.I further discuss three types of field data on which to build empirical models: (i) data that are proprietary to firms, (ii) data that can come from the public domain, or (iii) data that can be purchased from private research firms, and I discuss the benefits and limits of each. I then detail a process for obtaining proprietary data and the potential pitfalls inherent in the process.
Marketing researchers have used models of consumer demand to forecast future sales, to describe and test theories of behavior, and to measure the response to marketing interventions. The basic framework typically starts from microfoundations of expected utility theory to obtain an econometric system that describes consumers' choices over available options, and to thus characterize product demand. The basic framework has been augmented significantly to account for quantity choices, to accommodate purchases of several products on a single purchase occasion (multiple discreteness and multicategory purchases), and to allow for asymmetric switching between brands across different price tiers. These extensions have enabled researchers to bring the analysis to bear on several related marketing phenomena of interest. This paper has three main objectives. The first objective is to articulate the main goals of demand analysis—forecasting, measurement, and testing—and to highlight several considerations associated with these goals. Our second objective is to describe the main building blocks of individual-level demand models. We discuss approaches built on direct and indirect utility specifications of demand systems, and we review extensions that have appeared in the marketing literature. The third objective is to explore a few emerging directions in demand analysis, including considering demand-side dynamics, combining purchase data with primary information, and using semiparametric and nonparametric approaches. We hope researchers new to this literature will take away a broader perspective on these models and see the potential for new directions in future research.
This paper provides a critical review of the methods for estimating static discrete games and their relevance for quantitative marketing. We discuss the various modeling approaches, alternative assumptions, and relevant trade-offs involved in taking these empirical methods to data. We consider games of both complete and incomplete information, examine the primary methods for dealing with the coherency problems introduced by multiplicity of equilibria, and provide concrete examples from the literature. We illustrate the mechanics of estimation using a real-world example and provide the computer code and data set with which to replicate our results.
Digital rights management (DRM) is an important yet controversial issue in the information goods markets. Although DRM is supposed to help copyright owners by protecting digital content from illegal copying or distribution, it is controversial because DRM imposes restrictions on even legal users, and there are many industry practitioners who believe that the industry would be better off without DRM. In this paper, we model consumers' utilities and their incentives to purchase legal products versus pirate illegal ones. This allows us to endogenize the level of piracy and understand how it is influenced by the presence or absence of DRM. Our analysis suggests that, counterintuitively, download piracy might decrease when the firm allows legal DRM-free downloads. Furthermore, we find that a decrease in piracy does not guarantee an increase in firm profits and that that copyright owners do not always benefit from making it harder to copy music illegally. By analyzing the competition among the traditional retailer, the digital retailer, and pirated sources of information goods, we get a better understanding of the competitive forces in the market and provide insights into the role of digital rights management.
Whereas literature in marketing shows that individuals often use noncompensatory decision rules, existing research on dyadic choice is based on compensatory models. In this paper we present a dyadic consider-then-choose model that investigates both compensatory and noncompensatory aspects of the joint decision process. The intersection of individual consideration sets at the dyad level gives rise to dyadic decision processes (DDPs) where dyad members are in concordance or discordance about alternatives to consider. We empirically investigate the implications of different DDPs on outcomes such as decision efficiency and dyadic welfare. The methodological approach merges choice experiments with Bayesian statistical models to uncover nuances of the dyadic choice process. Data were collected using a multiphase nationwide study of 265 husband-and-wife dyads. Results across three categories indicate that both concordant and discordant dyads exist. Among concordant dyads, the noncompensatory dyads make quicker decisions that result in higher dyadic welfare. Among discordant dyads, those that restrict their consideration set make quicker decisions that result in higher welfare than those that expand their consideration set. These findings have important implications for buyers looking to maximize dyadic welfare when making joint choices and for sellers making pricing and new product design decisions.
How should forward-looking managers plan advertising if they envision a product-harm crisis in the future? To address this question, we propose a dynamic model of brand advertising in which, at each instant, a nonzero probability exists for the occurrence of a crisis event that damages the brand's baseline sales and may enhance or erode marketing effectiveness when the crisis occurs. Because managers do not know when the crisis will occur, its random time of occurrence induces a stochastic control problem, which we solve analytically in closed form. More importantly, the envisioning of a possible crisis alters managers' rate of time preference: anticipation enhances impatience. That is, forward-looking managers discount the present—even when the crisis has not occurred—more than they would in the absence of crisis. Building on this insight, we then derive the optimal feedback advertising strategies and assess the effects of crisis likelihood and damage rate. We discover the crossover interaction: the optimal precrisis advertising decreases, but the postcrisis advertising increases as the crisis likelihood (or damage rate) increases. In addition, we develop a new continuous-time estimation method to simultaneously estimate sales dynamics and feedback strategies using discrete-time data. Applying the method to market data from the Ford Explorer's rollover recall, we furnish evidence to support the proposed model. We detect compensatory effects in parametric shift: ad effectiveness increases, but carryover effect decreases (or vice versa). We also characterize the crisis occurrence distribution that shows that Ford Explorer should anticipate a crisis in 2.1 years and within 6.3 years at the 95% confidence level. Finally, we find a remarkable correspondence between the observed and optimal advertising decisions.
Commercial open source software (COSS) products—privately developed software based on publicly available source code—represent a rapidly growing, multibillion-dollar market. A unique aspect of competition in the COSS market is that many open source licenses require firms to make certain enhancements public, creating an incentive for firms to free ride on the contributions of others. This practice raises a number of puzzling issues. First, why should a firm further develop a product if competitors can freely appropriate these contributions? Second, how does a market based on free riding produce high-quality products? Third, from a public policy perspective, does the mandatory sharing of enhancements raise or lower consumer surplus and industry profits?We develop a two-sided model of competition between COSS firms to address these issues. Our model consists of (1) two firms competing in a vertically differentiated market, in which product quality is a mix of public and private components, and (2) a market for developers that firms hire after observing signals of their contributions to open source. We demonstrate that free-riding behavior is supported in equilibrium, that a mandatory sharing setting can result in high-quality products, and that free riding can actually increase profits and consumer surplus.
We discuss how regression discontinuity designs arise naturally in settings where firms target marketing activity at consumers, and we illustrate how this aspect may be exploited for econometric inference of causal effects of marketing effort. Our main insight is to use commonly observed discontinuities and kinks in the heuristics by which firms target such marketing activity to consumers for nonparametric identification. Such kinks, along with continuity restrictions that are typically satisfied in marketing and industrial organization applications, are sufficient for identification of local treatment effects. We review the theory of regression discontinuity estimation in the context of targeting and explore its applicability to several marketing settings. We discuss identifiability of causal marketing effects using the design and show that consideration of an underlying model of strategic consumer behavior reveals how identification hinges on model features such as the specification and value of structural parameters as well as belief structures. We emphasize the role of selection for identification. We present two empirical applications: the first measures the effect of casino e-mail promotions targeted to customers based on ranges of their expected profitability, and the second measures the effect of direct mail targeted by a business-to-consumer company to zip codes based on cutoffs of expected response. In both cases, we illustrate that exploiting the regression discontinuity design reveals negative effects of the marketing campaigns that would not have been uncovered using other approaches. Our results are nonparametric, easy to compute, and control for the endogeneity induced by the targeting rule.
Although prior literature has examined reactions to drastic negative news, we examine the situation in which decision makers receive contradictory information about products and they have to decide whether to persist with or abandon product usage. We investigate physician reactions to conflicting information concerning the cardiovascular risk of Avandia, a diabetes drug. We examine how beliefs about both drug effectiveness and drug safety are updated and speculate that experience, expertise, and self-efficacy impact how such information is integrated with current quality beliefs. Unlike previous Bayesian learning models, we consider that some signals, such as positive and negative news releases and the firm's marketing effort, may be biased in that they provide an opinionated point of view. The results show interesting differences in how physician types (specialists, hospital-based primary care physicians, heavy and light prescribers) update their beliefs and the information sources they use to do so. We find evidence that safety issues about Avandia resulted in spillover concern to close competitor Actos. The results have implication for determining who should be targeted and what vehicles should be used if a firm is faced with a situation where consumers are in a quandary because of receiving conflicting messages.
Market response models based on field-generated data need to address potential endogeneity in the regressors to obtain consistent parameter estimates. Another requirement is that market response models predict well in a holdout sample. With both requirements combined, it may seem reasonable to subject an endogeneity-corrected model to a holdout prediction task, and this is quite common in the academic marketing literature. One may be inclined to expect that the consistent parameter estimates obtained via instrumental variables (IV) estimation predict better than the biased ordinary least squares (OLS) estimates. This paper shows that this expectation is incorrect. That is, if the holdout sample is similar to the estimation sample so that the regressors are endogenous in both samples, holdout sample validation favors regression estimates that are not corrected for endogeneity (i.e., OLS) over estimates that are corrected for endogeneity (i.e., IV estimation). We also discuss ways in which holdout samples may be used sensibly in the presence of endogeneity. A key takeaway is that if consistent parameter estimates are the primary model objective, the model should be validated with an exogenous (rather than endogenous) holdout sample.
Neeraj Arora (“Noncompensatory Dyadic Choices”) is the John P. Morgridge Chair in Business Administration at University of Wisconsin—Madison, where he also serves as the executive director of the A.C. Nielsen Center for Marketing Research. He has an undergraduate degree in engineering from Delhi University, and an MBA and Ph.D. from the Ohio State University. He serves on the editorial boards of the Journal of Marketing Research, Marketing Science, and the Journal of Marketing. His papers have appeared in the Journal of Marketing Research, Marketing Science, the Journal of Marketing, the Journal of Consumer Research, the International Journal of Research in Marketing, and Marketing Letters.Eric T. Bradlow (“Foreword—Revisiting the Workshop on Quantitative Marketing and Structural Econometrics”) is a statistical methodologist and empirical researcher interested in the development of mathematical models of consumer behavior. He is interested in applying mathematical models to unique data structures in marketing, education, psychology, medicine, or whoever will give him interesting data. His wife Laura and three sons Ethan, Zach, and Ben, along with an undying passion for sports, are his greatest joys. Pradeep K. Chintagunta (“Structural Workshop Paper—Discrete-Choice Models of Consumer Demand in Marketing”) is the Joseph T. and Bernice S. Lewis Distinguished Service Professor of Marketing at the Booth School of Business, University of Chicago. He is interested in studying the effectiveness of marketing activities in pharmaceutical markets, investigating aspects of technology product markets, studying online and off-line purchase behavior, and analyzing household purchase behavior using scanner data. He graduated from Northwestern University and has also served on the faculty of the Johnson School, Cornell University.Preyas S. Desai (“Music Downloads and the Flip Side of Digital Rights Management”) is the Spencer R. Hassell Professor of Business Administration at the Fuqua School of Business, Duke University. His research covers a wide range of topics in marketing strategy, distribution channels, and marketing of durable products. His articles on these topics have appeared in journals such as Marketing Science, Management Science, the Journal of Marketing, the Journal of Marketing Research, and Quantitative Marketing and Economics. He is currently the editor-in-chief of Marketing Science.Jean-Pierre Dubé (“Foreword—Revisiting the Workshop on Quantitative Marketing and Structural Econometrics”) is the Sigmund E. Edelstone Professor of Marketing and Robert King Steel Faculty Fellow at the University of Chicago Booth School of Business. He is also a Faculty Research Fellow for the National Bureau of Economic Research (NBER) in the Industrial Organization program. He holds a B.Sc. in quantitative economics from the University of Toronto and a Ph.D. in economics from Northwestern University. He studies empirical quantitative marketing and empirical industrial organization, with specific interests in pricing, advertising, branding, Internet marketing, retailing, and dynamic decision making. Peter Ebbes (“The Sense and Non-Sense of Holdout Sample Validation in the Presence of Endogeneity”) is a visiting assistant professor of marketing at the Fisher College of Business at the Ohio State University and an assistant professor of marketing at the Smeal College of Business at the Pennsylvania State University. He has an undergraduate degree in econometrics and marketing and obtained a Ph.D. from the University of Groningen. His research focuses on understanding and modeling endogeneity in market response models, and heterogeneity and segmentation in consumer markets.Paul B. Ellickson (“Structural Workshop Paper—Estimating Discrete Games”) is an assistant professor of economics and of marketing at the University of Rochester. He received an A.B. in economics and mathematics from the University of California, Berkeley, and a Ph.D. from the Massachusetts Institute of Technology. His research interests lie at the intersection of quantitative marketing and industrial organization, with a focus on using structural modeling to understand the forces that drive strategic interaction and optimal decision making. His research has been published in various academic outlets including the RAND Journal of Economics, the American Economic Review, Marketing Science, Quantitative Marketing and Economics, the International Journal of Industrial Organization, and the Annual Review of Economics.Brett R. Gordon (“Foreword—Revisiting the Workshop on Quantitative Marketing and Structural Econometrics”; “Competitive Strategy for Open Source Software”) is an associate professor at Columbia Business School. He received his B.S. in information systems and economics and Ph.D. in economics from Carnegie Mellon University. He studies topics in empirical industrial organization and marketing, with a particular interest in how competition impacts firms' pricing and innovation decisions, especially in high-tech markets. More recently, he has examined the effects of competition on advertising in political elections.Wesley Hartmann (“Identifying Causal Marketing Mix Effects Using a Regression Discontinuity Design”) is an associate professor of marketing at the Stanford Graduate School of Business. He holds a Ph.D. in economics from the University of California, Los Angeles. He is interested in applying and developing econometric techniques to analyze questions relevant to marketing and economics. His current research focuses on dynamic choice contexts, pricing, advertising, social interactions, and targeted marketing.Ty Henderson (“Noncompensatory Dyadic Choices”) is an assistant professor at the McCombs School of Business, University of Texas at Austin. He earned his Ph.D. from the University of Wisconsin–Madison after experiencing the dot-com boom at two start-ups. His research interests include sales promotion and branding strategy in the context of public goods, noncompensatory choice, Bayesian econometric methods, and behavioral measurement technologies. His research has appeared in Marketing Science and the Journal of Marketing.Ajay Kalra (“Understanding Responses to Contradictory Information About Products”) is a professor of marketing at the Jesse H. Jones Graduate School of Business at Rice University. He received his Ph.D. from Duke University. His current research is oriented toward substantive topics such as communication strategies, sales-force management, and quality assessments. He has published in Marketing Science, Management Science, Journal of Marketing Research, Journal of Marketing and Journal of Consumer Research, and he has won the O'Dell Award from the Journal of Marketing Research and was a finalist for the John D. C. Little Award. Vineet Kumar (“Competitive Strategy for Open Source Software”) is an assistant professor at Harvard Business School. He received his undergraduate degree from the Indian Institute of Technology and completed his master's and doctoral studies at Carnegie Mellon University. His research has focused on understanding consumer and firm choices in industries that are highly influenced by technology. His current interests include investigating how value is created and captured when consumers, with the help of social media technologies, take a leading role in producing valuable user-generated content; he also examines issues including how firms can design and deploy marketing tools to leverage user-generated inputs to cocreate digital products.Shibo Li (“Understanding Responses to Contradictory Information About Products”) is an associate professor of marketing at the Kelley School of Business, Indiana University. He received a Ph.D. in industrial administration (marketing) from Carnegie Mellon University. His research interests are consumer dynamics, analytical customer relationship management, interactive marketing, and analytical and empirical analysis of signaling models. He was recognized as a MSI Young Scholar in 2009; received the 2004 John A. Howard AMA Doctoral Dissertation Award, the 2006 CART Research Frontier Award for Innovative Research from Carnegie Mellon University, the 3M Junior Faculty Grant Award from the Kelley School of Business, Indiana University from 2008 to 2010; and was a finalist for the 2004 John D. C. Little Award. Qing Liu (“Noncompensatory Dyadic Choices”) is an assistant professor of marketing at the University of Wisconsin–Madison. She received her B.S. degree from the University of Science and Technology of China and her M.S. and Ph.D. in statistics from the Ohio State University. Her research focuses on the application and development of statistical theories and methodology to help solve problems in marketing and marketing research; areas of interest include conjoint analysis, consumer choice, experimental design, and Bayesian methods. Her papers have appeared in Marketing Science, Quantitative Marketing and Economics, and Statistica Sinica.Carl F. Mela (“Structural Workshop Paper—Data Selection and Procurement”) is the T. Austin Finch Foundation Professor of Marketing at Duke University. His research focuses on the long-term effects of marketing activity, customer management, the Internet, and new media. His articles have appeared in the Journal of Marketing Research, Marketing Science, the Journal of Marketing, Harvard Business Review, and the Journal of Consumer Research, and they have received or been nominated for more than 20 best paper awards. His home page is located at http://www.duke.edu/~mela.Sanjog Misra (“Structural Workshop Paper—Estimating Discrete Games”) is an associate professor of marketing and applied statistics at the University of Rochester. His current research interests include the development and application of structural econometric methods to marketing problems. His research has been published in journals such as Marketing Science, Quantitative Marketing and Economics, the International Journal of Marketing Research, and the Journal of Law and Economics, among others.Prasad A. Naik (“Optimal Advertising When Envisioning a Product-Harm Crisis”) is a professor of marketing at the University of California, Davis. He studied chemical engineering (University of Bombay) and obtained an MBA (IIM Calcutta) and a Ph.D. (University of Florida); prior to the doctoral studies, he worked for several years with Dorr-Oliver and GlaxoSmithKline, where he acquired invaluable experience in sales and distribution management and brand management. He is a recipient of the Chancellor's Fellow, Frank Bass Award, O'Dell Award Finalist, JIM Best Paper Award, MSI Young Scholar, AMS Doctoral Dissertation Award, AMA Consortium Faculty, and Professor of the Year for outstanding teaching on multiple occasions. His Erdös number is 4; his Lehmann number is 2.Harikesh S. Nair (“Structural Workshop Paper—Discrete-Choice Models of Consumer Demand in Marketing”; “Identifying Causal Marketing Mix Effects Using a Regression Discontinuity Design”) is an associate professor of marketing at the Stanford Graduate School of Business. He is interested in the dynamic effects of marketing actions and in optimal marketing resource allocation in competitive markets. His research brings together applied economic theory and econometric tools with marketing data to quantitatively inform these decisions. His recent research is in the area of sales-force compensation design, social interactions, network effects, diffusion of technologies, and empirical industrial organization, especially as applied to the marketing of high-technology and entertainment goods.Sridhar Narayanan (“Identifying Causal Marketing Mix Effects Using a Regression Discontinuity Design”) is an associate professor of marketing at the Graduate School of Business at Stanford University. His research focuses on empirical analysis of marketing problems through the estimation of econometric models on behavioral data; his previous research has focused on problems such as consumer learning, nonlinear pricing, peer effects, market entry, pharmaceutical marketing, and online advertising. He has a particular interest in estimation of causal effects and in Bayesian estimation. His papers have been published in Marketing Science, the Journal of Marketing Research, Quantitative Marketing and Economics, the Journal of Marketing, and Marketing Letters.Dominik Papies (“The Sense and Non-Sense of Holdout Sample Validation in the Presence of Endogeneity”) is an assistant professor of marketing and media management at the Institute for Marketing and Media at the University of Hamburg, Germany. He holds a doctoral degree in marketing from the University of Hamburg. His research focuses on analyzing and modeling consumer demand in markets for media products and services. In addition, he analyzes subjective consumer perceptions of firm behavior as a predictor of future purchase decisions. Devavrat Purohit (“Music Downloads and the Flip Side of Digital Rights Management”) is the Bob J. White Professor of Business Administration at the Fuqua School of Business, Duke University. His teaching and research interests are in marketing high-technology products and marketing strategy. He has published extensively in journals such as Marketing Science, Management Science, the Journal of Consumer Research, and the Journal of Marketing Research.Peter C. Reiss (“Structural Workshop Paper—Descriptive, Structural, and Experimental Empirical Methods in Marketing Research”) is the MBA Class of 1963 Professor of Economics at the Stanford University Graduate School of Business. He has a courtesy appointment in the Stanford Economics department. He received his Ph.D. in economics from Yale University, and he has a B.A. with honors in applied mathematics and economics from Brown University. He is the recipient of a Sloan Fellowship and an NBER Olin Fellowship, and in 2011–2012, he will be a fellow at the Center for Advanced Study in the Behavioral Sciences.Olivier Rubel (“Optimal Advertising When Envisioning a Product-Harm Crisis”) is an assistant professor of marketing at the University of California, Davis; he obtained his Ph.D. in marketing from HEC Montréal. His research has appeared in Automatica and Annals of Dynamic Games. He is an alumnus of l'Ecole Normale Supérieure de Cachan, is a recipient of the French Agrégation, and serves on the editorial board of the Journal of African Business. Finally, he enjoys the Californian pauses café and visits to the Delta of Venus, where ideas blossom.Kannan Srinivasan (“Competitive Strategy for Open Source Software”) is the Rohet Tolani Distinguished Professor of International Business and H. J. Heinz II Professor of Management, Marketing and Information Systems at the Tepper School of Business at Carnegie Mellon University. He is currently an area editor for Marketing Science and Quantitative Marketing and Economics, and he is an associate editor for Management Science. He has published over 50 papers in leading journals.Shuba Srinivasan (“Optimal Advertising When Envisioning a Product-Harm Crisis”) is an associate professor of marketing and a Dean's Research Fellow at Boston University's School of Management. Her research focuses on strategic marketing problems, in particular, long-term marketing productivity, to which she applies her expertise in time-series analysis and econometrics. Her current research focuses on marketing's impact on financial performance and firm valuation and on metrics for gauging marketing performance. She has recently won several research awards including the Broderick Prize for Excellence in Research Scholarship at Boston University in 2010, the Google-WPP Research Award in 2010 for her work on audience-based online metrics, and the Syntec Management Consulting Best Academic Paper Award in 2011, among others.Richard Staelin (“Foreword—Revisiting the Workshop on Quantitative Marketing and Structural Econometrics”) has been an active researcher and educator for over four decades. In addition, he has taken on a number of leadership/administrative roles, both within his university and the marketing community at large. He is interested in a diverse set of problems, ranging from the quality of medical care, to managerial decision making, to channel management. His most recent nonacademic accomplishment was to climb Mt. Kilimanjaro, which is over 19,000 feet. Raphael Thomadsen (“Foreword—Revisiting the Workshop on Quantitative Marketing and Structural Econometrics”) is an assistant professor of marketing at the UCLA Anderson School of Management. He holds a Ph.D. in economics from Stanford University. His research primarily focuses on the interplay between product offerings and pricing. In particular, he studies how firms decide which products to offer and how these choices affect consumer choice and competition between firms.Harald J. van Heerde (“The Sense and Non-Sense of Holdout Sample Validation in the Presence of Endogeneity”) is a professor of marketing at the University of Waikato, Hamilton, New Zealand. In his research, he develops new econometric models to measure the effects of various elements of marketing strategy and tactics (price, promotion, advertising, innovation, assortment, loyalty programs) on purchase behavior and sales. His work has been awarded with the Paul E. Green Award, the William F. O'Dell Award (Journal of Marketing Research), and the IJRM Best Paper award, and it has been a best-paper award finalist for the Journal of Marketing Research or Marketing Science on 11 more occasions. He has been awarded prestigious national research grants both by the Netherlands Organisation for Scientific Research (2002–2006) and the New Zealand Royal Society (2010–2012; Marsden Fund 10-UOW-068).Dinah A. Vernik (“Music Downloads and the Flip Side of Digital Rights Management”) is an assistant professor of marketing at the Jesse H. Jones School of Business, Rice University. She graduated with a Ph.D. in business administration from Duke University in 2009. Her research interests lie in the area of quantitative marketing modeling. She applies economic concepts and a game theoretic approach to real-world marketing problems in order to provide insight and intuition about optimal pricing and distribution channel strategies.Wei Zhang (“Understanding Responses to Contradictory Information About Products”) is an assistant professor of marketing at Iowa State University's College of Business. He received his Ph.D. from Carnegie Mellon University; he was a consultant at McKinsey & Company and also worked in the pharmaceutical industry for Amgen and Bristol-Myers Squibb. His research interests include Bayesian statistics, pharmaceutical marketing, and sales-force management. His research has appeared in Management Science.
The goal of this paper is to study the behavior of consumers, dealers, and manufacturers in the car sector and present an approach that can be used by managers and policy makers to investigate the impact of significant demand shocks on profits, prices, and dealer networks. More specifically, we investigate consumer demand, substitution patterns, and price decisions across different cars and dealer locations to identify dealerships with low margins or high fixed costs and measure the value of closing down dealerships for manufacturers. We apply our model empirically to the San Diego area using a transactional data set with information about the locations of dealers and consumers, as well as manufacturer and retail prices. We find strong consumer disutility for travel and find that dealers have local demand areas that are shared with a small set of competitors. We show that a reduction of market demand by 30% over two years, similar to the economic crisis of 2008–2009, results in an annual drop in prices of approximately 11%. We discuss this price drop in the context of the 2009 federal policy measure known as the Car Allowance Rebate System program. We compare predictions and actual dealership closings in the General Motors and Chrysler dealer networks as an application of our approach.
This series of discussions presents commentaries and a rejoinder on strategic and managerial issues arising from Albuquerque and Bronnenberg [Albuquerque, P., B. J. Bronnenberg. 2012. Measuring the impact of negative demand shocks on car dealer networks. Marketing Sci.31(1) 4–23].
Consumers are often unable to resist the temptation of overconsuming certain products such as cookies, crackers, soft drinks, alcohol, etc. To control their consumption, some consumers buy small packages or abstain from purchasing the product altogether. Other consumers, however, still purchase large packages and overconsume. From a strategic perspective, firms have the option of introducing small packages or only offering large packages. We use the literature on hyperbolic discounting to model consumers' self-control problems and examine conditions under which firms will offer small packages to help consumers combat their self-control problem, and how this offering in turn affects prices, profits, consumer, and social welfare. Our results show that introducing small packages can increase firms' profits only when a small fraction of consumers have overconsumption problems or when small packages can bring in new customers. Additionally, we find that competition can sometimes reduce the incentives for firms to introduce small packages. This is particularly true when a large fraction of consumers is attracted to small packages. We also find that firms' profits can sometimes decrease if they produce healthier alternatives of their goods. Our analysis of consumer welfare reveals that small packages enhance consumer and social welfare, even though they sometimes increase the consumption of vice goods.
Jain [Jain, S. 2012. Marketing of vice goods: A strategic analysis of the package size decision. Marketing Sci.31(1) 36–51] examines the impact of consumers' self-control problem on the equilibrium package sizes offered by firms marketing vice goods. This series of discussions offers commentaries and a rejoinder that discuss competitive implications of firms offering small sizes and the impact of smaller sizes on the total consumer expenditure.
Most ads in practice receive no more than a single eye fixation. This study investigates the limits of what ads can communicate under such adverse exposure conditions. We find that consumers already know at maximum levels of accuracy and with high degree of certainty whether something is an ad or is editorial material after an exposure of less than 100 milliseconds and—if the ad is typical—which product is being advertised. Even after an extremely coarse visual presentation of 100 milliseconds, the product and brand in typical ads are identified well above chance levels, with atypical ads doing slightly better at the brand level. We propose a new metric that quantifies how effectively individual ads communicate their gist in adverse exposure conditions and that predicts the immediate interest that ads draw. Bayesian mediation analyses show that because of their better gist performance, typical ads rather than atypical ones raise immediate interest after very brief exposures. These findings challenge some of the received knowledge in advertising theory and practice, and they reveal the immediate communication benefits of typical ads.
There exists a dichotomy in the communication strategies of fashion firms—some firms purposefully cloak information on the tastefulness of their products, whereas others openly flaunt their tasteful or “it” products. This divide in communication strategies cannot be explained by existing wealth signaling models of fashion. In this paper, we offer a model of fashion that explains the above dichotomy. We model fashion as a social device that plays the dual role of allowing people to both fit in with their peers and differentiate themselves by signaling their good taste or access to information. In this context, we show that a fashion firm faces an interesting dilemma—if it restricts information, then only sophisticated consumers buy its products and use them to signal their taste. Cloaking thus preserves the signaling value of its products but reduces the number of social interactions enabled by them. In contrast, flaunting undermines the signaling value of its products but increases the interactions enabled by them. Given these trade-offs, we derive the conditions under which cloaking occurs. We also show that, in equilibrium, the most tasteful product endogenously emerges as the fashion hit or “it” product.
Households incur transaction costs when choosing among off-line stores for grocery purchases. They may incur additional transaction costs when buying groceries online versus off-line. We integrate the various transaction costs into a channel choice framework and empirically quantify the relative transaction costs when households choose between the online and off-line channels of the same grocery chain. The key challenges in quantifying these costs are (i) the complexity of channel choice decision and (ii) that several of the costs depend on the items a household expects to buy in the store, and unobserved factors that influence channel choice also likely influence the items purchased. We use the unique features of our empirical context to address the first issue and the plausibly exogenous approach in a hierarchical Bayesian framework to account for the endogeneity of the channel choice drivers. We find that transaction costs for grocery shopping can be sizable and play an important role in the choice between online and off-line channels. We provide monetary metrics for several types of transaction costs, such as travel time and transportation costs, in-store shopping time, item-picking costs, basket-carrying costs, quality inspection costs, and inconvenience costs. We find considerable household heterogeneity in these costs and characterize their distributions. We discuss the implications of our findings for the retailer's channel strategy.
In recent years there has been a growing stream of literature in marketing and economics that models consumers as Bayesian learners. Such learning behavior is often embedded within a discrete choice framework that is then calibrated on scanner panel data. At the same time, it is now accepted wisdom that disentangling preference heterogeneity and state dependence is critical in any attempt to understand either construct. We posit that this confounding between state dependence and heterogeneity often carries through to Bayesian learning models. That is, the failure to adequately account for preference heterogeneity may result in over- or underestimation of the learning process because this heterogeneity is also reflected in the initial conditions. Using a unique data set that contains stated preferences (survey) and actual purchase data (scanner panel) for the same group of consumers, we attempt to untangle the effects of preference heterogeneity and state dependence, where the latter arises from Bayesian learning. Our results are striking and suggest that measured brand beliefs can predict choices quite well and, moreover, that in the absence of such measured preference information, the Bayesian learning behavior for consumer packaged goods is vastly overstated. The inclusion of preference information significantly reduces evidence for aggregate-level learning and substantially changes the nature of individual-level learning. Using individual-level outcomes, we illustrate why the lack of preference information leads to faulty inferences.
We develop and test an incentive-compatible Conjoint Poker (CP) game. The preference data collected in the context of this game are comparable to incentive-compatible choice-based conjoint (CBC) analysis data. We develop a statistical efficiency measure and an algorithm to construct efficient CP designs. We compare incentive-compatible CP to incentive-compatible CBC in a series of three experiments (one online study and two eye-tracking studies). Our results suggest that CP induces respondents to consider more of the profile-related information presented to them compared with CBC.
Service providers and their customers are sometimes victims of failures caused by exogenous factors such as unexpected bad weather, power outages, or labor strikes. When such no-fault failures occur in confined zones, service providers may confine customers against their will if making arrangements for them to leave is very costly. Such confinements, however, can result in severe pain and suffering, and customer complaints put regulators under pressure to pass a customer bill of rights that allows captive customers to abort failed services. This paper shows that service providers are better off preempting such laws by voluntarily allowing customers to escape the service under failure. Moreover, service providers can profit by targeting compensation to customers based on whether they use or leave the service under failure.
This paper evaluates the joint impact of exclusive channels and revenue sharing on suppliers and retailers in a hybrid duopoly common retailer and exclusive channel model. The model bridges the gap in the literature on hybrid multichannel supply chains with bilateral complementary products and services with or without revenue sharing. The analysis indicates that, without revenue sharing, the suppliers are reluctant to form exclusive deals with the retailers; thus, no equilibrium results. With revenue sharing from the retailers to the suppliers, it can be an equilibrium strategy for the suppliers and retailers to form exclusive deals. Bargaining solutions are provided to determine the revenue sharing rates. Our additional results suggest forming exclusive deals becomes less desirable for the suppliers if revenue sharing is also in place under nonexclusivity. In our extended discussion, we also study the impact of channel asymmetry, an alternative model with fencing, composite package competition, and enhanced price-dependent revenue sharing.
Paulo Albuquerque (“Measuring the Impact of Negative Demand Shocks on Car Dealer Networks”; “Rejoinder to Commentaries on Albuquerque and Bronnenberg”) is an assistant professor of marketing at the Simon Graduate School of Business, University of Rochester. He holds a Ph.D. in management from the UCLA Anderson School of Management. He is currently interested in competition and consumer behavior in online markets, new product diffusion across markets, and spatial competition models. His articles have appeared in Marketing Science, the Journal of Marketing Research, and Management Science.Bart J. Bronnenberg (“Measuring the Impact of Negative Demand Shocks on Car Dealer Networks”; “Rejoinder to Commentaries on Albuquerque and Bronnenberg”) is a professor of marketing and CentER research fellow at Tilburg University. He holds Ph.D. and M.Sc. degrees in management from INSEAD, Fontainebleau, France and an M.Sc. in industrial engineering from Twente University, The Netherlands. He is currently interested in marketing strategy and multimarket competition in consumer goods and medical industries; he is also continuing to work on empirical analyses of new product growth and consumer choice behavior. His articles have appeared in the leading field journals, and he was named the recipient of the 2003 Paul Green Award, the 2003 IJRM Best Paper Award, the 2004 John D. C. Little Best Paper Award, and the 2008 Paul Green Award.Gangshu (George) Cai (“Exclusive Channels and Revenue Sharing in a Complementary Goods Market”) is an associate professor in the Department of Management at Kansas State University. He received his Ph.D. in operations research from North Carolina State University in 2005, and he received his M.S. in business statistics and economics in 1999 and B.S. in physics in 1996 from Peking University. His research is concentrated on multichannel supply chain management, with a particular focus on the interface between operations management and marketing, finance, and e-commerce.Javier Cebollada (“Quantifying Transaction Costs in Online/Off-line Grocery Channel Choice”) is an associate professor of marketing at the Public University of Navarra, Spain. He obtained a Ph.D. in management and a master's degree in economics, both from Pompeu Fabra University (Barcelona), Spain. He has been studying how manufacturers and retailers adapt their strategies to the multichannel online–off-line structure and how consumers behave in the multichannel environment. His research has been published in journals such as Marketing Science, the Journal of Interactive Marketing, and the International Journal of Research in Marketing. Rachel R. Chen (“Customer Bill of Rights Under No-Fault Service Failure: Confinement and Compensation”) is an associate professor at the Graduate School of Management, University of California, Davis. She received her Ph.D. from the Johnson Graduate School of Management, Cornell University. Her research addresses economic issues in managing supply chains and distribution channels, including procurement and the marketing–operations interface; she also analyzes decision making under uncertainty in service operations. Her previous research has appeared in Management Science, Marketing Science, Manufacturing & Service Operations Management, Production and Operations Management, IIE Transactions, and other research outlets.Pradeep K. Chintagunta (“Quantifying Transaction Costs in Online/Off-line Grocery Channel Choice”) is the Joseph T. and Bernice S. Lewis Distinguished Service Professor of Marketing at the Booth School of Business, University of Chicago. He earned a Ph.D. in marketing from Northwestern University in 1990. He is interested in empirically studying strategic interactions among firms in vertical and horizontal relationships, measuring the effectiveness of marketing activities in pharmaceutical markets, investigating aspects of technology product markets, and analyzing household purchase behavior. Junhong Chu (“Quantifying Transaction Costs in Online/Off-line Grocery Channel Choice”) is an assistant professor of marketing at the National University of Singapore (NUS) Business School. She earned a Ph.D. in marketing and an MBA from the University of Chicago Booth School of Business in 2006. Her research interests include structural modeling (both classic and Bayesian approaches) of consumer and firm behavior, distribution channels, e-commerce, and retailing. Her research has appeared in Marketing Science, the Journal of Marketing Research, the Journal of Marketing, and the Journal of Interactive Marketing; she was the 2011 MSI Young Scholar. Yue Dai (“Exclusive Channels and Revenue Sharing in a Complementary Goods Market”) is an associate professor at Fudan University, China. She received her Ph.D. in industrial engineering from North Carolina State University. Her scholarly work has appeared in Production and Operations Management and Naval Research Logistics.Martijn G. de Jong (“Measuring Consumer Preferences Using Conjoint Poker”) is the J. Tinbergen Associate Professor of Marketing, Erasmus University. He applies statistical and psychometric methods to improve marketing decision making; often his research is cross-cultural in nature, relying on large-scale data sets. He received several major research grants, including an NWO (Netherlands Organization for Scientific Research) innovation grant. His awards include the J. C. Ruigrok award (awarded once every four years to the most productive young scholar in the Economic Sciences in the Netherlands) and the Christiaan Huygens award (presented by HRH princess Máxima of the Netherlands; awarded once every five years to a young economist in the Netherlands). Johann Füller (“Measuring Consumer Preferences Using Conjoint Poker”) is the CEO of Hyve AG, a leading innovation and community agency in Germany, and a researcher at the Innsbruck University School of Management. His research explores innovation and cocreation communities from multiple perspectives. He advises and speaks to major corporations worldwide in the areas of innovation communities, social media, crowdsourcing, and cocreation. He has published in journals such as the Journal of Product Innovation Management, California Management Review, MIS Quarterly, the Journal of Business Research, and others. Eitan Gerstner (“Customer Bill of Rights Under No-Fault Service Failure: Confinement and Compensation”) is a professor of management at the Faculty of Industrial Engineering and Management, The Technion–Israel Institute of Technology. His research areas include marketing strategies and social responsibility. He contributes regularly to this journal; recent titles include “Should Captive Sardines Be Compensated? Serving Customers in a Confined Zone” and “For a Few Cents More: Why Supersize Unhealthy Food?” He served Marketing Science as an editorial board member and an area editor.Dominique M. Hanssens (“Response Models, Data Sources, and Dynamics”) is the Bud Knapp Professor of Marketing at the UCLA Anderson School of Management. His research focuses on strategic marketing problems—in particular, the assessment of long-term marketing impact on business performance. He received his Ph.D. from Purdue University, and from 2005 to 2007, he served as executive director of the Marketing Science Institute in Cambridge, MA. He is a fellow of the INFORMS Society for Marketing Science.Dan Horsky (“Disentangling Preferences and Learning in Brand Choice Models”) is the Benjamin L. Forman Professor of Marketing at the William E. Simon Graduate School of Business, University of Rochester. He has published on a wide variety of marketing topics and has twice won the John D. C. Little Best Paper Award. His outside interests include swimming and art collecting.Sanjay Jain (“Marketing of Vice Goods: A Strategic Analysis of the Package Size Decision”; “Rejoinder: Package Size Issues and Vice Goods”) is a professor and JCPenney Chair of Marketing and Retailing Studies at the Mays Business School, Texas A&M University. His research interests are in the areas of competitive strategy, behavioral economics, and experimental game theory. He has been a finalist for the Paul Green Award, the John D. C. Little Award, the INFORMS Society of Marketing Science Long Term Impact Award, and he has received the INFORMS Society of Marketing Science Practice Prize Award. He is an associate editor for Management Science and serves on the editorial boards of the Journal of Marketing Research and Marketing Science.Sanjog Misra (“Disentangling Preferences and Learning in Brand Choice Models”) is an associate professor of marketing and applied statistics at the William E. Simon School of Business, University of Rochester. His current research interests include the development and application of structural econometric methods to marketing problems. His research has been published in journals such as Marketing Science, Quantitative Marketing and Economics, the International Journal of Research in Marketing, and the Journal of Law and Economics, among others.Rik Pieters (“Ad Gist: Ad Communication in a Single Eye Fixation”) is a professor of marketing at Tilburg University. He holds a Ph.D. in social psychology from Leiden University. He researches consumer behavior to improve the effectiveness of marketing and public policy decisions; his work focuses on the determinants and implications of visual attention. He is in search of interesting main effects and surmountable hills. Devavrat Purohit (“A Strategic Perspective on Durable Goods”) is the Bob J. White Professor of Business Administration at Duke University's Fuqua School of Business. His research interests are in the area of durable goods and high-technology products, and in channel management issues. He has also studied the role of leases, sales, and inventory levels as ways to manage the manufacturer–retailer relationship. More recently, he has been studying the role of digital rights and information goods.Ram Rao (“Package Size and Competition”) is the Founders Professor in the Naveen Jindal School of Management at the University of Texas at Dallas. His current research is on competitive promotions, varied consumption, and marketing implications of social media. He is coeditor of the Internet journal of marketing science Review of Marketing Science, serves on the editorial boards of Marketing Science and the Journal of Marketing Research, and serves on the advisory boards of Quantitative Marketing and Economics and Management Research Network. Sangwoo Shin (“Disentangling Preferences and Learning in Brand Choice Models”) is an assistant professor of marketing at the Krannert School of Management, Purdue University. He holds an MBA from Seoul National University, Seoul, and a Ph.D. in marketing from University of Rochester, Rochester, NY. His research interests cover dynamic consumer decision under uncertainty, strategic firm decision in direct marketing, conjoint analysis, and applied Bayesian statistics.Richard Staelin (“A Strategic Perspective on Durable Goods”) has been an active researcher and educator for over four decades. He has supervised over 40 doctoral students during this time period. His research interests range from the quality of medical care, to managerial decision making, to channel management. A few years ago he was forced to give up running but now he takes yoga and pilates. Daniel Stieger (“Measuring Consumer Preferences Using Conjoint Poker”) is a managing partner of Modellwerkstatt, a company integrating customers in the engineering process. He received his Ph.D. in business administration at the Innsbruck University School of Management and holds a master's degree in economics and a degree in engineering. His research projects include quantitative methods in marketing, the study of consumer behavior within online environments, and the adoption of new technologies.Olivier Toubia (“Measuring Consumer Preferences Using Conjoint Poker”) is a professor of marketing at the Columbia Business School. He is a graduate from Ecole Centrale Paris, and he holds an M.S. in operations research and a Ph.D in marketing, both from MIT. His research interests include new product development, adaptive experimental design, conjoint analysis, preference measurement, idea generation, idea screening, the diffusion of innovation, behavioral economics, and social networks. Brian Wansink (“Package Size, Portion Size, Serving Size…Market Size: The Unconventional Case for Half-Size Servings”) is the John S. Dyson Endowed Chair at Cornell in the Applied Economics and Management Department at Cornell University, where he directs the Cornell Food and Brand Lab to uncover how the biases in our eating behavior and shopping behavior can be reversed in healthy, profitable, win-win ways. This is relevant to medicine, nutrition, obesity, public policy, and marketing. He received his Ph.D. from Stanford University in 1990. He is the former executive director of the U.S. Department of Agriculture's Center for Nutrition Policy and Promotion, a best-selling author, founder of the Smarter Lunchroom Movement, an Iowa native, and the current president of the Society for Nutrition Education and Behavior.Michel Wedel (“Ad Gist: Ad Communication in a Single Eye Fixation”) is the Pepsico Professor of Consumer Science at the Robert H. Smith School of Business of the University of Maryland. His main research interests are in the application of statistical and econometric methods to problems in marketing and marketing research. Much of his recent work addresses issues in visual marketing, using eye-tracking technology.Yinghui (Catherine) Yang (“Customer Bill of Rights Under No-Fault Service Failure: Confinement and Compensation”) is an assistant professor of the Graduate School of Management at University of California, Davis. She received her Ph.D. in operations and information management from the Wharton School of the University of Pennsylvania. Her current research focuses on service marketing and learning customer behavior from clickstream data. Her research has been published in Marketing Science, IEEE Transactions on Knowledge and Data Engineering, and the INFORMS Journal on Computing, among others.Hema Yoganarasimhan (“Cloak or Flaunt? The Fashion Dilemma”) is an assistant professor of marketing at the University of California, Davis. She has a Ph.D. in marketing from Yale University. Her research interests include fashion markets, consumer-generated media, empirical measurement of social influence, and online reputation systems.Sean X. Zhou (“Exclusive Channels and Revenue Sharing in a Complementary Goods Market”) is currently an assistant professor in the Department of Systems Engineering and Engineering Management, the Chinese University of Hong Kong. He received his B.S. in electrical engineering from Zhejiang University, China in 2001, and he received his M.S. and Ph.D. in operations research from North Carolina State University in 2002 and 2006, respectively. His main research interests include inventory control, pricing, and game-theoretic applications in supply chain management.
This study examines whether user-generated content (UGC) is related to stock market performance, which metric of UGC has the strongest relationship, and what the dynamics of the relationship are. We aggregate UGC from multiple websites over a four-year period across 6 markets and 15 firms. We derive multiple metrics of UGC and use multivariate time-series models to assess the relationship between UGC and stock market performance.Volume of chatter significantly leads abnormal returns by a few days (supported by Granger causality tests). Of all the metrics of UGC, volume of chatter has the strongest positive effect on abnormal returns and trading volume. The effect of negative and positive metrics of UGC on abnormal returns is asymmetric. Whereas negative UGC has a significant negative effect on abnormal returns with a short “wear-in” and long “wear-out,” positive UGC has no significant effect on these metrics. The volume of chatter and negative chatter have a significant positive effect on trading volume. Idiosyncratic risk increases significantly with negative information in UGC. Positive information does not have much influence on the risk of the firm. An increase in off-line advertising significantly increases the volume of chatter and decreases negative chatter. These results have important implications for managers and investors.
We estimate the joint impact of the frequency reward and customer tier components of a loyalty program on customer behavior and resultant sales. We provide an integrated analysis of a loyalty program incorporating customers' purchase and cash-in decisions, points pressure and rewarded behavior effects, heterogeneity, and forward-looking behavior. We focus on four key research questions: (1) How important is it to combine both components in one model? (2) Does points pressure exist in the context of a two-component loyalty program? (3) How is the market segmented in its response to the combined program? (4) Do the programs complement each other in terms of the incremental sales they produce?Our most basic message is that the frequency reward and customer tier components of loyalty programs should be modeled jointly rather than in separate models. We find strong evidence for points pressure for both the customer tier and frequency reward components using both model-based and model-free evidence. We find a two-segment solution revealing a “service-oriented” segment that highly values cash-ins for room upgrades and staying in “luxury” hotels, and a “price-oriented” segment that is more price sensitive and highly values the frequency reward aspects of the loyalty program. Furthermore, we find that both components generate incremental sales. Also, there was slight synergy between the programs but not a huge amount. Overall, each component contributes to increased revenues and does not interfere with the other.
When social influence plays a key role in the diffusion of new product, the value of a customer often goes beyond her own product purchase. We posit that a customer's value (CV) comes not only from her purchase value (PV) but also from her influence value (IV) (i.e., CV = PV + IV). Therefore, a customer's value can be far greater than her purchase value if she exerts a considerable influence on others. Building on a two-segment influential–imitator asymmetric influence model, we develop a model framework to derive closed-form expressions for PV, IV, and CV by customer segment as well as time of adoption, and we examine their comparative statics with respect to the diffusion parameters. A key parameter of our model framework is the social apportioning parameter, δ, which determines the credit a customer receives by influencing other potential adopters. We develop an endogenous method for determining δ as a function of the new product diffusion parameters. Our model framework allows us to investigate how a firm might accelerate product purchases by providing introductory discount offers to a targeted group of potential adopters at product launch. We find that purchase acceleration frequently leads to a significant increase in total customer value.
We investigate a business-to-business context and ask when and why a firm should announce a “reference program” that commits the firm to facilitating the flow of information about the efficacy of its products from early adopters to potential late adopters. We model a monopolist manufacturer with a new innovation that can be sold to two potential customers. We demonstrate here two benefits of a reference program that relate not to an increase in later adopters' willingness to pay but to an increase in the willingness to pay of the early adopters themselves. The impact on the early adopters' willingness to pay arises in two ways as a result of their observation of the firm's commitment to information transmission. First, in a model of symmetric uncertainty, we show that the announcement of a reference program facilitates dynamic pricing by the manufacturer in the sense that it allows the firm to provide temporary exclusive use of the technology to one of the customers. This creates more value, which the manufacturer can extract via a higher price. In this way, a reference program can serve as a partial substitute for an exclusive-use contract. In a model with asymmetric information, we demonstrate that under certain conditions, the firm is able to use the reference program as a signal—again, to the early adopting customer—that its technology is of high quality. However, such a signal requires significant discounts to early adopters to ensure separation. As a result, a pooling equilibrium dominates in which the manufacturer fosters references regardless of its quality. Finally, by allowing the firms' private information to be stochastic, we show that separation may be a dominant outcome.
When a firm can recognize its previous customers, it may use information about their past purchases to price discriminate. We study a model with a monopolist and a continuum of heterogeneous consumers, where consumers have the ability to maintain their anonymity and avoid being identified as past customers, possibly at a cost. When consumers can freely maintain their anonymity, they all individually choose to do so, which results in the highest profit for the monopolist. Increasing the cost of anonymity can benefit consumers but only up to a point, after which the effect is reversed. We show that if the monopolist or an independent third party controls the cost of anonymity, it often works to the detriment of consumers.
This study develops and estimates a dynamic model of consumer choice behavior in markets for seasonal goods, where products are sold over a finite season and availability is limited. In these markets, retailers often use dynamic markdown policies in which an initial retail price is announced at the beginning of the season and the price is subsequently marked down as the season progresses. Strategic consumers face a trade-off between purchasing early in the season, when prices are higher but goods are available, and purchasing later, when prices are lower but the stockout risk is higher. If the good starts providing utility as soon as it is purchased (e.g., apparel), consumers purchasing earlier in the season can also get more use from the product compared to those purchasing later.Our structural model incorporates three features essential for modeling the demand for seasonal goods: changing prices, limited availability, and possible dependence of total consumption utility on the time of purchase. In this model, heterogeneous consumers have expectations about future prices and product availability, and they strategically time their purchases. We estimate the model using aggregate sales and inventory data from a fashion goods retailer.The results indicate that, in the fashion goods context, ignoring consumers' expectations about future availability or the change in total consumption utility over the season can lead to biased demand estimates. We find that strategic consumers delay their purchases to take advantage of markdowns and that these strategic delays hurt the retailer's revenues. Retailer revenues facing strategic consumers are 9% lower than they would have been facing myopic consumers. Limited availability, on the other hand, reduces the extent of strategic delays by motivating consumers to purchase earlier. We find that the impact of strategic delays on retailer revenues would have been as high as 35% if there were no stockout risk. By means of counterfactual experiments, we show that the highest retailer profits are achieved by offering small markdowns early in the season. On the other hand, given current markdown percentages, the retailer can improve profits by carrying less stock as consumers accelerate purchases and purchase at higher prices when they anticipate scarcity in future periods. As long as the reduction in availability is not great, the profit gain from earlier higher-priced sales can overcome the loss resulting from the reduction in overall sales.
We use laboratory experiments to examine the relative performance of the English auction (EA) and the first-price sealed-bid auction (FPA) when procuring a commodity. The mean and variance of prices are lower in the FPA than in the EA. Bids and prices in the EA agree with game-theoretic predictions, but they do not agree in the FPA. To resolve these deviations found in the FPA, we introduce a mixture model with three bidding rules: constant absolute markup, constant percentage markup, and strategic best response. A dynamic specification in which bidders can switch strategies as they gain experience is estimated as a hidden Markov model. Initially, about three quarters of the subjects are strategic bidders, but over time, the number of strategic bidders falls to below 65%. There is a corresponding growth in those who use the constant absolute markup rule.
Quantity discount pricing is a common practice used by business-to-business and business-to-consumer companies. A key characteristic of quantity discount pricing is that the marginal price declines with higher purchase quantities. In this paper, we propose a choice-based conjoint model for estimating consumer-level willingness to pay (WTP) for varying quantities of a product and for designing optimal quantity discount pricing schemes. Our model can handle large quantity values and produces WTP estimates that are positive and increasing in quantity at a diminishing rate. In particular, we propose a tractable WTP function that depends on both product attributes and product quantity and that captures diminishing marginal WTP. We show how such a function embeds standard WTP functions in the quantity discount literature as special cases. We also demonstrate how to use the model to estimate the consumer value potential, which is the product of the premium a consumer is willing to pay and her volume potential. Finally, we propose a parsimonious experimental design approach for implementation.We illustrate the model using data from a conjoint study of online movie rental services. The empirical results show that the proposed model has good fit and predictive validity. In addition, we find that marginal WTP in this category decays rapidly with quantity. We also find that the standard choice-based conjoint model results in anomalous WTP distributions with negative WTP values and nondiminishing marginal willingness-to-pay curves. Finally, we identify four segments of consumers that differ in terms of magnitude of WTP and volume potential, and we derive optimal quantity discount schemes for a monopolist and a new entrant in a competitive market.
We propose a method to include seasonality in any diffusion model that has a closed-form solution. The resulting diffusion model captures seasonality in a way that naturally matches the original diffusion model's pattern. The method assumes that additional sales at seasonal peaks are drawn from previous or future periods. This implies that the seasonal pattern does not influence the underlying diffusion pattern. The model is compared with alternative approaches through simulations and empirical examples. As alternatives, we consider the standard Generalized Bass Model (GBM) and the basic Bass Model, which ignores seasonality. One of the main findings is that modeling seasonality in a GBM generates good predictions but gives biased estimates. In particular, the market potential parameter is underestimated. Ignoring seasonality in cases where data of the entire diffusion period are available gives unbiased parameter estimates in most relevant scenarios. However, ignoring seasonality leads to biased parameter estimates and predictions when only part of the diffusion period is available. We demonstrate that our model gives correct estimates and predictions even if the full diffusion process is not yet available.
Vincent Conitzer (“Hide and Seek: Costly Consumer Privacy in a Market with Repeat Purchases”) is the Sally Dalton Robinson Professor of Computer Science and Professor of Economics at Duke University. His research focuses on computational aspects of microeconomic theory—in particular, game theory, mechanism design, voting/social choice, and auctions. In 2011, he received the IJCAI Computers and Thought Award, which is awarded to outstanding young scientists in artificial intelligence.Dennis Fok (“Modeling Seasonality in New Product Diffusion”) is an associate professor of econometrics at the Erasmus School of Economics, Erasmus University Rotterdam. His research interests are in the fields of marketing and applied econometrics. These interests include modeling choice at an individual level as well as at an aggregated level; furthermore, he is interested in nonlinear panels and simulation-based estimation. He publishes on these topics in journals as Marketing Science, the Journal of Marketing Research, the Journal of AppliedEconometrics, and the Journal of Econometrics.Philip Hans Franses (“Modeling Seasonality in New Product Diffusion”) is a professor of applied econometrics and a professor of marketing research, both affiliated with the Erasmus School of Economics of the Erasmus University Rotterdam. His research interests include econometric models in marketing and forecasting. Currently, he serves as the dean of the Erasmus School of Economics.David Godes (“The Strategic Impact of References in Business Markets”) is an associate professor in the Department of Marketing at the Robert H. Smith School of Business, University of Maryland. He received a B.S. in economics at the University of Pennsylvania and an S.M. and Ph.D. in management science from the Massachusetts Institute of Technology. His research interests include word-of-mouth communication, social networks, media competition, and sales management. His work has appeared in Marketing Science, Management Science, Quantitative Marketing and Economics, and the Harvard Business Review.Teck-Hua Ho (“Customer Influence Value and Purchase Acceleration in New Product Diffusion”) is the William Halford Jr. Family Professor of Marketing and the director of the Asia Business Center at the Haas School of Business at the University of California, Berkeley. Currently on leave from UC Berkeley, he is working as the Vice President (Research Strategy) at the National University of Singapore, where he holds the Tan Chin Tuan Centennial Professorship. His research spans a wide range of topics in marketing and economics, and it has been internationally recognized; he was a finalist for several best paper awards, including the 2011 William F. O'Dell Award. Currently, he is the codepartment editor of behavioral economics for Management Science, and he serves as an area editor for the Journal of Marketing Research and Marketing Science.Raghuram Iyengar (“A Conjoint Model of Quantity Discounts”) is an assistant professor of marketing at the Wharton School of the University of Pennsylvania. He earned his Ph.D. from Columbia University and his B.Tech. from the Indian Institute of Technology Kanpur. His research focuses on pricing and social influence. His work has been published in Marketing Science, the Journal of Marketing Research, Quantitative Marketing and Economics, and Psychometrika.Kamel Jedidi (“A Conjoint Model of Quantity Discounts”) is the John A. Howard Professor of Marketing at Columbia Business School, Columbia University, New York. He holds a bachelor's degree in economics from the Faculté des Sciences Economiques de Tunis, Tunisia, and master's and Ph.D. degrees in marketing from the Wharton School of the University of Pennsylvania. His substantive research interests include pricing, product design and positioning, diffusion of innovations, market segmentation, and the long-term impact of advertising and promotions; his methodological interests lie in multidimensional scaling, classification, structural equation modeling, and Bayesian and finite-mixture models. He has extensively published in the leading marketing, statistics, and psychometric journals, the most recent of which include the Journal of Marketing Research, Marketing Science, Management Science, and Psychometrika.Praveen K. Kopalle (“The Joint Sales Impact of Frequency Reward and Customer Tier Components of Loyalty Programs”) is a professor of marketing at the Tuck School of Business at Dartmouth, Dartmouth College. He received his Ph.D. from Columbia University, New York, a PGDM from the Indian Institute of Management, Bangalore, and a B.E. from Osmania University, Hyderabad; prior to joining Tuck, he was on the faculty at the University of Arizona, Tucson. His research interests include new products/innovation, pricing and promotions, customer expectations, and e-commerce. He is on the editorial boards of Marketing Science, Marketing Letters, the Journal of Revenue and Pricing Management, the Journal of Interactive Marketing, the International Journal of Technology and Marketing, the International Journal of Electronic Commerce, and IIMB Management Review, and he serves as an associate editor of marketing science at the Journal of Retailing.Lakshman Krishnamurthi (“Demand Dynamics in the Seasonal Goods Industry: An Empirical Analysis”) is the Montgomery Ward Professor of Marketing at the Kellogg School of Management, Northwestern University, and has served as the chairman of the Marketing Department for 11 years. He is a past winner of the John D. C. Little award for the best paper published in Marketing Science. He is a coauthor (with Rakesh Vohra) of Principles of Pricing: An Analytical Approach, published by the Cambridge University Press.Shan Li (“Customer Influence Value and Purchase Acceleration in New Product Diffusion”) is a member of the research staff and a service scientist at Philips Research. She obtained her Ph.D. in industrial engineering and operations research at the University of California, Berkeley. Before she joined Philips Research, she was an operations research scientist at Amazon.com. Her research interests span a wide range of topics in business analytics and service science.Scott A. Neslin (“The Joint Sales Impact of Frequency Reward and Customer Tier Components of Loyalty Programs”) is the Albert Wesley Frey Professor of Marketing at the Tuck School of Business, Dartmouth College. His research applies statistical analysis to measuring marketing effectiveness and developing managerially relevant insights in the fields of customer relationship management and sales promotion. He is coauthor of Sales Promotion: Concepts, Methods and Strategies and Database Marketing: Analyzing and Managing Customers. He is an INFORMS Society for Marketing Science Fellow and a recipient of the John D. C. Little and Harold H. Maynard Awards.So-Eun Park (“Customer Influence Value and Purchase Acceleration in New Product Diffusion”) is a doctoral student in the marketing department at the Haas School of Business, University of California, Berkeley. Before coming to UC Berkeley, she earned her B.A. with honors in mathematics at Columbia University in 2009. Her research interest lies in new product diffusion and emerging markets, as well as relating behavioral economics to topics in marketing.Yuri Peers (“Modeling Seasonality in New Product Diffusion”) is a research fellow at the Waikato Management School, University of Waikato in Hamilton, New Zealand. He obtained his Ph.D. from the Erasmus University in Rotterdam in the field of quantitative marketing. His research interests are in marketing and applied econometrics.Jason Shachat (“Procuring Commodities: First-Price Sealed-Bid or English Auctions?”) is a professor of economics at the Wang Yanan Institute for Studies in Economics (WISE) and a member of the Ministry of Education (MOE) Key Laboratory in Econometrics at Xiamen University. He is also the director of the Finance and Economics Experimental Laboratory at Xiamen University. His research interests are in using laboratory experiments, applied microeconomic theory, and econometrics to study behavioral game theory, asset and commodity market performance, and auctions. His work has appeared in journals such as the Journal of Economic Theory, Games and Economic Behavior, the Journal of Mathematical Psychology, and Decision Analysis.Zuo-Jun Max Shen (“Customer Influence Value and Purchase Acceleration in New Product Diffusion”) is the Chancellor's Professor of Industrial Engineering and Operations Research at the University of California, Berkeley. He has been active in the following research areas: integrated supply chain design and management, market mechanism design, marketing–operations management interface issues, and decision making with limited information. He is currently on the editorial/advisory board for several leading journals.Gonca P. Soysal (“Demand Dynamics in the Seasonal Goods Industry: An Empirical Analysis”) is an assistant professor of marketing at the Naveen Jindal School of Management, University of Texas at Dallas. She received a B.S. degree in industrial engineering from the Middle East Technical University in Turkey, an M.E. degree in industrial and systems engineering from the University of Florida, and M.S. and Ph.D. degrees in marketing from the Kellogg School of Management, Northwestern University. Her research interests are focused on understanding dynamics in consumer and firm behavior with a special emphasis on the marketing of seasonal goods and retailing.Baohong Sun (“The Joint Sales Impact of Frequency Reward and Customer Tier Components of Loyalty Programs”) is the Dean's Distinguished Chair Professor of Marketing at Cheung Kong Graduate School of Business (CKGSB). Before joining CKGSB, she was the Carnegie Bosch Chair Professor of Marketing at the Tepper School of Business of Carnegie Mellon University. She develops dynamic structural models to investigate consumer choices and to evaluate the effectiveness of marketing mix and customer information management strategies. Her recent research interest focuses on modeling dynamic and interdependent consumer decisions on e-commerce and social media platforms.Yacheng Sun (“The Joint Sales Impact of Frequency Reward and Customer Tier Components of Loyalty Programs”) is an assistant professor of marketing at the Leeds School of Business, University of Colorado. He received a Ph.D. in marketing and an M.A. in economics from Indiana University at Bloomington and a B.A. in economics from Huazhong University of Science and Technology, China. His research interests include dynamic structural models of customer relationships, one-of-a-kind services (e.g., financial counseling), and peer influences in online social shopping networks.Vanitha Swaminathan (“The Joint Sales Impact of Frequency Reward and Customer Tier Components of Loyalty Programs”) is an associate professor of business administration and the Robert W. Murphy Faculty Fellow in Marketing at the University of Pittsburgh. Her research interests revolve around branding strategy and consumer–brand relationships; specifically, her focus is on understanding why consumers are attached to certain brands, the impact of various marketing actions on brand loyalty, and the conditions that foster consumer–brand relationships. Additionally, her research investigates how firms can successfully design brand strategies (e.g., cobranding, brand extensions, loyalty programs) to strengthen brand attachment. Her research has been published in journals such as the Journal of Marketing, Journal of Consumer Research, Journal of Marketing Research, Strategic Management Journal, and Journal of Consumer Psychology.Curtis R. Taylor (“Hide and Seek: Costly Consumer Privacy in a Market with Repeat Purchases”) is an economics faculty member at Duke University. He earned his Ph.D. from Yale in 1992. He has served as a national fellow of the Hoover Institution, a research fellow for The Alfred P. Sloan Foundation, and a research associate of the Private Enterprise Research Center at Texas A&M University. He has been awarded grants from the National Science Foundation, the United States Department of Agriculture, and the NET Institute, among others, and he has also held numerous editorial appointments.Gerard J. Tellis (“Does Chatter Really Matter? Dynamics of User-Generated Content and Stock Performance”) is a professor of marketing, management, and organization; Neely Chair of American Enterprise; and Director of the Center for Global Innovation at the USC Marshall School of Business. An expert in advertising, innovation, global market entry, new product growth, quality, and pricing, he has published four books and over 100 papers that have won over 20 awards, including the Frank M. Bass Award, the William F. O'Dell Award, and the Harold D. Maynard Award (twice). He is a Distinguished Professor of Marketing Research, Erasmus University, Rotterdam; a senior research Associate at the Judge Business School; a fellow of Sidney Sussex College, Cambridge University, United Kingdom; and a fellow of the INFORMS Society of Marketing Science. His Google cites number over 6000; more information can be found at http://www.gtellis.net.Seshadri Tirunillai (“Does Chatter Really Matter? Dynamics of User-Generated Content and Stock Performance”) is an assistant professor of marketing at the Bauer College of Business, University of Houston. He received his Ph.D. from the University of Southern California. He won an award for the best paper based on a doctoral dissertation at the 2009 European Marketing Academy Conference (EMAC) and an honorable mention in the 2008 Shankar-Spiegel Doctoral Dissertation Proposal Competition. His research interest is in online media, Internet marketing, and the financial impact of marketing.Liad Wagman (“Hide and Seek: Costly Consumer Privacy in a Market with Repeat Purchases”) is a member of the faculty at the Illinois Institute of Technology's Stuart School of Business. His research is focused on issues at the intersection of industrial organization, innovation, and information economics. He has served as a research fellow at the Social Science Research Institute and Computer Science Department at Duke University.Lijia Wei (“Procuring Commodities: First-Price Sealed-Bid or English Auctions?”) is a Ph.D. candidate at the Wang Yanan Institute for Studies in Economics (WISE) and member of the Ministry of Education (MOE) Key Laboratory in Econometrics at Xiamen University. His dissertation addresses the modeling of dynamic discrete heterogeneity of behavior in strategic situations. His research interests are in experimental economics and applied econometrics. He has published in Chinese-language journals on school matching and regime-switching models for conditional heteroskedasticity in financial markets.
The growth of the “social” Web has resulted in the enormous growth of what is referred to as user-generated content, or UGC. UGC takes the form of product reviews, descriptions of product usage, “homemade advertising,” blogs, and other consumer-initiated contributions. Following a research competition cosponsored by the Marketing Science Institute and the Wharton Interactive Media Initiative (now known as the Wharton Customer Analytics Initiative), a call for papers for a special issue of Marketing Science resulted in 69 submissions. Of these, eight papers were accepted, covering a range of issues such as how and why people make UGC contributions, the impact of UGC contributions, and new methods for analyzing UGC data.
Whereas recent research has demonstrated the impact of online product ratings and reviews on product sales, we still have a limited understanding of the individual's decision to contribute these opinions. In this research, we empirically model the individual's decision to provide a product rating and investigate factors that influence this decision. Specifically, we consider how previously posted ratings may affect an individual's posting behavior in terms of whether to contribute (incidence) and what to contribute (evaluation), and we identify selection effects that influence the incidence decision and adjustment effects that influence the evaluation decision.Across individuals, our results show that positive ratings environments increase posting incidence, whereas negative ratings environments discourage posting. Our results also indicate important differences across individuals in how they respond to previously posted ratings, with less frequent posters exhibiting bandwagon behavior and more active customers revealing differentiation behavior. These dynamics affect the evolution of online product opinions. Through simulations, we illustrate how the evolution of posted product opinions is shaped by the underlying customer base and show that customer bases with the same median opinion may evolve in substantially different ways because of the presence of a core group of “activists” posting increasingly negative opinions.
User-generated content is increasingly created through the collaborative efforts of multiple individuals. In this paper, we argue that the value of collaborative user-generated content is a function both of the direct efforts of its contributors and of its embeddedness in the content–contributor network that creates it. An analysis of Wikipedia's WikiProject Medicine reveals a curvilinear relationship between the number of distinct contributors to user-generated content and viewership. A two-mode social network analysis demonstrates that the embeddedness of the content in the content–contributor network is positively related to viewership. Specifically, locally central content—characterized by greater intensity of work by contributors to multiple content sources—is associated with increased viewership. Globally central content—characterized by shorter paths to the other collaborative content in the overall network—also generates greater viewership. However, within these overall effects, there is considerable heterogeneity in how network characteristics relate to viewership. In addition, network effects are stronger for newer collaborative user-generated content. These findings have implications for fostering collaborative user-generated content.
We measure the value of promotional activities and referrals by content creators to an online platform of user-generated content. To do so, we develop a modeling approach that explains individual-level choices of visiting the platform, creating, and purchasing content as a function of consumer characteristics and marketing activities, allowing for the possibility of interdependence of decisions within and across users. Empirically, we apply our model to Hewlett-Packard's (HP) print-on-demand service of user-created magazines, named MagCloud. We use two distinct data sets to show the applicability of our approach: an aggregate-level data set from Google Analytics, which is a widely available source of data to managers, and an individual-level data set from HP. Our results compare content creator activities, which include referrals and word-of-mouth efforts, with firm-based actions, such as price promotions and public relations. We show that price promotions have strong effects but are limited to the purchase decisions, whereas content creator referrals and public relations efforts have broader effects that impact all consumer decisions at the platform. We provide recommendations as to the level of a firm's investments when “free” promotional activities by content creators exist. These free marketing campaigns are likely to have a substantial presence in most online services of user-generated content.
The success of any user-generated content website depends crucially on its asset of content contributors. How firms should invest in the acquisition and retention of content contributors represents a novel question that is particularly important for these websites. We develop a vector autoregressive (VAR) model to measure the financial values of the retention and acquisition of both contributors and content consumers. In our empirical application to a customer-to-customer marketplace, we find that contributor (seller) acquisition has the largest financial value because of their strong network effects on content consumers (buyers) and other contributors. However, the wear-in of contributors' financial values takes longer because the network effects need time to be fully realized. Our simulation-based studies (i) shed light on the value implications of “enhancing network effects” and (ii) quantify the revenue contributions of marketing newsletter campaigns. Our results indicate that enhancing network effects in complementary ways can further increase the marginal benefits of acquisition and retention. We also find that simply tracking click-throughs may vastly underestimate the values of marketing newsletters—in our case, by more than a factor of 5—which may lead to suboptimal marketing effort allocation.
We investigate the evolution of online ratings over time and sequence. We first establish that there exist two distinct dynamic processes, one as a function of the amount of time a book has been available for review and another as a function of the sequence of reviews themselves. We find that, once we control for calendar date, the residual average temporal pattern is increasing. This is counter to existing findings that suggest that without this calendar-date control, the pattern is decreasing. With respect to sequential dynamics, we find that ratings decrease: the nth rating is, on average, lower than the n-1th when controlling for time, reviewer effects, and book effects. We test and find some support for existing theories for this decline based on motivation. We then offer two additional explanations for this “order effect.” We find support for the idea that one's ability to assess the diagnosticity of previous reviews decreases: when previous reviewers are very different, more reviews may thus lead to more purchase errors and lower ratings.
Volunteer users employ collaborative Internet technologies to develop open source products, a form of user-generated content, where time to product release is a crucial measure of project success. The open source community features two separate but related subcommunities: developer users who contribute time and effort to develop products and end users who act as collaborative testers and provide feedback. We develop hypotheses concerning how the location of the project's founders in the social network of developer users, the interplay of developer users and end users, and project and product characteristics affect time to product release. We use data on 817 development projects from SourceForge, a large open source community forum, to calibrate a split hazard model to test the hypotheses. That model supports the two-community conceptualization and most of the related hypotheses. The results have theoretical and managerial implications; for example, a pivotal position of founders in the developer user community can reduce time to product release by up to 31 and projects in which users are more engaged can experience an 11 time to product release compared with those projects in which they are not.
User-generated content on social media platforms and product search engines is changing the way consumers shop for goods online. However, current product search engines fail to effectively leverage information created across diverse social media platforms. Moreover, current ranking algorithms in these product search engines tend to induce consumers to focus on one single product characteristic dimension (e.g., price, star rating). This approach largely ignores consumers' multidimensional preferences for products. In this paper, we propose to generate a ranking system that recommends products that provide, on average, the best value for the consumer's money. The key idea is that products that provide a higher surplus should be ranked higher on the screen in response to consumer queries. We use a unique data set of U.S. hotel reservations made over a three-month period through Travelocity, which we supplement with data from various social media sources using techniques from text mining, image classification, social geotagging, human annotations, and geomapping. We propose a random coefficient hybrid structural model, taking into consideration the two sources of consumer heterogeneity the different travel occasions and different hotel characteristics introduce. Based on the estimates from the model, we infer the economic impact of various location and service characteristics of hotels. We then propose a new hotel ranking system based on the average utility gain a consumer receives from staying in a particular hotel. By doing so, we can provide customers with the “best-value” hotels early on. Our user studies, using ranking comparisons from several thousand users, validate the superiority of our ranking system relative to existing systems on several travel search engines. On a broader note, this paper illustrates how social media can be mined and incorporated into a demand estimation model in order to generate a new ranking system in product search engines. We thus highlight the tight linkages between user behavior on social media and search engines. Our interdisciplinary approach provides several insights for using machine learning techniques in economics and marketing research.
Web 2.0 provides gathering places for Internet users in blogs, forums, and chat rooms. These gathering places leave footprints in the form of colossal amounts of data regarding consumers' thoughts, beliefs, experiences, and even interactions. In this paper, we propose an approach for firms to explore online user-generated content and “listen” to what customers write about their and their competitors' products. Our objective is to convert the user-generated content to market structures and competitive landscape insights. The difficulty in obtaining such market-structure insights from online user-generated content is that consumers' postings are often not easy to syndicate. To address these issues, we employ a text-mining approach and combine it with semantic network analysis tools. We demonstrate this approach using two cases—sedan cars and diabetes drugs—generating market-structure perceptual maps and meaningful insights without interviewing a single consumer. We compare a market structure based on user-generated content data with a market structure derived from more traditional sales and survey-based data to establish validity and highlight meaningful differences.
Paulo Albuquerque (“Evaluating Promotional Activities in an Online Two-Sided Market of User-Generated Content”) is an assistant professor of marketing at the Simon Graduate School of Business, University of Rochester. He holds a Ph.D. in management from the UCLA Anderson School of Management. He is currently interested in competition and consumer behavior in online markets, new product diffusion across markets, and spatial competition models. He was named a 2011 MSI Young Scholar, and his articles have appeared in Marketing Science, the Journal of Marketing Research, and Management Science.Udi Chatow (“Evaluating Promotional Activities in an Online Two-Sided Market of User-Generated Content”) is a program and research manager at Hewlett-Packard (HP) Labs and a lead on MagCloud.com incubation, which he cofounded. He earned bachelor's and master's degrees in physics and medical physics from Tel Aviv University and an EMBA from Kellogg/Tel Aviv University in their international program. Since joining HP Labs in July 2005, he has led and supported several Web-to-print services and incubations; he previously spent 17 years at HP-Indigo, where he held various research and development positions such as research scientist, project manager, section manager, and director. He has over 30 patents awarded and is active in the information systems and technology organization and in nonimpact printing conferences.Kay-Yut Chen (“Evaluating Promotional Activities in an Online Two-Sided Market of User-Generated Content”) is a principal scientist at Hewlett-Packard (HP) Labs. He started behavioral economics research at HP Labs, a first in a corporation, after he received his Ph.D. from Caltech in 1994. He has pioneered the application of behavior economics to business issues in areas such as supply chain contracting and human-based forecasting, and his work has been featured in many popular publications such as Scientific American, Newsweek, the Wall Street Journal, and the Financial Times. He is the author of the book The Secrets of the Moneylab: How Behavioral Economics Can Improve Your Business, published by Portfolio in October 2010.Theodoros Evgeniou (“Content Contributor Management and Network Effects in a UGC Environment”) is an associate professor of decision sciences and technology management at INSEAD, Fontainebleau. His current research interests include preference measurement methods and market research, social networks, machine learning, and data analytics for marketing. He has published more than 30 top academic journal and conference papers.Moshe Fresko (“Mine Your Own Business: Market-Structure Surveillance Through Text Mining”) is a consulting expert on the topics of text mining, data mining, natural language programming, and machine learning. He holds a B.A. and an M.A. in computer engineering from Boğaziçi University, Istanbul, Turkey, and he received his Ph.D. in computer science from Bar-Ilan University, Israel. Between 2001 and 2010, he worked as a researcher and lecturer at Bar Ilan's Computer Science department, studying text mining, data mining, natural language programming, and machine learning, as well as teaching several programming-related courses; between 2007 and 2008, he worked as a visiting researcher and lecturer at the School of Business Administration at the Hebrew University of Jerusalem. He was active in the founding and progress of two text-mining related start-up companies.Ronen Feldman (“Mine Your Own Business: Market-Structure Surveillance Through Text Mining”) currently serves as the Head of the Internet Studies Department at the School of Business Administration of the Hebrew University of Jerusalem. He received his Ph.D. in computer science from Cornell University and his B.Sc. in math, physics, and computer science from the Hebrew University of Jerusalem. In 1997, he founded ClearForest, a Boston-based business intelligence company later acquired by Reuters. He coined the term “text mining” in 1995 and wrote the textbook The Text Mining Handbook: Advanced Approaches in Analyzing Unstructured Data (Cambridge University Press, 2007); he has given over 30 tutorials on text mining and information extraction and has written numerous scholarly papers on these topics.Anindya Ghose (“Designing Ranking Systems for Hotels on Travel Search Engines by Mining User-Generated and Crowdsourced Content”) is an associate professor in the Department of Information, Operations, and Management Sciences at the Stern School of Business of New York University. He received his Ph.D. from Carnegie Mellon University. His expertise is in analyzing how the massive amount of data generated by technological advances such as the Internet and mobile phones can influence marketing and advertising decisions, and his recent research interests include social media, mobile Internet, crowdfunding, Internet marketing, and digital advertising. He has received multiple best paper awards at premier conferences and journals, is a 2011 MSI Young Scholar, and is also a recipient of a National Science Foundation CAREER Award.David Godes (“Sequential and Temporal Dynamics of Online Opinion”) is an associate professor in the Marketing Department at the Robert H. Smith School of Business, University of Maryland. He received a B.S. in economics from the University of Pennsylvania and an S.M. and Ph.D. in management science from the Massachusetts Institute of Technology. His research interests include word-of-mouth communication, social networks, media competition, and sales management. His work has appeared in Marketing Science, Management Science, Quantitative Marketing and Economics, and the Harvard Business Review.Jacob Goldenberg (“Mine Your Own Business: Market-Structure Surveillance Through Text Mining”) is a professor of marketing at the School of Business Administration at the Hebrew University of Jerusalem and a visiting professor at the Columbia Business School. His research focuses on creativity, new product development, diffusion of innovation, complexity in market dynamics social networks effects, and social media. He has published papers in the Journal of Marketing, the Journal of Marketing Research, Management Science, Marketing Science, Nature Physics, and Science; in addition, he is an author of two books by the Cambridge University Press and one by the Chicago Press. His scientific work has been covered by the New York Times, the Wall Street Journal, the Boston Globe, the BBC News Harold Tribune, the Economist, and Wired Magazine.Rajdeep Grewal (“User-Generated Open Source Products: Founder's Social Capital and Time to Product Release”) is the Irving & Irene Bard Professor of Marketing at the Smeal College of Business at the Pennsylvania State University and is also the Associate Research Director of the Institute for the Study of Business Markets at the Smeal College of Business. He received his Ph.D. from the University of Cincinnati in 1998. His research focuses on empirical modeling of strategic marketing issues and has appeared in the top field journals. He has received several awards for his research, including a doctoral dissertation award from the Procter & Gamble Market Innovation Research Fund, an honorable mention award at the prestigious MSI/Journal of Marketing competition on “Linking Marketing to Financial Performance and Firm Value,” the 2003 Young Contributor Award from the Society of Consumer Psychology for his article in the Journal of Consumer Psychology, and the AMA Marketing Strategy SIG Early Career Award in 2007.Panagiotis G. Ipeirotis (“Designing Ranking Systems for Hotels on Travel Search Engines by Mining User-Generated and Crowdsourced Content”) is an associate professor in the Department of Information, Operations, and Management Sciences at the Stern School of Business of New York University. He received his Ph.D. degree in computer science from Columbia University in 2004, with distinction. His recent research interests focus on crowdsourcing and on mining user-generated content on the Internet. He has received three best paper awards (International Conference on Data Engineering 2005, ACM Special Interest Group on Management of Data 2006, and International World Wide Web Conference 2011), two best paper runner-up awards (Joint Conference on Digital Libraries 2002 and ACM Knowledge Discovery and Data Mining Conference 2008), and is also a recipient of a CAREER Award from the National Science Foundation.Zainab Jamal (“Evaluating Promotional Activities in an Online Two-Sided Market of User-Generated Content”) is a research scientist at Hewlett-Packard Labs. She holds a Ph.D. in marketing science from the University of California, Los Angeles. Her area of focus is in developing econometric and statistical models to understand and predict customer response behavior; this area feeds into the broader research stream of enabling businesses to optimize their marketing operations through analytical technologies in the backdrop of major paradigm shifts in the landscape such as personalized marketing. She brings deep industry experience to her research expertise, having worked in different roles in brand management and product development after receiving her master's in economics (Delhi School of Economics) and an MBA (Indian Institute of Management, Ahmedabad).Gerald C. Kane (“Network Characteristics and the Value of Collaborative User-Generated Content”) is an assistant professor of information systems at Boston College's Carroll School of Management. He received his Ph.D. from the Goizueta Business School of Emory University and his MBA in computer information systems from Georgia State University. His research interests include exploring the role of information systems in social networks, organizational applications and implications of social media, and the use of information technology in healthcare organizations; his published research has appeared in such journals as MIS Quarterly, Information Systems Research, Organization Science, and the Harvard Business Review. He is a recent recipient of a CAREER Award from the National Science Foundation for research on using social media to manage knowledge.Beibei Li (“Designing Ranking Systems for Hotels on Travel Search Engines by Mining User-Generated and Crowdsourced Content”) is a Ph.D. candidate in the Department of Information, Operations, and Management Sciences at the Stern School of Business, New York University. Her research interests lie at the intersection of economics of information technology, quantitative modeling, and machine learning; she is especially interested in the areas related to social media, search engines, and digital marketing. Recently, she received the Best Paper Award at the 20th International World Wide Web Conference (WWW 2011). She will be joining the Heinz College at Carnegie Mellon University in the fall of 2012 as an assistant professor.Gary Lilien (“User-Generated Open Source Products: Founder's Social Capital and Time to Product Release”) is the Distinguished Research Professor at Pennsylvania State University and cofounder and research director of the Institute for the Study of Business Markets (http://www.isbm.org). He is the author or coauthor of 12 books and over 100 professional articles. He is the former President as well as Vice President/Publications for The Institute of Management Sciences, the Vice President for External Relations and a Fellow of the European Marketing Academy, and is also the Vice President, External Relations for the INFORMS Society for Marketing Science (ISMS). He is an inaugural INFORMS Fellow, an Inaugural ISMS Fellow, was honored as a Morse Lecturer for INFORMS, and also received the Kimball Medal from INFORMS for distinguished contributions to the field of operations research; in 2010, the ISMS-MSI Practice Prize for the best applied work in marketing science globally was renamed the Gary Lilien ISMS-MSI Practice Prize in his honor.Nicholas H. Lurie (“Network Characteristics and the Value of Collaborative User-Generated Content”) is the ING Global Professor and associate professor of marketing at the University of Connecticut at Storrs. He received his Ph.D. from the Haas School at the University of California at Berkeley, his MBA from the Kellogg School at Northwestern University, and his A.B. from Vassar College. He conducts research on how consumers search for information and make decisions in information-rich environments, and his research has been published or is forthcoming in the Journal of Consumer Research, the Journal of Marketing Research, Marketing Science, the Journal of Marketing, and Organizational Behavior and Human Decision Processes. His article “Decision Making in Information Rich Environments: The Role of Information Structure” won the Ferber Award for the best article in the Journal of Consumer Research based on a doctoral dissertation.Girish Mallapragada (“User-Generated Open Source Products: Founder's Social Capital and Time to Product Release”) is an assistant professor of marketing at the Kelley School of Business at Indiana University. He received his Ph.D. in marketing from the Pennsylvania State University, and his primary research focus is on the topics of open innovation, social networks, marketing channels, and patent pools. His research on open source social networks has appeared in Management Science. He was on the editorial board of Marketing Science in 2007 and has served as a reviewer for Marketing Science, Information Systems Research, Management Science, the Journal of Marketing, and the Journal of Interactive Marketing; he also serves on the advisory board of Content Syndicate, a content syndication platform provider.Wendy W. Moe (“Online Product Opinions: Incidence, Evaluation, and Evolution”) is an associate professor of marketing at the Robert H. Smith School of Business at the University of Maryland. She earned her Ph.D., M.S., and B.S. from the Wharton School of the University of Pennsylvania and has her MBA from Georgetown University. She is an expert in the area of online behavior and early sales forecasting. Her research has focused on developing statistical methods and models for Internet clickstream data, online advertising, social media/user-generated content, and entertainment sales (e.g., sales of music, event tickets).Oded Netzer (“Mine Your Own Business: Market-Structure Surveillance Through Text Mining”) is the Phillip H. Geier Jr. Associate Professor of Business at Columbia University. He received an M.Sc. in statistics and a Ph.D. in business, both from Stanford University, and he also holds a B.Sc. in industrial engineering and management from the Technion (Israel Institute of Technology). His research interests focus on modeling customer relationships, preference measurement methods, and modeling various aspects of choice behavior, including how choices change over time, contexts, and customers; his research has appeared in Marketing Science, the Journal of Marketing Research, Marketing Letters, and the Journal of Consumer Psychology. He is the recipient of the John D. C. Little Award, the Frank M. Bass Award, and the Society of Consumer Psychology Best Competitive Paper Award.V. Padmanabhan (“Content Contributor Management and Network Effects in a UGC Environment”) is the John H. Loudon Professor of International Management at INSEAD, Singapore. His current research interests include the implications of economic crises, business opportunities and challenges in developing economies, and social networks. His research has generated numerous honors, including recognition as among the top 10 most influential papers published in the 50 years of publication of Management Science (1954–2004).Polykarpos Pavlidis (“Evaluating Promotional Activities in an Online Two-Sided Market of User-Generated Content”) is currently a Ph.D. candidate in marketing at the Simon Graduate School of Business, University of Rochester. He holds a university degree in economics and a postgraduate degree in business administration from the University of Macedonia, Greece, as well as an M.Sc. in applied economics from the University of Rochester. He has presented his research at the Marketing Science Conference, the Marketing Dynamics Conference, and at various universities. He has worked in the sector of consumer packaged goods distribution, has been a research intern with Hewlett-Packard Labs, and has also taught for Simon Graduate School of Business.Sam Ransbotham (“Network Characteristics and the Value of Collaborative User-Generated Content”) is an assistant professor at the Carroll School of Management at Boston College. He received his Ph.D., MSM, and BChE degrees from Georgia Tech. His current research interests include information technology (IT) security, social media, and the strategic use of IT; his research has appeared in Information Systems Research, Management Science, the MIS Quarterly, and the INFORMS Journal on Computing. He was also awarded 1 of 11 inaugural Google and WPP Marketing Awards to support research into how online media influences consumer behavior, attitudes, and decision making.Emile Richard (“Content Contributor Management and Network Effects in a UGC Environment”) is a Ph.D. candidate at Ecole Normale Supérieure de Cachan (France) and INSEAD in the area of machine learning for e-marketing applications. He is also a research assistant at the 1000mercis Research Lab.David A. Schweidel (“Online Product Opinions: Incidence, Evaluation, and Evolution”) is an assistant professor of marketing at the University of Wisconsin–Madison's School of Business. He earned a B.A. in mathematics in 2001 from the University of Pennsylvania, an M.A. in statistics in 2004, and a Ph.D. in 2006 from the Wharton School of the University of Pennsylvania. His research interests are in the development of stochastic models for media and customer relationship management applications. His current research projects include examining dynamics in social media.José C. Silva (“Sequential and Temporal Dynamics of Online Opinion”) is an associate researcher at the Fuqua School of Business, Duke University, and a Visiting Professor at The Lisbon MBA. He has undergraduate and graduate degrees in electrical engineering and computer science and an MBA from Portuguese universities, and he has a Ph.D. in management science from the Massachusetts Institute of Technology. Before joining Duke, he taught at the University of California, Berkeley and the Washington University in St. Louis. His research interests include advanced marketing analytics, social media, customer-driven innovation, and behavioral economics, and his work has appeared in the Journal of Experimental Psychology: General, Marketing Letters, and Microprocessors and Microsystems.Kaifu Zhang (“Content Contributor Management and Network Effects in a UGC Environment”) is a Ph.D. student in marketing at INSEAD, Fontainebleau. His current research interests include user-generated content, Internet communities, and contextual advertising.
This paper develops and calibrates a simple yet comprehensive set of models for the evolution of binary attribute importance weights, based on a cue–goal association framework. We argue that the utility a consumer ascribes to an attribute comes from its association with the achievement of a goal. We investigate how associations may be represented and then track back the relationship of these associations to the utility function. We explain why we believe this to be an important problem before providing an overview of the extensive literature on learning models. This literature identifies key phenomena and provides a foundation for our modeling of binary attribute importance learning, which can test for three departures from “rational” learning—bias, existence of priors, and the unequal weighting of sample observations (order effects). We apply our models in a laboratory setting under a number of different relationship strengths, and we find that, in our application, consumers' learning about attribute–goal associations exhibits bias and the effects of prior beliefs when the sample realizations occur with and without noise, and order effects when the sample realizations occur with noise. We provide an example of how our models can be extended to learning about more than one attribute.
We propose a new statistical instrument-free method to tackle the endogeneity problem. The proposed method models the joint distribution of the endogenous regressor and the error term in the structural equation of interest (the structural error) using a copula method, and it makes inferences on the model parameters by maximizing the likelihood derived from the joint distribution. Similar to the “exclusion restriction” in instrumental variable methods, extant instrument-free methods require the assumption that the unobserved instruments are exogenous, a requirement that is difficult to meet. The proposed method does not require such an assumption. Other benefits of the proposed method are that it allows the modeling of discrete endogenous regressors and offers a new solution to the slope endogeneity problem. In addition to linear models, the method is applicable to the popular random coefficient logit model with either aggregate-level or individual-level data. We demonstrate the performance of the proposed method via a series of simulation studies and an empirical example.
This paper analyses firms' decisions to provide connectivity to their customers. We distinguish between intraconnectivity—the ability of one firm's customers to connect to each other—and interconnectivity—the ability of one firm's customers to connect with another firm's customers. The profitability implications of allowing connectivity are not a straightforward consequence of the consumer value of connectivity, because connectivity affects not only the customer value but also the intensity of competition by creating or changing network externality. We find that if sales are driven by brand switching rather than by category expansion, a firm may find it optimal not to provide intraconnectivity even if providing it is not costly and may find it optimal to provide interconnectivity even at a cost exceeding the consumer value of connectivity. On the other hand, if category expansion is possible, providing intraconnectivity may be profitable. In this case, either the equilibrium intraconnectivity provision may be asymmetric or both firms may find it (individually) optimal to provide intraconnectivity. Under certain conditions in the latter case, the firms' choice of intraconnectivity is a prisoner's dilemma game.
Social sharing of information goods—wherein a single good is purchased and shared through a network of acquaintances such as friends or coworkers—is a significant concern for the providers of these goods. The effect of social sharing on firm pricing and profits depends critically on two elements: the structure of the underlying consumer network and the mechanism used by groups to decide whether to purchase at a given price. We examine the effect of social sharing under different network structures (decentralized, centralized, and complete), which reflect a range of market conditions. Moreover, we draw from the mechanism design literature to examine several approaches to group decision making. Our results suggest that a firm can benefit from increased social sharing if the level of sharing is already high, enabling a pricing strategy targeted primarily at sharing groups rather than individuals. However, the point at which sharing becomes marginally beneficial for a firm depends on both the distribution of group sizes (which derives from the network structure) and the group decision mechanism. Additional insights are obtained when we extend the model to capture homophily in group formation and the potential that a subset of consumers will never share for ethical reasons.
Each year in the postsecondary education industry, schools offer admission to nearly 3 million new students and scholarships totaling nearly $100 billion. This is a large, understudied targeted marketing and price discrimination problem. This problem falls into a broader class of configuration utility problems (CUPs), which typically require an approach tailored to exploit the particular setting. This paper provides such an approach for the admission and scholarship decisions problem. The approach accounts for the key distinguishing feature of this industry—schools value the average features of the matriculating students such as percent female, percent from different regions of the world, average test scores, and average grade point average. Thus, as in any CUP, the value of one object (i.e., student) cannot be separated from the composition of all of the objects (other students in the enrolling class). This goal of achieving a class with a desirable set of average characteristics greatly complicates the optimization problem and does not allow the application of standard approaches. We develop a new approach that solves this more complex optimization problem using an empirical system to estimate each student's choice and the focal school's utility function. We test the approach in a field study of an MBA scholarship process and implement adjusted scholarship decisions. Using a holdout sample, we provide evidence that the methodology can lead to improvements over current management decisions. Finally, by comparing our solution to what management would do on its own, we provide insight into how to improve management decisions in this setting.
The question as to the optimality of advertising pulsing has attracted many researchers over the last half-century. In this paper we specify a market share model in which there are two advertising-setting firms as well as a no-purchase option. The framework is that of a first-order Markov process with three states. The objective of both firms is to maximize profits. We are able to demonstrate, for a diminishing returns advertising function, that the optimal advertising strategy is pulsing. The frequency of the advertising pulse is shown to depend on the magnitude of the market share retention rate (state dependence); the higher it is, the less frequent the advertising. We further find that the optimal advertising budgets do not remain the same when the frequency of pulsing changes. Finally, we show that it is optimal for both firms to advertise in phase.
We study a model of film distribution and consumption. The studio can release two goods, a theatrical version and a video version, and has to decide on its versioning and sequencing strategy. In contrast with the previous literature, we allow for the possibility that some consumers may watch both versions. This simple extension leads to novel results. It now becomes optimal to introduce versioning if the goods are not too substitute for one another, even when production costs are zero (pure information goods). We also demonstrate that the simultaneous release of the versions (“day-and-date” strategy) can be optimal when the studio is integrated with the exhibition and distribution channels. In contrast, a sequential release (“video window” strategy) is typically the outcome when the studio negotiates with independent distributors and exhibitors.
Consumer new product adoption and preference evolution or learning may be influenced by intrinsic or internal factors (e.g., usage experiences, personal characteristics), external influences (e.g., social effects, media), and marketing activities of the firm. Moreover, the preference evolution in a certain category can spill over to other categories; i.e., consumers can exhibit cross-category learning. In this paper, we develop a multicategory framework to analyze the role of the above elements in the formation and evolution of consumer preferences across categories. We analyze these elements by employing multiple data sets, i.e., by combining revealed preference data (from scanner panel), stated data (from surveys measuring consumer lifestyle variables and demographics), and external influences (e.g., media mentions) in a completely heterogeneous framework while considering other facets of the learning process. By jointly estimating the model for organic purchases in six distinct food categories, we also explore the role of category differences. Results show that consumer new product adoption and learning is indeed impacted significantly and to various degrees by the aforementioned factors. We show how, by selectively encouraging purchases under various scenarios, firms can accelerate the learning process, not only for the focal category but also for other categories, thereby realizing considerable incremental profits. These results can be used by both manufacturers and retailers for more efficient allocation of marketing budgets across (new) products.
We show how networks modify the diffusion curve by affecting its symmetry. We demonstrate that a network's degree distribution has a significant impact on the contagion properties of the subsequent adoption process, and we propose a method for uncovering the degree distribution of the adopter network underlying the dissemination process, based exclusively on limited early-stage penetration data. In this paper we propose and empirically validate a unified network-based growth model that links network structure and penetration patterns. Specifically, using external sources of information, we confirm that each network degree distribution identified by the model matches the actual social network that is underlying the dissemination process. We also show empirically that the same method can be used to forecast adoption using an estimation of the degree distribution and the diffusion parameters at an early stage (15%) of the penetration process. We confirm that these forecasts are significantly superior to those of three benchmark models of diffusion.Our empirical analysis indicates that under heavily right-skewed degree distribution conditions (such as scale-free networks), the majority of adopters (in some cases, up to 75%) join the process after the sales peak. This strong asymmetry is a result of the unique interaction between the dissemination process and the degree distribution of its underlying network.
Alexandre Belloni (“Optimal Admission and Scholarship Decisions: Choosing Customized Marketing Offers to Attract a Desirable Mix of Customers”) is an associate professor of decision sciences at the Fuqua School of Business, Duke University. He received a Ph.D. from the Massachusetts Institute of Technology and was awarded an IBM Herman Goldstine Postdoctoral Fellowship. His research interests include high-dimensional statistics and econometrics, mechanism design, and marketing applications. Ram Bezawada (“Investigating the Drivers of Consumer Cross-Category Learning for New Products Using Multiple Data Sets”) is an assistant professor of marketing at the School of Management, State University of New York at Buffalo. He has a Ph.D. in marketing from the Krannert Graduate School of Management and an M.S. in statistics from the College of Science, respectively, from Purdue University. His research interests include aspects related to shopper marketing, health behavior, new product development, and branding. He is also exploring topics related to multichannel shopping, social media, digital marketing, and new media.William Boulding (“Optimal Admission and Scholarship Decisions: Choosing Customized Marketing Offers to Attract a Desirable Mix of Customers”) is the Dean and JB Fuqua Professor of Business Administration at the Fuqua School of Business, Duke University. His research interests lie at the intersection of management, marketing, and strategy. He won the 1998 William F. O'Dell Award and the 2006 Harold H. Maynard Award. He is thrilled to see his longtime research collaborator, Rick Staelin, happily teaching core marketing!Joan Calzada (“Intertemporal Movie Distribution: Versioning When Customers Can Buy Both Versions”) is an associate professor of economics at the University of Barcelona and a member of the research group on governments and markets. He holds an M.S. in economics from the University College London and a Ph.D. in economics from the University of Barcelona. His research interests are industrial organization and public economics, and he has published in academic journals such as the Economic Journal, Information Economics and Policy, Journal of Regulatory Economics, and Review of Industrial Organization.Mathew B. Chylinski (“Consumer Learning of New Binary Attribute Importance Accounting for Priors, Bias, and Order Effects”) is a faculty member in the School of Marketing at the Australian School of Business, the University of New South Wales. He is a graduate of the University of New South Wales with honours in economics, and he earned a Ph.D. from the Australian Graduate School of Management. He studies how consumers learn to develop preferences over time and how innovations are adopted based on the structure of social networks. Yaniv Dover (“Network Traces on Penetration: Uncovering Degree Distribution from Adoption Data”) is a postdoctoral associate at the Yale School of Management at Yale University. His research interests include word of mouth, social networks, economics of information, and diffusion of innovation.Marshall Freimer (“Periodic Advertising Pulsing in a Competitive Market”) is a professor of management science and computers and information systems at the William E. Simon Graduate School of Business Administration at the University of Rochester. He holds a Ph.D. in mathematics from Harvard University. He uses applied probability and decision sciences to analyze problems in information management, electronic commerce, and marketing. With Len Simon, he coauthored Analytical Marketing, which appeared in 1970 and was one of the first published books in the area of marketing science.Michael R. Galbreth (“Social Sharing of Information Goods: Implications for Pricing and Profits”) is an associate professor of management science at the Moore School of Business, University of South Carolina. He holds a Ph.D. from Vanderbilt University and has been awarded a Fulbright Scholarship for his research on sustainable operations. His research interests include sustainable business practices, the business impact of emerging technologies, and the interface between operational and marketing decision making. His work has appeared in journals such as Production & Operations Management, MIS Quarterly, and Interfaces.Bikram Ghosh (“Social Sharing of Information Goods: Implications for Pricing and Profits”) is an assistant professor of marketing at the Moore School of Business, University of South Carolina. He holds an M.S. in economics and Ph.D. in marketing from Purdue University. His research focuses on analytical and empirical models of product bundling, competitive effects of advertising avoidance technologies such as digital video recorders, and the effect of sustainable business practices on product market competition; he is also interested in analytical models of social networks. His research has appeared in Marketing Science and Management Science.Jacob Goldenberg (“Network Traces on Penetration: Uncovering Degree Distribution from Adoption Data”) is a professor of marketing at the School of Business Administration at the Hebrew University of Jerusalem and a visiting professor at the Columbia Business School. His research focuses on creativity, new product development, diffusion of innovation, complexity in market dynamics, and social networks effects. He has published in journals such as the Journal of Marketing, Journal of Marketing Research, Management Science, Marketing Science, Nature Physics, and Science; in addition, he is an author of two books published by the Cambridge University Press. His scientific work has been featured in the New York Times, the Wall Street Journal, the Boston Globe, the BBC News Herald Tribune, the Economist, and Wired Magazine.Sachin Gupta (“Handling Endogenous Regressors by Joint Estimation Using Copulas”) is the Henrietta Johnson Louis Professor of Management and Professor of Marketing at the Samuel Curtis Johnson Graduate School of Management at Cornell University. He received his Ph.D. from Cornell as well. His earlier papers have been honored with the O'Dell Award and the Paul Green Award of the American Marketing Association, and he is also the recipient of several teaching awards. He is on the editorial boards of the Journal of Marketing Research and Marketing Science.Bruce G. S. Hardie (“Consumer Learning of New Binary Attribute Importance Accounting for Priors, Bias, and Order Effects”) is a professor of marketing at the London Business School. His primary research interest lies in the development of data-based models to support marketing analysts and decision makers, with a particular interest in models that are easy to implement. Most of his current projects focus on the development of probability models for customer-base analysis.Tingting He (“Intraconnectivity and Interconnectivity: When Value Creation May Reduce Profits”) is an assistant professor in marketing at the Lubar Business School, University of Wisconsin–Milwaukee. In 2008, she obtained a Ph.D. in marketing from the Olin Business School, Washington University in St. Louis. Her research interests include quantitative analysis of competitor and consumer behavior.Dan Horsky (“Periodic Advertising Pulsing in a Competitive Market”) is the Benjamin L. Forman Professor of Marketing at the William E. Simon Graduate School of Business, University of Rochester. He has published on a wide variety of marketing topics and has twice won the John D. C. Little Best Paper Award. His outside interests include swimming and art collecting.Dmitri Kuksov (“Intraconnectivity and Interconnectivity: When Value Creation May Reduce Profits”) is an associate professor at the Olin Business School of the Washington University in St. Louis and holds a Ph.D. in marketing from Haas Business School of the University of California, Berkeley. His research interests include studying competitive strategy, markets with incomplete information, consumer communication and networks, branding and product line strategy, and customer satisfaction. His work has appeared in a number of journals, including Marketing Science, Management Science, the Journal of Marketing Research, and the Journal of Economic Theory. He received the 2005 Frank M. Bass Dissertation Award, and two of his papers were finalists for the 2007 John D. C. Little Award.Mitchell J. Lovett (“Optimal Admission and Scholarship Decisions: Choosing Customized Marketing Offers to Attract a Desirable Mix of Customers”) is an assistant professor of marketing at the University of Rochester's Simon Graduate School of Business. He received a Ph.D. in business administration from Duke University and an MBA from Boise State University. His research interests include advertising, targeted marketing, consumer learning, and political marketing.Chakravarthi Narasimhan (“Intraconnectivity and Interconnectivity: When Value Creation May Reduce Profits”) is the Philip L. Siteman Professor of Marketing in the Olin Business School at the Washington University in St. Louis. His current research interests are in modeling demand in pharmaceutical and telecommunication markets, understanding the impact of promotions on brands, examining interaction of multiple marketing strategies, and investigating supply chain contracts, especially supply chain strategies under uncertainty. He is an associate editor of Marketing Science and Quantitative Marketing and Economics, and his work has published in Marketing Science, Management Science, the Journal of Marketing Research, the Journal of Marketing, the Journal of Business, the Journal of Econometrics, and Harvard Business Review, among others. Sungho Park (“Handling Endogenous Regressors by Joint Estimation Using Copulas”) is an assistant professor of marketing at the W. P. Carey School of Business, Arizona State University. He received his Ph.D. in marketing from the Johnson School of Management, Cornell University. His research interests include dynamics in consumer choice and applications of structural econometric models to marketing. His research has been published in journals such as Marketing Science, the Journal of Marketing Research, and the Journal of Forecasting. John H. Roberts (“Consumer Learning of New Binary Attribute Importance Accounting for Priors, Bias, and Order Effects”) holds a joint appointment as professor of marketing at the Australian National University and the London Business School. He is a winner of the American Marketing Association's William O'Dell Award, its John Howard Award, and its ART Forum Best Paper Award. He has been a finalist for the John D. C. Little Award three times and the Gary Lilien Marketing Science Practice Prize twice. He sits on the editorial boards of Marketing Science, the Journal of Marketing Research, the Australian Journal of Management, and the Australasian Marketing Journal of Management, and he is an associate editor of the International Journal of Research in Marketing and the Journal of Forecasting. Daniel Shapira (“Network Traces on Penetration: Uncovering Degree Distribution from Adoption Data”) is a lecturer for the Department of Business Administration at the Guilford Glazer Faculty of Business and Management in Ben-Gurion University. His research concentrates on the complexity of market dynamics, diffusion of innovation, and emergence of collective behavior in social systems. He applies in his interdisciplinary work ideas and methodologies that are rooted in the field of statistical mechanics to the social sciences.Mikhael Shor (“Social Sharing of Information Goods: Implications for Pricing and Profits”) is an assistant professor of economics at the University of Connecticut. He earned his bachelor's degree in economics and foreign affairs at the University of Virginia and his Ph.D. in economics at Rutgers University.Karthik Sridhar (“Investigating the Drivers of Consumer Cross-Category Learning for New Products Using Multiple Data Sets”) is an assistant professor of marketing at the Dauch College of Business and Economics, Ashland University, Ohio. He holds a Ph.D. in marketing with a minor in economics from the School of Management, State University of New York at Buffalo. His research interests lies primarily in areas related to consumer shopping behavior and learning, contemporary retail promotions, new product development, and health product marketing. He also explores topics associated with social influences and new age digital media influences on consumer choice.Richard Staelin (“Optimal Admission and Scholarship Decisions: Choosing Customized Marketing Offers to Attract a Desirable Mix of Customers”) is the Edward and Rose Donnell Professor of Business Administration at the Fuqua School of Business, Duke University. A former editor of Marketing Science and president of the INFORMS Society for Marketing Science, he has won best paper awards for Marketing Science, the Journal of Marketing Research, and the Journal of Marketing, and he was awarded the Converse Award and the AMA's Best Educators award. After serving as an administrator for many years, he has gone back to the classroom to teach the core marketing course and finds it a great pleasure. A former runner, he now finds himself limited to the elliptical machine and interacting with his five grandchildren.Minakshi Trivedi (“Investigating the Drivers of Consumer Cross-Category Learning for New Products Using Multiple Data Sets”) is a professor of marketing at the School of Management, State University of New York at Buffalo. She holds a Ph.D. in management science with a minor in operations research from the University of Texas at Dallas and master's degrees in business administration and chemistry, with distinction in both. Her research interests lie in the areas of distribution channels and in the empirical application of quantitative modeling to study the impact of various factors such as format, pricing, health orientation, and more recently, communications and social media, on retailing. She has published in a variety of journals, including Management Science, Marketing Science, and the Journal of Retailing. Tommaso M. Valletti (“Intertemporal Movie Distribution: Versioning When Customers Can Buy Both Versions”) is a professor of economics both at Imperial College London and at the University of Rome. He has a magna cum laude degree in engineering from Turin and holds an M.S. and a Ph.D. in economics from the London School of Economics. His research interests are in industrial economics, regulation, and telecoms economics. He was a board director of Consip, the Italian Public Procurement Agency, in 2002—2006, and he has advised numerous bodies, including the European Commission, Organisation for Economic Co-operation and Development, and World Bank, on topics such as network interconnection, mobile telephony markets, and spectrum auctions.
This paper investigates how firms responded to standardized nutrition labels on food products required by the Nutrition Labeling and Education Act (NLEA). Using a longitudinal quasi-experimental design, we test our predictions using two large-scale samples that span 30 product categories. Results indicate that the NLEA reduced brand nutritional quality relative to a control group of products not regulated by the NLEA. At the same time, among regulated products, brand taste increased. Although this reduction in nutrition represents an unintended consequence of regulation, there were a set of category, firm, and brand conditions under which the NLEA produced a positive effect on brand nutritional quality. We find that firms were more likely to improve brand nutrition when firm risk or firm power is low. Lower risk occurs when the firm is introducing a new brand rather than changing an existing brand, and weaker power in a category is reflected by lower market share in a category. Furthermore, firms competing in low-health categories (e.g., potato chips) or small-portion categories (e.g., peanut butter) improved nutrition more than firms competing in high-health categories (e.g., bread) or large-portion categories (e.g., frozen dinners). Recommendations for firm strategy and the design of consumer information policy are examined in light of these surprising firm responses.
This series of discussions presents commentaries and a response on the impact the Nutrition Labeling and Education Act of 1990 has had on brand nutritional quality and taste as raised in Moorman et al. [Moorman C, Ferraro R, Huber J (2012) Unintended nutrition consequences: Firm responses to the Nutrition Labeling and Education Act. Marketing Sci. 31(5):717–737].
This paper asks whether brand extension can serve as a signal of product quality given that it costs less than a new brand. (Existing literature has assumed either that brand extension is cost-neutral or that it costs more.) I show that it can as a perfect Bayesian equilibrium, but the argument is unconvincing. For one thing, the separating equilibrium is not unique; a pooling equilibrium also exists in which brand extension signals nothing. For another, the separating equilibrium relies on off-equilibrium beliefs that are poorly motivated in the model. I propose a refinement of the perfect Bayesian equilibrium that resolves both issues. Empirical off-equilibrium beliefs require that consumers' off-equilibrium beliefs be justifiable on the basis of their prior beliefs and product performance observations. With empirical off-equilibrium beliefs, two necessary conditions for brand extension to signal product quality are identified: (i) consumers must perceive old and new products of the firm to be positively correlated in quality, and (ii) at least some consumers must identify with brands and not the firm behind the brands. Even with these conditions in place, the signaling argument is fragile: firm observability of past performance diminishes brand extension's signaling capability; an arbitrarily small probability of failure for good products eliminates it. My results suggest that, going forward, the case for brand extension must rest on foundations other than signaling product quality.
This series of discussions presents commentaries and a rejoinder on the economic perspectives on branding arising from Moorthy [Moorthy S (2012) Can brand extension signal product quality? Marketing Sci. 31(5):756–770].
This research examines how individual differences and institutional practices influence consumer bidding in auctions. Bidders may be motivated by different goals, e.g., thrill (of winning the item, with minimal attention to what they pay for it) versus prudence (winning the item at a price at or below its perceived value). Also, innate or auctioneer-induced differences may exist in the precision and salience of bidder cognitions about the item's value. We report two studies on how these motivational and cognitive factors influence bids in descending and ascending auctions, respectively. Each study also manipulated a situational variable (wait time at each price step). The two auctions realized different average prices for the same item set. Average bids were higher in the descending (versus ascending) auction in several study conditions. In both auction formats, bidders primed with thrill (versus prudence) bid higher, but more precise and/or salient values attenuated this goal effect. Among other results, in the descending auction, longer wait times elicited higher bids from bidders primed with thrill (but not prudence). In the ascending auction, longer wait times produced lower bids for bidders primed with prudence (but not thrill). These findings on consumer bidding behavior have practical implications for auction design.
Firms in a variety of industries offer add-on products to consumers who have previously purchased a base product. We posit that consumers, in making their decisions as to whether to purchase add-ons that complement the base products, find a greater need for the value offered by the add-ons when the “unrecovered” value (i.e., price paid minus the benefits obtained so far) associated with the base products is higher. We conduct experiments that test the proposed hypothesis and examine the strategic implications of such consumer decision making to a firm that sells base product add-on pairs.Consistent with our hypothesis, the experiments show that a consumer's unrecovered value associated with the base product is positively correlated to his likelihood of purchasing the add-on. Formal modeling of this bias shows that firms may find penetration pricing strategies (such as loss leader pricing) suboptimal. Furthermore, the identified bias leads the firm to spend more resources toward enhancing both the base product and the add-on quality, especially so when the add-on will be offered before the consumer has a chance to extensively use the base product. Finally, the effect of competition in the base product market is also considered.
Firms that sell via a direct channel and via indirect channels have to decide whether to allow third-party sellers to use trademarked brand names of products in their advertising. This question has been particularly controversial for advertising on search engines. In June 2009, Google started allowing any third-party reseller of a product to use a trademark such as “DoubleTree” in the text of its ad, even if the reseller did not have the trademark holder's permission. We study the effects of this change empirically within the hotel industry. We find some evidence that allowing third-party sellers to use a trademark in their online search advertising weakly reduced the likelihood of a consumer clicking on a trademark holder's paid search ads. However, the decrease in paid clicks was outweighed by a large increase in consumers clicking on the unpaid links to the hotelier's website within the main search results. Our evidence shows that when a third-party seller focuses on a trademarked brand in its ads, the ads become less distinct, and customers are more likely to ignore the advertised offers and buy from the direct channel.
In recent years academic research has focused on understanding and modeling the survey response process. This paper examines an understudied systematic response tendency in surveys: the extent to which observed responses are subject to state dependence, i.e., response carryover from one item to another independent of specific item content. We develop a statistical model that simultaneously accounts for state dependence, item content, and scale usage heterogeneity. The paper explores how state dependence varies by response category, item characteristics, item sequence, respondent characteristics, and whether it becomes stronger as the survey progresses. Two empirical applications provide evidence of substantial and significant state dependence. We find that the degree of state dependence depends on item characteristics and item sequence, and it varies across individuals and countries. The article demonstrates that ignoring state dependence may affect reliability and predictive validity, and it provides recommendations for survey researchers.
It is well known that individuals often fail to exert proper self-control. In organizational settings, this can lead to reduced productivity and profits. We use the literature on present-biased preferences to model employees' self-control problems and examine how firms can design compensation plans to reduce the negative consequences of their employees' self-control problems. Our results suggest that firms can mitigate self-control problems by delaying payment to the employees. This can be achieved by using multiperiod quotas (such as annual quotas) to compensate employees for their cumulative performance. Although such plans are prevalent in the market, there is little theoretical research that shows when multiperiod quota plans can be optimal. The paper provides one potential explanation for the widespread use of such quota plans. Interestingly, we find that such plans may be optimal despite the fact that they encourage more procrastination. We also find that such plans lead to higher effort by the employees and can sometimes improve the welfare of not only the firm but also the employees.
Sreekumar R. Bhaskaran (“Consumer Mental Accounts and Implications to Selling Base Products and Add-ons”) is an assistant professor of operations management at the Cox School of Business, Southern Methodist University. He has a B.E. in mechanical engineering from the Indian Institute of Technology Madras, an MBA in operations and marketing from the Indian Institute of Management Calcutta, and a Ph.D. in supply chain and operations management from the McCombs School of Business, University of Texas at Austin. His primary research interests include new product development, supply chain management, and marketing and operation interfaces. His work has previously appeared in Management Science, Marketing Science, and Production and Operations Management.Dondeena Bradley (“Further Examining the Impact of the NLEA on Nutrition”) is the Vice President, Global R&D and Nutrition Ventures, at PepsiCo, where she is responsible for designing new solutions that target the special needs of consumers with diverse health and nutrition challenges. Prior to joining PepsiCo in 2007, she held numerous roles in the areas of strategy, nutrition, and health with Johnson & Johnson, Mars Inc., the Stepan Company, and the Campbell Soup Company. She received her Ph.D. in food science from The Ohio State University, her M.S. in nutrition from Purdue University, and her B.S. from Anderson University.Dipankar Chakravarti (“Bidding Behavior in Descending and Ascending Auctions”) is a professor of marketing at the Johns Hopkins Carey Business School, where he served as Vice Dean, Programs, and is also a professor emeritus at the University of Colorado, Boulder, where he was the Ortloff Professor of Business. He holds a Ph.D. in industrial administration from Carnegie Mellon University and has taught previously at the University of Arizona, Duke, and University of Florida. His current research examines marketing and consumer behavior issues in emerging economies, with a focus on the psychology of consumption in poverty and development. His research on consumer and managerial decision making in marketing contexts has been published in the field's leading scholarly journals and received several significant academic recognitions. Among his other contributions to the marketing field are two sons—one a practitioner and the other an academic; he also has three grandsons who he hopes will also publish in Marketing Science one day.Amar Cheema (“Bidding Behavior in Descending and Ascending Auctions”) is an associate professor of marketing at the McIntire School of Commerce, University of Virginia. He received his Ph.D. from the University of Colorado, Boulder. His research interests include auctions and online purchase behavior, pricing and promotion effects, behavioral decision theory, and word-of-mouth influences.Lesley Chiou (“How Does the Use of Trademarks by Third-Party Sellers Affect Online Search?”) is an associate professor of economics at Occidental College. She received her Ph.D. in economics from the Massachusetts Institute of Technology. She is interested in industrial organization and applied econometrics, and her research focuses on online advertising and competition in the retail sector.Martijn G. de Jong (“State-Dependence Effects in Surveys”) holds a Chair in Marketing Research at the Erasmus School of Economics, Erasmus University, and is a Tinbergen Research Fellow. He has a B.Sc. and M.Sc. in econometrics from Erasmus University and a Ph.D. in marketing from Tilburg University. He is mainly interested in consumer preference measurement; often his research is cross-cultural in nature, relying on large-scale data sets. He received several major research grants, including an NWO (Netherlands Organization for Scientific Research) innovation grant. His awards include the J. C. Ruigrok Prize (awarded once every four years to the most productive young scholar in the Economic Sciences in the Netherlands) and the Christiaan Huygens Science Award (presented by HRH Princess Máxima of the Netherlands; awarded once every five years to a young economist in the Netherlands).Sanjiv Erat (“Consumer Mental Accounts and Implications to Selling Base Products and Add-ons”) is an assistant professor of innovation, technology, and operations management at the Rady School of Management, University of California, San Diego. He has a B.E. in computer science from the Indian Institute of Technology Madras and a Ph.D. in operations management from the College of Management, Georgia Institute of Technology. His primary research interests include new product development, marketing and operation interfaces, and behavioral economics. His work has previously appeared in Management Science.Rosellina Ferraro (“Unintended Nutrition Consequences: Firm Responses to the Nutrition Labeling and Education Act”; “From Consumer Information Regulation to Nutrition Competition: A Response”) is an associate professor of marketing at the Robert H. Smith School of Business, University of Maryland. Her research focuses on consumer behavior—specifically, on the effects of social influence on choice and preference and the effects of external threats on consumption behavior. Her work has been published in the Journal of Consumer Research, Journal of Marketing, and Journal of Consumer Psychology. She serves on the editorial review board for the Journal of Consumer Research and was named a 2011 MSI Young Scholar.Joel Huber (“Unintended Nutrition Consequences: Firm Responses to the Nutrition Labeling and Education Act”; “From Consumer Information Regulation to Nutrition Competition: A Response”) is the Alan D. Schwartz Professor of Business Administration at the Fuqua School of Business at Duke University. He has a B.A. from Princeton University and an MBA and Ph.D. from the Wharton School of the University of Pennsylvania. His research centers on ways relatively minor changes in the competitive context can have a large impact on market choice and the impact of this context dependency on appropriate ways to measure value. Recent work has focused on valuation of environmental changes, insurance programs, and health systems. He has been an associate editor for the Journal of Consumer Research for 12 years and the editor of Journal of Marketing Research for 3 years.Sanjay Jain (“Self-Control and Incentives: An Analysis of Multiperiod Quota Plans”) is a professor and the JCPenney Chair of Marketing and Retailing Studies at the Mays Business School, Texas A&M University. His research interests are in the areas of competitive strategy, behavioral economics, and experimental game theory. His research has been published in the Journal of Marketing Research, Management Science, and Marketing Science. He is an associate editor for Management Science and serves on the editorial boards of the Journal of Marketing Research and Marketing Science.Kevin Lane Keller (“Economic and Behavioral Perspectives on Brand Extension”) is the E. B. Osborn Professor of Marketing at the Tuck School of Business at Dartmouth College. His academic resume includes degrees from Cornell, Duke, and Carnegie Mellon universities, award-winning research, and faculty positions at the University of California at Berkeley, Stanford, and the University of North Carolina. His textbook, Strategic Brand Management, has been adopted at the top business schools and leading firms around the world. He is also the coauthor (with Philip Kotler) of the all-time best-selling introductory marketing textbook, Marketing Management.Donald R. Lehmann (“State-Dependence Effects in Surveys”) is the George E. Warren Professor of Business at the Columbia Business School. He has a B.S. in mathematics from Union College, Schenectady, NY, and an MSIA and Ph.D. from the Krannert School of Purdue University. His research interests include individual and group choice and decision making, empirical generalizations and meta-analysis, the introduction and adoption of new products and innovations, and measuring the value of marketing assets such as brands and customers. He has published numerous journal articles and six books. He was the founding editor of Marketing Letters; has served on the editorial boards of the Journal of Consumer Research, the Journal of Marketing, the Journal of Marketing Research, Management Science, and Marketing Science; and has served as executive director of the Marketing Science Institute and as president of the Association for Consumer Research.Christine Moorman (“Unintended Nutrition Consequences: Firm Responses to the Nutrition Labeling and Education Act”; “From Consumer Information Regulation to Nutrition Competition: A Response”) is the T. Austin Finch, Sr. Professor of Business Administration at the Fuqua School of Business, Duke University. She has published research on consumers, managers, and organization learning and the use of information in a range of marketing strategy and public policy contexts. Founder of the CMO Survey, author of the book Strategy from the Outside In: Profiting from Customer Value (recipient of the 2011 Berry Book Prize), and winner of the Paul D. Converse award, she has also served as a trustee for the Marketing Science Institute and on the Board of Directors of the American Marketing Association.Sridhar Moorthy (“Can Brand Extension Signal Product Quality?”; “On Brand Extension as a Signal of Product Quality: A Reply to Keller and Wernerfelt”) is the Manny Rotman Professor of Marketing at the Rotman School of Management, University of Toronto. He received his Ph.D. from Stanford University, and he has taught previously at the University of Rochester, Yale School of Management, INSEAD, the University of California at Los Angeles, the Wharton School, and the Indian School of Business. His current research focuses on branding, advertising, and retailing issues; previous work published here and in other journals has examined the relationship between advertising and product quality, product differentiation in a competitive environment, and price-matching guarantees in retailing. He is coeditor of Quantitative Marketing and Economics, associate editor of Management Science, and a member of the editorial board of Journal of Marketing Research. He is a coauthor (with Philip Kotler and Gary Lilien) of Marketing Models (Prentice-Hall 1992).Oded Netzer (“State-Dependence Effects in Surveys”) is the Phillip H. Geier Jr. Associate Professor of Business at Columbia University. He received an M.Sc. in statistics and a Ph.D. in business, both from Stanford University, and he also holds a B.Sc. in industrial engineering and management from the Technion (Israel Institute of Technology). His research interests focus on modeling customer relationships, preference measurement methods, and modeling various aspects of choice behavior, including how choices change over time, contexts, and customers. His research has appeared in the top academic journals. He is the recipient of the John D. C. Little, Frank M. Bass, and Society of Consumer Psychology Best Competitive Paper awards.Janis K. Pappalardo (“Are Unintended Effects of the Marketing Regulations Unexpected?”) is the Assistant Director for Consumer Protection in the Bureau of Economics at the Federal Trade Commission. She majored in economics at Catholic University and received her Ph.D. from Cornell University in 1986, with a primary focus in consumer economics and secondary fields in statistics and industrial organization. Research that she coauthored on health claims regulation earned her two outstanding article awards from the Journal of Public Policy and Marketing. Her research on mortgage disclosures, coauthored with James Lacko, has been published in the American Economic Review (Papers and Proceedings) and has been cited in congressional testimony and newspapers such as the Washington Post, USA Today, and the Wall Street Journal. She currently serves on the editorial boards of the Journal of Public Policy and Marketing and the Journal of Consumer Affairs.Brian T. Ratchford (“Suggestions for Further Research on Firm Responses to NLEA and Other Disclosure Laws”) is the Charles and Nancy Davidson Professor of Marketing, University of Texas at Dallas. He has MBA and Ph.D. degrees from the University of Rochester. His research interests are in economics applied to the study of consumer behavior, information economics, marketing productivity, marketing research, and electronic commerce. He has published over 80 articles in marketing and related fields. He was the Editor of Marketing Science (from 1998 to 2002); is currently an associate editor of the Journal of Consumer Research; serves on the editorial review boards of the Journal of Marketing Research, Journal of Marketing, Journal of Retailing, Journal of Interactive Marketing, Journal of Public Policy and Marketing, and Journal of Service Research; and serves on the advisory editorial board of Marketing Science.Atanu R. Sinha (“Bidding Behavior in Descending and Ascending Auctions”) is an associate professor of marketing at the Leeds School of Business, University of Colorado, Boulder. His research interests include, among others, pricing, theoretical and empirical models of auctions, negotiations, social media, online two-sided markets, and loyalty programs.Catherine Tucker (“How Does the Use of Trademarks by Third-Party Sellers Affect Online Search?”) is currently the Douglas Drane Career Development Professor in IT and Management and an associate professor of marketing at the MIT Sloan School of Management, and she is a faculty research fellow at the National Bureau of Economic Research. She received her Ph.D. in economics from Stanford University. She specializes in understanding how the huge amounts of data generated by the information and communication technology revolution can better guide marketing and advertising decisions. She has also done substantial research into how healthcare information technology is transforming the healthcare sector. She also focuses on the privacy concerns that such data raise and how firms and policy makers can best address these; she received a National Science Foundation CAREER award for her work on digital privacy.Birger Wernerfelt (“On Brand Extension as a Signal of Product Quality”) is the JC Penney Professor of Management at the MIT Sloan School of Management. He has taught marketing, strategy, and economics, and he has published in all three areas.
Although the assumption of utility-maximizing consumers has been challenged for decades, empirical applications of alternative choice rules are still very new. We add to this growing body of literature by proposing a model based on the idea of a “satisficing” decision maker. In contrast to previous models (including recent models implementing alternative choice rules), satisficing depends on the order in which alternatives are evaluated. We therefore conduct a visual conjoint experiment to collect search and choice data. We model search and product evaluation jointly and allow for interdependence between them. The choice rule incorporates a conjunctive rule for the evaluations and, contrary to most previous models, does not rely on compensatory trade-offs at all. The results strongly support the proposed model. For instance, we find that search is indeed influenced by product evaluations. More importantly, the model results strongly support the satisficing stopping rule. Finally, we perform a holdout prediction task and find that the proposed model outperforms a standard multinomial logit model.
Social interaction (peer) effects are recognized as a potentially important factor in the diffusion of new products. In the case of environmentally friendly goods or technologies, both marketers and policy makers are interested in the presence of causal peer effects as social spillovers can be used to expedite adoption. We provide a methodology for the simple, straightforward identification of peer effects with sufficiently rich data, avoiding the biases that occur with traditional fixed effects estimation when using the past installed base of consumers in the reference group. We study the diffusion of solar photovoltaic panels in California and find that at the average number of owner-occupied homes in a zip code, an additional installation increases the probability of an adoption in the zip code by 0.78 percentage points. Our results provide valuable guidance to marketers designing strategies to increase referrals and reduce customer acquisition costs. They also provide insights into the diffusion process of environmentally friendly technologies.
This paper analyzes the competitive role of retail shopping experience in markets with consumer search costs. We examine how a retailer's advantage in providing consumer shopping experience affects its equilibrium pricing and price advertising strategies. We find that if the consumer valuation of a shopping experience is sufficiently low, its effect on retailer strategy is similar to that of quality, and the retailer with the advantage in shopping experience then deploys higher levels of price advertising. On the other hand, when the shopping experience is valuable enough for consumers, it acts akin to price advertising in that it makes it optimal for the retailer with the advantage in shopping experience to eschew price advertising. The optimal competitive investments in consumer shopping experience can be higher than that of a monopoly. The profit impact of shopping experience for a retailer depends on the level of shopping experience: for low levels, the profit impact depends on the difference in the levels between the retailers, but for high enough levels, it depends only on whether the retailer's shopping experience level is higher than that of its competitor. In this case, even small differences in shopping experience levels can result in large differences in equilibrium profits.
We consider how public firms influence their stock market valuations by timing the introduction of innovative new products. Our focus is on innovation ratchet strategy—firms timing the introduction of innovations in order to demonstrate an improvement in the number of introductions over time. We document that public firms use an innovation ratchet strategy more often than do private firms and that the stock market rewards public firms for doing so. These rewards from the stock market, however, come at the expense of performance in product markets. Specifically, because firms using an innovation ratchet strategy delay some product introductions, they have significantly lower sales growth in the year they ratchet. Finally, we identify firm and market characteristics that influence the likelihood that a public firm will engage in an innovation ratchet strategy.
Word-of-mouth (WOM) plays an increasingly important role in shaping consumers' attitudes and buying behaviors. Prior work in marketing has mainly focused on the aggregate impact of WOM on product sales as well as the generation of WOM. Very little attention has been paid to the consumption or usage of WOM. In this paper, utilizing a unique data set that collects information from the automobile category on whether a consumer generates WOM to others and uses WOM for making purchase decisions, we build a discrete-choice model to study consumer WOM generation and WOM consumption decisions simultaneously and empirically answer questions that have not been explored previously. We are particularly interested in studying the key drivers of WOM generation/consumption and the synergy effect between the two WOM-related activities. We apply the proposed model to survey data collected on the automobile category. We find a strong synergy between WOM generation and WOM consumption. Although some consumers view WOM generation and WOM consumption as complementary to each other, others tend to perceive the two activities as competing with each other. We also find that consumer product experience and media exposure are positively correlated with their propensity to generate WOM. However, their effect on WOM consumption is mixed. Our empirical analysis also provides evidence of unobserved heterogeneity in the way consumer WOM activities are related to consumer product experience. Overall, these findings lead to important managerial implications on targeting for effective use of WOM as a marketing tool.
Competition is intense among rival technologies, and success depends on predicting their future trajectory of performance. To resolve this challenge, managers often follow popular heuristics, generalizations, or “laws” such as Moore's law. We propose a model, Step And Wait (SAW), for predicting the path of technological innovation, and we compare its performance against eight models for 25 technologies and 804 technologies-years across six markets. The estimates of the model provide four important results. First, Moore's law and Kryder's law do not generalize across markets; neither holds for all technologies even in a single market. Second, SAW produces superior predictions over traditional methods, such as the Bass model or Gompertz law, and can form predictions for a completely new technology by incorporating information from other categories on time-varying covariates. Third, analysis of the model parameters suggests that (i) recent technologies improve at a faster rate than old technologies; (ii) as the number of competitors increases, performance improves in smaller steps and longer waits; (iii) later entrants and technologies that have a number of prior steps tend to have smaller steps and shorter waits; but (iv) technologies with a long average wait time continue to have large steps. Fourth, technologies cluster in their performance by market.
Contextual advertising entails the display of relevant ads based on the content that consumers view, exploiting the potential that consumers' content preferences are indicative of their product preferences. This paper studies the strategic aspects of such advertising, considering an intermediary who has access to a content base, sells advertising space to advertisers who compete in the product market, and provides the targeting technology. The results show that contextual targeting impacts advertiser profit in two ways: First, advertising through relevant content topics helps advertisers reach consumers with a strong preference for their product. Second, heterogeneity in consumers' content preferences can be leveraged to reduce product market competition, especially when competition is intense. The intermediary has incentives to strategically design its targeting technology, sometimes at the cost of the advertisers. When product market competition is moderate, the intermediary offers accurate targeting such that the consumers see the most relevant ads. When competition is high, the intermediary lowers the targeting accuracy such that the consumers see less relevant ads. Doing so intensifies competition and encourages advertisers to bid for multiple content topics in order to prevent their competitors from reaching consumers. In some cases, this may lead to an asymmetric equilibrium where one advertiser bids high even for the content topic that is more relevant to its competitor.
This paper studies optimal product line design when consumers need to incur costly deliberation to uncover their valuations for quality. To induce deliberation, a firm must maintain quality dispersion and cut the price of the high-end product so that consumers are motivated to deliberate in the hope that high-end consumption fits their needs. To prevent deliberation, the firm may have to offer downgraded quality at a low price so that an impulsive purchase will not appear too wasteful. Whether the firm should induce deliberation depends on how much surplus it creates by aligning the supply of quality with heterogeneous demand for quality and how much surplus it captures during this process. Interestingly, equilibrium firm profit, consumer surplus, and social welfare can all increase with the cost of deliberation. We extend the model to accommodate consumers' heterogeneous prior beliefs of their valuations for quality. We also discuss how market research could benefit from taking into account the endogeneity of consumer deliberation.
This paper describes two new data sets available to academic researchers (at http://www.informs.org/Community/ISMS). The first is a panel data set containing the transactions of 19,936 households made over the period from December 1998 to November 2004 at a major U.S. consumer electronics retailer. There are a total of 173,262 transactions, including purchases and returns of products as well as extended warranties. There are 16 product categories and 292 subcategories, ranging from big-ticket items such as televisions to small-ticket items such as CDs and batteries. The second data set features a field experiment for a Christmas promotion that took place in December 2003 in the form of a direct mailing sent to a randomly selected group of households at the end of November 2003. We describe the data and the potential research issues that can be studied using these two durable goods data sets.
This paper studies product bundling in a distribution channel where a downstream retailer combines component goods produced by separate manufacturers acting independently. Past literature offers deep insights about bundling by a single firm whose unit costs are not impacted by choice of selling strategy. But when the retailer bundles goods from separate manufacturers, unit costs for the bundler (retailer) are, being the prices set by the manufacturers, no longer exogenous. This alters the economic balance with respect to bundling. I show that channel conflicts weaken the case for bundling. Although bundling is better than component selling for the integrated firm, it is no longer so in the decentralized channel. The culprit is a combination of vertical channel conflict (incentive misalignment with respect to bundle versus component sales) and horizontal conflict (each manufacturer wants a higher share of profits from bundle sales), with the latter playing a dominant role. They cause manufacturers to overprice component goods, weakening the retailer's incentives to bundle. The competitive interplay between firms when one (retailer) merges the prices of several (manufacturers) leads to lower profits for all. Price coordination between the firms could partially restore the role of bundling and improve the firms' profits as well as consumer surplus.
Henry Assael (“An Empirical Study of Word-of-Mouth Generation and Consumption”) is a professor of marketing at the Stern School of Business, New York University. He has written over 30 articles for scholarly journals, and he edited a 33-volume series on the history of marketing and a 30-volume series on the history of advertising. He is the author of three widely used texts: Consumer Behavior: A Strategic Approach (seven editions), Marketing: Principles and Strategy (three editions), and Marketing Management: Strategy and Action.Hemant K. Bhargava (“Retailer-Driven Product Bundling in a Distribution Channel”) is an associate dean and the Jerome and Elsie Suran Professor of Technology Management at the Graduate School of Management, University of California, Davis. He studies business strategy and competition for technology products such as information goods, online services, software, electronic gadgets, media and entertainment goods, and alternative energy technologies.Peter Boatwright (“A Satisficing Choice Model”) is an associate professor of marketing at the Tepper School of Business at Carnegie Mellon University. He also has a courtesy faculty appointment in mechanical engineering at Carnegie Mellon University. He received his Ph.D. from University of Chicago's Booth School of Business, and his research interests include product development processes and marketing of new products, Bayesian modeling, and consumer response to product assortment.Bryan Bollinger (“Peer Effects in the Diffusion of Solar Photovoltaic Panels”) is an assistant professor of marketing at New York University's Stern School of Business. His research interests lie at the intersection of marketing, empirical industrial organization, and economic policy, including empirical methods, dynamics, technology adoption, demand- and supply-side spillover effects, and the effectiveness of marketing mix variables and policy tools in affecting consumer and firm behavior. He received both a B.A. and B.E. in engineering from Dartmouth College, and an M.A. in economics and a Ph.D. in marketing from Stanford University.Xiaohong Chen (“An Empirical Study of Word-of-Mouth Generation and Consumption”) is a professor of management science at the Business School of Central South University, China. She received a B.S. in computer science and an M.S. in management science from Central South University, China, and a Ph.D. in management science from the Tokyo Institute of Technology, Japan. She is the principle professor of national first-level key principles “Management Science and Engineering” and “Innovation Group” of the National Natural Science Foundation in China. She is also the winner of “State Science Fund for Outstanding Youth” and named one of China's “National Outstanding Women” and “National Prominent Social Scientists.” Her research has been published in several top journals.John Deighton (“Editorial—Research Priorities of the Marketing Science Institute: 2012–2014”) is the Executive Director of the Marketing Science Institute and the Harold M. Brierley Professor of Business Administration at the Harvard Business School. His Ph.D. is from the Wharton School of the University of Pennsylvania, and he served previously on the faculties of the University of Chicago and Dartmouth College.Kenneth Gillingham (“Peer Effects in the Diffusion of Solar Photovoltaic Panels”) is an assistant professor of economics at Yale University, with appointments in the School of Forestry and Environmental Studies (primary) and the Department of Economics (courtesy). He holds a Ph.D. from Stanford University and a B.A. from Dartmouth College. His research focuses on the adoption of new technologies, including renewable energy, energy efficiency, and green transportation technologies. He was a Fulbright Fellow in New Zealand and has worked at the White House Council of Economic Advisers and Resources for the Future.Liang Guo (“Consumer Deliberation and Product Line Design”) is an associate professor of marketing and Senior Wei Lun Fellow at Hong Kong University of Science and Technology. He received a Ph.D. in business administration from the University of California, Berkeley, and a B.A. in economics from Beijing University. His research interests include behavioral economics, channel interaction, information acquisition and sharing, and marketing strategy. His research work has been accepted for publication at the Journal of Economics and Management Strategy, Management Science, and Marketing Science; he serves on the editorial boards of Marketing Science and Management Science (associate editor). He was named an MSI Young Scholar in 2009.Mantian (Mandy) Hu (“An Empirical Study of Word-of-Mouth Generation and Consumption”) is an assistant professor in the Department of Marketing at the Chinese University of Hong Kong, Hong Kong. She received a B.A. in economics from Fudan University, China, an M.A. in economics from Tufts University, and a Ph.D. in marketing from New York University. She is the 2011 recipient of the Best Proposal Award in the Society for Marketing Advances (SMA) Dissertation Proposal Competition.Ganesh Iyer (“Competition in Consumer Shopping Experience”) is the Edgar F. Kaiser Professor of Business Administration at the Haas School of Business, University of California, Berkeley. He received his Ph.D. from the University of Toronto and was previously on the faculty at Washington University in St. Louis. His research uses economic theory to study marketing strategy problems; his areas of research are the coordination of product distribution, marketing information, Internet strategy, strategic communication, and bounded rationality in marketing strategy. He is currently an associate editor for Marketing Science,Management Science, and Quantitative Marketing and Economics. He received the 2000 John D. C. Little Award and was a finalist for the Little award on three other occasions, and two of his papers have been finalists for the INFORMS Long Term Impact Award.Gareth M. James (“Predicting the Path of Technological Innovation: SAW vs. Moore, Bass, Gompertz, and Kryder”) is an expert on statistical methodology with particular application to marketing problems such as prediction of technology evolution. He teaches both M.B.A. and Ph.D. courses ranging from introductory statistics to advanced modern nonlinear regression techniques. He was recently elected a fellow of the American Statistical Association, the nation's preeminent professional statistical society, in recognition of his outstanding professional contributions to and leadership in the field of statistical science. He has also earned numerous accolades from USC Marshall, including the Evan C. Thompson Faculty Teaching and Learning Innovation Award, and he is a two-time winner of both the Dean's Award for Research Excellence and the Golden Apple Award for teaching excellence in his M.B.A. courses. He has published numerous articles in leading journals such as the Journal of the American Statistical Association, for which he also serves on the editorial review board.Zsolt Katona (“Contextual Advertising”) is an assistant professor of marketing at the Haas School of Business, University of California, Berkeley. He has a Ph.D. in management from INSEAD; he also earned a Ph.D. in computer science from Eotvos University, Budapest. His current research focuses on understanding the interaction between websites' online advertising strategies. He also studies the role that link structure of social networks plays in word-of-mouth effects and community formation. Previously, he had analyzed characteristics of different random networks and published his work in such journals as the Journal of Applied Probability, Statistics and Probability Letters, and Random Structures and Algorithms.Susan Keane (“Editorial—Research Priorities of the Marketing Science Institute: 2012–2014”) is the Editorial Director at the Marketing Science Institute, where she manages the development of the Relevant Knowledge book series, the working paper series, and other print and digital content.Dmitri Kuksov (“Competition in Consumer Shopping Experience”) is a professor of marketing at the Naveen Jindal School of Management, the University of Texas at Dallas. He previously worked at Washington University in St. Louis, and he holds a Ph.D. in marketing from the Haas Business School of the University of California, Berkeley. His research interests include competitive strategy, markets with incomplete information, consumer communication and networks, branding and product line strategy, and customer satisfaction. His work has appeared in a number of journals, including Marketing Science, Management Science, the Journal of Marketing Research, and the Journal of Economic Theory. He received the 2005 Frank M. Bass Dissertation Award for his work on search costs and product differentiation, which was also a finalist for the INFORMS Long Term Impact Award, and two of his papers were finalists for 2007 John D. C. Little Award.Natalie Mizik (“Firm Innovation and the Ratchet Effect Among Consumer Packaged Goods Firms”) is the Shansby Associate Professor of Marketing at the Foster School of Business, University of Washington (UW). She has published research in a broad set of substantive areas including branding, strategy, managerial myopia, customer satisfaction, and direct-to-physician pharmaceutical marketing. An award-winning teacher and researcher, she has served on the faculty of the Columbia Graduate School of Business and UNC Kenan-Flagler Business School, and she was a visiting professor at the MIT Sloan School of Management before she joined UW.Robert T. Monroe (“A Satisficing Choice Model”) is an associate teaching professor of information systems at Carnegie Mellon University's Tepper School of Business, as well as the associate dean for Carnegie Mellon University's Qatar campus. He holds a Ph.D. and M.S. in computer science from Carnegie Mellon University and a B.S. in philosophy and computer science from the University of Michigan.Christine Moorman (“Firm Innovation and the Ratchet Effect Among Consumer Packaged Goods Firms”) is the T. Austin Finch, Sr. Professor of Business Administration, Fuqua School of Business, Duke University. She has published research on consumer, manager, and organizational learning and the use of information in a range of marketing strategy and public policy contexts. Founder of The CMO Survey™ (http://www.cmosurvey.org) and winner of the Paul D. Converse award, she has also served as a trustee for the Marketing Science Institute and on the board of directors for the American Marketing Association.Scott A. Neslin (“Database Submission—The ISMS Durable Goods Data Sets”) is the Albert Wesley Frey Professor of Marketing at the Tuck School of Business, Dartmouth College. He received his Ph.D. from the MIT Sloan School of Management. His research focuses on measuring and enhancing marketing productivity, particularly in the areas of sales promotion, advertising, and customer relationship management.Jian Ni (“Database Submission—The ISMS Durable Goods Data Sets”) is an assistant professor of marketing at Carey Business School, Johns Hopkins University. He received his doctoral degree from Tepper School of Business at Carnegie Mellon University. His recent research focuses on empirical and theoretical studies of consumer choices and firm behavior.Ross Rizley (“Editorial—Research Priorities of the Marketing Science Institute: 2012–2014”) is the Research Director of the Marketing Science Institute. His doctorate is in clinical psychology from Yale University, and he was a faculty member in the Department of Psychology and Social Relations at Harvard University, in the Department of Psychology at Boston University, and in the Department of Marketing at the Boston University School of Management.Ashish Sood (“Predicting the Path of Technological Innovation: SAW vs. Moore, Bass, Gompertz, and Kryder”) is a professor of marketing at the Emory University and an expert in the areas of technology evolution, product innovation, and new product diffusion in emerging markets. He regularly chairs special sessions in research conferences and has been invited to talk at more than 10 top international schools. Prior to joining the academia, he worked in the industry for 12 years in India and Singapore. His research has been published in the top field journals, including Marketing Science and the Journal of Marketing, and has won numerous research awards and grants. His papers are highly cited, and the findings from his research have been published and reprinted in more than 40 books (e.g., Springer, Blackwell, Wiley), thought leadership publications (e.g., E&Y, Montgomery Research, Accenture, MSI's Knowledge series), and the business press (the New York Times and the Wall Street Journal).Fredrika J. Spencer (“Firm Innovation and the Ratchet Effect Among Consumer Packaged Goods Firms”) is an assistant professor of marketing at the Cameron School of Business at the University of North Carolina at Wilmington. She holds a Ph.D. from Duke University, an M.B.A. from Wake Forest University, and a B.S. from the University of North Carolina at Chapel Hill. Her current research interests include evaluating the impact of individual- and portfolio-level product introduction behavior on financial markets and examining the flow of information within the firm.Peter Stüttgen (“A Satisficing Choice Model”) is a visiting assistant professor in marketing at Carnegie Mellon University's Qatar campus. He holds a Ph.D. and M.S. in industrial administration (marketing) from Carnegie Mellon University's Tepper School of Business as well as a B.S. in marketing from the Campbell School of Business at Berry College. His research focuses on empirical models of consumer behavior—in particular, noncompensatory choice models.Baohong Sun (“Database Submission—The ISMS Durable Goods Data Sets”) is the Dean's Distinguished Chair Professor of Marketing at the Cheong Kong Graduate School of Business (New York). She develops empirical models to study rational consumer choice, evaluate promotion effect, and measure impact on short-term and long-term sales. Her recent research focuses on studying the economic foundation of consumer networking behavior.Gerard J. Tellis (“Predicting the Path of Technological Innovation: SAW vs. Moore, Bass, Gompertz, and Kryder”) is a professor of marketing, management, and organization; Neely Chair of American Enterprise; and Director of the Center for Global Innovation, at the USC Marshall School of Business. An expert in innovation, new product growth, emerging markets, global market entry, advertising, quality, and pricing, he has published four books and over 100 papers that have won over 20 awards, including the Frank M. Bass Award, the William F. O'Dell Award, the Harold D. Maynard Award (twice), and Converse award for lifetime contributions to research. He is a Distinguished Professor of Marketing Research, Erasmus University, Rotterdam; a senior research associate at the Judge Business School; and a fellow of Sidney Sussex College, Cambridge University, United Kingdom. He is an associate editor of Marketing Science and the Journal of Marketing Research. More information can be found at http://www.gtellis.net.Simone Wies (“Firm Innovation and the Ratchet Effect Among Consumer Packaged Goods Firms”) is a doctoral candidate in the Department of Finance at the School of Business and Economics and a member of the Marketing-Finance Research Lab, Maastricht University, the Netherlands. Her research deals with the interaction of capital markets and marketing investments, with a special emphasis on innovation.Russell S. Winer (“An Empirical Study of Word-of-Mouth Generation and Consumption”) is the William Joyce Professor and Chair of the Department of Marketing at the Stern School of Business, New York University. He received a B.A. in economics from Union College and an M.S. and Ph.D. in industrial administration from Carnegie Mellon University. He is a past executive director of the Marketing Science Institute in Cambridge, Massachusetts. He is a founding fellow of the INFORMS Society for Marketing Science and is the 2011 recipient of the American Marketing Association/Irwin/McGraw-Hill Distinguished Marketing Educator Award.Sha Yang (“An Empirical Study of Word-of-Mouth Generation and Consumption”) is a professor of marketing at the Marshall School of Business, University of Southern California. She received a B.A. in international economics from Renmin University, China, and an M.S. in statistics, M.A. in marketing, and Ph.D. in marketing from the Ohio State University. Her primary research focuses on understanding and modeling household purchase behavior (especially interdependent consumer decision making) and market competition. Her recent research interest focuses on Internet advertising. Her research has been published in leading journals such as Marketing Science, Management Science, the Journal of Marketing Research, the Journal of Marketing, and Quantitative Marketing and Economics.Juanjuan Zhang (“Consumer Deliberation and Product Line Design”) is the Class of 1948 Career Development Professor and an associate professor of marketing at the MIT Sloan School of Management. She holds a B.E. from Tsinghua University and a Ph.D. in business administration from the University of California, Berkeley. Her research interests include observational learning (i.e., learning by observing others' choices), the interaction of information and incentives, and product development. She is the recipient of the 2010 Frank M. Bass Award and a finalist for the 2010 and 2011 John D. C. Little Award.Kaifu Zhang (“Contextual Advertising”) is an assistant professor of marketing at Cheung Kong Graduate School of Business, Beijing, China. He holds a Ph.D. degree in management from INSEAD. His most recent research explores both theoretical and empirical issues related to media, advertising, and the Internet.Ji Zhu (“Predicting the Path of Technological Innovation: SAW vs. Moore, Bass, Gompertz, and Kryder”) is a professor in the Department of Statistics at the University of Michigan. A well-recognized researcher in the areas of statistical machine learning and high-dimensional data analysis, he is also interested in applications in computational biology, marketing, finance, engineering, and physics. He publishes regularly in the leading statistics journals, and he received a CAREER Award from the National Science of Foundation.
Marketing Science is in a great competitive position with a strong editorial board and infrastructure support. This editorial summarizes the state of the journal as perceived by its stakeholders. They believe that the journal should strive to remain a premier international journal and embrace diverse topics, methods, and foci. It should continue to draw upon allied fields while being open to the various methods and philosophies as recognized in those fields. It should strive to avoid silos and embrace applications and relevance while not sacrificing rigor. The path may not be easy, but we can move forward successfully. We highlight potential threats to success and recommend how the journal might overcome those threats.
This paper considers the history of keywords used in Marketing Science to develop insights on the evolution of marketing science. Several findings emerge. First, “pricing” and “game theory” are the most ubiquitous words. More generally, the three C's and four P's predominate, suggesting that keywords and common practical frameworks align. Various trends exist. Some words, like “pricing,” remain popular over time. Others, like “game theory” and “hierarchical Bayes,” have become more popular. Finally, some words are superseded by others, like “diffusion” by “social networking.” Second, the overall rate of new keyword introductions has increased, but the likelihood they will remain in use has decreased. This suggests a maturation of the discipline or a long-tail effect. Third, a correspondence analysis indicates three distinct eras of marketing modeling, comporting roughly with each of the past three decades. These eras are driven by the emergence of new data and business problems, suggesting a fluid field responsive to practical problems. Fourth, we consider author publication survival rates, which increase up to six papers and then decline, possibly as a result of changes in ability or motivation. Fifth, survival rates vary with the recency and nature of words. We conclude by discussing the implications for additional journal space and the utility of standardized classification codes.
Presidential elections provide both an important context in which to study advertising and a setting that mitigates the challenges of dynamics and endogeneity. We use the 2000 and 2004 general elections to analyze the effect of market-level advertising on county-level vote shares. The results indicate significant positive effects of advertising exposures. Both instrumental variables and fixed effects alter the ad coefficient. Advertising elasticities are smaller than are typical for branded goods yet significant enough to shift election outcomes. For example, if advertising were set to zero and all other factors held constant, three states' electoral votes would have changed parties in 2000. Given the narrow margin of victory in 2000, this shift would have resulted in a different president.
The extensive adoption of uniform pricing for branded variants is a puzzling phenomenon, considering that firms may improve profitability through price discrimination. In this paper, we incorporate consumers' concerns of peer-induced price fairness into a model of price competition and show that a uniform price for branded variants may emerge in equilibrium. Interestingly, we find that uniform pricing induced by consumers' concerns of fairness can actually help mitigate price competition and hence increase firms' profits if the demand of the product category is expandable. Furthermore, an individual firm may not have an incentive to unilaterally mitigate consumers' concerns of price fairness to its own branded variants, which suggests the long-run sustainability of the uniform pricing strategy. As a result, fairness concerns from consumers provide a natural mechanism for firms to commit to uniform pricing and enhance their profits.
One of the characteristics of the fashion marketplace is the unpredictability and apparent randomness of fashion hits. Another one is the information asymmetry among consumers. In this paper, we consider fashion as a means consumers use to signal belonging to a higher social rank and propose an analytical model of fashion hits in the presence of competition and consumers who can coordinate on which product to use. We show that, consistent with the observed market phenomenon, in equilibrium, consumer coordination involves randomization between products chosen, i.e., in randomness of fashion hits. Analyzing optimal consumer choice, we find that whenever low-type consumer demand for a product is positive, a price increase results in a higher probability of high-type consumers choosing this product but lower low-type consumer demand. We also show that although high-type consumers may prefer (higher) prices that would lead to complete separation of the high- and the low-type consumers through product use, in equilibrium, firms always price as to attract positive demand from low-type consumers. The equilibrium price and profits turn out to be nonmonotonic in the low-type consumer valuation of being recognized as belonging to a higher social rank. Equilibrium profits first increase and then decrease in this valuation.
Recognizing that initial public offerings (IPOs) represent the debut of private firms on the public stage, this study investigates how pre-IPO customer and competitor orientations (CCOs) affect IPO outcomes. Building on information economics, we propose that CCOs influence investors' sentiments toward an IPO and that both IPO-specific variables (which influence the credibility of CCO information) and facets of the organizational institutional and task environments (which influence the appropriateness of CCO information) moderate this influence. We test the framework using data collected from computer-aided text analysis, expert coders, and secondary sources for 543 IPOs across 43 industries between 2000 and 2004. A Bayesian shrinkage model, which accounts for industry-specific effects and uses latent instrumental variables to address CCO endogeneity, shows that CCOs positively influence IPO outcomes. Furthermore, (1) underwriter reputation and venture funding positively moderate the effects of CCOs; (2) technological and market turbulence positively and institutional complexity negatively moderate the effect of customer orientation; and (3) technological turbulence, competitive intensity, and institutional complexity positively moderate the effect of competitor orientation. Also, accounting for endogeneity using latent instrumental variables substantially improves the predictive validity of the model, relative to alternative model specifications.
Patients increasingly request their physicians to prescribe specific brands of pharmaceutical drugs. A popular belief is that requests are triggered by direct-to-consumer advertising (DTCA). We examine the relationship between DTCA, patient requests, and prescriptions for statins. We find that although the effect of requests on prescriptions is significantly positive, the mean effect of DTCA on patient requests is negative, yet very small. More interestingly, both effects show substantial heterogeneity across physicians, which we uncover using a hierarchical Bayes estimation procedure. We find that specialists receive more requests than primary care physicians but translate them less into prescriptions. In addition, we find that the sociodemographic profile of the area a physician practices in moderates the effects of DTCA on requests and of requests on prescriptions. For instance, physicians from areas with a higher proportion of minorities (i.e., blacks and Hispanics) receive more requests that are less triggered by DTCA and are accomodated less frequently than physicians from areas with a lower proportion of minorities. Our results challenge managers to revisit the role of DTCA in stimulating patient requests. At the same time, they may trigger public policy concerns regarding physicians' accommodation of patient requests and the inequalities they may induce.
Firms that serve a large market with many diverse consumer types use discriminatory or nonlinear pricing to extract higher revenue, inducing consumers to separate by self-selecting from a large number of tariff options. But the extent of price discrimination must often be tempered by the high costs of devising and managing discriminatory tariffs, including costs of supporting consumers in understanding and making selection from a complex menu of choices. These tariff design trade-offs occur in many industries where firms face many consumer types and each consumer picks the number of units to consume over time. Examples include wireless communication services, other telecom and information technology products, legal plans, fitness clubs, automobile clubs, parking, healthcare plans, and many services and utilities. This paper evaluates alternative ways to price discriminate while accounting for both revenues and tariff management costs. The revenue-maximizing menu of quantity-price bundles can be very (or infinitely) large and hence not practical. Instead, two-part tariffs (2PTs), which charge a fixed entry fee and a per-unit fee, can extract a large fraction of the optimal revenue with a small menu of choices, and they become more attractive once the costs of tariff management are factored in. We show that three-part tariffs (3PTs), which use an additional instrument, the “free allowance,” are an even more efficient way to price discriminate. A relatively small menu of 3PTs can be more profitable than a menu of 2PTs of any size. This 3PT menu can be designed with less information about consumer preferences relative to the menu of two-part tariffs, which, in order to segment customers optimally, needs fine-grained information about preferences. Our analysis reveals a counterintuitive insight that more-complex tariffs need not always be more profitable; it matters whether the complexity is from many choices or more pricing instruments.
This article examines cross-price promotional effects in a dynamic context. Among other things, we investigate whether previously established findings hold when consumer and competitive dynamics are taken into account. Five main influential effects (asymmetric price effect, neighborhood price effect, asymmetric share effect, neighborhood share effect, and private label versus national brand asymmetry) appear jointly in the second layer of a pooled HB-VEC-VARX model, together with brand- and category-specific variables. This study tests the relative importance of these key factors across three scenarios: with no market dynamics, when only consumer dynamics are considered, and when competitive reactions are also taken into account. The results confirm all five influential effects, even if they are jointly estimated, and consumer and competitive dynamics are taken into account. National brand/private label asymmetry has the strongest influence on the cross-price promotional effects and becomes significantly stronger when consumer and competitive dynamics are taken into account. Dynamic consumer responses and competitive reactions both affect cross-brand price elasticities, and contrary to expectations, competitive reactions accumulate rather than diminish cross-price elasticities. Preemptive switching does occur; i.e., a brand's promotion in period t hurts a competitor's sales in subsequent periods. Our findings are based on an extensive data set. To attain generalizable results, we analyze 33 categories in five stores—that is, 165 store/category combinations.
We propose a structural model to study the effect of online product reviews on consumer purchases of experiential products. Such purchases are characterized by limited repeat purchase behavior of the same product item (such as a book title) but significant past usage experience with other products of the same type (such as books of the same genre). To cope with the uncertainty in quality of the product item, we posit that consumers may learn from their experience with the same type of product and others' experiences with the product item. We model the review credibility as the precision with which product reviews reflect the consumer's own product evaluation. The higher the precision, the more credible the information obtained from product reviews for the consumer, and the larger the effect of reviews on the consumer's choice probabilities. We extend the Bayesian learning framework to model consumer learning on both product quality and review credibility. We apply the model to a panel data set of 1,919 book purchases by 243 consumers. We find that consumers learn more from online reviews of book titles than from their own experience with other books of the same genre. In the counterfactual analysis, we illustrate the profit impact of product reviews and how it varies with the number of reviews. We also study the phenomenon of fake reviews. We find that fake reviews increase consumer uncertainty. The effects of more positive reviews and more numerous reviews on consumer choice are smaller on online retailing platforms that have fake product reviews.
We reanalyze endogenous sample selection in the context of customer scoring, targeting, and influencing decisions. Scoring relies on ordered lists of probabilities that customers act in a way that contributes revenues, e.g., purchase something from the firm. Targeting identifies constrained sets of covariate patterns associated with high probabilities of these acts. Influencing aims at changing the probabilities that individual customers act accordingly through marketing activities. We show that successful targeting and influencing decisions require inference that controls for endogenous selection, whereas scoring can proceed relatively successfully based on simpler models that provide (local) approximations, capitalizing on spurious effects of observed covariates. To facilitate the type of inference required for targeting and influencing, we develop a prior that frees the analyst from having to specify (often arbitrary) exclusion restrictions for model identification a priori or to explicitly compare all possible models. We cover exclusions of observed as well as unobserved covariates that may cause the successive selections to be dependent. We automatically infer the dependence structure among selection stages using Markov chain Monte Carlo-based variable selection, before identifying the scale of latent variables. The adaptive parsimony achieved through our prior is particularly helpful in applications where the number of successive selections exceeds two, a relevant but underresearched situation.
Adib Bagh (“How to Price Discriminate When Tariff Size Matters”) is an assistant professor with a joint appointment in the departments of mathematics and economics at the University of Kentucky. His research interests include price discrimination using nonlinear pricing mechanisms, game theory, and mathematical economics.Hemant K. Bhargava (“How to Price Discriminate When Tariff Size Matters”) is an associate dean and the Jerome and Elsie Suran Professor of Technology Management at the Graduate School of Management, University of California, Davis. He studies business strategy and competition for technology products such as information goods, online services, software, electronic gadgets, media and entertainment goods, and alternative energy technologies.Yuxin Chen (“The Benefit of Uniform Price for Branded Variants”) is the Polk Brothers Professor in Retailing and professor of marketing at the Kellogg School of Management, Northwestern University. Currently, he is visiting the China Europe International Business School as the Zhongkun Group Visiting Chair Professor of Marketing. His primary research areas include competitive strategies, database marketing, structural empirical models, Bayesian econometric methods, and behavioral economics. His research has appeared in journals such as Marketing Science, the Journal of Marketing Research, Management Science, and Quantitative Marketing and Economics.Pradeep Chintagunta (“Editorial—Marketing Science: A Strategic Review”) is the Joseph T. and Bernice S. Lewis Distinguished Service Professor of Marketing at the Booth School of Business, University of Chicago. He graduated from Northwestern University and has also served on the faculty of the Johnson School, Cornell University. He is interested in studying consumer, agent, and firm behavior. In particular, he is interested in measuring the effectiveness of marketing activities in pharmaceutical markets, investigating aspects of technology product markets, studying online and off-line purchase behavior, and analyzing household purchase behavior using scanner data.Tony Haitao Cui (“The Benefit of Uniform Price for Branded Variants”) is an assistant professor of marketing at the Carlson School of Management, University of Minnesota, where he teaches Ph.D., EMBA, MBA, and undergraduate courses. He received a B.Eng. in fluid machinery and fluid engineering, a B.Eng. in industrial engineering, and an IMBA, all from Tsinghua University; he holds an M.S. in operations and information management and a Ph.D. in managerial science and applied economics, both from the Wharton School. His research focuses on behavioral modeling in marketing, behavioral and experimental economics, competitive strategies, distribution channels, pricing, and marketing-operations interfaces. His research has appeared in journals such as Marketing Science, Management Science, and Marketing Letters. He was named the 2011 Marketing Science Institute Young Scholar.Yiting Deng (“Invited Paper—A Keyword History of Marketing Science”) is a Ph.D. candidate in marketing at the Fuqua School of Business, Duke University. She received her B.A. in economics, B.S. in statistics, and M.A. in economics from Peking University before joining the Ph.D. program. She also holds a M.S. in statistics from Duke University. Her research interests include social media, advertising, online search, and choices.Dennis Fok (“Moderating Factors of Immediate, Gross, and Net Cross-Brand Effects of Price Promotions”) is a professor of applied econometrics at the Econometric Institute, Erasmus University Rotterdam. His research interests are in the fields of marketing and applied econometrics. These interests include modeling choice at an individual level as well as at an aggregated level; furthermore, he is interested in nonlinear panels and simulation-based estimation. He publishes on these topics in journals as Marketing Science, the Journal of Marketing Research, the Journal of AppliedEconometrics, and the Journal of Econometrics.Brett R. Gordon (“Advertising Effects in Presidential Elections”) is the Class of 1967 Associate Professor of Business at Columbia Business School. He received his Ph.D. in economics from Carnegie Mellon University in 2007 and joined Columbia Business School the same year. His research focuses on empirical industrial organization, with an emphasis on questions pertaining to pricing, innovation, advertising, dynamic oligopoly, and competitive strategy.Rajdeep Grewal (“Stock Market Reactions to Customer and Competitor Orientations: The Case of Initial Public Offerings”) is the Irving & Irene Bard Professor of Marketing at the Smeal College of Business at the Pennsylvania State University; he is also the associate research director of the Institute for the Study of Business Markets there. He received his Ph.D. in 1998 from the University of Cincinnati. His research focuses on empirically modeling strategic marketing issues and has appeared in prestigious journals such as the Journal of Marketing, Journal of Marketing Research, Marketing Science, Management Science, Quantitative Marketing and Economics, and Strategic Management Journal. Currently, he serves as an associate editor for the Journal of Marketing Research and an area editor for the Journal of Marketing.Dominique Hanssens (“Editorial—Marketing Science: A Strategic Review”) is the Bud Knapp Distinguished Professor of Marketing at the UCLA Anderson School of Management, where he has been on the faculty since 1977. His research focuses on quantitative models that improve our understanding of marketing impact on business performance. From 2005 to 2007, he served as Executive Director of the Marketing Science Institute in Cambridge, MA. In 2010, he was elected a fellow of the INFORMS Society for Marketing Science.Wesley R. Hartmann (“Advertising Effects in Presidential Elections”) is an associate professor of marketing at the Stanford Graduate School of Business. He holds a Ph.D. in economics from the University of California, Los Angeles. He is interested in applying and developing econometric techniques to analyze questions relevant to marketing and economics. His current research focuses on dynamic choice contexts, pricing, social interactions, and targeted marketing.John R. Hauser (“Editorial—Marketing Science: A Strategic Review”) is the Kirin Professor of Marketing at the MIT Sloan School of Management, where he teaches new product development, marketing management, competitive marketing strategy, and research methodology. He has consulted for a variety of corporations on product development, sales forecasting, marketing research, voice of the customer, defensive strategy, and research and development management. Among his awards include the Converse Award for contributions to the science of marketing and the Parlin Award for contributions to marketing research. He is a founder and principal at Applied Marketing Science, Inc., a former trustee of the Marketing Science Institute, a fellow of INFORMS and of the INFORMS Society of Marketing Science, and serves on many editorial boards. He enjoys sailing, NASCAR, opera, and country music.Csilla Horváth (“Moderating Factors of Immediate, Gross, and Net Cross-Brand Effects of Price Promotions”) is an assistant professor of marketing at the Institute for Management Research, Radboud University, Nijmegen, the Netherlands. Her research interests include modeling dynamic marketing processes, branding, self-control, and harmful consumer behavior. She publishes in journals such as the Journal of Marketing Research, International Journal of Research in Marketing,Marketing Letters, and International Journal of Forecasting.Dmitri Kuksov (“A Model of the “It' Products in Fashion”) is a professor of marketing at the Naveen Jindal School of Management, the University of Texas at Dallas; previously, he worked at Washington University in St. Louis. He holds a Ph.D. in marketing from Haas Business School of the University of California, Berkeley. His research interests include competitive strategy, markets with incomplete information, consumer communication and networks, branding and product line strategy, and customer satisfaction. He received 2005 Frank M. Bass Dissertation Award; two of his papers were finalists for 2007 John D. C. Little Award, and one was a finalist for INFORMS 2012 Long Term Impact Award. He is an associate editor of Marketing Science, Management Science, and Quantitative Marketing and Economics.Vardit Landsman (“The Relationship Between DTCA, Drug Requests, and Prescriptions: Uncovering Variation in Specialty and Space”) is an assistant professor of marketing at the Recanati Business School, Tel Aviv University (Israel), and the Erasmus School of Economics, Erasmus University Rotterdam (the Netherlands). Her fields of interest include the implementation of new modeling approaches to the study of marketing phenomena. Her work involves the study consumer choice and, in particular, the analysis of choice processes within new markets, as well as the study of marketing issues in the context of life sciences. Her work has been published in the Journal ofMarketing and QuantitativeMarketing and Economics.Carl F. Mela (“Invited Paper—A Keyword History of Marketing Science”) is the T. Austin Finch Foundation Professor of Marketing at Duke University, where he teaches brand management and the marketing core. His research focuses on the long-term effects of marketing activity and new media. His articles appear in the Journal of Marketing Research, Marketing Science, the Journal of Marketing, the Harvard Business Review, and the Journal of Consumer Research, and he has received or been nominated for more than 25 best paper awards. His home page is located at http://www.duke.edu/∼mela.Vishal Narayan (“Modeling Consumer Learning from Online Product Reviews”) is an assistant professor of marketing at the Johnson School at Cornell University. His primary research interests lies in studying social interactions and how such interactions affect consumer preferences and choices. He seeks to develop econometric methods that would lead to improved managerial decision making in the areas of pricing, product development, and demand estimation. More recently, he has developed an interest in studying marketing issues specifically pertaining to shopping behavior in emerging markets.Thomas Otter (“Successive Sample Selection and Its Relevance for Management Decisions”) is a professor of marketing in the faculty of business and economics of Goethe University Frankfurt. He received his Ph.D. from the Vienna University of Economics and Business Administration (WU-Wien). His research interests are in the development and application of Bayesian techniques to help conceptualize and solve problems in marketing and marketing research.Jagmohan Singh Raju (“Editorial—Marketing Science: A Strategic Review”) is the Joseph J. Aresty Professor of Marketing at the Wharton School and the Chair of Wharton's Marketing Department. He has a Ph.D. from the Graduate School of Business at Stanford University. His previous research has won the Frank M. Bass Award as well as the John D. C. Little Award. He has served as a departmental editor for the marketing department of Management Science, president of INFORMS Society for Marketing Science, and secretary treasurer of the INFORMS College of Marketing.Jason Roos (“Invited Paper—A Keyword History of Marketing Science”) is an assistant professor of marketing at the Rotterdam School of Management, Erasmus University Rotterdam. His dissertation, “Hyper-Media Search and Consumption” (Duke University, 2012), was a winner of the 2012 ISMS Doctoral Dissertation Proposal Competition. His research focuses on issues related to new media and the Internet, as well as the entertainment industry. His website is located at http://www.jasonmtroos.com.Alok R. Saboo (“Stock Market Reactions to Customer and Competitor Orientations: The Case of Initial Public Offerings”) is an assistant professor of marketing at the J. Mack Robinson College of Business, Georgia State University. He received a Ph.D. in marketing from the Smeal College of Business, Pennsylvania State University. His research interest lies in exploring the performance effectiveness of firms' marketing strategies.Kannan Srinivasan (“Editorial—Marketing Science: A Strategic Review”) is the Rohet Tolani Distinguished Professor of International Business and the H.J. Heinz II Professor of Management, Marketing and Information Systems, Tepper School of Business, Carnegie Mellon University. He has published more than 60 articles in leading marketing, management, and statistics journals and holds five patents. He serves as the president of INFORMS Society for Marketing Science and as an advisory board member for Marketing Science. He is also an associate editor of Management Science and Quantitative Marketing and Economics and a member of the editorial board of the Journal of Marketing Research.Richard Staelin (“Editorial—Marketing Science: A Strategic Review”) is the Edward and Rose Donnell Professor of Business Administration at the Fuqua School of Business, Duke University. He graduated many years ago from the University of Michigan and taught at Carnegie Mellon University (for 13 years), the University of Chicago (for 1 semester), and the Australian Graduate School of Management (for 1 year) prior to his arrival at Duke in 1982; since then, he has been deputy dean (twice), associate dean of executive education, executive director for the Teradata Center for CRM, and the initial managing director of Global Executive MBA (GEMBA) at Duke. He also was deeply involved in setting up the Duke Goethe, Duke Seoul University, and Nazarbayev University alliances. He has taught in the daytime MBA program and every executive MBA program ever offered by Duke; he is currently teaching the core marketing course in the new Master of Management Studies (MMS) program. He has published more than 80 papers in academic journals and has received best paper awards at the Journal ofMarketing Research, the Journal of Marketing, and Marketing Science as well as the Outstanding Educator Award and the Converse Award from the American Marketing Association.Stefan Stremersch (“The Relationship Between DTCA, Drug Requests, and Prescriptions: Uncovering Variation in Specialty and Space”) holds a chair in marketing and is the Desiderius Erasmus Distinguished Chair of Economics at the Erasmus School of Economics, Erasmus University Rotterdam (the Netherlands) and a professor of marketing at the IESE Business School, Universidad de Navarra (Spain). His current research interests are in innovation acceptance/diffusion, marketing of innovations, marketing of technology and life sciences, and international marketing. He has won several awards, such as the Harold H. Maynard Best Paper Award of the Journal of Marketing (2002), the J.C. Ruigrok Prize (2005) for the most productive young researcher in the social sciences in the Netherlands (only once in four years awarded to an economist), and the AMA Early Career Award in Marketing Strategy (2008). He also received the 2004 Research Prize at Erasmus University Rotterdam for outstanding research performance, selected among all Erasmus faculty across all disciplines and schools.Sriram Venkataraman (“The Relationship Between DTCA, Drug Requests, and Prescriptions: Uncovering Variation in Specialty and Space”) is an assistant professor of marketing at the Kenan-Flagler Business School, University of North Carolina at Chapel Hill. He received his Ph.D. from Cornell University. His research investigates marketing pertinent questions in the entertainment, pharmaceutical, and U.S. automobile industries. His work has appeared in journals such as Marketing Science, Management Science, and American Economic Review.Stephan Wachtel (“Successive Sample Selection and Its Relevance for Management Decisions”) studied at Christian-Albrechts-Universität zu Kiel, specializing in innovation, new media and marketing, distributive trade, statistics, and econometrics. Until January 2012, he was research associate at the chair of services marketing at Goethe University Frankfurt, Germany. After finishing his Ph.D, he is working as a business analyst.Kangkang Wang (“A Model of the “It' Products in Fashion”) is a doctoral student at the Olin Business School, Washington University in St. Louis.Sha Yang (“Modeling Consumer Learning from Online Product Reviews”) is a professor of marketing at the Marshall School of Business, University of Southern California. She received a B.A. in international economics from Renmin University, China; and an M.S. in statistics, M.A. in marketing, and Ph.D. in marketing from the Ohio State University. Her primary research focuses on understanding and modeling household purchase behavior (especially the interdependent consumer decision making) and market competition. Her recent research interest focuses on Internet advertising. Her research has been published in leading journals such as Marketing Science, Management Science, the Journal of Marketing Research, the Journal of Marketing, and Quantitative Marketing and Economics.Yi Zhao (“Modeling Consumer Learning from Online Product Reviews”) is an assistant professor of marketing at the J. Mack Robinson College of Business, Georgia State University. He received his Ph.D. in marketing from the Hong Kong University of Science and Technology. His research interests include understanding dynamics and interaction of consumers' preferences, empirically the modeling of firms' competitive strategy and Bayesian estimation methods. His papers have appeared in leading journals such as Marketing Science and the Journal of Marketing Research.Ying Zhao (“Modeling Consumer Learning from Online Product Reviews”) is an associate professor of marketing at the Hong Kong University of Science and Technology. She received her Ph.D. from University of California, Berkeley. Her research interests include empirical modeling of competitive strategies, pricing, consumer choice models, and consumer decision making. Her papers have appeared in the Journal of Marketing Research, Management Science, Marketing Science, and Journal of Business.
I introduce the work of the finalists in the 2011–2012 Gary L. Lilien ISMS-MSI Practice Prize Competition, representing once again the best combinations of rigor and relevance produced by marketing scientists. The winning paper is by a team who developed an innovative approach to measuring the impact of a social media campaign for an Indian premium ice cream retailer, Hokey Pokey. The other two finalists are from a team that developed a category management tool for a leading wine brand, Beringer, and a team that developed a system called PROSAD, which determines optimal bids to maximize an advertiser's profit per search engine advertising keyword.
Hokey Pokey, a popular “super premium” ice cream retailer, has over a dozen outlets based in India. Hokey Pokey offers “customized mix-in” flavors and realizes the importance of social media platforms to connect with its target consumers and create an engaging brand experience. However, with a limited marketing budget, the retailer needed to measure the success of its social media marketing efforts and create an optimized strategy. To accomplish this, we proposed and implemented a methodology to measure social media return on investment (ROI) and a customer's word-of-mouth (WOM) value by first creating a unique metric to measure the net influence wielded by a user in a social network, customer influence effect (CIE), and then predicting the user's ability to generate the spread of viral information. We then link WOM to the actual sales that it generates through a second metric, customer influence value (CIV), and we implement a strategy at Hokey Pokey to measure these metrics and identify their individual drivers. Finally, we refine our strategy to increase CIE and CIV, thereby impacting the profit. Our research shows that social media can be used to generate growth in sales, ROI, and positive word of mouth and can spread brand knowledge further.
This paper reports on a large-scale implementation of marketing science models to solve the bidding problem in search engine advertising. In cooperation with the online marketing agency SoQuero, we developed a fully automated bidding decision support system, PROSAD (PRofit Optimizing Search engine ADvertising; see http://www.prosad.de), and implemented it through the agency's bid management software. The PROSAD system maximizes an advertiser's profit per keyword without the need for human intervention. A closed-form solution for the optimized bid and a newly developed “costs-per-profit” heuristic enable advertisers to submit good bids even when there is significant noise in the data. A field experiment demonstrates that PROSAD can increase the return on investment by 21 percentage points and improve the yearly profit potential for SoQuero and its clients by €2.7 million.
The purpose of this paper is to describe the implementation of a category management tool known as Category Optimizer™ at Foster's Wine Estates Americas for one of its brands, the Beringer California Collection. Foster's was facing a common management problem: harnessing its portfolio of Beringer California Collection wines to increase profitability, improve its competitive position, and defend against a disruptive new entrant in the U.S. wine market called Yellow Tail. Category Optimizer combines the parsimony of an internal market structure with the advances that have been made in assortment planning in operations research, assortment and stock-keeping-unit–level modeling, mixed logits, and the marketing literature on the perceptions of variety of assortment to develop and estimate a model on readily available store scanner data. The model subsequently uses these results to inform strategic and tactical decision making. This approach led to recommendations that initially seemed counterintuitive; the normal response would be for Foster's to consider lowering prices to maintain share and volume, a strategy not inconsistent with many of the recommendations of past models. However, considering the additional degrees of freedom that a product range offered for defense, we demonstrated that a combination of price increases together with the introduction of a volume-flanker product in a new channel would improve profits, increase revenue, and protect and enhance market share. These were successfully implemented in early 2008, earning rich dividends for the company; increasing profitability by 70%, revenue by 3%, and earnings before interest and taxes by 8.5%; and having a positive impact on its brand ranking. In fact, in 2008, it debuted as sixth among the international wine brands. It also managed to play an important role in deposing Yellow Tail, the market share leader, from its dominant position. We conclude the paper by providing examples of other companies where this approach has also been successfully implemented and by discussing some avenues for future research.
From 2003 to 2012, the ISMS-MSI Practice Prize/Award competition has documented 25 impactful projects, with associated papers appearing in Marketing Science. This article reviews these papers and projects, examines their influence on the relevant organizations, and provides a perspective on the diffusion and impact of marketing science models within the organizations. We base our analysis on three sources of data—the articles, authors' responses to a survey, and in-depth interviews with the authors. We draw some conclusions about how marketing science models can create more impact without losing academic rigor while maintaining strong relevance to practice.We find that the application and diffusion of marketing science models are not restricted to the well-known choice models, conjoint analysis, mapping, and promotional analysis—there are very effective applications across a wide range of managerial problems using an array of marketing science techniques. There is no one successful approach, and although some factors are correlated with impactful marketing science models, there are a number of pathways by which a project can add value to its client organization. Simpler, easier-to-use models that offer robust and improved results can have a stronger impact than academically sophisticated models can. Organizational buy-in is critical and can be achieved through recognizing high-level champions, holding in-house presentations and dialogues, doing pilot assignments, involving multidepartment personnel, and speaking the same language as the influential executives. And we find that intermediaries often, but not always, play a key role in the transportability and diffusion of models across organizations.Although these applications are impressive and reflect profitable academic–practitioner partnerships, changes in the knowledge base and reward systems for academics, intermediaries, and practitioners are required for marketing science approaches to realize their potential impact on a much larger scale than the highly selective sample that we have been able to analyze.
In many markets, a handset vendor and a service provider may enter into a tie-in for a handset to be available exclusively through the service provider. We examine when and why a service provider and a handset vendor may find this arrangement mutually profitable. We find that an exclusive handset arrangement (EHA) may serve a dual strategic purpose. By restricting its handsets to one service provider, a handset vendor may be able to induce a rival handset vendor to compete less aggressively. At the same time, the service provider may be able to essentially raise a rival service provider's handset costs by limiting the handsets available to the rival. Interestingly, the handset vendor's market share may be higher when its handset is sold exclusively than when it is not. Our results might explain why EHAs seem more attractive in some markets than in others, why some service providers have exclusive arrangements even for handset models that do not seem popular, and how some handset vendors enjoy high market shares despite having many exclusive models. Furthermore, an EHA may lower the handset vendor's incentives to improve handset quality, supporting concerns raised by proponents of wireless network neutrality.
In this paper we quantify the economic worth of celebrity endorsements by studying the sales of endorsed products. We do so with the use of two unique data sets consisting of monthly golf ball sales and professional golfer (celebrity) rankings. In particular, we examine the impact Tiger Woods had on sales of Nike golf balls. Our identification of the causal effect of a celebrity is grounded in the celebrity's random performance over time.Using two different approaches, reduced form and structural, we find that there are substantial celebrity endorsement effects. From our structural model, we determine that endorsements not only induce consumers to switch brands, a business stealing effect, but also have a primary demand effect. We determine that from 2000 to 2010, the Nike golf ball division reaped an additional profit of $103 million through the acquisition of 9.9 million in sales from Tiger Woods' endorsement effect. Moreover, having Tiger Woods' endorsement led to a price premium of roughly 2.5%. As a result, approximately 57% of Nike's investment in Woods' $181 million endorsement deal was recovered just in U.S. golf ball sales alone.
Until recently, brand identities were built by firms via brand image advertising. However, the flourishing consumer communication weakened the firms' grip on their brands. The interaction between advertising and consumer communications and their joint impact on brand identity is the focal point of this paper.We present a model in which consumer preference for functional attributes may correlate with the identity they desire to project of themselves. This correlation is known to the firm but not to the consumers. Both the firm and the consumers can communicate their desired brand identity, although the actual brand identity is determined endogeneously by the composition of consumers who purchase it (i.e., what types of people consume the brand).We find that sometimes the firm can strengthen the identity of its brand by refraining from advertising. This result is based on the following intermediate finding: advertising can diminish the endogeneous informativeness of consumer communications by making it one-sided. Furthermore, it turns out that refraining from brand image advertising may be optimal for the firm when the product is especially well positioned to create a strong identity—i.e., when consumer preferences for functional and self-expressive attributes are highly correlated.
There is a growing trend among consumers to serially consume small, incomplete “chunks” of multiple media types—television, radio, Internet, and print—within a short time period. We refer to this behavior as media multiplexing and note that key challenges for integrated marketing communications media planners are (1) predicting which media or combination of media their target audience is likely to consume at any given time and (2) understanding potential substitutions and complementarities in their joint consumption. We propose a forecasting model that incorporates media-multiplexing behavior of both traditional and new media, their interdependencies, and consumer heterogeneity, and we calibrate the model using a rich database of individual-specific media activity diaries. The results suggest that accounting for media synergies within a single utility specification significantly improves model forecasts. We also introduce a utility function that directly models cross-channel media complementarities via interactive effects of the satiation parameters of own and joint consumption of various media types. Finally, our individual-level analyses generate unique insights on consumer-level media switching, multiplexing, and individual heterogeneity often ignored in aggregate data.
A setting is considered where consumers keep track of the extent to which brands care about them, which is modeled as altruism of brands toward their target consumers. Consumers who purchase an experience good of high quality reasonably deduce that the supplier of this good is relatively altruistic toward them, and they are therefore more keen to purchase a brand extension that is also directed at them. As a result, the success of brand extensions depends on the overlap between the customers of the original product and the target customers of the extension product. The quality and demand for a brand extension can be higher if the brand is perceived as caring only for its most quality-conscious consumers rather than for all possible buyers of the good.
A firm may want to preannounce its plans to develop a new product in order to stimulate future demand. But given that such communications can affect rivals' incentives to develop the same new product, a firm may decide to preannounce untruthfully in order to deter competitors. We examine an incumbent's preannouncement strategy when there is uncertainty regarding the commercial viability of a new product opportunity and a threat of rival entry. Each firm has a private assessment of the market potential for the new product. Two competitive incentives arise for the incumbent in terms of discouraging rival entry: it can use preemptive communication or it can remain silent and instill a pessimistic market potential outlook. We find that an incumbent prefers to follow a vaporware strategy—i.e., declares plans to pursue a new product opportunity even when it may have no development intentions—when its market forecasting capabilities are weak and the demand-side benefits from preannouncing are small. By contrast, when the incumbent has strong market forecasting capabilities and the demand-side benefits are small, the incumbent adopts a suddenware strategy—i.e., remains silent about its new product plans even when it actually plans to develop the new product. Finally, when its market forecasting capabilities are strong and the demand-side benefits are large, the incumbent prefers to engage in a trueware strategy—i.e., truthfully preannounces development plans. We show that an interplay between competition-related and demand-related considerations is what allows trueware to emerge as an equilibrium in the absence of any ex post cost to engaging in vaporware. In an extension, we let the incumbent's actual development plans leak out and allow the entrant to wait and learn those plans prior to setting a research and development level. We identify conditions for the entrant to postpone development despite the risk of being late to market, as well as conditions for the entrant to commence development immediately despite not knowing the incumbent's plans based on the observed preannouncement strategy.
Nadia Abou Nabout (“Practice Prize Paper—PROSAD: A Bidding Decision Support System for Profit Optimizing Search Engine Advertising”) received her Ph.D. from Goethe University Frankfurt, Germany, where she now holds a position as an assistant professor. Her research projects focus on online marketing—in particular, search engine advertising, Facebook advertising, and advertising exchanges. Her publications appeared in journals such as Marketing Science, the International Journal of Research in Marketing, and the Journal of Interactive Marketing.Vikram Bhaskaran (“Practice Prize Winner—Creating a Measurable Social Media Marketing Strategy: Increasing the Value and ROI of Intangibles and Tangibles for Hokey Pokey”) is an analyst at Freshdesk, Inc., in Chennai, India. He graduated with a master's degree in marketing from the Robinson College of Business at Georgia State University.Kevin YC Chung (“Economic Value of Celebrity Endorsements: Tiger Woods' Impact on Sales of Nike Golf Balls”) is a doctoral candidate in the Tepper School of Business at Carnegie Mellon University. He will earn his Ph.D. in the spring of 2013. He also holds an M.S. in statistics from the University of Chicago and a B.A. and B.S. degrees from the University of Pennsylvania. Timothy P. Derdenger (“Economic Value of Celebrity Endorsements: Tiger Woods' Impact on Sales of Nike Golf Balls”) is an assistant professor of marketing and strategy in the Tepper School of Business at Carnegie Mellon University. He holds a Ph.D. in economics from the University of Southern California and a B.B.A. from the George Washington University.Sandy D. Jap (“Media Multiplexing Behavior: Implications for Targeting and Media Planning”) is the Goizueta Term Chair Professor of Marketing at Emory University. Her research focuses on the development and management of interorganizational relationships, multichannel management and design, and e-procurement processes such as online reverse auctions; this research has been published in a variety of books and journals, including the Journal of Marketing Research, Journal of Marketing, Marketing Science, Management Science, and Organization Science. She has received numerous awards and distinctions, including the Lou Stern Award, an MSI Young Scholar award and an O'Dell Award finalist. Currently, she is an editorial board member at the Journal of Marketing Research and Marketing Letters and an area editor for the International Journal of Research in Marketing. Dmitri Kuksov (“Advertising and Consumers' Communications”) is a professor of marketing at the Naveen Jindal School of Management, the University of Texas at Dallas; he previously worked at the Washington University in St. Louis. He holds a Ph.D. in marketing from Haas Business School of the University of California, Berkeley. His research interests include competitive strategy, markets with incomplete information, consumer communication and networks, branding and product line strategy, and customer satisfaction. His work has appeared in a number of journals including Marketing Science, Management Science, the Journal of Marketing Research, and the Journal of Economic Theory. He received the 2005 Frank M. Bass Dissertation Award; two of his papers were finalists for 2007 John D. C. Little Award, and one was a finalist for INFORMS 2012 Long Term Impact Award.V. Kumar (“Practice Prize Winner—Creating a Measurable Social Media Marketing Strategy: Increasing the Value and ROI of Intangibles and Tangibles for Hokey Pokey”) is the Regents Professor, Lenny Distinguished Chair, Professor of Marketing, Executive Director of the Center for Excellence in Brand and Customer Management, and Director of the Ph.D. Program in Marketing at the J. Mack Robinson College of Business, Georgia State University. He has been recognized with seven lifetime achievement awards in marketing, the Paul D. Converse Award, the Sheth Foundation/Journal of Marketing Long Term Impact Award, and the Gary L. Lilien ISMS-MSI Practice Prize Award. He has published over 200 articles and books in many scholarly journals in marketing including the Harvard Business Review, Sloan Management Review, Journal of Marketing, Journal of Marketing Research, Marketing Science, Management Science, and Operations Research. He leads the marketing science to marketing practice initiative at the INFORMS Society for Marketing Science and has worked with Global Fortune 1000 firms to maximize their profits. He spends his “free” time visiting business leaders to identify challenging problems to solve.Gary L. Lilien (“Effective Marketing Science Applications: Insights from the ISMS-MSI Practice Prize Finalist Papers and Projects”) is a Distinguished Research Professor at Pennsylvania State University, and cofounder and research director of the Institute for the Study of Business Markets (http://www.isbm.org). He is an inaugural Fellow of the Institute for Operations Research and the Management Sciences (INFORMS), the INFORMS Society for Marketing Science (ISMS), and the European Marketing Academy. He was honored as a Morse Lecturer for INFORMS and received the Kimball medal from INFORMS for distinguished contributions to the field of operations research. He has received honorary doctorates from the University of Liege, the University of Ghent and Aston University, as well as the 2008 Educator of the Year Award from the American Marketing Association. He received the 2012 Gilbert Churchill Award for Lifetime Contributions to Marketing Research; in 2010, the ISMS-MSI Practice Prize for the best applied work in marketing science globally was renamed the Gary Lilien ISMS-MSI Practice Prize in his honor.Chen Lin (“Media Multiplexing Behavior: Implications for Targeting and Media Planning”) is an assistant professor of marketing at the Eli Broad College of Business, Michigan State University, where she teaches undergraduate marketing strategy and a Ph.D. modeling seminar. She received her Ph.D. in marketing from the Goizueta Business School at Emory University in 2012 and her B.S. in computing from the National University of Singapore in 2007. Her research interest lies in examining complex decisions under multiple consumption scenarios; this includes consumer decisions in traditional multicategory and multichannel media consumption, multichannel advertising setting, as well as in broader applications where product categories are seemingly disparate. Her work has been published in Marketing Science. Sharat K. Mathur (“Practice Prize Paper—Category Optimizer: A Dynamic-Assortment, New-Product-Introduction, Mix-Optimization, and Demand-Planning System”) is a senior vice president with the SymphonyIRI group in Chicago. His work focuses on helping Fortune 100 companies achieve their sales and marketing strategies using advanced, real-time analytics. Previously he worked with Booz & Company and Archstone Consulting, where he served clients across a range of industries. Prior to becoming a management consultant, he was an assistant professor of marketing at the Australian Graduate School of Management in Sydney, Australia. He has a Ph.D. in marketing from the University of Iowa.Rohan Mirchandani (“Practice Prize Winner—Creating a Measurable Social Media Marketing Strategy: Increasing the Value and ROI of Intangibles and Tangibles for Hokey Pokey”) is the director of the Ross Group and Drumsfood International (owner of Hokey Pokey) and is responsible for facilitating the field experiment in India. He graduated with a bachelor's degree in finance from New York University and a master's degree in business administration from the Wharton School of the University of Pennsylvania. He has worked in a private equity firm and currently pursues his passion of entrepreneurship. Elie Ofek (“Vaporware, Suddenware, and Trueware: New Product Preannouncements Under Market Uncertainty”) is the T. J. Dermot Dunphy Professor of Business Administration at the Harvard Business School. He received his Ph.D. in business and M.A. in economics from Stanford University. His research focuses on how marketing decisions and input can impact innovation strategy and on how firms can leverage novel technologies or major trends to deliver value to customers. His research has appeared in Marketing Science, Management Science, the Journal of Marketing Research, the Journal of Consumer Research, and the Journal of Economics and Management Strategy. He is an associate editor at Management Science and serves on the editorial boards of Marketing Science, the Journal of Marketing Research, and the International Journal of Research in Marketing.Jagmohan S. Raju (“Exclusive Handset Arrangements in the Wireless Industry: A Competitive Analysis”) is the Joseph J. Aresty Professor of Marketing at the Wharton School and the chair of Wharton's Marketing Department. He has a Ph.D. from the Graduate School of Business at Stanford University. His previous work has won the Frank M. Bass Award as well as the John D. C. Little Award. This paper is based on the Ph.D. thesis of Upender Subramanian, who is his 12th doctoral student.John H. Roberts (“Effective Marketing Science Applications: Insights from the ISMS-MSI Practice Prize Finalist Papers and Projects”) holds a joint appointment as professor of marketing at the Australian National University and the London Business School; he is also an Emeritus Scientia Professor at the University of New South Wales. His research concerns marketing strategy, marketing models and their adoption by industry, new product marketing and brand equity, high-technology marketing, and diffusion of new products. He has won the American Marketing Association's John A. Howard Award, its William O'Dell Award, and its Advanced Research Techniques Forum Best Paper Award; he has been a finalist for the INFORMS Society for Marketing Science's John D. C. Little Award three times and its ISMS-MSI Practice Prize twice. He was the Inaugural Distinguished Researcher of the Year of the Australia/New Zealand Marketing Academy and is the only academic to have won the Australian Marketing Institute's Sir Charles McGrath Award. He sits on the editorial boards of the Journal of Marketing Research, Journal of Forecasting, Marketing Science, and International Journal of Research in Marketing.Julio J. Rotemberg (“Expected Firm Altruism, Quality Provision, and Brand Extensions”) is a professor of business administration at Harvard Business School. He has a Ph.D. in economics from Princeton University. His research interests include pricing, with an emphasis on its impact on the economy as a whole, as well as the effect of consumer and worker psychology on firm decision making more generally. He has been editor of the Review of Economics and Statistics and has published in a variety of journals, including Management Science.Anna Sahgal (“Practice Prize Paper—Category Optimizer: A Dynamic-Assortment, New-Product-Introduction, Mix-Optimization, and Demand-Planning System”) is a principal at AS Marketing International, Sydney, Australia, where she specializes in the development of quantitative and qualitative methodologies that help marketing managers make better decisions. Apart from presenting her work at elite conferences, she has published in the Journal of Retailing and is also the recipient of William R. Davidson Award (honorable mention) for the Best Journal of Retailing Paper.Ron Shachar (“Advertising and Consumers' Communications”) is the dean of the Arison School of Business at the Interdisciplinary Center, Israel. Previously he was a faculty member at Yale University and Tel Aviv University. His research interests include advertising, branding, choice modeling, the entertainment industries, identity marketing, and political marketing. His work has been published in numerous academic journals such as the American Economic Review, American Journal of Political Science, Journal of Marketing Research, Marketing Science, Quantitative Marketing and Economics, and the RAND Journal of Economics. He serves as an area editor for the Quantitative Marketing and Economics, is on the editorial boards of Marketing Science and the International Journal of Research in Marketing, and has served as an associate editor in the Journal of Marketing Research. Milap Shah (“Practice Prize Winner—Creating a Measurable Social Media Marketing Strategy: Increasing the Value and ROI of Intangibles and Tangibles for Hokey Pokey”) is the cofounder and director of Hokey Pokey, India, who assisted in the implementation of the concept proposed in this study.Venkatesh Shankar (“Effective Marketing Science Applications: Insights from the ISMS-MSI Practice Prize Finalist Papers and Projects”) is the Professor & Coleman Chair in Marketing and director of research at the Center for Retailing Studies, Mays Business School, Texas A&M University. His research areas include e-business, competitive strategy, international marketing, branding, pricing, innovation management, and channel and supply chain management. He is the winner of the 2012 Mahajan Award for Lifetime Contributions to Marketing Strategy Research, the 2006 Robert B. Clarke Outstanding Educator Award, the 2001 IBM Faculty Partnership Award, the 1999 Paul Green Award for the best article in the Journal of Marketing Research, and the 2000 Don Lehmann Award for the best dissertation-based article in an American Marketing Association journal. He is a twotime finalist for the ISMS-MSI Practice Prize. He is Editor Emeritus of the Journal of Interactive Marketing, is a former associate editor of Management Science, and is on the editorial boards of the Journal of Marketing, Journal of Marketing Research, Marketing Science, International Journal of Research in Marketing, and Journal of Retailing.Ashish Sinha (“Practice Prize Paper—Category Optimizer: Dynamic-Assortment, New-Product-Introduction, Mix-Optimization, and Demand-Planning System”) is a professor and Head, School of Marketing, University of New South Wales, Sydney, Australia. He obtained his Ph.D. from the University of Alberta, Canada. His major area of interest lies in developing models that help managers make better decisions. His work has appeared in journals such as Marketing Science, the Journal of Marketing Research, the Journal of Retailing, and Marketing Letters. He is also the recipient of several academic awards, including the William R. Davidson Award for the Best Journal of Retailing Paper (honorable mention), Academy of Marketing Science Best Dissertation Award, and AMA Best Market Research Paper Award; he is also a twice-finalist for the prestigious Gary L. Lilien ISMS-MSI Practice Prize in recognition of his work in the areas of category management, assortment, and disruptive innovation.Bernd Skiera (“Practice Prize Paper—PROSAD: A Bidding Decision Support System for Profit Optimizing Search Engine Advertising”) is director at the E-Finance Lab (http://www.efinancelab.de). He received his Ph.D. and his habilitation (venia legendi) from the University of Kiel (Germany) and took over the very first chair of electronic commerce at the School of Business and Economics at Goethe University Frankfurt, Germany. His research projects focus on online marketing—in particular, search engine advertising, social media, pricing, and customer management. His publications appeared in the top management and marketing journals.Kannan Srinivasan (“Economic Value of Celebrity Endorsements: Tiger Woods' Impact on Sales of Nike Golf Balls”) is the Rohet Tolani Distinguished Professor in International Business and the H. J. Heinz II Professor of Management, Marketing and Information Systems in the Tepper School of Business at Carnegie Mellon University. He holds a Ph.D. from the University of California, Los Angeles.Upender Subramanian (“Exclusive Handset Arrangements in the Wireless Industry: A Competitive Analysis”) is an assistant professor of marketing at the Jindal School of Management of the University of Texas at Dallas. He obtained his Ph.D. in marketing from the Wharton School, University of Pennsylvania. His research examines how firms compete and collaborate through various arrangements in retailing, telecommunication and other industries.Özge Turut (“Vaporware, Suddenware, and Trueware: New Product Preannouncements Under Market Uncertainty”) is an assistant professor of marketing at the School of Management, Sabancı University, Turkey. She received her doctoral degree from the Harvard Business School in 2006. Before joining Sabancı University, she was an assistant professor of marketing at the Olin Business School, Washington University in St. Louis. Her research has appeared in leading marketing and strategy journals such as the Journal of Marketing Research and Journal of Economics and Management Strategy.Sriram Venkataraman (“Media Multiplexing Behavior: Implications for Targeting and Media Planning”) is an assistant professor of marketing at the Kenan-Flagler Business School, University of North Carolina at Chapel Hill. He received his Ph.D. from Cornell University. His research investigates marketing pertinent questions in the entertainment, pharmaceutical, and U.S. automobile industries. His work has appeared in journals such as Marketing Science, Management Science, and the American Economic Review.Kangkang Wang (“Advertising and Consumers' Communications”) is a marketing Ph.D. student at the Washington University in St. Louis; she is expecting to graduate in 2013. Her research interests include quantitative modeling, status goods marketing, durable goods marketing, and strategic implications of consumer behavioral anomalies.Z. John Zhang (“Exclusive Handset Arrangements in the Wireless Industry: A Competitive Analysis”) is a professor of marketing and the Murrel J. Ades Professor at the Wharton School. He has published many articles in top marketing and management journals on various pricing issues. He has also been an iPhone lover, user, and commentator from its get-go.
Marketing Science greatly benefited from the admirable and fastidious efforts of more than 200 different individuals who provided manuscript reviews last year. Beyond those individuals already recognized on the editorial board, the editor-in-chief and guest editors of Marketing Science are indebted to the many guest editors-in-chief, guest associate editors, and ad hoc reviewers who provided expert counsel and guidance on a voluntary basis. The following list acknowledges the contribution of guest editors-in-chiefs, guest associate editors, and ad hoc reviewers who served from December 1, 2011 to December 31, 2012. Finally, let us not forget to thank the authors. Marketing Science requires and receives outstanding submissions from many leading researchers and prestigious organizations.
We empirically study the motivations of users to contribute content to social media in the context of the popular microblogging site Twitter. We focus on noncommercial users who do not benefit financially from their contributions. Previous literature suggests that there are two main types of utility that motivate these users to post content: intrinsic utility and image-related utility. We leverage the fact that these two types of utility give rise to different predictions as to whether users should increase their contributions when their number of followers increases. To address the issue that the number of followers is endogenous, we conducted a field experiment in which we exogenously added followers (or follow requests, in the case of protected accounts) to a set of users over a period of time and compared their posting activities to those of a control group. We estimated each treated user's utility function using a dynamic discrete choice model. Although our results are consistent with both types of utility being at play, our model suggests that image-related utility is larger for most users. We discuss the implications of our findings for the evolution of Twitter and the type of value firms may derive from such platforms in the future.
Laboratory and field experiments show that when choosing among a set of objects, consumers could be subjected to context-dependent preferences and evaluate options by considering both the absolute utilities and their relative standing in the choice set. Using this premise we construct a game-theoretic model of competition between two firms and investigate how a firm's decision to differentiate or imitate is affected when consumers' preferences are context-dependent. We consider two horizontally differentiated firms where some consumers own the product of one firm and the rest own the other firm's product. One firm upgrades its existing product by adding a new feature. In the absence of any cost or capability constraints, to protect its competitive position, the other firm would prefer to upgrade its product by adding a differentiated new feature. We show that if consumers' preferences are context-dependent and the new feature is of an incremental type, the second mover prefers to imitate the first mover by adding the same feature even when there is no cost disadvantage to differentiate itself. This happens because context-dependent preferences cause consumers to dislike brands that are very differentiated from one another. Thus, if the second mover mimics the first mover, both firms can charge higher prices for their upgraded products (i.e., imitation leads to higher prices). This outcome, in turn, leads the first mover to pick a new feature such that it would induce the second mover to imitate. Therefore, our analysis shows that the need for context management leads not just to imitation but also to accommodating imitation.
We examine multilateral bargaining in vertical supply relationships that involve an upstream manufacturer who sells through two competing retailers. In these relationships the negotiations are interdependent, and bargaining externality may arise across the retailers. In addition, the timing by which the manufacturer negotiates with the retailers becomes important. In simultaneous bargaining the retailers negotiate without knowing if an agreement has been reached in the other retail channel, whereas in sequential bargaining the retailer in the second negotiation is able to observe whether an agreement was reached in the first negotiation. We show that simultaneous bargaining is optimal for the manufacturer when the retail prices (and profitability) are similar, and sequential bargaining is preferred when the dispersion in the retail prices is sufficiently large. As a result of ex post renegotiations, the manufacturer may strategically stock out the less profitable retailer who charged a relatively low retail price and exclusively supply only the retailer who charged a relatively high retail price and maintained high channel profitability. Moreover, ex post multilateral bargaining can buffer downstream competition and thus lead to positive retail profits even in markets that are close to perfect competition.
The market structure of platform competition is critically important to managers and policy makers. Network effects in these markets predict concentrated industry structures, whereas competitive effects and differentiation suggest the opposite. Standard theory offers little guidance—full rationality models have multiple equilibria with wildly varying market concentration. We relax full rationality in favor of a boundedly rational cognitive hierarchy model. Even small departures from full rationality allow sharp predictions—there is a unique equilibrium in every case. When participants single-home and platforms are vertically differentiated, a single dominant platform emerges. Multihoming can give rise to a strong–weak market structure: one platform is accessed by all, and the other is used as a backup by some agents. Horizontal differentiation, in contrast, leads to fragmentation. Differentiation, rather than competitive effects, mainly determines market structure.
A symmetric complements refer to goods where one good is more dependent on the other, yet consumers receive enhanced utility from consuming both. Examples include garden hoses and sprinklers, chips and dip, and routine versus personalized services where the former has a broader base for utility generation and the latter is more dependent on the other's presence. Measuring asymmetric effects is difficult when all that is observed are the purchase quantities present in a consumer's market basket. We propose a direct utility model with a latent decision sequence for measuring asymmetric effects that allows us to capture differential responses to cross-category purchases and inventories. Scanner panel data of milk and cereal purchases are used to investigate the presence of asymmetric complementarity, and implications are explored through counterfactual analyses involving cross-price elasticities and spillover effects of merchandising variables.
When defection is unobserved, latent attrition models provide useful insights about customer behavior and accurate forecasts of customer value. Yet extant models ignore direct marketing efforts. Response models incorporate the effects of direct marketing, but because they ignore latent attrition, they may lead firms to waste resources on inactive customers.We propose a parsimonious model that allows direct marketing to impact three relevant behaviors in latent attrition models—the frequency with which customers conduct transactions, the size of the transactions, and the duration for which customers remain active. Our model also accounts for how the organization targets its direct marketing across individuals and over time.Using donation data from a nonprofit organization, we find that direct marketing increases donation incidence for active donors. However, our analysis also shows that direct marketing has the potential to shorten the length of a donor's relationship. We find that our proposed model offers superior predictive performance compared with models that ignore the impact of direct marketing activity or latent attrition. We demonstrate the managerial applicability of our modeling approach by estimating the impact of direct marketing on donation behavior and identifying those donors most likely to conduct transactions in the future.
We focus on destination categories, so named because they have the greatest impact on where households choose to shop and, more generally, on how category positioning affects which store a household chooses. We propose a reduced-form model-based analytical approach to identify categories that fill the destination role. Our approach determines which categories are most important to shoppers' store choice decisions and helps determine in which categories the retailer provides superior value. In addition, our approach allows us to understand the impact of the retailer's long-run merchandising policy decisions on the value it provides. Previous store choice research considered the effects of pricing, assortment and other merchandising decisions at the store level but did not focus on the effect of specific categories on store choice. This focus leads us to formulate a model that can (1) measure and explain the differential impact that specific categories have on shoppers' store choice decisions and (2) measure the relative value of retailers' category offerings, partitioning that value into the component resulting from retailer merchandising and the component that is nonmerchandising related. The model form captures differences in category value across stores (i.e., the store's category positioning) by specifying a spatial model for the store choice and category incidence intercepts. Our spatial model recognizes that stores position their offering vis-à-vis the category ideal based on long-run category merchandising decisions and that not all categories have the same importance in store choice decisions. We explore these issues for five retailers in the Charlotte, North Carolina market. We find that (1) category impact on store choice is highly skewed; (2) although categories with higher sales generally have a higher impact on store choice decisions, there are exceptions; (3) impact on store choice decisions does not vary systematically by the type of category (e.g., perishable versus dry grocery); and (4) our measure of category impact on store choice, although correlated with the category development index between retailers, is superior in that it provides a basis for comparing category impact within a retailer and how relative category value, based on long-run merchandising decisions, attracts shoppers to a store.
The predominant perception on commonality strategy in product line design is that it entails a trade-off decision for a firm between cost savings and product differentiation. Adopting the commonality strategy may lower a firm's manufacturing costs, but it blurs the distinction between products targeting different consumer segments and makes consumer switching between products more likely such that cannibalization is always intensified. We show that this view in the literature is based on a crucial assumption that the quality valuation of one consumer segment is greater than that of another segment for all product attributes; i.e., one segment's preference structure dominates the other segment's preference structure. In this paper we consider the case of a nondominating preference structure where each segment has an attribute it values more than the other segment does. Interestingly, we show that the effect of commonality strategy is more diverse in this nondominating preference structure and that commonality can actually relieve cannibalization in the product line design. This finding gives rise to a previously unrecognized opportunity for firms to redesign their product lines to improve profits.
Physicians may learn about prescription drug effectiveness directly from the firm via detailing or from patient experience. Patient-mediated learning is aided by the use of free drug samples. The effective use of samples is hampered by a lack of understanding of its exact return on investment implications. We seek to fill this gap by incorporating the physician's sample allocation behavior in the firm's decision making. We uncover the following implications for firms as well as policy makers. First, we find that the optimal sampling level for a drug category is a nonmonotonic function of patient payment ability and the price of the drug. Second, an increase in the cost of samples can lead to an increase in sampling and a decrease in detailing when the physician's propensity to provide sample subsidies is high. Third, when future market growth is expected to be high (early stage product life cycle and/or chronic drugs) and sampling efficiency is low, the use of sampling is profitable for the firm but leads to lower market coverage than when sampling is disallowed.
Greg M. Allenby (“A Direct Utility Model for Asymmetric Complements”) is the Kurtz Chair in Marketing at the Ohio State University. He is a fellow of the American Statistical Association and the 2012 recipient of the AMA Parlin Award for his contributions to the field of marketing research. He is editor of Quantitative Marketing and Economics and past area/associate editor for Marketing Science, the Journal of Marketing Research, and the Journal of Business and Economic Statistics.Ram Bala (“Offering Pharmaceutical Samples: The Role of Physician Learning and Patient Payment Ability”) is an assistant professor of operations management and information systems at the Leavey School of Business, Santa Clara University. He holds a Ph.D. in management science from the UCLA Anderson School of Management. His main research areas are product line design, promotional effort allocation, global product development, and pricing and contracting strategies for services. His research cuts across disciplinary lines, particularly operations management, marketing, and information systems.Pradeep Bhardwaj (“Offering Pharmaceutical Samples: The Role of Physician Learning and Patient Payment Ability”) is an associate professor of marketing at the College of Business Administration, University of Central Florida. His research interests include sales force management, distribution channels, customer relationship management, and pricing policies. His research has been published in leading journals such as Marketing Science, Management Science, and Quantitative Marketing and Economics. His ideas have been developed into several cross-functional projects that are applicable to sales force management, distribution channels, and customer lifetime value management.Richard A. Briesch (“Category Positioning and Store Choice: The Role of Destination Categories”) is the Marilyn and Leo Corrigan Endowed Associate Professor of Marketing, Cox School of Business, Southern Methodist University. His primary areas of research are the modeling consumer decision making, sales promotions, and nonparametric methods.Yuxin Chen (“Offering Pharmaceutical Samples: The Role of Physician Learning and Patient Payment Ability”) is the Polk Bros. Professor in Retailing at the Kellogg School of Management at Northwestern University. His primary research areas include database marketing, Internet marketing, pricing, retailing, competitive strategies, structural empirical models, Bayesian econometric methods, and behavioral economics. His research has appeared in journals such as the Journal of Marketing Research, Management Science, Marketing Science, and Quantitative Marketing and Economics. He received the Frank M. Bass Dissertation Paper Award for best marketing paper derived from a Ph.D. thesis published in an INFORMS-sponsored journal and the 2001 John D. C. Little Award for the best marketing paper published in Marketing Science or Management Science for his research on targeted marketing.Dilip Chhajed (“Can Commonality Relieve Cannibalization in Product Line Design?”) is a professor and associate head in the Department of Business Administration at the University of Illinois at Urbana–Champaign. He is also the executive director of the masters programs. He got his Ph.D. from Purdue University. His research interests are in supply chain management and product and process design problems. He coedited (with Timothy J. Lowe) Building Intuition: Insights from Basic Operations Management Models and Principles.William R. Dillon (“Category Positioning and Store Choice: The Role of Destination Categories”) is the Herman W. Lay Professor of Marketing and Professor of Statistics at the Cox School of Business, Southern Methodist University. He also serves as senior associate dean. He received his Ph.D. in marketing and quantitative methods from the City University of New York. His research interests are in the areas of segmentation, positioning, brand equity, market structure, and issues related to the development and use of latent-class/mixture models and covariance structure models.Edward J. Fox (“Category Positioning and Store Choice: The Role of Destination Categories”) is an associate professor of marketing and the Corrigan Research Professor at the Edwin L. Cox School of Business at Southern Methodist University. He is also the W.R. & Judy Howell Director of Southern Methodist University's JCPenney Center for Retail Excellence. His research interests include retail management, consumer shopping behavior, and statistical modeling. His recent projects have focused on retail assortment management, pricing and inventory management, and consumer “cherry-picking” behavior.Liang Guo  (“Multilateral Bargaining and Downstream Competition”) is an associate professor of marketing and the Senior Wei Lun Fellow at the Hong Kong University of Science and Technology. He received a Ph.D. in business administration from the University of California, Berkeley, and a B.A. in economics from Beijing University. His research interests include economics of psychology, marketing strategy, industrial organization, and applied economics. His research work has been accepted for publication at the Journal of Economics and Management Strategy, Management Science, and Marketing Science. He serves as an associate editor on the editorial boards of the International Journal of Research in Marketing, Marketing Science, and Management Science.Tanjim Hossain (“When Do Markets Tip? A Cognitive Hierarchy Approach”) is an assistant professor of marketing at the University of Toronto. He has conducted research on online auctions, incentive effects in the lab and in the field, and two-sided markets. He has published papers in the top economics, management, and marketing journals, and his research has also been featured in USA Today, the Christian Science Monitor, the Boston Globe, U.S. News and World Report, and the Economist. He has won awards for excellence in refereeing from the American Economic Review and Management Science.Ganesh Iyer (“Multilateral Bargaining and Downstream Competition”) is the Edgar F. Kaiser Professor of Business Administration at the Haas School of Business, University of California, Berkeley. He received his Ph.D. from the University of Toronto and was previously on the faculty at the Washington University in St. Louis. His research uses economic theory to study marketing strategy problems; his areas of research are the coordination of product distribution, marketing information, Internet strategy, strategic communication, and bounded rationality in marketing strategy. His research has been published in several journals, including Marketing Science, Management Science, the Journal of Marketing Research, and Quantitative Marketing and Economics. He is currently an associate editor for Marketing Science,Management Science, and Quantitative Marketing and Economics.Jaehwan Kim (“A Direct Utility Model for Asymmetric Complements”) is an associate professor of marketing at the Korea University Business School. He received his Ph.D. from the Ohio State University. His research interests are in modeling consumer demand at the micro level using an economic utility approach and its applications to product line formation, and advertising content decisions.Kilsun Kim (“Can Commonality Relieve Cannibalization in Product Line Design?”) is a professor and associate dean at the College of Business, Sogang University, Seoul, South Korea. He is also the director of the Research Institute for Management of Technology and a cofounder of the Graduate School of Management of Technology at Sogang University. He received a Ph.D. from the University of Illinois at Urbana–Champaign. His research interests are in new product development and the management of technology.George Knox (“Incorporating Direct Marketing Activity into Latent Attrition Models”) is an assistant professor of marketing at the LeBow College of Business, Drexel University. He holds a Ph.D. in marketing from the Wharton School of the University of Pennsylvania and was previously on the faculty of Tilburg University. His current research includes quantifying the impact of customer complaints on lifetime value and exploring unplanned and impulse buying in developed and emerging markets. He has published in the Journal of Marketing, International Journal of Research in Marketing, and Manufacturing and Service Operations Management.Sanghak Lee (“A Direct Utility Model for Asymmetric Complements”) is an assistant professor of marketing at the University of Iowa. He received a B.S. in chemical engineering from Seoul National University, an M.S. in management engineering from KAIST (Korean Advanced Institute of Science and Technology), and a Ph.D. in marketing from the Ohio State University. His research focuses on the development and estimation of empirical models for consumer behavior.Yunchuan Liu (“Can Commonality Relieve Cannibalization in Product Line Design?”) is an associate professor of business administration at the College of Business, University of Illinois at Urbana–Champaign. He received a Ph.D. in marketing from Columbia University. His research interests include distribution channels, retailing, product strategy, and pricing strategy. Many of his papers have been published in Marketing Science and Management Science.John Morgan (“When Do Markets Tip? A Cognitive Hierarchy Approach”) is the Gary and Sherron Kalbach Professor of Entrepreneurship at the Haas School of Business, University of California, Berkeley. His research includes studies of the economics of the Internet, tournaments and contests, information flows within organizations, and auctions. His recent paper, “Tournaments for Ideas,” received the Accenture Award for the best paper published in 2010 in the California Management Review. His research has been published in the top academic journals and has also been featured in the New York Times, Money, and the Economist. He is the Founding Director of Xlab, Berkeley's experimental social sciences lab, and is the Faculty Director of Berkeley's Center for Executive Education.Chakravarthi Narasimhan (“Differentiate or Imitate? The Role of Context-Dependent Preferences”) is the Philip L. Siteman Professor of Marketing at the Olin Business School, Washington University in St. Louis. He received his Ph.D. from the University of Rochester in 1982. He was an assistant and associate professor at the University of Chicago from 1981 to 1988. Several of his papers have won or were finalists for best paper of the year awards. He was chosen Teacher of the Year by the MBA Graduating Class of 1991 and has been recognized by the Graduate Student Senate for excellence in mentoring three times.David A. Schweidel (“Incorporating Direct Marketing Activity into Latent Attrition Models”) is an associate professor of marketing at Emory University's Goizueta Business School. He serves as codirector of the Emory Marketing Analytics Center (EmoryMAC). He holds a Ph.D. in marketing from the Wharton School of the University of Pennsylvania. His current research focuses on the examining the value of social media as a source of marketing intelligence.Andrew T. Stephen (“Intrinsic vs. Image-Related Utility in Social Media: Why Do People Contribute Content to Twitter?”) is an assistant professor of business administration and the Katz Fellow in Marketing at the Joseph M. Katz Graduate School of Business, University of Pittsburgh. He was previously on the faculty at INSEAD, and he received his Ph.D. in marketing from Columbia University. His research interests lie in how consumers contribute to and use social media and online social networks, and how marketers can encourage and benefit from consumers' online and off-line social interactions.Olivier Toubia (“Intrinsic vs. Image-Related Utility in Social Media: Why Do People Contribute Content to Twitter?”) is the Glaubinger Professor of Business at the Columbia Business School. He is a graduate from École Centrale Paris (Paris, France) and holds an M.S. in operations research and a Ph.D in marketing, both from the Massachusetts Institute of Technology. His research interests include new product development, adaptive experimental design, conjoint analysis, preference measurement, idea generation, idea screening, the diffusion of innovation, behavioral economics, and social networks.Özge Turut (“Differentiate or Imitate? The Role of Context-Dependent Preferences”) is an assistant professor of marketing at the School of Management, Sabancı University, Turkey. She received her DBA from the Harvard Business School in 2006. Before joining Sabancı University, she was an assistant professor of marketing at the Olin Business School, Washington University in St. Louis.
The canonical design of customer satisfaction surveys asks for global satisfaction with a product or service and for evaluations of its distinct attributes. Users of these surveys are often interested in the relationship between global satisfaction and attributes; regression analysis is commonly used to measure the conditional associations. Regression analysis is only appropriate when the global satisfaction measure results from the attribute evaluations and is not appropriate when the covariance of the items lie in a low-dimensional subspace, such as in a factor model. Potential reasons for low-dimensional responses are that responses may be haloed from overall satisfaction and there may be an unintended lack of item specificity. In this paper we develop a Bayesian mixture model that facilitates the empirical distinction between regression models and relatively much lower-dimensional factor models. The model uses the dimensionality of the covariance among items in a survey as the primary classification criterion while accounting for the heterogeneous usage of rating scales. We apply the model to four different customer satisfaction surveys that evaluate hospitals, an academic program, smartphones, and theme parks, respectively. We show that correctly assessing the heterogeneous dimensionality of responses is critical for meaningful inferences by comparing our results to those from regression models.
This paper studies the strategic interaction between firms producing strictly complementary products. With strict complements, a consumer derives positive utility only when both products are used together. We show that value-capture and value-creation problems arise when such products are developed and sold by separate firms (“nonintegrated” producers). Although the firms tend to price higher for given quality levels, their provision of quality is so low that, in equilibrium, prices are set well below what an integrated monopolist would choose. When one firm can mandate a royalty fee from the complementor producer (as often occurs in arrangements between hardware and software makers), we find that the value-capture problem is mitigated to some extent and consumer surplus rises. However, because royalty fees greatly reduce the incentives of the firm paying them to invest in quality, the arrangement exacerbates the value-creation problem and leads to even lower total quality. Surprisingly, this result can reverse with competition. Specifically, when the firm charging the royalty fee faces a vertically differentiated competitor, the value-creation problem is greatly reduced—opening the door for the possibility of a Pareto-improving outcome in which all firms and consumers benefit. It is worth noting that this outcome cannot be achieved by giving firms the option of introducing a line of product variants; competition serves as a necessary “commitment” ingredient.
As firms become more customer-centric, concepts such as customer equity come to the fore. Any serious attempt to quantify customer equity requires modeling techniques that can provide accurate multiperiod forecasts of customer behavior. Although a number of researchers have explored the problem of modeling customer churn in contractual settings, there is surprisingly limited research on the modeling of usage while under contract. The present work contributes to the existing literature by developing an integrated model of usage and retention in contractual settings. The proposed method fully leverages the interdependencies between these two behaviors even when they occur on different time scales (or “clocks”), as is typically the case in most contractual/subscription-based business settings.We propose a model in which usage and renewal are modeled simultaneously by assuming that both behaviors reflect a common latent variable that evolves over time. We capture the dynamics in the latent variable using a hidden Markov model with a heterogeneous transition matrix and allow for unobserved heterogeneity in the associated usage process to capture time-invariant differences across customers.The model is validated using data from an organization in which an annual membership is required to gain the right to buy its products and services. We show that the proposed model outperforms a set of benchmark models on several important dimensions. Furthermore, the model provides several insights that can be useful for managers. For example, we show how our model can be used to dynamically segment the customer base and identify the most common “paths to death” (i.e., stages that customers go through before churn).
Nearly a quarter of all products purchased in U.S. supermarkets and drug stores are store brands (SBs). Although the presence of SBs benefits both consumers and retailers, it is a threat to the dominance of the incumbent national brand manufacturers (NBMs). When considering the potential threat of an SB, an NBM generally pursues one of three strategies: accommodate, displace, or buffer. Under the accommodation strategy, the NBM repositions the products in his existing product line. Under the displacement strategy, the NBM elects to supply the SB to preempt the entry of the SB supplier. Under the buffering strategy, the NBM adds a defender product, which competes with his own product offering and the new SB. Using a game-theoretic model, we consider a market where consumers are heterogeneous in their valuation of product quality and analyze an NBM's response to an SB threat. We focus on two important drivers: the NBM's ability to differentiate on the quality dimensions and his cost advantage over the outside supplier of SB. To completely characterize the NBM's response, we consider two regimes. In the first regime, the NBM is a monopolist producer. In the second regime, the retailer has the added option of procuring an SB product from an independent, nonstrategic SB manufacturer. By comparing the results from both regimes, we develop a descriptive theory that clarifies the incentives of the NBM to accommodate, displace, or buffer. In doing this, we determine how the NBM's whole product portfolio should be designed, i.e., the positioning (quality levels) and prices of all its offerings.
I consider a cheap-talk model in which a firm has a chance to communicate its product quality to consumers. The model describes how advertising can be both informative to consumers and profitable for the firm through its content in a vertically differentiated market. I find that advertising content may be effective in inducing search even if incentives for misrepresentation exist. In particular, a firm with an undesirable (low-quality) product is able to attract consumers who would have not incurred a search cost had they known its true quality. In this case, a semiseparating equilibrium occurs where the lowest firm types pool upward in order to increase the expected product quality while simultaneously signaling that the product is affordable. Although consumers always benefit from truth in advertising, total welfare may decrease if an undesirable firm is required to reveal its type. Finally, I show that the extent to which misrepresentation can take place increases with the cost of advertising coverage.
Many durable products with relatively short selling seasons have been using returns policies between manufacturers and retailers as the contractual protocol for some time. Recently, these sectors have witnessed the growing popularity of peer-to-peer Web-based used goods markets as important transaction channels between buyers and sellers. Given that these two issues are critically linked from both supply and demand perspectives, in this paper we study the role that consumer valuation of used products plays in shaping a manufacturer's incentive to offer a returns policy option to a retailer when used goods might be devalued compared to new ones as a result of physical deterioration (or obsolescence). We do so through a two-period dyadic channel framework where the retailer faces uncertain demand for a durable product from a renewable set of customers who are impatient but forward looking. The manufacturer, on the other hand, needs to decide whether or not to offer a returns contract to the retailer. We first characterize the necessary and sufficient condition under which a returns contract is the equilibrium strategy as well as the corresponding channel decisions. Further analysis of this condition reveals that a higher consumer valuation of used products increases the likelihood of a returns contract being the equilibrium strategy. This result seems to be robust except when the potential demands for the two periods are quite deterministic and uncorrelated. However, it contradicts the burgeoning managerial trend to replace returns contracts with price-only ones in sectors where used goods are valued relatively highly by the consumers. We also discuss how used goods markets affect the equilibrium channel decisions as well as how demand uncertainty and logistics costs associated with returns influence the equilibrium contracting strategy.
This paper examines the impact of search engine optimization (SEO) on the competition between advertisers for organic and sponsored search results. The results show that a positive level of search engine optimization may improve the search engine's ranking quality and thus the satisfaction of its visitors. In the absence of sponsored links, the organic ranking is improved by SEO if and only if the quality provided by a website is sufficiently positively correlated with its valuation for consumers. In the presence of sponsored links, the results are accentuated and hold regardless of the correlation. When sponsored links serve as a second chance to acquire clicks from the search engine, low-quality websites have a reduced incentive to invest in SEO, giving an advantage to their high-quality counterparts. As a result of the high expected quality on the organic side, consumers begin their search with an organic click. Although SEO can improve consumer welfare and the payoff of high-quality sites, we find that the search engine's revenues are typically lower when advertisers spend more on SEO and thus less on sponsored links. Modeling the impact of the minimum bid set by the search engine reveals an inverse U-shaped relationship between the minimum bid and search engine profits, suggesting an optimal minimum bid that is decreasing in the level of SEO activity.
This study examines the strategic implications of retailer shelf layout decisions in a market characterized by consumer fit uncertainty. A retailer can display competing products in the same location, allowing consumers to inspect various products all at once or in distant locations, which induces consumers to inspect one product first and then decide whether to incur the travel cost to inspect another product. We consider a model in which two competing manufacturers distribute two horizontally differentiated products through a common retailer. Our analysis shows when the two manufacturers offer products of the same fit probabilities, the retailer obtains a greater profit by displaying competing products in distant locations if the products' fit probabilities are not too high; otherwise, the retailer is better off displaying competing products in the same location. When manufacturers offer products of differentiated fit probabilities, a retailer is more likely to benefit from displaying competing products in distant locations with an increased fit difference between products. Finally, a retailer is more likely to benefit from displaying competing products in distant locations when facing less competition from other retailers.
A widely debated question in recent years by both strategy theorists and antitrust practitioners is what role product differentiation between firms plays in their ability to sustain a collusive agreement in order to reduce the strength of competition and gain higher profits. This paper addresses the following question: What happens to the “product differentiation–collusion sustainability” relationship when setting up and maintaining an agreement is costly? We show that introducing collusion costs into the discussion has relevant implications. Indeed, sufficiently high collusion costs modify the underlying market structure, thus altering the product differentiation–collusion sustainability relationship with respect to the case where collusion costs are absent or low. In particular, if the gains from collusion are increasing (decreasing) with the degree of product differentiation, the relationship between product differentiation and collusion sustainability is always positive (negative), whereas if the gains from collusion are inverted U-shaped, the relationship is inverted U-shaped too. These results stress the importance of considering those markets where the coordination between firms is sufficiently costly as structurally different from those markets where coordination has no costs for firms.
Greg M. Allenby (“The Dimensionality of Customer Satisfaction Survey Responses and Implications for Driver Analysis”) is the Helen C. Kurtz Chair of Marketing at the Max M. Fisher College of Business, the Ohio State University.Eva Ascarza (“A Joint Model of Usage and Churn in Contractual Settings”) is an assistant professor of marketing at the Columbia Business School. She is a marketing modeler who uses tools from statistics and economics to answer marketing questions. Her main research areas are customer analytics and pricing in the context of subscription businesses.Ron Berman (“The Role of Search Engine Optimization in Search Marketing”) is a doctoral candidate at the Haas School of Business, University of California, Berkeley. He holds an M.Sc. in computer science from Tel Aviv University and a B.Sc. in physics, math and computer science from the Hebrew University in Jerusalem. His research focuses on new media, online advertising, start-ups, and online phenomena in general.Eyal Biyalogorsky (“Complementary Goods: Creating, Capturing, and Competing for Value”) is an associate professor of marketing at the Arison School of Business, the Interdisciplinary Center (IDC) Herzliya, Israel. He received his Ph.D. in marketing from the Fuqua School of Business, Duke University, and was on the faculty of the University of California, Davis, before joining the Arison School of Business. He wants to point out that the work with his complementary coauthors on this paper was an exception to the quality issues discussed in the paper.Joachim Büschken (“The Dimensionality of Customer Satisfaction Survey Responses and Implications for Driver Analysis”) is a professor of marketing at the Ingolstadt School of Management, Catholic University of Eichstätt-Ingolstadt, Germany.Stefano Colombo (“Product Differentiation and Collusion Sustainability When Collusion Is Costly”) is an assistant professor of economics at the Università Cattolica del Sacro Cuore, Milan. He has a magna cum laude degree in economics from Bocconi University and holds a Ph.D. in economics (DEFAP) from the Università Cattolica del Sacro Cuore. His research interests are industrial economics, regional economics and spatial methods. He has published in academic journals such as Papers in Regional Science, Games and Economic Behavior, and Annals of Regional Science.Pedro M. Gardete (“Cheap-Talk Advertising and Misrepresentation in Vertically Differentiated Markets”) is an assistant professor of marketing at the Stanford Graduate School of Business. His research focuses on marketing strategies related to advertising and on the role of market information in strategic contexts.Zheyin (Jane) Gu (“Consumer Fit Search, Retailer Shelf Layout, and Channel Interaction”) is an assistant professor of marketing at the School of Business, University at Albany, State University of New York. She received Ph.D. in marketing from the Stern School of Business, New York University. Her research interests include distribution channel, retailing, e-commerce, and competitive strategies. She has published in the Journal of Marketing Research, Management Science, and Marketing Science.Mehmet Gümüş (“Returns Policies Between Channel Partners for Durable Products”) is an assistant professor in the Operations Management Area at the Desautels Faculty of Management, McGill University. He joined McGill in 2007 from the University of California, Berkeley, where he completed his Ph.D. in industrial engineering and operations research and his M.A. in economics. In his research, he explores the impact of customer behavior and information asymmetry on supply chain management, dynamic pricing, and risk management. His research has been published in Management Science, Operations Research, Manufacturing & Service Operations Management, Marketing Science, and Production and Operations Management.Bruce G. S. Hardie (“A Joint Model of Usage and Churn in Contractual Settings”) is a professor of marketing at the London Business School. His primary research interest lies in the development of data-based models to support marketing analysts and decision makers, with a particular interest in models that are easy to implement. Most of his current projects focus on the development of probability models for customer-base analysis.Zsolt Katona (“The Role of Search Engine Optimization in Search Marketing”) is an assistant professor of marketing at the Haas School of Business, University of California, Berkeley. He has a Ph.D. in management from INSEAD; he previously earned a Ph.D. in computer science from Eotvos University, Budapest. His current research focuses on understanding the interaction between websites' online advertising strategies, and he also studies the role that link structure of social networks plays in word-of-mouth effects and community formation. His research on online marketing has been published in Marketing Science, Management Science, and the Journal of Marketing Research. Previously, he had analyzed characteristics of different random networks and published his work in journals such as the Journal of Applied Probability, Statistics and Probability Letters, and Random Structures and Algorithms. Oded Koenigsberg (“Complementary Goods: Creating, Capturing, and Competing for Value”) is an associate professor of marketing at the London Business School. His research interest is the marketing–manufacturing interface—in particular, in incorporating operational constraints into firms' marketing decisions (e.g., pricing, channel, product design and product line). His research has appeared in Quantitative Marketing and Economics, Management Science, Production and Operations Management, the Journal of Marketing Research, and Marketing Science. He is an associate editor at the International Journal of Research in Marketing and serves on the editorial boards of Marketing Science and Production and Operations Management.Yunchuan Liu (“Consumer Fit Search, Retailer Shelf Layout, and Channel Interaction”) is an associate professor of business administration at the College of Business, University of Illinois at Urbana–Champaign. He received Ph.D. in marketing from Columbia University. His research interests include distribution channels, retailing, product strategy, and pricing strategy. Many of his papers have been published in Marketing Science and Management Science.Chakravarthi Narasimhan (“National Brand's Response to Store Brands: Throw In the Towel or Fight Back?”) is the Philip L. Siteman Professor of Marketing at the Olin Business School, Washington University in St. Louis. His current research interests are in strategic value of information, incorporating non-microeconomic foundations in strategic models, understanding the impact of promotions on brands, examining the interaction of multiple marketing strategies, and supply chain contracts, especially supply chain strategies under uncertainty. He has published in Marketing Science, Management Science, the Journal of Marketing Research, the Journal of Marketing, the Journal of Business, the Journal of Econometrics, and Harvard Business Review, among others. He is an area editor of Marketing Science and is an associate editor of Quantitative Marketing and Economics.Sherif Nasser (“National Brand's Response to Store Brands: Throw In the Towel or Fight Back?”) is an assistant professor of marketing at the Olin Business School, Washington University in St. Louis. He holds a Ph.D. in marketing from New York University's Stern School of Business. His research interests are in product differentiation, media and advertising, distribution channels, and the interface of marketing and operations management.Elie Ofek (“Complementary Goods: Creating, Capturing, and Competing for Value”) is the T.J. Dermot Dunphy Professor of Business Administration at the Harvard Business School. He received his Ph.D. in business and M.A. in economics from Stanford University. His research focuses on the relationship between marketing and innovation strategy and on how firms can leverage novel technologies or major trends to deliver value to customers. His research has appeared in Marketing Science, Management Science, the Journal of Marketing Research, the Journal of Consumer Research, and the Journal of Economics and Management Strategy. He is an associate editor at Management Science and serves on the editorial boards of Marketing Science, the Journal of Marketing Research, and the International Journal of Research in Marketing.Thomas Otter (“The Dimensionality of Customer Satisfaction Survey Responses and Implications for Driver Analysis”) is a professor of marketing at the Faculty of Business and Economics, Johann Wolfgang Goethe University, Frankfurt am Main, Germany. Saibal Ray (“Returns Policies Between Channel Partners for Durable Products”) is an associate professor in the Operations Management Area at the Desautels Faculty of Management, McGill University. His research interest can broadly be categorized as value chain management; he is specifically interested in studying value chain risk management, contracting/competition issues in value chains, time-based competition, issues related to used goods markets, capacity and inventory management, and dynamic pricing. His research has been published in such reputed journals as Management Science, Operations Research, Manufacturing & Service Operations Management, Marketing Science, and Production and Operations Management. In addition, he has held (or presently holds) a number of grants from the governments of Canada and Quebec. He has been awarded the Desautels Faculty Scholar award for his research achievements. Danko Turcic (“National Brand's Response to Store Brands: Throw In the Towel or Fight Back?”) is an assistant professor of operations and manufacturing management at the Olin Business School, Washington University in St. Louis. He holds a Ph.D. in operations research from Case Western Reserve University. His research is on the interface of operations management with marketing and on supply chain contracting. Taylan Yalcin (“Complementary Goods: Creating, Capturing, and Competing for Value”) is an assistant professor at the George L. Argyros School of Business and Economics and a research associate in the Economic Science Institute at Chapman University. He received his DBA degree from Harvard University and holds an MBA and a B.Sc. in engineering from Bilkent University. His research focuses on firm's marketing strategies when quality differentiation is involved. He is especially interested in the effect of quality improvement decisions on competitive strategy. Shuya Yin (“Returns Policies Between Channel Partners for Durable Products”) is an associate professor in the Operations and Decision Technologies program at the Paul Merage School of Business, University of California, Irvine. She joined the school in 2005. Her recent research addresses various problems in decentralized retail supply chains by using noncooperative and cooperative game theory. She is interested in issues including product return policies between channel members, secondary markets for durable products, alliances of players in selling and buying, timing of operational decisions, and management of information flow on uncertain demand. Her research has been published in academic journals such as Management Science, Manufacturing & Service Operations Management, Marketing Science, and Operations Research.
I measure the spillover effect of intercollegiate athletics on the quantity and quality of applicants to institutions of higher education in the United States—an effect popularly known as the “Flutie effect.” I treat athletic success as a stock of goodwill that decays over time, similar to that of advertising. A major challenge is that privacy laws prevent us from observing information about the applicant pool. I overcome this challenge by using order statistic distribution to infer applicant quality from information on enrolled students. Using a flexible random-coefficients aggregate discrete choice model that accommodates heterogeneity in preferences for school quality and athletic success, as well as an extensive set of school fixed effects to control for unobserved quality in athletics and academics, I estimate the impact of athletic success on applicant quality and quantity. Overall, athletic success has a significant, long-term goodwill effect on future applications and quality. However, students with lower-than-average SAT scores tend to have a stronger preference for athletic success, whereas students with higher SAT scores have a greater preference for academic quality. Furthermore, the decay rate of athletics' goodwill is significant only for students with lower SAT scores, suggesting that the goodwill created by intercollegiate athletics resides more extensively with lower-scoring students than with their higher-scoring counterparts. But, surprisingly, athletic success impacts applications even among academically stronger students.
Retailers face the problem of finding the assortment that maximizes category profit. This is a challenging task because the number of potential assortments is very large when there are many stock-keeping units (SKUs) to choose from. Moreover, SKU sales can be cannibalized by other SKUs in the assortment, and the more similar SKUs are, the more this happens. This paper develops an implementable and scalable assortment optimization method that allows for theory-based substitution patterns and optimizes real-life, large-scale assortments at the store level. We achieve this by adopting an attribute-based approach to capture preferences, substitution patterns, and cross-marketing mix effects. To solve the optimization problem, we propose new very large neighborhood search heuristics. We apply our methodology to store-level scanner data on liquid laundry detergent. The optimal assortments are expected to enhance retailer profit considerably (37.3%), and this profit increases even more (to 43.7%) when SKU prices are optimized simultaneously.
The information processing literature provides a wealth of laboratory evidence on the effects that the choice task and individual characteristics have on the extent to which consumers engage in alternative-based versus attribute-based information processing. Less attention has been paid to studying how the processing pattern at the point of purchase is associated with a consumer's propensity to buy in shopping settings. To understand this relationship, we formulate a discrete choice model and perform formal model comparisons to distinguish among several possible dependence structures. We consider models involving an existing measure of information processing, PATTERN; a latent variable version of this measure; and several new refinements and generalizations. Analysis of a unique data set of 895 shoppers on a popular electronics website supports the latent variable specification and provides validation for several hypotheses and modeling components. We find a positive relationship between alternative-based processing and purchase, as well as a tendency of shoppers in the lower price category to engage in alternative-based processing. The results also support the case for joint modeling and estimation. These findings can be useful for future work in information processing and suggest that likely buyers can be identified while engaged in information processing prior to purchase commitment, an important first step in targeting decisions.
When we think of colas, Coca-Cola first comes to mind. Products such as Cola-Cola, Tide laundry detergent, and Chapstick lip balm are the prototypical products in their respective categories. For more than three decades, research in consumer psychology has accumulated evidence on how prototypicality influences memory, shapes the composition of consideration set, and affects purchase decision. Yet there is no research on how it changes the competitive behavior of firms in a horizontally differentiated market. For example, some prototypical products are priced lower than other products in their category, whereas in certain other categories the prototypical product is priced higher. We propose a novel model of spatial competition, where the prototypicality of a product influences the probability of the product being included in consumers' consideration sets without affecting its valuation. Using the model, we examine theoretically the impact of prototypicality on the pricing decisions of competing firms. Our analysis shows that when consumer valuations are low, the prototypical product is priced lower than a nonprototypical product and earns more profits. However, when consumer valuations are high, the rank order of the prices of the prototypical product and a nonprototypical product is reversed, but not the order of profits. We subject these predictions to an empirical test. The experimental results lend support for the qualitative predictions of the model.
Online advertising campaigns often consist of multiple ads, each with different creative content. We consider how various creatives in a campaign differentially affect behavior given the targeted individual's ad impression history, as characterized by the timing and mix of previously seen ad creatives. Specifically, we examine the impact that each ad impression has on visiting and conversion behavior at the advertised brand's website. We accommodate both observed and unobserved individual heterogeneity and take into account correlations among the rates of ad impressions, website visits, and conversions. We also allow for the accumulation and decay of advertising effects, as well as ad wearout and restoration effects. Our results highlight the importance of accommodating both the existence of multiple ad creatives in an ad campaign and the impact of an individual's ad impression history. Simulation results suggest that online advertisers can increase the number of website visits and conversions by varying the creative content shown to an individual according to that person's history of previous ad impressions. For our data, we show a 12.7% increase in the expected number of visits and a 13.8% increase in the expected number of conversions.
Should a firm favor a weaker or stronger employee in a contest? Despite a widespread emphasis on rewarding the best employees, managers continue to tolerate and even favor poor performers. Contest theory reveals that evenly matched contests are the most intense, which implies that a contest designer can maximize each player's effort by artificially boosting the underdog's chances. We apply this type of “handicapping” to a two-period repeated contest between employees, in which the only information available about their abilities is their performance in the first period. In this setting, employees are strategic and forward looking, such that they fully anticipate the potential impact of the first-period contest result on the second-period contest and thus adjust their behaviors accordingly. The manager also incorporates these strategic behaviors of employees when determining an optimal handicapping policy. If employees' abilities are sufficiently different, favoring the first-period loser in the second period increases the total effort over both periods. However, if abilities are sufficiently similar, we find the opposite result occurs: total effort increases the most in response to a handicapping strategy of favoring the first-period winner.
How do firms develop marketing strategy when consumers seek to satisfy both quality and status-related considerations? We develop an analytical model to study this issue, examining both pricing and product management decisions in markets for conspicuous durable goods. Our analysis yields many interesting and nontrivial insights. First, we demonstrate that high intrinsic quality indirectly generates exclusivity via pricing effects; in turn, this exclusivity generates considerable social payoffs where consumers value status. This insight reverses the direction of causality in the existing literature, wherein only status considerations matter and mere price increases may enhance consumer utility. Second, our dynamic model indicates that where consumers prioritize status benefits, producers incur substantial price depreciation in equilibrium. Third, we examine the product management strategies used by firms to preserve early adopter exclusivity. Finally, we discuss the boundary conditions of our results as well as our results' implications for managerial and policy issues.
Co-creation, the participation of customers in the design and production of goods and services, has been gaining popularity in recent years. In this research we incorporate firm pricing into the joint production process allowing us to study (1) production externalities between firm and customers, (2) production externalities among customers, and (3) optimal pricing by firms.We show that given a choice, a monopoly firm will opt for co-creation with customers rather than deal with passive price-taking consumers. Furthermore, the firm will increase the effort it devotes to co-creation as the number of potential co-creating customers increases.We show that the profit of a firm facing a centralized pattern of externalities among customers (with an expert, or lead user, in the center) can be higher than its profit when facing a decentralized pattern of externalities among customers and clearly dominates its profit when customers do not have any cross externalities. Thus, we provide a different justification for the use of lead users, one that depends on their network centrality and not on having lower cost, more information, or greater ability than the firm. Because the decentralized pattern has more links than the centralized pattern, our results demonstrate the importance of the pattern of links between customers, and not just their number, in determining the profitability of co-creation.Furthermore, we find that the lead user's externality spillover to other connected users, her neighbors, acts as a force multiplier on the efforts exerted by all participants in equilibrium. Specifically, a higher spillover from the lead user increases the efforts of the firm, the neighbors, and the lead user herself, and this may lead to beneficial outcomes for all.Finally, we show that in co-creation environments, a monopolist firm may benefit by committing to a single price rather than exercising price discrimination. This is because the pricing structure affects customers' incentive to invest effort in the innovation-production stage.
The recent article by Bandyopadhyay and Paul [Bandyopadhyay S, Paul AA (2010) Equilibrium returns policies in the presence of supplier competition. Marketing Sci. 29(5):846–857] searched for an explanation for the phenomenon the authors termed the “Pasternack paradox,” i.e., why full-credit return policies, which were considered suboptimal from the perspective of channel coordination, are prevalent in practice. The authors argued that the underlying reason is that it is the competition between suppliers rather than the coordination among channel members that dominates business practice. We show that their model actually fails to generate the claimed results. Counterexamples are given. Alternative explanations are therefore needed for the seemingly suboptimal business practice.
Wilfred Amaldoss (“Pricing Prototypical Products”) is a professor of marketing at the Fuqua School of Business of Duke University. He holds an MBA from the Indian Institute of Management (Ahmedabad), and an M.A. (applied economics) and a Ph.D. from the Wharton School of the University of Pennsylvania. His research interests include behavioral game theory, experimental economics, advertising, pricing, new product development, and social effects in consumption. His recent publications have appeared in Marketing Science, Management Science,the Journal of Marketing Research,the Journal of Economic Behavior and Organization, and the Journal of Mathematical Psychology. His work has received the John D. C. Little and the Frank Bass awards.Tammo H. A. Bijmolt (“Optimizing Retail Assortments”) is a professor of marketing research at the Department of Marketing and director of the research school SOM, Faculty of Economics and Business Administration, University of Groningen, the Netherlands. His research interests include conceptual and methodological issues such as retailing, loyalty programs, pricing, and meta-analysis. His work has appeared in prestigious journals, such as the Journal of Marketing Research, Journal of Marketing, Journal of Consumer Research, International Journal of Research in Marketing (IJRM), Psychometrika, and the Journal of the Royal Statistical Society Series A. He serves as an associate editor for IJRM.Michael Braun (“Online Display Advertising: Modeling the Effects of Multiple Creatives and Individual Impression Histories”) is an associate professor of marketing at the Cox School of Business at Southern Methodist University. He earned his Ph.D. from the Wharton School of the University of Pennsylvania, and he holds an A.B. with honors in economics from Princeton University and an MBA from the Fuqua School of Business at Duke University. He is a noted expert on the statistical analysis of large and complex customer databases; he has written on, spoken on, and taught about management topics such as sales forecasting, customer retention and valuation, marketing return on investment, social networking models, segmentation and targeting strategies, online advertising, and insurance decisions. From 2006 to 2013, he served on the marketing faculty of the MIT Sloan School of Management.Doug J. Chung (“The Dynamic Advertising Effect of Collegiate Athletics”) is an assistant professor of business administration in the Marketing Unit at Harvard Business School. He received his Ph.D. in management from Yale University and is also the recipient of the ISMS Doctoral Dissertation Award, ISBM Doctoral Support Award, and the Mary Kay Doctoral Dissertation Award. His research primarily focuses on sales force management and incentive compensation. Prior to pursuing a career in academics, he served as an officer and platoon commander in the South Korean Special Forces before continuing on to a variety of industry positions with several multinational companies.Imran S. Currim (“Information Processing Pattern and Propensity to Buy: An Investigation of Online Point-of-Purchase Behavior”) is the Chancellor's Professor and Associate Dean at the Paul Merage School of Business at University of California, Irvine. He received his B.Eng. from Victoria Jubilee Technical Institute of the University of Bombay, an MBA from the University of Wisconsin, and M.S. (statistics) and Ph.D. (business) degrees from Stanford. He received two American Marketing Association awards: the William O'Dell Award for a paper published in the Journal of Marketing Research and the Houghton Mifflin Distinguished Teaching in Marketing Award. He has published 40 articles on consumer choice models in the leading field journals, served as an associate editor for Marketing Science and Management Science, and served on the editorial boards of the Journal of Marketing Research and the International Journal of Research in Marketing. For the past five years he has served as associate dean of master's, executive, and undergraduate programs; the Wall Street Journal cited him as a Favorite Professor in an Executive MBA Program, and Business Week ranked his executive MBA marketing class third in the world.Chuan He (“Pricing Prototypical Products”) is an associate professor of marketing at the Leeds School of Business, University of Colorado at Boulder; he is also visiting associate professor of marketing at the Cheung Kong Graduate School of Business in Beijing, China. He holds a Ph.D. in marketing from Washington University in St. Louis and an M.A. in economics from the University of Toronto. His fields of specialization and research include advertising, search, pricing strategies, and channel contracts. His recent studies have appeared in Marketing Science, the Journal of Marketing Research, and the Economic Journal. He serves on the editorial board of Marketing Science.Zhongsheng Hua (“Commentary—On ‘Equilibrium Returns Policies in the Presence of Supplier Competition’ ”) is a professor at and vice dean of the School of Management at University of Science and Technology of China. He has published extensively on supply chain management, channel management, and production research.Ivan Jeliazkov (“Information Processing Pattern and Propensity to Buy: An Investigation of Online Point-of-Purchase Behavior”) is an associate professor of economics and statistics at the University of California, Irvine. His research is in the area of Bayesian econometrics, with an emphasis on modeling, Markov chain Monte Carlo estimation, model comparison, and discrete data analysis.Yongquan Lan (“Commentary—On ‘Equilibrium Returns Policies in the Presence of Supplier Competition’ ”) is a Ph.D. candidate affiliated with the joint Ph.D. program between the City University of Hong Kong and the University of Science and Technology of China. His research interest is on operations management and operations/marketing interface research.Yanzhi Li (“Commentary—On ‘Equilibrium Returns Policies in the Presence of Supplier Competition’ ”) is an associate professor at the City University of Hong Kong. He received his Ph.D. and B.S. from the Hong Kong University of Science and Technology and Tsinghua University, respectively. His research interest is primarily on operations and supply chain management, and he has been recently focusing on interface research between operations and marketing and operations and finance.Ofer Mintz (“Information Processing Pattern and Propensity to Buy: An Investigation of Online Point-of-Purchase Behavior”) is an assistant professor of marketing at the E. J. Ourso College of Business at Louisiana State University (LSU). He completed his Ph.D. in marketing at the University of California, Irvine; M.Sc. in finance at the University of London; and BBA in marketing at Texas A&M University. Before coming to LSU, he was a visiting faculty member at the Interdisciplinary Center (IDC), Herzliya, Israel, for the spring 2012 semester. His research on marketing strategy/analytics and social media/online marketing has appeared (or is forthcoming) in Marketing Science and the Journal of Marketing. In addition, his teaching in social media/online marketing and international marketing has received high peer evaluation grades, and his courses have also been highlighted by the media.Wendy W. Moe (“Online Display Advertising: Modeling the Effects of Multiple Creatives and Individual Impression Histories”) is an associate professor of marketing and director of the M.S. in Marketing Analytics program at the Robert H. Smith School of Business, University of Maryland. She holds a Ph.D., M.A., and B.S. from the Wharton School at the University of Pennsylvania, as well as an MBA from Georgetown University. She is a recognized expert in online marketing and social media and has been on the faculty at the University of Maryland since 2004. Prior to that, she was on the faculty at the University of Texas at Austin. In addition to her academic work, she has consulted for numerous corporations and government agencies, helping them develop and implement state-of-the-art statistical models in the context of Web analytics, social media intelligence, and forecasting.Amit Pazgal (“Co-Creation with Production Externalities”) is a professor of marketing at the Jones Graduate School of Business, Rice University. He received his Ph.D. from the Kellogg School of Management, Northwestern University. His current research focuses on the analysis and characterization of optimal price-setting procedures employed by firms in various strategic environments. His research has appeared in the leading marketing, management, operations, and economics journals.Raghunath Singh Rao (“Conspicuous Consumption and Dynamic Pricing”) is an assistant professor of marketing at the McCombs School of Business at the University of Texas at Austin. He received a master's in applied economics and a Ph.D. in business administration from the University of Minnesota. His research interests include information asymmetry and bounded rationality issues in marketing in relation to substantive topics such as durable goods markets, pricing, sales management, and innovation. His research has been published in the Journal of Marketing Research, Marketing Science, and the Journal of Marketing. He was honored as a Marketing Science Institute (MSI) Young Scholar in 2011.Robert Ridlon (“Favoring the Winner or Loser in Repeated Contests”) is an assistant professor of business economics and public policy in the Kelley School of Business at Indiana University. He received his doctoral degree in business economics from Indiana University, where he also holds a master's of science and bachelor's in business. His research focuses on analytical modeling of contests in competitive environments such as workforce incentives, advertising, and procurement. He particularly examines how the structure of these contests alters strategies when players are relatively different in abilities.Robert P. Rooderkerk (“Optimizing Retail Assortments”) is an associate professor of marketing at TiasNimbas Business School; researcher at the Tilburg School of Economics and Management, Tilburg, the Netherlands; and visiting assistant professor of business administration at the Tuck School of Business at Dartmouth. He received his Ph.D. from Tilburg University. His research develops econometric models and optimization heuristics to improve product strategies and extends choice models to account for behavioral phenomena such as context dependence. His work has appeared in Marketing Science, the Journal of Marketing Research (JMR), and Marketing Letters. He is a recipient of the Dutch Marketing Science award and a finalist for JMR's Paul Green Award.Richard Schaefer (“Conspicuous Consumption and Dynamic Pricing”) is a marketing Ph.D. student at the McCombs School of Business at the University of Texas at Austin. Using analytical methods, his research examines the strategic implications of consumers' social interactions.Jiwoong Shin (“Favoring the Winner or Loser in Repeated Contests”) is an associate professor of marketing in the Yale School of Management at Yale University. He received his doctoral degree in marketing from the Massachusetts Institute of Technology, and he holds an M.S. and a bachelor's degree in business from Seoul National University. His research focuses on analytical modeling of strategic interactions between firms and consumers—in particular, consumer search theory, advertising, pricing strategies, and customer relationship management. His current work in communication strategy investigates the role of vague messages and offers novel explanations for why and how those vague messages can convey price and quality information to consumers, as well as the relative roles of consumer search and firm advertising in signaling product quality. His work in customer management strategy addresses a long-standing puzzle in practice: Should a firm offer a lower price to its own customers or to competitors' customers? When is it profitable to reward one's own customers?Niladri B. Syam (“Co-Creation with Production Externalities”) is a professor of marketing at the University of Houston. His current research focuses on product customization, product co-creation, and analytical and empirical modeling of issues in salesforce management.Harald J. van Heerde (“Optimizing Retail Assortments”) is a research professor of marketing at Massey University, Auckland, New Zealand. He received his Ph.D. from the University of Groningen, the Netherlands. His work develops econometric models to improve marketing decision making, and it has appeared in the leading journals such as the Journal of Marketing, Journal of Marketing Research (JMR), and Marketing Science. He is the recipient of the William O'Dell Award and JMR's Paul Green Award. He serves as an associate editor for Marketing Science and the International Journal of Research in Marketing and as an editorial board member for the Journal of Marketing and JMR.
Several key questions in bundling have not been empirically examined in marketing: Is mixed bundling more effective than pure bundling or pure components? Does correlation in consumer valuations make bundling more or less effective? Does bundling serve as a complement or substitute to network effects? To address these questions, we develop a consumer-choice model from microfoundations to capture the essentials of our setting, the handheld video game market. We provide a framework to understand the dynamic, long-term effects of bundling on demand. The primary explanation for the profitability of bundling relies on homogenization of consumer valuations for the bundle, allowing the firm to extract more surplus. We find that bundling can be effective through a novel and previously unexamined mechanism of dynamic consumer segmentation, which operates independent of the homogenization effect, and can in fact be stronger when the homogenization effect is weaker. We also find that bundles are treated as separate products (distinct from component products) by consumers. Sales of both hardware and software components decrease in the absence of bundling, and consumers who had previously purchased bundles might delay purchases, resulting in lower revenues. We also find that mixed bundling dominates pure bundling and pure components in terms of both hardware and software revenues. Investigating the link between bundling and indirect network effects, we find that they act as substitute strategies, with a lower relative effectiveness for bundling when network effects are stronger.
Online freelance marketplaces are websites that match buyers of electronically deliverable services with freelancers. Although freelancing has grown in recent years, it faces the classic “information asymmetry” problem—buyers face uncertainty over seller quality. Typically, these markets use reputation systems to alleviate this issue, but the effectiveness of these systems is open to debate. We present a dynamic structural framework to estimate the returns to seller reputations in freelance sites. In our model, a buyer decides in each period whether to choose a bid from her current set of bids, cancel the auction, or wait for more bids. In the process, she trades off sellers' price, reputation, and other attributes, as well as the costs of waiting and canceling. Our framework addresses dynamic selection, which can lead to underestimation of reputation, through two types of persistent unobserved heterogeneities: bid arrival rates and buyers' unobserved preference for bids. We apply our framework to data from a leading freelance firm. We find that buyers are forward looking, that they place significant weight on seller reputation, and that not controlling for dynamics and selection can bias reputation estimates. Using counterfactual simulations, we infer the dollar value of seller reputations and provide guidelines to managers of freelance firms.
When a television advertisement causes viewers to switch channels, it reduces the audience available to subsequent advertisers. This audience loss is not reflected in the advertisement price, resulting in an audience externality. The present article analyzes the television network's problem of how to select, order, and price advertisements in a break of endogenous length in order to correct audience externalities. It proposes the Audience Value Maximization Algorithm (AVMA), which considers many possible advertisement orderings within a dynamic programming framework with a strategy-proof pricing mechanism. Two data sets are used to estimate heterogeneity in viewer-switching probabilities and advertiser willingness-to-pay parameters in order to evaluate the algorithm's performance. A series of simulations shows that AVMA typically maximizes audience value to advertisers, increases network revenue relative to several alternatives, and runs quickly enough to implement.
Learning models extend the traditional discrete choice framework by postulating that consumers have incomplete information about product attributes and that they learn about these attributes over time. In this survey we describe the literature on learning models that has developed over the past 20 years, using the model of Erdem and Keane as a unifying framework [Erdem T, Keane M (1996) Decision-making under uncertainty: Capturing dynamic brand choice processes in turbulent consumer goods markets. Marketing Sci. 15(1):1–20]. We describe how subsequent work has extended their modeling framework and applied learning models to a wide range of different products and markets. We argue that learning models have contributed greatly to our understanding of consumer behavior—in particular, in enhancing our understanding of brand loyalty and long-run advertising effects. We also discuss the limitations of existing learning models and potential extensions. One key challenge is to disentangle learning as a source of dynamics from other key mechanisms that may generate choice dynamics (inventories, habit persistence, etc.). Another is to enhance identification of learning models by collecting and using direct measures of signals, perceptions, and expectations.
We examine whether cobranding—the practice of using two established brand names on the same product—increases the market value of parent firms. Using data from the consumer packaged goods industry, we document that the average stock market reaction to the announcement of cobranded new products is approximately +1.0%. We hypothesize that this reaction is significantly higher than it would have been if these same products were single branded, and we find evidence consistent with this hypothesis. We also examine the determinants of this stock market reaction. We find that the consistency between the two brand images, the innovativeness of the product, and the exclusivity of the cobranding relationship significantly increase the market reaction to cobranding announcements. Our findings provide important managerial guidelines for enhancing firm value through cobranding partnerships.
Social learning can occur when information is transferred from existing customers to potential customers. It is especially important when the information that is conveyed pertains to experience attributes, i.e., attributes of products that cannot be fully verified prior to the first purchase. Experience attributes are prevalent and salient when consumers shop through catalogs, on home shopping networks, and over the Internet. Firms therefore employ creative and sometimes costly methods to help consumers resolve uncertainty; we argue that uncertainty can be partially resolved through social learning processes that occur naturally and emanate from local neighborhood characteristics. Using data from Bonobos, a leading U.S. online fashion retailer, we find not only that local social learning facilitates customer trial but also that the effect is economically important because about half of all trials were partially attributable to it. Merging data from the Social Capital Community Benchmark Survey, we find that neighborhood social capital, i.e., the propensity for neighbors to trust each other and communicate with each other, enhances the social learning process and makes it more efficient. Social capital does not operate on trials directly; rather, it improves the learning process and therefore indirectly drives sales when what is communicated is favorable.
This paper investigates how individuals' product choices are influenced by the product choices of their connected others and how the influence mechanism may differ for fashion- versus technology-related products. We conduct a novel field experiment to induce and observe choice interdependence in a closed social network. In our experiment, we conceptualize individuals' choices to be driven by multiattribute utilities, and we measure their initial attribute preferences prior to observing their choice interdependence and collecting network information. These design elements help alleviate concerns in identifying social interaction effects from other confounds. Given that we have complete information on choices and their sequence, we use a discrete-time Markov chain model. Nonetheless, we also use a Markov random field (MRF) model as an alternative when the information on choice sequence is missing. We find significant social interaction effects. Our findings show that whereas experts exert asymmetrically greater influence on a technology-related product, popular individuals exert greater influence on a fashion-related product. In addition, we find choices made by early decision makers to be more influential than choices made later for the technology-related product. Finally, using the MRF with snapshot data can also provide good out-of-sample predictions for a technology-related product.
This paper examines the phenomenon of profit-increasing consumer exit and the related phenomenon of profit-decreasing consumer entry. We demonstrate that firms can be better off in shrinking markets and worse off in growing markets, even in the absence of competitive entry or exit. Specifically, firms may benefit if a segment of consumers who are relatively indifferent about consuming any product in the category leave the market. Profits can increase for all firms even if the exiting consumers have strong preferences for only one of the products in the market. In shrinking markets, it is reasonable to assume that the people who are likely to exit the market first are people who are “least committed” to the category. In particular, people who are the least satisfied with the existing offers are the most likely to change their behavior by finding an alternative or adopting a new technology. Similarly, in growing markets, consumers who enter the market late are generally the least committed to the category. Such exiting can relax the competitive pressure between firms and lead to increased profitability. Our findings provide an explanation for profit growth that has been observed in product industries exhibiting slow and predictable declines over time, including vacuum tubes, cigarettes, and soft drinks.
Anocha Aribarg (“Modeling Choice Interdependence in a Social Network”) is an associate professor at the Ross School of Business, University of Michigan. She received a B.S. in statistics from Chulalongkorn University, Thailand and an MBA from the University of Wisconsin–Milwaukee; she received a Ph.D. degree in marketing from the University of Wisconsin. Her research focuses on developing econometric and statistical models; she integrates statistical modeling, Bayesian inference, conjoint experiments, and consumer behavior theories to study challenging marketing problems that involve understanding consumers' complex decision making and improving managerial decisions.Yves F. Atchadé (“Modeling Choice Interdependence in a Social Network”) is an associate professor in the Department of Statistics at the University of Michigan, Ann Arbor. He received his Ph.D. from the Université de Montréal in 2003. His research focuses on computational methods for high-dimensional and intractable statistical models, with applications in social sciences and environmental sciences. He did his undergraduate studies at the Université d'Abomey-Calavi in Benin and at the Ecole Nationale Supérieure de Statistique et d' Economie Appliquée, Abidjan, Cote d'Ivoire.David R. Bell (“Neighborhood Social Capital and Social Learning for Experience Attributes of Products”) is the Xinmei Zhang and Yongge Dai Professor at the Wharton School, University of Pennsylvania. He received a Ph.D. from the Graduate School of Business at Stanford University. His research focuses on consumer behavior on the Internet, and recent articles explain how and why geography matters for Internet retailers. He owns multiple Bonobos.com products and does most of his shopping online.Zixia Cao (“Wedded Bliss or Tainted Love? Stock Market Reactions to the Introduction of Cobranded Products”) is an assistant professor of marketing at the College of Business, West Texas A&M University. She joined the faculty of the College of Business in 2012 after receiving her Ph.D. at Texas A&M University. Her research interests include innovation, advertising, and the marketing–finance interface.Andrew T. Ching (“Invited Paper—Learning Models: An Assessment of Progress, Challenges, and New Developments”) is an associate professor of marketing at the Rotman School of Management, University of Toronto. His research focuses on developing new empirical models and estimation methods to understand the forward-looking behavior of consumers and firms. He has applied these techniques to study new technology adoption decisions by consumers and the demand for new and used digital products. He has also applied these techniques to study how marketing mix and word-of-mouth (WOM) affect consumer choice in prescription drug markets, how firms decide their marketing mix to take advantage of WOM, and how firms interact with each other in a dynamic game environment. He received the Young Economist Award from the European Economic Association in 2003, an Honorable Mention for the Dick Wittink Prize Award in 2011, the Excellence in Teaching Award in 2012, and a major grant from the Social Sciences and Humanities Research Council.Timothy Derdenger (“The Dynamic Effects of Bundling as a Product Strategy”) holds a Ph.D. in economics from the University of Southern California and a B.B.A. from the George Washington University. His research interests are divided into two areas: the study of technology and sports markets. Broadly, his research focuses on the marketing and strategy associated with a given product/product line both empirically and theoretically. He has publications in both Marketing Science and Management Science.Tülin Erdem (“Invited Paper—Learning Models: An Assessment of Progress, Challenges, and New Developments”) is the Leonard N. Stern Professor of Business Administration and Professor of Marketing at the Stern School of Business, New York University. Her research interests include advertising, brand management and equity, consumer choice, decision making under uncertainty, econometric modeling, and marketing mix effectiveness. She has received best paper awards, as well major research grants, including two major National Science Foundation grants. She has been an area editor at Marketing Science, associate editor at Quantitative Marketing and Economics and the Journal of Consumer Research, and editor-in-chief of the Journal of Marketing Research (July 2009–June 2012). She also served as the president of INFORMS Society for Marketing Science.Michael P. Keane (“Invited Paper—Learning Models: An Assessment of Progress, Challenges, and New Developments”) is the Nuffield Professor of Economics at the University of Oxford. He has made significant contributions in several areas including simulation methods, discrete choice modeling, human capital investment, labor supply, health economics, and incentive effects of taxes. Honors include being named a fellow of the Econometric Society in 2005, election to the Council of the Econometric Society in 2009, the John D. C. Little Award for best paper in Marketing Science in 1996, and the Arrow Award for best paper in health economics in 2008. He was named an Australian Federation Fellow in 2005 and Laureate Fellow in 2011, and he has received 10 major grants from the National Science Foundation, National Institutes of Health, and Australian Research Council. He has been an associate editor at Econometrica (2002–2008) and the Journal of Econometrics (2009–present).David Kempe (“Correcting Audience Externalities in Television Advertising”) has been on the faculty in the Department of Computer Science at the University of Southern California (USC) since the fall of 2004, where he is currently an associate professor. He received his Ph.D. from Cornell University in 2003. His primary research interests are in computer science theory and the design and analysis of algorithms, with a particular emphasis on social networks, algorithms for feature selection, and game-theoretic and pricing questions. He is a recipient of the National Science Foundation CAREER award, the Viterbi School of Engineering (VSoE) Junior Research Award, the Office of Naval Research (ONR) Young Investigator Award, a Sloan Fellowship, and an Okawa Fellowship, in addition to several USC mentoring awards.Vineet Kumar (“The Dynamic Effects of Bundling as a Product Strategy”) is an assistant professor of business administration in the Marketing Unit at Harvard Business School. He received his undergraduate degree from the Indian Institute of Technology, and completed his doctoral study in the Tepper School at Carnegie Mellon University. His research focus is on understanding the drivers of value in technology products and services, with a specific interest in complementary products, and strategies that firms should adopt in designing such products.Jae Young Lee (“Neighborhood Social Capital and Social Learning for Experience Attributes of Products”) is a Ph.D. candidate in marketing at the Wharton School, University of Pennsylvania, and is joining Yonsei University as an assistant professor. He received his M.S. in statistics from the University of Michigan and his B.A. in economics from Seoul National University. His research focuses on understanding the impact of social capital, opinion leadership, homophily, and triadic balance on social learning.Amit Pazgal (“Profit-Increasing Consumer Exit”) is a professor of marketing at the Jones Graduate School of Business at Rice University. He received his Ph.D. from the Kellogg School of Management, Northwestern University. His current research focuses on the analysis and characterization of optimal price setting procedures employed by firms in various strategic environments. His research appeared in leading marketing, management, operations, and economics journals.David Soberman (“Profit-Increasing Consumer Exit”) is a professor of marketing and the CN Chair in Strategic Management at the Rotman School of Management at the University of Toronto. He is a licensed professional engineer (Ontario), and he holds a Ph.D. (management) from the University of Toronto and an MBA and a B.S. in chemical engineering from Queen's University in Kingston. His research consists of using applied microeconomics and game theory to analyze a number of marketing phenomena. His research has appeared in Marketing Science, Management Science, the Journal of Marketing Research, the Journal of Marketing, and the California Management Review. Before his doctoral studies, he held a number of positions in marketing management, sales, and engineering with Molson Breweries, Nabisco Brands Ltd., and Imperial Oil Ltd.Alina Sorescu (“Wedded Bliss or Tainted Love? Stock Market Reactions to the Introduction of Cobranded Products”) is the Rebecca U. '74 and William S. Nichols III '74 Associate Professor of Marketing at the Mays Business School, Texas A&M University. She holds a B.S. in mathematics from the University of Bucharest, an M.S. in statistics from the University of Florida, and a Ph.D. from the University of Houston. Her research focuses on radical innovations, product portfolio decisions, branding, acquisitions and alliances, and measuring the financial value of marketing actions; it has been published in the Journal of Marketing Research, Journal of Marketing, and Journal of Retailing, among others. Her research awards include the American Marketing Association (AMA) John A. Howard Dissertation Award, the Academy of Marketing Science Mary Kay Cosmetics Dissertation Award, and best paper awards at AMA and Strategic Management Society conferences.Raphael Thomadsen (“Profit-Increasing Consumer Exit”) is an associate professor at the Olin Business School at Washington University in St. Louis. He holds a Ph.D. and master's in economics from Stanford University and a B.A. in math and economics from the University of Wisconsin. His research focuses on pricing and product offerings in differentiated competitive markets and the interplay between the two. His research has appeared in top marketing and economics journals. Before joining the Washington University faculty, he taught at Columbia and the University of California, Los Angeles.Jing Wang (“Modeling Choice Interdependence in a Social Network”) is a specialist of Asia Pacific Marketing & Sales practice in McKinsey & Company. She received a Ph.D. in statistics from the University of Michigan and worked at Google as a data scientist. Her research interests include Markov chain Monte Carlo and the social network and its application in marketing.Kenneth C. Wilbur (“Correcting Audience Externalities in Television Advertising”) teaches core marketing in the MBA program at the University of California, San Diego, Rady School of Management. He produces original, useful research at the intersection of advertising, media, and technology. His work has been published in leading journals, won major awards, influenced practice, and been presented at conferences, universities, and companies worldwide. Complete details are available at http://kennethcwilbur.com.Linli Xu (“Correcting Audience Externalities in Television Advertising”) is an assistant professor at the Carlson School of Management, University of Minnesota. She joined the faculty of the marketing department in 2012 after receiving her Ph.D. at the University of Southern California (USC). While in the Ph.D. program at USC, she was awarded the 2010 James S. Ford/Commerce Associate Ph.D. Fellowship in recognition of her outstanding scholastic achievement and research. Her research interests focus on advertising and new product development using quantitative modeling methods. She has presented her works at several marketing conferences, including the UT Dallas FORMS conference, the Marketing Dynamics Conference, and the INFORMS Marketing Science Conference.Hema Yoganarasimhan (“The Value of Reputation in an Online Freelance Marketplace”) is an assistant professor at the University of California, Davis. She has an undergraduate degree from the Indian Institute of Technology, Madras and Ph.D. and M.Phil. degrees from Yale University. Her research interests include empirical measurement and analytical modeling of social influence, network effects, fashion, and auctions. In 2008 she won the Marketing Science Institute's Alden G. Clayton Doctoral Dissertation Proposal Award. She has also won the 2012 Frank M. Bass Outstanding Dissertation Award and was the finalist for the 2012 John D. C. Little Best Paper Award.
This special section is the result of an effort by several scholars to move marketing academic research toward greater practical relevance. This initiative, called Theory + Practice in Marketing (TPM), started with a conference at Columbia Business School in 2011, and the five papers published in this special section were presented at the second TPM conference held at Harvard Business School in 2012.
Although store brands (SBs) are becoming increasingly important across the world, their success varies dramatically across consumer packaged goods categories and countries. The purpose of this paper is to provide insight into how such differences in SB success originate. Using a unique data set that combines scanner data for a three- to five-year period with consumer survey data (n = 20,987) for scores of food, household care, and personal care categories from 23 countries around the world, we identify cross-national regularities as to the role of nine manufacturer and retailer factors in explaining SB market share. For each manufacturer and retailer factor, we determine whether it can be part of a global integration strategy, whether it can be included in a local adaptation strategy, or whether it is a candidate for worldwide learning. Our findings have important implications for national brand manufacturers and retailers.
Researchers and practitioners devote substantial effort to targeting banner advertisements to consumers, but they focus less effort on how to communicate with consumers once targeted. Morphing enables a website to learn, automatically and near optimally, which banner advertisements to serve to consumers to maximize click-through rates, brand consideration, and purchase likelihood. Banners are matched to consumers based on posterior probabilities of latent segment membership, which are identified from consumers' clickstreams.This paper describes the first large-sample random-assignment field test of banner morphing—more than 100,000 consumers viewed more than 450,000 banners on CNET.com. On relevant Web pages, CNET's click-through rates almost doubled relative to control banners. We supplement the CNET field test with an experiment on an automotive information-and-recommendation website. The automotive experiment replaces automated learning with a longitudinal design that implements morph-to-segment matching. Banners matched to cognitive styles, as well as the stage of the consumer's buying process and body-type preference, significantly increase click-through rates, brand consideration, and purchase likelihood relative to a control. The CNET field test and automotive experiment demonstrate that matching banners to cognitive-style segments is feasible and provides significant benefits above and beyond traditional targeting. Improved banner effectiveness has strategic implications for allocations of budgets among media.
Market growth is fundamental to marketing. Frank Bass's seminal diffusion theory explains growth in new product markets. We develop an analogous theory for established markets exhibiting sporadic growth or intermittent declines.Our theory suggests that market participants repeatedly take successful and unsuccessful actions that cause them to change or to mutate in myriad and often unpredictable ways. The environment sorts these mutations, determining winners and losers. Abundant mutations often cause different market participants to become winners, displacing past winners. Abundant mutations also often cause market growth because the natural selection mechanism leaves more surviving favorable mutations. So one nonobvious falsifiable implication of our theory is that displacement precedes growth and stability precedes decline. Another is that risk taking, diversity of opinions, and experimentation should precede growth.We develop a metric for measuring displacement. Using multiple publicly available data sets (one including sales for top firms for 55 years and another including sales for all automobile models for 25 years), we find that our metric provides a practical way to measure the rate of mutation and confirm our theory's predictions. Our easily replicated tests show that our displacement metric can predict intermittent market growth or decline in very different contexts without the need for exogenous idiosyncratic explanations. Moreover, other alternative covariates (trends, lagged growth, new product entry, macroeconomic indicators, etc.) are unable to predict growth or decline.
It has been argued that retailers lack both the resources and capabilities to maximize category performance. Retailers may seek category management (CM) advice from a manufacturer, referred to as a category captain (CC). A CC's recommendations affect all brands in the category, not just her own. Despite an increase in the number of CC collaborations, retailers are still concerned about manufacturer opportunism and militant behavior by manufacturers not selected as CCs, whereas government agencies are worried about anticompetitive behavior that could harm consumers. The Federal Trade Commission recommends strictly enforced information firewalls within a CC's organization as a best-practice guideline. In this study we develop an empirical model and use policy simulations to quantify the impact of CC arrangements with information firewalls on retailers, manufacturers, and consumers. We show how these effects could be influenced by the (de)activation of vertical and horizontal information firewalls within the CC's organization.
With the availability of social network data, it has become possible to relate the behavior of individuals to that of their acquaintances on a large scale. Although the similarity of connected individuals is well established, it is unclear whether behavioral predictions based on social data are more accurate than those arising from current marketing practices. We employ a communications network of over 100 million people to forecast highly diverse behaviors, from patronizing an off-line department store to responding to advertising to joining a recreational league. Across all domains, we find that social data are informative in identifying individuals who are most likely to undertake various actions, and moreover, such data improve on both demographic and behavioral models. There are, however, limits to the utility of social data.In particular, when rich transactional data were available, social data did little to improve prediction.
An examination of brand prices in several categories reveals that the distribution of prices is multimodal, with firms offering shallow and deep discounts. Another interesting feature of these distributions is that they may have holes in the interior of the support. These pricing distributions do not occur in extant theoretical models of price promotions. We develop a dynamic model of competition in which some price-sensitive consumers stockpile during periods of deep discounts. A game-theoretic analysis of our model generates a multimodal pricing distribution with a hole in the interior of the support. Consumer stockpiling in our model also gives rise to negative serial correlation in prices. This is consistent with our empirical observation of the pricing distribution of several brands across multiple categories in the IRI marketing data set.We generate several interesting insights into firms' optimal promotional strategies and their interplay with the clientele mix, market structure, and other market factors. We find that, in equilibrium, stockpiling by price-sensitive consumers neither harms nor benefits firms when they adopt equilibrium strategies. Interestingly, when price-sensitive consumers stockpile, even increased consumption as a result of stockpiling does not lead to higher profits for firms.
Firms constantly grapple with the question of whether to make, buy, or ally for innovations. The literature has not, to our knowledge, analyzed the choice of and payoff from these alternate routes to innovation for the same firm. To address this issue, we collect, code, and analyze the choice of and payoff from 3,522 announcements of make, buy, and ally for 192 firms across 108 industries over five years.We find that announcements to make or ally generate positive and higher payoffs than announcements to buy, which generate negative payoffs. Nevertheless, firms continue to buy for two reasons. First, firms seem to have no memory of the payoff from buy, even though they have a memory of the payoff from make. Second, firms tend to buy when they lack commercializations, even though this strategy does not always seem to pay off. These results suggest that firms see buy as a signal to investors that they have a solution for what may be a deep strategic problem. Nevertheless, the negative returns to a buy can be mitigated if the acquirer is experienced, and the target is related and offers high customer benefit. We offer explanations for and implications of the results.
Paid search has become the mainstream platform for online advertising, further intensifying competition between advertisers. The main objective of this research is twofold. On the one hand, we want to understand, in the context of paid-search advertising, the effects of competition (measured by the number of ads on the paid-search listings) on click volume and the cost per click (CPC) of paid-search ads. On the other hand, we are interested in understanding the determinants of competition, that is, how various demand and supply factors affect the entry probability of firms and, consequently, the total number of entrants for a keyword. We regard each keyword as a market and build an integrative model consisting of three key components: (i) the realized click volume of each entrant as a function of the baseline click volume and the decay factor; (ii) the vector of realized CPCs of those entrants as a function of the decay factor and the order statistics of the value per click at an equilibrium condition; and (iii) the number of entrants, the product of the number of potential entrants multiplied by the entry probability; the entry probability is determined by the expected revenue (a function of expected click volume, CPC, and value per click) and the entry cost at the equilibrium condition of an incomplete information game. The proposed modeling framework entails several econometric challenges. To cope with these challenges, we develop a Bayesian estimation approach to make model inferences. Our proposed model is applied to a data set of 1,597 keywords associated with digital camera/video and their accessories with full information on competition. Our empirical analysis indicates that the number of competing ads has a significant impact on the baseline click volume, decay factor, and value per click. These findings help paid-search advertisers assess the impact of competition on their entry decisions and advertising profitability. In the counterfactual analysis, we investigate the profit implication of two polices for the paid-search host: raising the decay factor by encouraging consumers to engage in more in-depth search/click-through and providing coupons to advertisers.
This is an abridged version of an evaluation report for Marketing Science, which was commissioned by the INFORMS Publications Committee as part of its periodic review of every INFORMS journal. The coauthors listed here comprised the task force that conducted the research project and strategic analysis described below.
We estimate a dynamic structural model of sales force response to a bonus-based compensation plan. This paper provides substantive insight into how different elements of the compensation plan enhance productivity. We find evidence that (1) bonuses enhance productivity across all segments; (2) overachievement commissions help sustain the high productivity of the best performers, even after attaining quotas; and (3) quarterly bonuses help improve performance of the weak performers by serving as pacers to keep the sales force on track in achieving its annual sales quotas. The paper also introduces two main methodological innovations to the marketing literature: First, we implement empirically the method proposed by Arcidiacono and Miller [Arcidiacono P, Miller RA (2011) Conditional choice probability estimation of dynamic discrete choice models with unobserved heterogeneity. Econometrica 79(6):1823–1867] to accommodate unobserved latent-class heterogeneity using a computationally light two-step estimator. Second, we illustrate how discount factors can be estimated in a dynamic structural model using field data through a combination of (1) an exclusion restriction separating current and future payoff and (2) a finite-horizon model in which there is no forward-looking behavior in the last period.
When managers and researchers encounter a data set, they typically ask two key questions: (1) Which model (from a candidate set) should I use? And (2) if I use a particular model, when is it going to likely work well for my business goal? This research addresses those two questions and provides a rule, i.e., a decision tree, for data analysts to portend the “winning model” before having to fit any of them for longitudinal incidence data. We characterize data sets based on managerially relevant (and easy-to-compute) summary statistics, and we use classification techniques from machine learning to provide a decision tree that recommends when to use which model. By doing the “legwork” of obtaining this decision tree for model selection, we provide a time-saving tool to analysts. We illustrate this method for a common marketing problem (i.e., forecasting repeat purchasing incidence for a cohort of new customers) and demonstrate the method's ability to discriminate among an integrated family of a hidden Markov model (HMM) and its constrained variants. We observe a strong ability for data set characteristics to guide the choice of the most appropriate model, and we observe that some model features (e.g., the “back-and-forth” migration between latent states) are more important to accommodate than are others (e.g., the inclusion of an “off” state with no activity). We also demonstrate the method's broad potential by providing a general “recipe” for researchers to replicate this kind of model classification task in other managerial contexts (outside of repeat purchasing incidence data and the HMM framework).
The nature of marketing science is changing in a systematic, predictable, and irrevocable way. As information technology enables ubiquitous customer communication and big customer data, the fundamental nature of the firm's connection to the customer changes: better, more personalized service can be offered, from which service relationships are deepened, and consequently, more profitable customers grow the influence of service within the goods sector and expand the service sector in the economy. Marketing is becoming more personalized, and marketing science techniques that exploit customer heterogeneity are becoming more important. Information technology improvements also guarantee the increasing importance and usage of computationally intensive data processing and “big data.” Most importantly, these trends have already lasted for more than a century, and they will become even more pronounced in the coming years as a result of the monotonic nature of technology improvement. These changes imply a transformation of marketing science in both the topics to be emphasized and the methods to be employed. Increasingly, and inevitably, all of marketing will come to resemble to a greater degree the formerly specialized area of service marketing, only with an increased emphasis on marketing analytics.
Researchers often collect continuous consumer feedback (moment-to-moment, or MTM, data) to understand how consumers respond to a variety of experiences (e.g., viewing a TV show, undergoing a colonoscopy). Analyzing how MTM judgments are integrated into overall evaluations allows researchers to determine how the structure of an experience influences consumers' post-experience satisfaction. However, this analysis is challenging because of the functional nature of MTM data. As such, previous research has typically been limited to identifying the influence of heuristics, such as relying on the average intensity, peak, and ending.We develop a Bayesian functional linear model to study how the different “moments” in the MTM data contribute to the overall judgment. Our approach incorporates a (temporally) weighted average of MTM data as well as specific “patterns” such as peak and trough, thus nesting previous approaches such as the “peak-end” rule as special cases. We apply our methodology to analyze data on TV show pilots collected by CBS. Our results reveal several interesting empirical findings. First, the last quintile of a TV show is weighted about four times as much as each of the first four quintiles. Second, patterns such as peak and trough do not play substantial roles in driving overall evaluations for TV shows. Finally, the last quintile is more important for procedural dramas than for serial dramas. We discuss the managerial implications of our results and other potential applications of our general methodology.
We study the relative importance of online word of mouth and advertising on firm performance over time since product introduction. The current research separates the volume of consumer-generated online word of mouth (OWOM) from its valence, which has three dimensions—attribute, emotion, and recommendation oriented. Firm-initiated advertising content is also classified as attribute or emotion advertising. We also shed light on the role played by advertising content on generating the different types of OWOM conversations. We use a dynamic hierarchical linear model (DHLM) for our analysis. The proposed model is compared with a dynamic linear model, vector autoregressive/system of equations model, and a generalized Bass model. Our estimation accounts for potential endogeneity in the key measures. Among the different OWOM measures, only the valence of recommendation OWOM is found to have a direct impact on sales; i.e., not all OWOM is the same. This impact increases over time. In contrast, the impact of attribute advertising and emotion advertising decreases over time. Also, consistent with prior research, we observe that rational messages (i.e., attribute-oriented advertising) wears out a bit faster than emotion-oriented advertising. Moreover, the volume of OWOM does not have a significant impact on sales. This suggests that, in our data, “what people say” is more important than “how much people say.” Next, we find that recommendation OWOM valence is driven primarily by the valence of attribute OWOM when the product is new and driven by the valence of emotion OWOM when the product is more mature. Our brand-level results help us classify brands as consumer driven or firm driven, depending on the relative importance of the OWOM and advertising measures, respectively.
We identify the conditions under which a problem of optimal advance selling strategy can be mathematically transformed into a problem of optimal bundle pricing. These conditions are as follows: (i) consumers and sellers have common priors on the probability of each state being realized in the future, (ii) consumers are risk-neutral, (iii) sellers can commit to spot prices, and (iv) consumers and sellers discount the future at the same rate. The result allows both researchers and practitioners to extend and/or apply the findings from the vast literature on bundling to advance selling problems, and vice versa. We highlight several insights that are particularly relevant, such as the importance of the dependence of consumer valuations across states on the profitability of advance selling in the base case of two states as well as in the cases of more than two states or with possible competition in some of the states.
Customer base analysis is a key element in customer valuation and can provide guidance for decisions such as resource allocation. Yet extant models often focus on a single activity, such as purchases from a retailer or donations to a nonprofit organization. These models do not consider other ways that an individual may engage with an organization, such as purchasing in multiple brands or contributing user-generated content. In this research, we propose a framework to generalize extant models for customer base analysis to multiple activities.Using the data from a website that allows users to purchase digital content and/or post digital content at no charge, we develop a flexible “buy ‘til you die” model to empirically examine how the two activities are related. Compared with benchmarks, our model more accurately forecasts the future behavior for both types of activities. In addition to finding evidence of coincidence between the activities while customers are “alive,” we find that the latent attrition processes are related. This suggests that conducting one type of activity is informative of whether customers are still alive to conduct another type of activity and, consequently, affects inferences of customer value.
In this paper, we study the entry and expansion decisions of McDonald's and KFC in China using an originally assembled data set on the two chains' expansion in the China market from their initial entry up to year 2007. We analyze how the presence of a rival affects each firm's strategies. The results indicate that a rival's presence has a net positive effect on a chain's expansion decision. We focus on testing two possible explanations for a positive rival impact: market learning and demand expansion. First, we derive a set of theoretical predictions on how a chain's optimal expansion decision would react to its rival's expansion patterns when market learning versus demand expansion is the driving force of the rival's positive influence. The empirical analysis based on these predictions consistently suggests that market learning is more likely to explain the positive effect of KFC on McDonald's and that demand expansion is more plausible with McDonald's positive spillover on KFC. In other words, the results are consistent with the presence of KFC signaling market demand potential and growth to McDonald's and the presence of McDonald's helping to cultivate consumer taste and generate demand for Western fast food, which benefits KFC.
Marketing Science greatly benefited from the admirable and fastidious efforts of more than 200 different individuals who provided manuscript reviews last year. Beyond those individuals already recognized on the editorial board, the editor-in-chief and guest editors of Marketing Science are indebted to the many guest editors-in-chief, guest associate editors, and ad hoc reviewers who provided expert counsel and guidance on a voluntary basis. The following list acknowledges the contribution of guest editors-in-chief, guest associate editors, and ad hoc reviewers who served from January 1, 2013 to December 31, 2013. Finally, let us not forget to thank the authors. Marketing Science requires and receives outstanding submissions from many leading researchers and prestigious organizations.Preyas S. DesaiDuke University
This editorial acknowledges notable contributions of many individuals to Marketing Science during 2013.
We model the multifaceted impact of pricing decisions in business-to-business (B2B) relationships that are governed by trust. We show how a seller can develop optimal intertemporal targeted pricing strategies to maximize profits over time while taking into consideration the impact of pricing decisions on short-term profit margin, reference price formation, and long-term relationships. Our modeling framework uses a hierarchical Bayesian approach to weave together a multivariate nonhomogeneous hidden Markov model, buyer heterogeneity, and control functions to facilitate targeting, capture the evolution of trust, and control for price endogeneity. We estimate our model on longitudinal transactions data from a retailer in the industrial consumables domain. We find that buyers in our data set can be best represented by two latent states of trust toward the seller—a “vigilant” state that is characterized by heightened price sensitivity and a cautious approach to ordering and a “relaxed” state with purchase behaviors that are consistent with high relational trust. The seller's pricing decisions can transition buyers between these two states. An optimal dynamic and targeted pricing strategy based on our model suggests a 52% improvement in profitability compared with the status quo. Furthermore, a counterfactual analysis examines the seller's optimal pricing policy under fluctuating commodity prices.
Human faces are used extensively in print advertisements. In prior literature, researchers have studied spokespersons in general, but few have studied faces explicitly. This paper aims to answer three questions that are important to both researchers and practitioners: (1) Do faces affect how a viewer reacts to an advertisement on the metrics that advertisers care about? (2) If faces do have an effect, is it large enough to warrant careful selection of faces when constructing print advertisements? (3) If faces do have an effect and the effect is large, what facial features elicit such differential reactions on these metrics, and are such reactions different across individuals and/or product categories? Relying on the eigenface method, a holistic approach widely used in the computer science field for face recognition, we conducted an empirical study to answer these three questions. The results show that different faces do have an effect on people's attitude toward the advertisement, attitude toward the brand, and purchase intention and that the effect is nontrivial. Multiple segments were identified and substantial differences were found among people's reactions to the faces in the ads across those segments. We also found that the effect of faces interacts with product categories and is mediated by various facial traits such as attractiveness, trustworthiness, and competence. Implications and directions for future research are discussed.
I develop a dynamic investment game with a “memoryless” research and development process in which an incumbent and an entrant can invest in a new technology, and the entrant can also invest in the old technology. I show that an increase in the probability of successfully implementing a technology can cause the incumbent to reduce its investment. Under certain conditions, if the success probability is high, the incumbent allows the entrant to win the new technology so that firms reach an equilibrium in which they use different technologies, and threats of retaliation prevent attacks; but if the success probability is low, such an equilibrium cannot be sustained, and both firms eventually implement both technologies.
Disaggregate demand in the marketplace exists on a grid determined by the package sizes offered by manufacturers and retailers. Although consumers may want to purchase a continuous-valued amount of a product, realized purchases are constrained by available packages. This constraint might not be problematic for high-volume demand, but it is potentially troubling when demand is small. Despite the prevalence of packaging constraints on choice, economic models of choice have been slow to deal with their effects on parameter estimates and policy implications. In this paper we propose a general framework for dealing with indivisible demand in economic models of choice, and we show how to estimate model parameters using Bayesian methods. Analyses of simulated data and a scanner-panel data set of yogurt purchases indicate that ignoring packaging constraints can bias parameter estimates and measures of model fit, which results in the inaccurate measures of metrics such as price elasticity and compensating value. We also show that a portion of nonpurchase in the data (e.g., 2.27% for Yoplait Original) reflects the restriction of indivisibility, not the lack of preference. The importance of demand indivisibility is also highlighted by the counterfactual study where the removal of the smallest package size (i.e., 4 oz) mainly results in nonpurchase in the yogurt category instead of switching to larger package sizes.
Sellers often make claims about product strengths without providing evidence. Even though such claims are mere puffery, we show that they can be credible because talking up any one strength comes at the implicit trade-off of not talking up another potential strength. Puffery pulls in some buyers who value product attributes that are talked up or emphasized while pushing away other buyers who infer that the attributes they value are relative weaknesses. When the initial probability of making a sale is low, there are more potential buyers to pull in than to push away, so puffery is persuasive overall. This persuasiveness requires that buyers have some privacy about their preferences so that the seller does not completely pander to them. More generally, the results show how comparative cheap talk by an expert to a decision maker can be credible and persuasive in standard discrete choice models used throughout marketing, economics, and other disciplines.
This study examines the dynamics of online buzz over time before product release. Employing functional data analysis, we treat the curve of prerelease buzz evolution trajectory as the unit of analysis and find that the shape of the curve significantly adds power in predicting new product performance compared with using product characteristics and firm advertising alone. Moreover, daily prerelease buzz evolution data enable accurate sales forecasting long before product release, which allows sufficient time for managers to adjust product design and/or marketing strategy. For example, the forecasting accuracy using an early buzz evolution curve ending on the 61st day before product release is not only higher than that using accumulated buzz volume until then but also higher than that using the total volume of all buzz up until product release. Beyond the sales outcome, we find that prerelease buzz is quickly reflected in firm stock returns before product release and reduces the absolute amount of postrelease stock price correction. The model accounts for endogeneity, and the results are robust after controlling for buzz sentiment. We also explore the factors influencing prerelease buzz evolution patterns, thus generating insights into how to manage prerelease buzz dynamics to enhance new product performance.
A manufacturer will often limit competition among downstream partners by authorizing only a select group of retailers to carry its product. However, it is not uncommon for authorized retailers to create an additional competitor by diverting units to an unauthorized seller. This paper presents an analytical model that demonstrates how diversion from authorized retailers to an unauthorized direct competitor can occur under circumstances not considered by the prior literature. In fact, diversion can represent a prisoner's dilemma whereby retailers diminish their own profit by selling to the unauthorized direct seller. The authorized retailer's profit loss actually increases as the per-unit diversion costs incurred by the authorized retailer decrease. The model also shows that the unauthorized direct seller earns greater profit by strategically procuring a unilaterally constraining quantity, even though this procurement strategy results in an equivalent increase in the quantity sold by the retailers. Combined, the results identify a new reason for diversion and its consequences for retailers and the unauthorized direct seller.
Firms and organizations often need to collect and analyze sensitive consumer data. A common problem encountered in such evidence-based research is that they cannot collect all essential information from one sample, and they may need to link nonoverlapping data items across independent samples. We propose an automated nonparametric data fusion solution to this problem. The proposed methods are not restricted to specific types of variables and distributions. They require no prior knowledge about how data at hand may behave differently from standard theoretical distributions, and they automate the process of generating suitable distributions that match data, therefore making our methods particularly useful for linking data with complex distributional shapes. In addition, these methods have strong theoretical support; permit highly efficient direct fusion to relate a mixture of continuous, semicontinuous, and discrete variables; and enable nonparametric identification of entire distributions of fusion variables, including higher moments and tail percentiles. These novel and promising features overcome important limitations of existing methods and have the potential to increase fusion effectiveness. We apply the proposed methods to overcome data constraints in a study of counterfeiting. By combining data sets from multiple sources, data fusion provides a feasible approach to studying the relationship between counterfeit purchases and various marketing elements, such as consumers' purchase motivations, behaviors, and attitudes; brand marketing channels; promotions; and advertisements. Therefore, data fusion sheds light on counterfeit purchase behaviors and suggests ways to counter counterfeits that would not be available if these data sets were analyzed separately.
Our Tablet Computer data set, collected from various websites, contains market dynamics related to 2,163 products, characteristics of 794 products, more than 40,000 consumer-generated product reviews, and information about 39,278 reviewers. The market dynamic information was collected weekly for 24 weeks starting February 1, 2012. Our Tablet Computer data set comprises four tables: the Market Dynamics of Products, Product Characteristic Information, Consumer-Generated Product Reviews, and Reviewer Information tables. In turn, it offers three unique properties. First, it contains both structured product information and unstructured product reviews. Second, it comprises product characteristic information and market dynamic information. Third, this data set integrates user-generated content with manufacturer-provided content. This integrated data set (available at http://pubsonline.informs.org/page/mksc/online-databases) is valuable for both academics and practitioners who conduct research related to marketing, information systems, computer science, and other fields using digital data readily available through the Internet.
We study how peers impact worker productivity growth among salespeople in the cosmetics department of a department store. We first exploit a shift assignment policy that creates exogenous variation in salespersons' peers each week to identify and quantify sources of worker learning. We find that peer-based learning is more important than learning-by-doing for individuals, and there is no evidence of forgetting. Working with high-ability peers substantially increases the long-term productivity growth of new salespeople. We then examine possible mechanisms behind peer-based learning by exploiting the multiple colocated firms in our setting that sell products with different task difficulties and compensate their sales forces using either team-based or individual-based compensation systems. The variation in incentives to compete and cooperate within and across firm boundaries, combined with variation in sales difficulty for different product classes, allows us to suggest two mechanisms behind peer-based learning: observing successful sales techniques of peers and direct teaching. Our paper advocates the importance of learning from one another in the workplace and suggests that individual peer-based learning is a foundation of both organizational learning curves and knowledge spillovers across firms.
In search advertising, brand names are often purchased as keywords by the brand owner or a competitor. We aim to understand the strategic benefits and costs of a firm buying its own brand name or a competitor's brand name as a keyword. We model the effect of search advertising to depend on the presence or absence of a competitor's advertisement on the same results page. We find that the quality difference between the brand owner and the competitor moderates the purchase decision of both firms. Interestingly, in some cases, a firm may buy its own brand name only to defend itself from the competitor's threat. It is also possible that the brand owner, by buying its own branded keyword, precludes the competitor from buying the same keyword. Our result also implies that the practice of bidding on the competitor's brand name creates a prisoner's dilemma, and thus both firms may be worse off, but the search engine captures the lost profits. We also discuss the difference in our results when the search is for a generic keyword instead of a branded keyword. Finally, we find some empirical support for our theory from the observation of actual purchase patterns on Google AdWords.
We investigate how the tendency to adopt a new product independently of social influence, the recipients' susceptibility to such influence, and the sources' strength of influence vary with social status. Leveraging insights from social psychology and sociology about middle-status anxiety and conformity, we propose that for products that potential adopters expect to boost their status, both the tendency to adopt independently from others and the susceptibility to contagion is higher for middle-status than for low- and high-status customers. Applying a nested case-control design to the adoption of commercial kits used in genetic engineering, we find evidence that status affects (i) how early or late one adopts regardless of social influence, (ii) how susceptible one is to such influence operating through social ties, and (iii) how influential one's own behavior is in triggering adoption by others. The inverse-U patterns in (i) and (ii) are consistent with middle-status anxiety and conformity. The findings have implications for how to use status to better understand adoption and contagion mechanisms, and for targeting customers when launching new products.
Marketing managers often use consumer attitude metrics such as awareness, consideration, and preference as performance indicators because they represent their brand's health and are readily connected to marketing activity. However, this does not mean that financially focused executives know how such metrics translate into sales performance, which would allow them to make beneficial marketing mix decisions. We propose four criteria—potential, responsiveness, stickiness, and sales conversion—that determine the connection between marketing actions, attitudinal metrics, and sales outcomes.We test our approach with a rich data set of four-weekly marketing actions, attitude metrics, and sales for several consumer brands in four categories over a seven-year period. The results quantify how marketing actions affect sales performance through their differential impact on attitudinal metrics, as captured by our proposed criteria. We find that marketing–attitude and attitude–sales relationships are predominantly stable over time but differ substantially across brands and product categories. We also establish that combining marketing and attitudinal metrics criteria improves the prediction of brand sales performance, often substantially so. Based on these insights, we provide specific recommendations on improving the marketing mix for different brands, and we validate them in a holdout sample. For managers and researchers alike, our criteria offer a verifiable explanation for differences in marketing elasticities and an actionable connection between marketing and financial performance metrics.
We investigate whether partners in a brand alliance should be similar or dissimilar in brand image to foster favorable perceptions of brand fit. Using a Bayesian nonlinear structural equation model and evaluations of 1,200 brand alliances, we find that the conceptual coherence in brand personality profiles predicts attitudes towards a brand alliance. More specifically, we find that similarity in Sophistication and Ruggedness and moderate dissimilarity in Sincerity and Competence result in more favorable brand alliance evaluations. Overall, we find that similarity effects are more pronounced than dissimilarity effects. Implications for brand alliance strategies and marketing managers are discussed.
Past research has established that just surveying individuals or measuring consumers' intentions can influence their subsequent behaviors. Building on self-generated validity theory and extant studies on the survey participation effect, we examine the behavioral phenomenon in a setting where consumers repeatedly participate in brand-specific surveys of all competing brands in a product category. We also investigate the existence and magnitude of the survey participation effect at the individual decision maker level while accounting for marketing communication efforts of the focal and competing brands. We test our proposed individual-consumer-level model using unique behavioral panel data with survey participation and marketing communication information. Our results suggest that the survey participation effect exists in a competitive marketplace setting where consumers' intentions toward a focal brand and all the competing brands are measured. We find evidence of a backlash effect wherein survey participation and marketing communication work against each other. We also find that consumers' participation in surveys of competing brands does not positively spill over to their choice of the focal brand. Based on our results, we suggest important implications for coordination between marketing communication efforts and marketing research activities.
Traditional advertising, such as TV and print advertising, primarily builds awareness of a firm's product among consumers, whereas sponsored search advertising on a search engine can target consumers closer to making a purchase because they reveal their interest by searching for a relevant keyword. Increased consumer targetability in sponsored search advertising induces a firm to “poach” a competing firm's consumers by directly advertising on the competing firm's keywords; in other words, the poaching firm tries to obtain more than its “fair share” of sales through sponsored search advertising by free riding on the market created by the firm being poached. Using a game theory model with firms of different advertising budgets, we study the phenomenon of poaching, its impact on how firms allocate their advertising budgets to traditional and sponsored search advertising, and the search engine's policy on poaching. We find that, as budget asymmetry increases, the smaller-budget firm poaches more on the keywords of the larger-budget firm. This may induce the larger-budget firm to allocate more of its budget to traditional advertising, which, in turn, hurts the search engine's advertising revenues. Therefore, paradoxically, even though poaching increases competition in sponsored search advertising, the search engine can benefit from limiting the extent of poaching. This explains why major search engines use “ad relevance” measures to handicap poaching on trademarked keywords.
Brands stand at the core of marketing. They are central to positioning, marketing communications, word of mouth, customer relationships, and firm profits. Brands have been studied from multiple perspectives using a variety of measures and scales. We offer a data set that contains 136 different measures of the brand characteristics for almost 700 of the top U.S. national brands across 16 categories measured by 2010. These measures cover a broad range of characteristics including brand personality, satisfaction, age, attributes related to Rogers' innovation scheme such as complexity, and the four brand equity pillars of Young and Rubicam's BrandAsset Valuator. The data were collected from a combination of sources including an original survey on 4,769 subjects. In addition, we provide quarterly data on the variables available from the BrandAsset Valuator for two and a half years between 2008 and 2010. These data can be used as a building block in research that aims to explore the antecedents of brand perceptions or connect brand characteristics with market and financial outcomes. This paper describes the data and some relevant research questions.Data, as supplemental material, are available at http://dx.doi.org/10.1287/mksc.2014.0861.
The customer relationship management allocation in marketing budgets is potentially misleading when it uses individual customer lifetime value estimations from historical data. Planned marketing interventions would change the purchasing behavior of different customers, and history-based decisions would thus be suboptimal. To cope with this inherent endogeneity, we model the optimal allocation of the marketing mix by accounting simultaneously for mass interventions and direct marketing interventions for each customer. This is a large stochastic dynamic problem that, in general, is computationally rather intractable as a result of the “curse of dimensionality.” We present an algorithm to derive the optimal marketing policies (how the firm should allocate its marketing resources) and the expected present value of those decisions, which maximize the long-term profitability of firms. This allows the firm to value customers/segments and helps the firm to target those that maximize long-term profitability given the optimal marketing resources allocation. We apply the proposed approach in the context of a kitchen appliance manufacturer. The results identify the most effective marketing policies and the endogenous customer values. It is in this context that we also dynamically identify the most profitable customer and the short- and long-term effects of marketing activities on each customer.
In many service markets such as consulting, auto repair, financial planning, and healthcare, the service provider may have more information about the customer’s problem than the customer, and different customers may impose different costs on the service provider. In principle, the service provider should ethically care about the customer’s welfare, but it is possible that a provider may maximize only its own profit. Moreover, the customer may not know ex ante whether the provider is ethical or purely self-interested. We develop a game-theoretic model to investigate pricing strategies and the market outcome in service markets where the provider has two-dimensional private information about her own type (whether ethical or self-interested) and about the customer’s condition (whether serious or minor). We show that in a less ethical market, a self-interested provider will charge different prices based on the customer’s condition, whereas an ethical provider will charge the same price for both conditions. In contrast, in a more ethical market, both the self-interested and the ethical provider will charge the same uniform price to both types of customers. Interestingly, both market efficiency and the customer’s ex ante expected surplus might be lower in a more ethical market than in a less ethical one.
Marketing is a field that is rich in data. Our data is of high quality, often at a highly disaggregate level, and there is considerable variation in the key variables for which estimates of effects on outcomes such as sales and profits are desired. The recognition that, in some general sense, marketing variables are set by firms on the basis of information not always observable by the researcher has led to concerns regarding endogeneity and widespread pressure to implement instrumental variables methods in marketing problems. The instruments used in our empirical literature are rarely valid and the IV methods used can have poor sampling properties, including substantial finite sample bias and large sampling errors. Given the problems with IV methods, a convincing argument must be made that there is a first order endogeneity problem and that we have strong and valid instruments before these methods should be used. If strong and valid instruments are not available, then researchers need to look toward supplementing the information available to them. For example, if there are concerns about unobservable advertising or promotional variables, then the researcher is much better off measuring these variables rather than using instruments (such as lagged marketing variables) that are clearly invalid. Ultimately, only randomized variation in marketing variables (with proper implementation and large samples) can be argued to be a valid instrument without further assumptions.
Past studies have overlooked the joint effects of economic and customer experience factors on service purchase behaviors. Furthermore, service firms tend to make substantial investments in enhancing customer experience, mitigating the negative effects of service failures through recovery efforts and increasing overall customer satisfaction. Yet, largely due to a paucity of data, we know little about how the state of the economy influences the way in which customers use past service experiences to make future purchase decisions. We hypothesize that the state of the economy moderates the effects of customer experience factors on customers’ service purchase behaviors. In addition, we examine how personal income influences the degree to which the aggregate economy influences service purchase decisions. We test the proposed model using panel survey and transaction data from an international airline carrier. Our findings demonstrate that, contrary to wisdom in the popular press, customer experience matters more when the economy is doing better, not worse. Furthermore, lower income consumers are more sensitive to changes in the economy than higher income consumers. We validate the hypothesized model using a controlled experiment and establish that aggregate measures of the economy can be used to predict individual perceptions and purchase intentions.
A well-established phenomenon of consumer buying behavior is that consumers evaluate prices relative to a reference point and exhibit loss aversion; i.e., their propensity to buy is more negatively affected by prices above the reference point than it is positively affected by prices below the reference point. The objective of this paper is to analytically examine how the competitive strategy and profitability of firms are affected by the presence of consumer loss aversion in the price dimension. Although we assume that consumer loss aversion increases consumer propensity to search for lower prices, we find that it does not necessarily lead to lower prices or profits when firms compete over multiple periods and when the consumer reference price in subsequent periods is affected by current prices. Specifically, consumer loss aversion could lead to higher prices and profits when consumer valuation is sufficiently high relative to search costs and the proportion of consumers with positive search costs is in an intermediate range. We also show that when forward-looking firms incorporate the negative effect of price promotions on future profits, the equilibrium range of price promotions may actually increase.
The potential demand in a new industry evolves over time. Demand is initially low, but advertising by the industry’s early entrants can speed up demand growth. However, there is intrinsic uncertainty of the demand level in each period and uncertainty of the demand evolution path, which can be affected by the underlying economic environment. We construct a dynamic model that features the stochastically and endogenously expanding demand of a new industry, and we investigate the optimal entry and exit behavior of firms as the industry evolves. We find that firms’ incentive to enter early depends critically on the cost that early entrants have to pay in developing the market. When the cost is high and the benefit spills over to potential entrants, firms have an incentive to wait, and the probability of entry can increase with the number of incumbents under certain circumstances. Firms’ entry strategy is also influenced by the transition of economic states. Firms are more likely to enter under a state that shows the prospect of demand taking off soon. We also find that, in the early stage of an industry, higher demand uncertainty can induce faster entry.
In a dynamic model with overlapping generations of consumers, we study duopolistic competition when firms can price discriminate, at each period, between their previous customers and the consumers that they have never served. Long-term contracts are not enforceable. In (Markov-perfect) equilibrium, one firm charges higher prices to its past customers than to its new customers, as past customers have revealed their strong preferences for the firm; the other firm, however, rewards its previous customers by charging lower prices to them than to its new customers. This loyalty reward strategy comes from the interplay between the firms’ usual incentive to extract surplus from consumers with revealed strong preferences and their incentives to acquire information and to recognize their young loyal customers. The result also relies on the firms’ inability a priori to tell different generations apart. It is the outcome of the unique equilibrium of a simplified two-period (or T-period) version of the game and holds with forward-looking consumers who are impatient enough.
Probabilistic or opaque selling, whereby a seller hides the exact identity of a product until after the buyer makes a payment, has been used in practice and received considerable attention in the literature. Under what conditions, and why, is probabilistic selling attractive to firms? The extant literature has offered the following explanations: to price discriminate heterogeneous consumers, to reduce supply–demand mismatches, and to soften price competition. In this paper, we provide a new explanation: to exploit consumer bounded rationality in the sense of anecdotal reasoning. We build a simple model where the firm is a monopoly, consumers are homogeneous, and there is no demand uncertainty or capacity constraint. This model allows us to isolate the impact of consumer bounded rationality on the adoption of opaque selling. We find that although it is never optimal to use opaque selling when consumers have rational expectations, it can be optimal when consumers are boundedly rational. We show that opaque selling may soften price competition and increase the industry profits as a result of consumer bounded rationality. Our findings underscore the importance of consumer bounded rationality and show that opaque selling might be even more attractive than previously thought.
Recent years have seen a considerable rise in the use of product placement in television shows. Taking advantage of second-by-second product placement, advertising, and audience tuning data, this research explores the impact of such product placement on the extent to which viewers tune away from downstream advertisements. Motivated by the behavioral priming literature, we examine how this impact relates to the brand- and category-match between product placement and subsequent advertising, as well as the temporal distance between them. Our analysis suggests that the coveted first position of a commercial break holds a greater audience when preceded by product placement from the same brand. This indicates a positive synergy between the two activities that can reduce audience decline by more than 10%. Product placements by other brands, however, can actually exacerbate audience loss, thus interfering with the reach of advertisements by competitors. Significantly, these changes in audience size are not temporary, but are retained across the remaining commercials in the same break. We discuss the managerial implications of these findings and directions for future research in the rapidly changing media landscape.Data, as supplemental material, are available at http://dx/doi.org/10.1287.mksc.2014.0864.
Competitor orientation, i.e., the focus on beating the competition rather than maximizing profits, seems to thrive in business situations despite being, by definition, suboptimal for profit-maximizing firms. Our research explains how a competitor orientation can persist and even thrive in equilibrium in markets that reward only profits. We apply evolutionary game theory to business markets where reputation matters. We use three games that represent classic interactions in business marketing: Chicken (to illustrate competition for product adoption), the Battle of the Sexes (channel negotiations), and the Prisoners' Dilemma (pricing battles).Initial populations are assumed to have both profit-maximizing managers and competitor-oriented managers (i.e., those who gain additional utility from beating others). We demonstrate that a competitor orientation can survive in equilibrium despite selection that is based solely on profits. Using Chicken, we show that a competitor orientation thrives and can even overrun the population. We use the Battle of the Sexes to show that a competitor orientation will overrun one population in a two-sided negotiation (e.g., all retailers in a retailer/manufacturer dyad). Last, using the Prisoners' Dilemma, we show that competitor orientation is not selected against. We conclude that evolutionary profit-driven selection pressures cannot be assumed to eliminate nonprofit-maximizing behavior even when selection is based purely on profitability.
In business-to-business settings a company's sales force often spends considerable time lobbying internally for authorization to charge lower prices. These internal lobbying activities are time consuming, and divert attention from other tasks, such as interacting with customers. We explain why internal lobbying activities serve an important role. They help the firm elicit truthful reporting of demand information from the sales force. As a result, it may be profitable for the firm to require lobbying (and make the requirement onerous), even though lobbying is a nonproductive activity that creates an additional administrative burden and imposes a deadweight loss.
The presence of positive entertainment (e.g., visual imagery, upbeat music, humor) in TV advertisements can make them more attractive and persuasive. However, little is known about the downside of too much entertainment. This research focuses on why, when, and how much to entertain consumers in TV advertisements. We collected data in a large scale field study using 82 ads with various levels of entertainment shown to 178 consumers in their homes and workplaces. Using a novel web-based face tracking system, we continuously measure consumers' smile responses, viewing interest, and purchase intent. A simultaneous Bayesian hierarchical model is estimated to assess how different levels of entertainment affect purchases by endogenizing viewing interest. We find that entertainment has an inverted U-shape relationship to purchase intent. Importantly, we separate entertainment into that which comes before the brand versus that which comes after, and find that the latter is positively associated with purchase intent while the former is not.
We examine the nature of best-worst data for modeling consumer preferences and predicting their choices. We show that contrary to the assumption of widely used models, the best and worst responses do not originate from the same data generating process. We propose a sequential evaluation model and show that people are likely to engage in a two-step evaluation process and are more likely to select the worst alternative first before selecting the best. We find that later choices have systematically larger coefficients as compared to earlier choices. We also find the presence of an elicitation effect that leads to larger coefficients when respondents are asked to select the worst alternative, meaning that respondents are surer about what they like least than what they like most. Finally, we investigate global inference retrieval in choice tasks, which can be represented by the central limit theorem and normally distributed errors, versus episodic retrieval represented by extreme value errors. We find that both specifications of the error term are plausible and advise using the proposed sequential logit model for practical reasons. We apply our model to data from a national survey investigating the concerns associated with hair care. We find that accounting for the sequential evaluation in the best-worst tasks and the presence of the scaling effects leads to different managerial implications compared to the results from currently used models.
Conventional wisdom suggests that when firms face a negative externality like gray marketing (i.e., the selling of branded goods outside of the manufacturer’s authorized channels), an effective strategy to reduce the negative impact is to centralize decision making. Nevertheless, in industries with significant gray marketing, we observe many firms with decentralized decision making. Our study assesses whether decentralized decision making can be optimal when a manufacturer faces gray market distribution. We consider a market where a focal firm competes with an existing competitor that produces a differentiated product and a gray marketer that sources an identical product from a lower-priced foreign market. We find that decentralization is optimal under quantity-based competition, provided the gray market is relatively uncompetitive and the level of competitive intensity between the focal firm and the competitor is high. Decentralization leads a firm to make aggressive production decisions, which leads to lower prices, yet it also leads to higher market share for the firm compared to centralization. When the level of competitive intensity between a firm and its competitor is high, the gain in market share more than offsets the loss due to lower prices. As a result, the focal firm is better off decentralizing its operations independent of (a) whether the competitor operates in the foreign market, and (b) the competitor’s organizational structure. This finding contradicts the belief that centralized decision making is always optimal when authorized manufacturers attempt to limit the negative impact of gray markets. The findings also provide insight to understand why firms might employ decentralized decision making in industries where gray markets are active.
This study investigates how prior usage experience with various decision aids available in an Internet shopping environment contributes to online purchase behavior evolution. Four types of decision aids are examined: those for (1) nutritional needs, (2) brand preference, (3) economic needs, and (4) personalized shopping lists. We construct and estimate nonhomogeneous hidden Markov models of store- and category-level purchase decisions, in which parameters vary over time across hidden states as driven by usage experience with different decision aids. We find that consumers evolve through distinct behavioral states over time, and the evolution is attributable to their prior usage experience with various decision aids. Moreover, the impact varies by the specific decision aid, behavioral state, and category characteristics. In addition, consumers gravitate toward habitual decision processes in online grocery stores, and their average price and promotion sensitivities increase first and then decrease but the level of heterogeneity rises continuously. We identify beneficial versus potentially undesirable decision aids and demonstrate how the proposed research method can help online retailers improve their store environments, design customized promotions, and quantify the payoffs of these strategies.
There is substantial academic interest in modeling consumer experiential learning. However, (approximately) optimal solutions to forward-looking experiential learning problems are complex, limiting their behavioral plausibility and empirical feasibility. We propose that consumers use cognitively simple heuristic strategies. We explore one viable heuristic—index strategies—and demonstrate that they are intuitive, tractable, and plausible. Index strategies are much simpler for consumers to use but provide close-to-optimal utility. They also avoid exponential growth in computational complexity, enabling researchers to study learning models in more complex situations.Well-defined index strategies depend on a structural property called indexability. We prove the indexability of a canonical forward-looking experiential learning model in which consumers learn brand quality while facing random utility shocks. Following an index strategy, consumers develop an index for each brand separately and choose the brand with the highest index. Using synthetic data, we demonstrate that an index strategy achieves nearly optimal utility at substantially lower computational costs. Using IRI data for diapers, we find that an index strategy performs as well as an approximately optimal solution and better than myopic learning. We extend the analysis to incorporate risk aversion, other cognitively simple heuristics, heterogeneous foresight, and an alternative specification of brands.
This paper investigates a determinant of location choice for multistore retailing firms: the trade-off between the business-stealing effect and the cost-saving effect from clustering their own stores. I present an empirical model of network choice by two multistore firms. I use lattice-theoretical results to address the computational burden of solving for an equilibrium in store networks. The framework integrates the static entry game of complete information with post-entry outcome data while using simulations to correct for the selection of entrants. I present an application of the model to the case of the convenience store industry in Okinawa Island, Japan, using unique cross-sectional data on store networks and revenues. I use parameter estimates to examine the impact of a hypothetical horizontal merger on store configurations, costs, and profits. Results suggest a retailer's trade-off between cost savings and lost revenues from clustering its stores is positive across markets and negative within a market. I find an acquirer of a hypothetical merger of two multistore firms would decrease its number of stores in suburbs but increase its number in the city center.
We disentangle and study the relative importance of different risk preferences in explaining extended warranty purchases and the high premia paid for them. Empirical and behavioral research on insurance is at odds with whether diminishing returns (curvature of the utility function), or loss aversion and nonlinear probability weighting lead to observed consumer behavior. This lack of consensus is primarily due to the inability of standard choice data to separate different risk preferences, and the consequent need to rely on strong parametric assumptions. We design two conjoint studies (consistent with simultaneous and sequential decision making) with choices about washing machines and extended warranties, where subjects are given failure probabilities and repair costs. Using stated choice data from the surveys, we can nonparametrically identify product and risk preferences. We find that loss aversion is significantly more important than curvature and probability weights in explaining extended warranty choices. Importantly, failure to decompose risk-averse behavior into that arising from curvature, loss aversion, and probability weighting leads to lower washer prices and profits. These findings are robust to alternate reference point assumptions. We rationalize the premia paid for warranties by exploring retailer incentives to price discriminate, and test the theory on complementary goods pricing. Finally, based on counterfactual analysis, forcing separate retailers to sell washers and extended warranties is not necessarily welfare enhancing as cited in the media and previous literature.
Recently there has been significant interest in studying consumer behavior in sponsored search advertising (SSA). Researchers have typically used daily data from search engines containing measures such as average bid, average ad position, total impressions, clicks, and cost for each keyword in the advertiser’s campaign. A variety of random utility models have been estimated using such data and the results have helped researchers explore the factors that drive consumer click and conversion propensities. However, virtually every analysis of this kind has ignored the intraday variation in ad position. We show that estimating random utility models on aggregated (daily) data without accounting for this variation will lead to systematically biased estimates. Specifically, the impact of ad position on click-through rate (CTR) is attenuated and the predicted CTR is higher than the actual CTR. We analytically demonstrate the existence of the bias and show the effect of the bias on the equilibrium of the SSA auction. Using a large data set from a major search engine, we measure the magnitude of bias and quantify the losses suffered by the search engine and an advertiser using aggregate data. The search engine revenue loss can be as high as 11% due to aggregation bias. We also present a few data summarization techniques that can be used by search engines to reduce or eliminate the bias.
This paper investigates the importance of network effects in the demand for ethanol-compatible vehicles and the supply of ethanol fuel. An indirect network effect, or positive feedback loop, arises in this context due to spatially-dependent complementarities in the availability of ethanol fuel and the installed base of ethanol-compatible vehicles. Marketers and social planners are interested in whether these effects exist, and if so, how policy might accelerate adoption of the ethanol fuel standard within a targeted population. To measure these feedback effects, I develop an econometric framework that considers the simultaneous determination of ethanol-compatible vehicle demand and ethanol fuel supply in local markets. The demand-side model considers the automobile purchase decisions of consumers and fleet operators; the supply-side model considers the ethanol market entry decisions of competing fuel retailers. The framework extends extant market entry models by endogenizing the market size shifting fuel retailer profits. I estimate the model using zip code panel data from four states over a nine-year period. The model estimates provide evidence of a network effect. Under typical market conditions, entry of an additional ethanol fuel retailer leads to a 6% increase in the probability of ethanol-compatible vehicle purchase. The entry model estimates imply that the first entrant requires a local installed base of approximately 300 ethanol-compatible vehicles to be profitable. As an application, I demonstrate that subsidizing fuel retailers to offer ethanol in selective geographic markets can be an effective policy to indirectly increase ethanol-compatible vehicle sales.
In the last decade, design innovation has gained increasing prominence in the marketplace, with a growing number of firms innovating not only through technology but also through novel product forms (i.e., design). However, while the effect of technological innovation on product sales is a heavily studied topic, a defining theory of how design innovation influences product sales is still missing. This paper provides demand- and supply-side theories to formulate a set of coherent hypotheses about the effect of design innovativeness, i.e., the degree of novelty in a product’s design, on sales' evolution over time. The hypotheses are tested in two different samples. In the first, car models introduced in the United States from 1978 to 2006 (for a total of 2,757 model-year data) are analyzed. In the second, motorcycle models introduced in the United States from 1980 to 2008 (for a total of 2,847 model-year observations) are analyzed. I find that design innovativeness diminishes initial sales' status but increases sales' growth rates. Furthermore, design and technological innovativeness have a negative interaction effect on sales' initial status, but a positive effect on sales' growth rates. Finally, brand strength and brand advertising expenditures worsen the negative effect of design innovativeness on initial sales' status, but boost its positive effect on sales' growth rates.
Multipart tariffs are widely favored within service industries as an efficient means of mapping prices to differential levels of consumer demand. Whether they benefit consumers, however, is far less clear as they pose individuals with a potentially difficult task of dynamically allocating usage over the course of each billing cycle. In this paper we explore this welfare issue by examining the ability of individuals to optimally allocate consumption over time in a stylized cellular-phone usage task for which there exists a known optimal dynamic utilization policy. Actual call behavior over time is modeled using a dynamic choice model that allows decision makers to both discount the future (be myopic) and be subject to random errors when making call decisions. Our analysis provides a “half empty, half full” view of intuitive optimality. Participants rapidly learn to exhibit farsightedness, yet learning is incomplete with some level of allocation errors persisting even after repeated experience. We also find evidence for an asymmetric effect in which participants who are exogenously switched from a low (high) to high (low) allowance plan make more (fewer) errors in the new plan. The effect persists even when participants make their own plan choices. Finally, interventions that provide usage information to help participants eradicate errors have limited effectiveness.
What is the role that color plays in consumers’ perception of the gist of ads during the increasingly brief and blurred exposures in practice? Two studies address this question. The first study manipulates the level of blur of the exposure and the presence or absence of color in the ad image, during exposures that lasted 100 milliseconds (msec). It reveals a buffer effect of color: color contributes little to gist perception when sufficient visual detail is available and ads are typical, but color enables consumers to continue to perceive the gist of ads accurately when the exposure is blurred. The second study finds that color inversion of the entire ad deteriorates gist perception, but that color inversion of the background scene does not affect gist perception when the exposure is blurred. This provides evidence that the color composition of the central object in the ad scene plays a key role in protecting the gist perception of advertising under adverse exposure conditions. The underlying mechanism is likely to be cognitive rather than sensory. Implications for advertising theory and design are discussed.
Different objectives such as category demand expansion or market share stealing warrant the use of different marketing instruments. To help brand managers make informed decisions, it is essential that marketing mix models appropriately measure their effects. Random Utility Models that have been applied to this problem might not be adequate because they do not allow the effects of marketing instruments of one brand to spillover to preference for competing alternatives. Additionally, they have the Invariant Proportion of Substitution (IPS) property, which in some situations imposes counter-intuitive restrictions on individual choice behavior. Recognizing that effects of marketing instruments can spill across brands in a category, we propose an alternative choice model that relaxes the IPS property: the cross attributes flexible substitution logit model. We apply the model in two very different empirical settings, i.e., consumer choices of brands of refrigerated yogurt, and prescription-writing choices of physicians in the hyperlipidemia category. In both settings the proposed model provides consistent evidence that certain marketing instruments produce sales gains primarily from growing the category pie, while others produce gains from stealing share. By contrast, the random coefficient logit and generalized nested logit models both predict that gains from all marketing instruments would have similar sources.
The focus of this paper is dual distribution channels in business-to-business markets. We take the perspective of the distributor, and examine how different forms of competition with a manufacturer-owned channel impact distributor opportunism. Next, we consider how the same forms of competition impact the distributor’s end customers. Based on a multi-industry field study of industrial distributors, we highlight the complex processes that characterize dual distribution systems. We show that while competition with a manufacturer-owned channel increases distributor opportunism, it also has the potential to benefit the distributor’s end customers. In addition, although actions taken by a manufacturer to create vertical separation between channels limit competition, such actions also reduce end customer satisfaction.
In many consumption settings (e.g., restaurants), individuals consume products either alone or with their peers (e.g., friends). In this study, we propose a general framework for modeling peer effects by including two new peer effects: the exogenous peer effect (exogenous factors that could change the peer’s behavior) and the peer presence effect (when the peer is present but not consuming). We also include the well known endogenous peer effect. We develop an empirical model that allows us to identify all three effects simultaneously and apply the model to behavioral data from a casino setting. It is a simultaneous equation model with the structural parameters expressed as a function of the ratio of the reduced form parameters. This necessitates the use of the Minimum Expected Loss approach, allowing us to obtain consistent estimates at the individual level. Our data comprise detailed gambling activity for a panel of individuals at a single casino over a two-year period. Our results show that all three types of peer effects exist. These effects vary across individuals and exhibit considerable asymmetry within pairs of peers. We discuss how our results can help managers allocate resources more effectively and policy makers formulate regulatory guidelines with more complete information.
In recent years, customer lifetime value (CLV) has gained increasing importance in both academia and practice. Although many advanced techniques have been proposed, the recency/frequency/monetary value (RFM) segmentation framework, and its related probability models, remain a CLV mainstay. In this article, we demonstrate the deficiency in RFM as a basis for summarizing customer history (data compression), and extend the framework to include clumpiness (C) by a metric-based approach. Our main empirical finding is that C adds to the predictive power, above and beyond RFM and firm marketing action, of both the churn, incidence, and monetary value parts of CLV. Hence, we recommend a significant implementation change: from RFM to RFMC.This work is also motivated by noting that although statistical models based on RFM summaries can fit well in aggregate, their use can lead to significant micro-level (e.g., ranking of customers) prediction errors unless C is captured. A set of detailed empirical studies using data from a large North American retailer, in addition to six companies that vary in their business model: two traditional (e.g., CDNow.com) and four Internet (e.g., Hulu.com), demonstrate that the “clumpiness phenomena” is widely prevalent, and that companies with “bingeable content” have both high potential and high risk segments, previously unseen, but now uncovered because of the new framework: RFM to RFMC.
This series of discussions presents commentaries and a reply on Zhang et al. [Zhang Y, Bradlow ET, Small DS (2015) Predicting customer value using clumpiness: From RFM to RFMC. Marketing Sci. 34(2):195–208].
We analyze two pricing mechanisms for information goods. These mechanisms are selling, where up-front payment allows unrestricted use, and pay-per-use, where payments are tailored to use. We analytically model a market where consumers differ in use frequency and where use on a pay-per-use basis invokes a psychological cost associated with the well known “ticking meter” effect. We demonstrate that pay-per-use yields higher profits in a monopoly provided the associated psychological cost is low. In a duopoly, one firm uses selling and the other uses pay-per-use. Here, in contrast to the monopoly, selling yields higher profits than pay-per-use. We demonstrate that, surprisingly, the profits of both duopolists can increase as the psychological cost associated with pay-per-use increases. Next, we show that uncertainty in consumer use frequency does not affect pay-per-use in a monopoly, but lowers profits from selling. In a duopoly, both the seller and the pay-per-use provider obtain lower profits when use frequency is uncertain. We also analyze how pricing mechanism performance is affected if the firms cannot commit to prices, if the pay-per-use provider offers a two-part tariff, and if consumers are risk-averse.
Current complex dynamic markets are characterized by numerous brands, each with multiple products and price points, and differentiated on a variety of product attributes plus a large number of new product introductions. This study seeks to analyze dynamic pricing paths in a highly complex branded market, consisting of 663 products under 79 brand names of digital cameras. The authors develop a method to classify dynamic pricing strategies and analyze the choice and correlates of observed pricing paths in the introduction and early growth phase of this market. The authors find that, despite numerous recommendations in the literature for skimming or penetration pricing, market pricing dominates in practice. In particular, the authors find five patterns: skimming (20% frequency), penetration (20% frequency), and three variants of market-pricing patterns (60% frequency), where new products are launched at market prices. Skimming pricing launches the new product 16% above the market price and subsequently increases the price relative to the market price. Penetration pricing launches the new product 18% below the market price and subsequently lowers the price relative to the market price. Firms exhibit a mix of these pricing paths across their portfolios. The specific pricing paths correlate with market, firm, and brand characteristics such as competitive intensity, market pioneering, brand reputation, and experience effects. The authors discuss managerial implications.
Firms with a customer-centric structure—an organizational design that aligns each business unit with a distinct customer group—are expected to exhibit superior performance compared to firms that are internally structured. Top executives invoke these customer-centric beliefs when initiating corporate reorganizations. However, a lack of empirical evidence linking these customer-centric structures to better long-term financial performance raises doubts if corporate structure can truly foster customer centricity and better position a firm to satisfy customers and hence exhibit superior performance. The current research addresses this question by using longitudinal data (1998–2010) that links Fortune 500 firms’ corporate-level structure to performance. Utilizing a dueling mediator model with allowance for endogeneity in a firm’s organizational structure choice, the study reveals that a corporate-level customer-centric structure translates to greater customer satisfaction, but simultaneously adds coordinating costs. Further explaining customer-centric structure’s record of mixed success, the benefits of increased customer satisfaction diminish (1) as competitors have already adopted customer-centric structures, (2) in fragmented markets where competitors leave few unique customer needs unaddressed, and (3) in less profitable industries. Ultimately, we show that aligning corporate structure around customers pays off only in specific competitive environments.Data, as supplemental material, are available at http://dx.doi.org/10.1287/mksc.2014.0878.
Blood banks rely on marketing to encourage donors to give blood. Many, if not most, blood banks in the United States are community-based not-for-profit organizations with limited marketing budgets. As a result, blood banks increasingly use novel and inexpensive online media, i.e., paid, owned, and earned (POE) media, in their marketing efforts. We propose a dynamic model to help blood bank marketing managers understand how blood donations can be managed via online POE media. We analytically characterize the optimal forward-looking paid media strategies, taking into account the asymmetric costs related to shortage and excess of blood, as well as the possibility of a cost-free target donation zone. We detail new advertising resource allocation rules for blood banks and show when traditional allocation recommendations do not apply. Additionally, we discover that under certain circumstances, owned/earned media activities hurt the blood bank’s performance, despite being (predominantly) free. We validate our analytical model by using daily donation data from a community-based blood bank and measure the effects of POE media activities on the level of blood donated.
Many video ads are designed to go viral so that the total number of views they receive depends on customers sharing the ads with their friends. This paper explores the relationship between the number of views and how persuasive the ad is at convincing consumers to purchase or to adopt a favorable attitude towards the product. The analysis combines data on the total views of 400 video ads, and crowd-sourced measurement of advertising persuasiveness among 24,000 survey responses. Persuasiveness is measured by randomly exposing half of these consumers to a video ad and half to a similar placebo video ad, and then surveying their attitudes towards the focal product. Relative ad persuasiveness is on average 10% lower for every one million views that the video ad achieves. The exceptions to this pattern were ads that generated views and large numbers of comments, and video ads that attracted comments that mentioned the product by name. Evidence suggests that such ads remained effective because they attracted views due to humor rather than because they were outrageous.
In our paper about optimal reverse pricing mechanisms [Spann M, Zeithammer R, Häubl G (2010) Optimal reverse-pricing mechanisms. Marketing Sci. 29(6):1058–1070] (hereafter, ORPM), some of the mathematical derivations implicitly assume that the name-your-own-price seller interprets the outside-market posted price p differently than the buyers. This note shows that all of the qualitative results in ORPM continue to hold under the more natural assumption of common knowledge that p is the upper bound of wholesale cost. Interestingly, the proofs and algebraic expressions are often simpler than those in ORPM.
Marketing Science greatly benefited from the admirable and fastidious efforts of more than 200 different individuals who provided manuscript reviews last year. Beyond those individuals already recognized on the editorial board, the editor-in-chief and guest editors of Marketing Science are indebted to the many guest editors-in-chief, guest associate editors, and ad hoc reviewers who provided expert counsel and guidance on a voluntary basis. The following list acknowledges the contribution of guest editors-in-chiefs, guest associate editors, and ad hoc reviewers who served from January 1, 2014 to December 31, 2014. Finally, let us not forget to thank the authors. Marketing Science requires and receives outstanding submissions from many leading researchers and prestigious organizations.Preyas S. DesaiDuke University
Media multitasking competes with television advertising for consumers’ attention, but may also facilitate immediate and measurable response to some advertisements. This paper explores whether and how television advertising influences online shopping. We construct a massive data set spanning $3.4 billion in spending by 20 brands, measures of brands’ website traffic and transactions, and ad content measures for 1,224 commercials. We use a quasi-experimental design to estimate whether and how TV advertising influences changes in online shopping within two-minute pre/post windows of time. We use nonadvertising competitors’ online shopping in a difference-in-differences approach to measure the same effects in two-hour windows around the time of the ad. The findings indicate that television advertising does influence online shopping and that advertising content plays a key role. Action-focus content increases direct website traffic and sales. Information-focus and emotion-focus ad content actually reduce website traffic while simultaneously increasing purchases, with a positive net effect on sales for most brands. These results imply that brands seeking to attract multitaskers’ attention and dollars must select their advertising copy carefully.Data, as supplemental material, are available at http://dx.doi.org/10.1287/mksc.2014.0899.
This paper studies the optimal product and pricing decisions in a crowdfunding mechanism by which a project between a creator and many buyers will be realized only if the total funds committed by the buyers reach a specified goal. When the buyers are sufficiently heterogeneous in their product valuations, the creator should offer a line of products with different levels of product quality. Compared to the traditional situation where orders are placed and fulfilled individually, with the crowdfunding mechanism, a product line is more likely than a single product to be optimal and the quality gap between products is smaller. This paper also shows the effect of the crowdfunding mechanism on pricing dynamics over time. Together, these results underscore the substantial influence of the emerging crowdfunding mechanisms on common marketing decisions.
Extant research on choice designs in marketing focuses on the construction of efficient homogeneous designs where all respondents get the same design. Recently marketing scholars proposed the construction of efficient heterogeneous designs where different respondents or groups of respondents get different subdesigns, and demonstrated substantial efficiency gain when such heterogeneous designs are employed. A significant hurdle in the widespread adoption of heterogeneous designs is the high computation cost, even when the number of subdesigns contained in the heterogeneous design is restricted to be small. In this paper we propose a new approach for the construction of efficient heterogeneous choice designs. In contrast to extant approaches that are based on an exact design framework where it is computationally prohibitive to do an exhaustive search to find a globally optimal design, our proposed approach is based on the continuous design framework where well-established mathematical theories can be leveraged for quick identification of a globally optimal design. The proposed approach makes it feasible to generate a highly efficient choice design that is completely heterogeneous—a unique subdesign for each individual respondent in the choice experiment. The proposed approach is the first in the marketing literature to find a completely heterogeneous choice design with assured high global design efficiency using the continuous design framework. Results from simulation and empirical studies demonstrate superior performance of the proposed approach over extant approaches in constructing efficient heterogeneous choice designs.Data, as supplemental material, are available at http://dx.doi.org/10.1287/mksc.2014.0897.
Many firms operate customer communities online. This is motivated by the belief that customers who join the community become more engaged with the firm and/or its products, and as a result, increase their economic activity with the firm. We describe this potential economic benefit as “social dollars.” This paper contributes evidence for the existence and source of social dollars using data from a multichannel entertainment products retailer that launched a customer community online. We find a significant increase in customer expenditures attributable to customers joining the firm’s community. While self-selection is a concern with field data, we rule out multiple alternative explanations. Social dollars persist over the time period observed and arose primarily in the online channel. To assess the source of the social dollar, we hypothesize and test whether it is moderated by participation behaviors conceptually linked to common attributes of customer communities. Our results reveal that posters (versus lurkers) of community content and those with more (versus fewer) social ties in the community generated more (fewer) social dollars. We found a null effect for our measure of the informational advantage expected to accrue to products that differentially benefit from content posted by like-minded community members. This overall pattern of results suggests a stronger social than informational source of economic benefits for firm operators of customer communities. Several implications for firms considering investments in and/or managing online customer communities are discussed.
We investigate the causal effect of position in search engine advertising listings on outcomes such as click-through rates and sales orders. Because positions are determined through an auction, there are significant selection issues in measuring position effects. A simple mean comparison of outcomes at two positions is likely to be biased due to these selection issues. Additionally, experimentation is rendered difficult in this situation by competitors’ bidding behavior, which induces selection biases that cannot be eliminated by randomizing the bids for the focal advertiser. Econometric approaches to address the selection are rendered infeasible due to the difficulty of finding suitable instruments in this context. We show that a regression discontinuity (RD) approach is feasible to measure causal effects in this important context. We apply the approach to a large and unique data set of daily observations containing information on a focal advertiser as well as its major competitors. Our RD estimates demonstrate that there are significant selection biases in the more naive estimates. While a mean comparison of outcomes across positions would indicate very large position effects, we find that our RD estimates of these effects are much smaller, and exist only in some of the positions. We further investigate moderators of these effects. Position effects are stronger when the advertiser is smaller, and when the consumer has low prior experience with the keyword for the advertiser. They are weaker when the keyword phrase has specific brand or product information, when the ad copy is more specific as in exact matching options, and on weekends compared to weekdays.
The notion of peer influence in new product adoption or trial is well accepted. We propose that peer influence may affect repeat behavior as well, though the process and source of influence are likely to differ between trial and repeat. Our analysis of the acceptance of a risky prescription drug by physicians provides three novel findings. First, there is evidence of contagion not only in trial but also in repeat. Second, who is most influential varies across stages. Physicians with high centrality in the discussion and referral network and with high prescription volume are influential in trial but not repeat. In contrast, immediate colleagues, few of whom are nominated as a discussion or referral partner, are influential in both trial and repeat. Third, who is most influenceable also varies across stages. For trial, it is physicians who do not consider themselves to be opinion leaders, whereas for repeat, it is those located towards the middle of the status distribution as measured by network centrality. The pattern of results is consistent with informational social influence reducing risk in trial and normative social influence increasing conformity in repeat.
Digitization of content is changing how consumers and firms use purchase and rental markets. Low transaction costs make accessing content easier for consumers. Digital technology enables firms to create nondurable “rental” versions of their content and to restrict content to the purchasing consumer, effectively shutting down resale markets. To empirically analyze the interaction of purchase and rental markets, I design a preference measurement tool to recover consumers’ intertemporal preferences through current-period choices alone. I then use these preferences to solve for a dynamic equilibrium between consumers and the firm. In the context of the online home video market, I find that when the firm is able to commit to holding prices fixed forever, providing content through the purchase market alone is sufficient. However, when the firm is unable to commit, it should serve both purchase and rental markets. Canonical theory models would predict exclusive rentals, but the purchase option enables indirect price discrimination in practice. I also find that when consumers place a premium on accessing new content, they are less likely to intertemporally substitute, thereby increasing the firm’s pricing power. Consistent with theory, commitment to future prices increases profits considerably. This finding supports the rigid pricing structure of such retailers as Apple, despite studios’ push toward more pricing flexibility.
Addiction creates an intertemporal link between a consumer’s past and present decisions, altering their responsiveness to price changes relative to nonaddictive products. We construct a dynamic model of rational addiction and endogenous consumption to investigate how consumers respond to policy interventions that aim to reduce purchases of cigarettes. We find that, on average, the category elasticity is about 35% higher when the model correctly accounts for addiction. However, some policies spur substitution from more expensive single packs to less expensive cartons of cigarettes, resulting in higher overall consumption for some consumers.
In this editorial accompanying the Special Section on Marketing Science in Emerging Markets (MSEM), we describe how research on emerging markets can contribute to richer theoretical and substantive understanding of markets and marketing. Such research can also aid in providing managerial guidance on how to operate in emerging markets. We conclude with a description of the selection and review process for the special section and an overview of the four papers being published.
There are few marketing studies of social learning about new technologies in low-income countries. This paper examines how learning through opinion leaders and social networks influences demand for nontraditional cookstoves—a technology with important health and environmental consequences for developing country populations. We conduct marketing interventions in rural Bangladesh to assess how stove adoption decisions respond to (a) learning the adoption choices of locally identified “opinion leaders” and (b) learning about stove attributes and performance through social networks. We find that households generally draw negative inferences about stoves through social learning and that social learning is more important for stoves with less evident benefits. In an institutional environment where consumers are distrustful of new products and brands, consumers appear to rely on their networks more to learn about negative product attributes. Overall, our findings imply that external information and marketing campaigns can induce initial adoption and experiential learning about unfamiliar technologies, but sustained use ultimately requires that new technologies match local preferences.
Firms invest in technology to increase productivity. Yet in emerging markets, where a culture of informality is widespread, information technology (IT) investments leading to greater transparency can impose a cost through higher taxes and the need for regulatory compliance. The tendency of firms to avoid productivity-enhancing technologies and remain small to avoid transparency has been dubbed the “Peter Pan Syndrome.” We examine whether firms make the trade-off between productivity and transparency by examining IT adoption in the Indian retail sector. We find that computer technology adoption is lower when firms are motivated to avoid transparency. Specifically, technology adoption is lower when there is greater corruption, but higher when there is better enforcement and auditing. So, firms have a higher productivity gain threshold to adopt computers in corrupt business environments that suffer from patchy and variable enforcement of the tax laws. Not accounting for this motivation to hide from the formal sector underestimates productivity gains from computer adoption. Thus, in addition to their direct effects on the economy, enforcement, auditing, and corruption can have indirect effects through their negative impact on adoption of productivity-enhancing technologies that also increase operational transparency.
In this paper, we untangle the searchable and experiential dimensions of quality responses to entry by counterfeiters in emerging markets with weak intellectual property rights. Our theoretical framework analyzes market equilibria under competition from counterfeiting as well as under monopoly branding. A key theoretical prediction is that emerging markets can be self-corrective with respect to counterfeiting issues in the following sense: First, counterfeiters can earn positive profits by pooling with authentic brands only when consumers have good faith in the market (i.e., they believe there is low probability that any product is a counterfeit). When the proportion of counterfeits in the market exceeds a cutoff value, brands invest in self-differentiation from the competitive-fringe counterfeiters. Second, to attain a separating equilibrium with counterfeiters, branded incumbents upgrade the searchable quality (e.g., appearance) of their products more and improve the experiential quality (e.g., functionality) less compared with monopoly equilibrium. However, in the pooling equilibrium with sporadic counterfeits, authentic firms instead may invest in experiential quality to attract more of the expert consumers who are well versed in quality. This prediction uncovers the nature of product differentiation in the searchable dimension and helps with analyzing real-world innovation strategies employed by authentic firms in response to entries by counterfeit entities. In addition, welfare analysis hints at a nonlinear relationship between social welfare and intellectual property enforcement.
Consumers in many emerging markets exhibit a pronounced preference for western and global brands, while domestic brands are often associated with a cheap but low quality image. Frustrated with the negative country-of-origin (COO) stereotype imposed on them, many domestic brands from emerging markets follow a variety of approaches to disguise their COO and pretend to be foreign. This paper studies the strategic aspects of this phenomenon. We consider an experience good market where consumers learn about each brand’s quality, using its COO as a prior. Each firm can invest in product quality or COO dissociation; the former generates better quality signals while the latter simply masks the firm’s COO identity. The analysis reveals a few main insights. Sharing a reputation leads to a common good problem where firms free-ride on each other’s quality investments. As a high quality firm dissociates itself from the stereotype, it ceases to contribute to the COO image but also prevents its low quality peers from free-riding on it. This incurs a negative direct effect but a positive strategic effect on the group image. Consequently, a country’s COO image may actually improve when more high quality brands shun their identities and pretend to be foreign. In equilibrium, COO image improves monotonically as firms become more efficient in providing quality. However, the prevalence of COO dissociation may first increase then decrease. We discuss the implications of these results for emerging market brands as well as policy makers.
Research has shown that advertising assets and R&D (research and development) assets increase shareholder value. Although one might conclude that their impacts on bankruptcy risk are merely the inverse of their impacts on shareholder value, we argue otherwise and show that the differences hinge on the fact that shareholder value is a function of expected cash flows from all future periods, whereas bankruptcy risk is a function of expected cash flow from only the next period. We show that current market turbulence moderates the impacts of advertising assets and R&D assets on expected cash flow from the next period but not on expected cash flows from more distant future periods. Therefore, market turbulence moderates the impacts of advertising assets and R&D assets on bankruptcy risk but not shareholder value. Market stability increases the impact of advertising assets on reducing bankruptcy risk, whereas market turbulence increases the impact of R&D assets on reducing bankruptcy risk. Using a data set of more than 1,000 firms covering three decades, we find support for our hypotheses. Out-of-sample validation indicates that bankruptcy prediction performance improves when including marketing variables in addition to the usual financial predictors.Data, as supplemental material, are available at http://dx.doi.org/10.1287/mksc.2015.0913.
Networks and the embedded relationships are critical determinants of how people communicate and form beliefs. The explosion of social media has significantly increased the scope and impact of social learning among consumers. This paper studies observational learning in networks of friends versus strangers. A consumer decides whether to adopt a product after receiving a private signal about product quality and observing the actions of others. The preference for the product has greater heterogeneity in the stranger-network than in the friend-network. We show that when the network is small, observing friends’ actions helps the consumer make more accurate inferences about quality. As the network grows, however, the stranger-network becomes more effective. Underlying these results are two competing effects of network heterogeneity on social learning. These are the individual preference effect, which allows one to make a better quality judgment when the preference element of past actions is more certain, and the social conforming effect, wherein private signals are underused in quality judgment as people follow others’ actions. We find cascading is more likely to occur in the friend-network than in the stranger-network. For a high-quality firm, the stranger-network generates greater sales than the friend-network when the network size is sufficiently large or the private signal is sufficiently accurate. We also examine the existence of experts and firms using advertising to influence consumers. Finally, we show how networks that are highly homogeneous or heterogeneous could impede observational learning.
This research examines how prepurchase information that reduces consumer uncertainty about a product or service can affect consumer decisions to reverse an initial product purchase or service enrollment decision. One belief commonly held by retailers is that provision of greater amounts of information before the purchase reduces decision reversals. We provide theory and evidence showing conditions under which uncertainty-reducing information provided before the purchase decision can actually increase the number of decision reversals. Predictions generated from an analytical model of consumer behavior incorporating behavioral theory of reference-dependence are complemented by empirical evidence from both a controlled behavioral experiment and econometric analysis of archival data. Combined, the theory and evidence suggest that managers should be aware that their information provision decisions taken to reduce decision reversals may actually increase them.Data, as supplemental material, are available at http://dx.doi.org/10.1287/mksc.2015.0906.
Consumer search activities can be endogenously determined by the ad positions in sponsored search advertising. We model how advertisers compete for ad positions in sponsored listings and, conditional on the list of sponsored ads, how online consumers search for information and make purchase decisions. On the consumer side, assuming that users browse information from top to bottom and adopt a sequential search strategy, we develop a two-stage model of consumer search (whether to click and whether to stop the search) that extends the standard sequential search framework in economics literature. On the advertiser side, it is very difficult to fully specify the optimal strategies of advertisers because the equilibrium outcome depends on variables that researchers do not observe. As we have an incomplete model of advertiser competition, we propose using the necessary condition that, at equilibrium, no advertiser will find another available ad position more valuable than the one it has chosen. Using a data set obtained from a search engine, we find that consumers can be classified into two segments that exhibit distinct search behaviors. For advertisers, the value of search advertising comes primarily from terminal clicks, which represent the last link (including organic results) clicked by an online user. We also demonstrate that the value of ad positions depends not only on the identities and positions of the advertisers in sponsored listings but also the composition of online consumers who exhibit distinct search behaviors.
Firms are increasingly engaging with customers on social media. Despite this heightened interest, guidance for effective engagement is lacking. In this study, we investigate customers’ compliments and complaints and firms’ service interventions on social media. We develop a dynamic choice model that explicitly accounts for the evolutions of both customers’ voicing decisions and their relationships with the firm. Voices are driven by both the customers’ underlying relationships and other factors such as redress seeking. We estimate the model using a unique data set of customer voices and service interventions on Twitter. We find that redress seeking is a major driver of customer complaints, and although service intervention improves relationships, it also encourages more complaints later. Because of this dual effect, firms are likely to underestimate the returns on service intervention if measured using only voices. Furthermore, we find an “error-correction” effect in certain situations, where customers compliment or complain when others voice the opposite opinions. Finally, we characterize the distinct voicing tendencies in different relationship states, and show that uncovering the underlying relationship states enables effective targeting. We are among the first to analyze individual customer level voice dynamics and to evaluate the effects of service intervention on social media.
Bucket-based price discrimination is a unique price format that involves monthly subscription fees and instantaneous quotas (the number of rental products that can be checked out). We propose an empirical model in which consumers make dynamic purchase decisions under consumption uncertainty, accounting for the constraints imposed by the instantaneous quota. Applying the model to an online DVD rental data set, we find that (1) consumers incur a large disutility (∼$8) from stockout (i.e., unmet consumption needs); (2) such a disutility drives consumers' overpurchase of the service quota as a way to avoid potential stockout situations; and (3) the dynamics of overpurchase are driven by the interplay between trends in consumption needs and the magnitude of consumers' plan-switching costs. We run counterfactual exercises to better understand how the instantaneous quota and stockout risk affect consumers' consumption rates, purchase decisions, and firm profitability. We find that the instantaneous quota induces a greater stockout compared with a monthly quota. We further demonstrate that the company should recognize the drivers of the dynamics in overpurchase to balance short- and long-term profitability—for example, by offering targeted discounts to customers with excess overpurchase.
Firms track consumers’ shopping behaviors in their online stores to provide individually personalized banners through a method called retargeting. We use data from two large-scale field experiments and two lab experiments to show that, although personalization can substantially enhance banner effectiveness, its impact hinges on its interplay with timing and placement factors. First, personalization increases click-through especially at an early information state of the purchase decision process. Here, banners with a high degree of content personalization (DCP) are most effective when a consumer has just visited the advertiser’s online store, but quickly lose effectiveness as time passes since that last visit. We call this phenomenon overpersonalization. Medium DCP banners, on the other hand, are initially less effective, but more persistent, so that they outperform high DCP banners over time. Second, personalization increases click-through irrespective of whether banners appear on motive congruent or incongruent display websites. In terms of view-through, however, personalization increases ad effectiveness only on motive congruent websites, but decreases it on incongruent websites. We demonstrate in the lab how perceptions of ad informativeness and intrusiveness drive these results depending on consumers’ experiential or goal-directed Web browsing modes.
In managing service failures such as stockouts, most research has emphasized preventive mechanisms, whereas stockout recovery mechanisms have been largely ignored. We propose and examine a failure-recovery mechanism (i.e., contractual stockout recovery) in the presence of demand uncertainty and compare it with failure-prevention mechanisms in a dyadic distribution channel.We find that stockout recovery mechanisms can improve channel profitability under certain conditions. More importantly, we find that stockout recovery may outperform stockout prevention mechanisms such as return policy and vendor managed inventory in improving manufacturer and channel profitability. This is because stockout recovery reduces channel-wide stockout risks and allows benefits from the reduced risks to be shared between the manufacturer and the retailer, helping alleviate double marginalization. Although return policy also reduces stockout risks, it does so by increasing inventory risks in the channel without reducing channel exposure to demand uncertainty. Thus, our research suggests that stockout recovery can be an effective alternative in managing stockouts to those common methods of stockout prevention mechanisms.
Beverage consumption occurs many times a day in response to short-run needs that fluctuate. We develop a model in which consumers are heterogeneous in self-regulating consumption by balancing short-run needs (e.g., hydration and mood) with long-term goals (e.g., health). The model has two novel features: (1) utility depends on the match between occasion-specific needs and product attributes, and (2) dynamics of consumption and stockpiling are at the level of product attributes. We estimate the model using unique intraday beverage consumption, activity, and psychological needs data. We find that only a third of individuals do not self-regulate. Of the two-thirds who self-regulate, over 40% self-regulate adaptively based on past choice, whereas 25% self-regulate both adaptively and anticipating future needs. Our attribute–need match model enables us to assess unmet demand for new products with attributes that match co-occurring occasion-specific needs. Specifically, we find that a product satisfying a combination of “health-hydrating” needs expands overall beverage consumption by as much as 5%. Our framework of modeling heterogeneity in self-regulation by balancing short-run needs with long-term goals is more broadly applicable in contexts where situational needs vary, and long-term effects are gradual and hard to discern (e.g., nutrition, smoking, and preventive health care).
Building on the structural two-sided matching model, we develop a framework to study the sourcing market in the context of marketing firms matching with manufacturers. Both sides prefer partners that could generate significant values with better sourcing process abilities. Moreover, experienced manufacturers are preferred by the branded marketing firms who may even be willing to compensate the matching intermediary more for facilitating that preference. Empirical research, measuring the values of such matching and the intermediary’s pricing (through commission) on observed marketing firms’ characteristics and the low-cost manufacturers and the deals that result, is problematic, when some of the characteristics are only partially observed and the matching is endogenous. With the matching model, we can control for endogenous matching. We find evidence of positive assortative matching of pairs’ size on both sides of the market. We also find that manufacturers’ location and tenure, and whether the marketing firms are listed, are important factors in identifying the preferred matching partners and the related ranking. Without controlling for endogenous matching, the intermediary’s pricing equation estimates are biased, especially for the marketing firms that specialize in luxury products.
This paper investigates the economic value of online reviews for consumers and restaurants. We use a data set from Dianping.com, a leading Chinese website providing user-generated reviews, to study how consumers learn, from reading online reviews, the quality and cost of restaurant dining. We propose a learning model with three novel features: (1) different reviews offer different informational value to different types of consumers; (2) consumers learn their own preferences, and not the distribution of preferences among the entire population, for multiple product attributes; and (3) consumers update not only the expectation but also the variance of their preferences. Based on estimation results, we conduct a series of counterfactual experiments and find that the value from Dianping is about 7 CNY for each user, and about 8.6 CNY from each user for the reviewed restaurants in this study. The majority of the value comes from reviews on restaurant quality, and contextual comments are more valuable than numerical ratings in reviews.
The window between a film’s theatrical and video releases has been steadily declining with some studios now testing day-and-date strategies (i.e., when a film is released across multiple channels at once). We present a model of consumer choice that examines trade-offs between substitutable products (theatrical and video forms), the possibility of purchasing both alternatives, a congestion externality affecting consumption at theaters with heterogeneous consumer groups, and a decay in the quality of the content over time. Our model permits a normative study of the impact of shorter release windows (zero–three months) for which there is a scarcity of relevant data. We characterize the market conditions under which a studio makes video release time and price selections indicative of direct-to-video, day-and-date, and delayed video release tactics. During seasons of peak congestion, we establish that day-and-date strategies are optimal for high-quality films with high content durability (i.e., films whose content tends to lead consumers to purchase both alternatives) whereas prices are set to perfectly segment the consumer market for films with low content durability. We find that lower congestion effects provide studios with incentives to delay release and price the video to induce multiple purchasing behavior for films with higher content durability. However, an increase in congestion effects can, in certain cases, actually lead to higher studio profitability. We also show that, at the lower range of quality, an increase in movie quality should often be accompanied by a later video release time. Surprisingly, however, we observe the opposite result at the upper range of movie quality: an increase in quality can justify an earlier release of the video.
Using the University of Texas at Dallas database on publications in the top 24 business journals, we examine the evolution of Marketing Science as reflected by participation of faculty from top ranked business schools on one hand, and diversity on the other hand, as evidenced by contributions from different countries and from faculty of a wider set of schools. We show that faculty from top-ranked business schools have always published in Marketing Science, and continue to do so. We also show that the variety of schools with authors who publish in Marketing Science has increased, and that much of this expansion has occurred outside of North America. This international expansion appears to be driven by collaborations between authors in North America and those in other areas. One of the factors that may be fueling the increase in the variety of schools with authors publishing in Marketing Science is an increased tendency for collaboration by three or more authors. In general, Marketing Science has remained an outlet for authors from top schools, and has also become a place where authors from a much broader array of schools, especially those outside the United States, can publish.Data, as supplemental material, are available at http://dx.doi.org/10.1287/mksc.2015.0957.
I introduce the special section highlighting the background of the competition and the work of the finalists in the 2013–2014 Gary L. Lilien ISMS-MSI Practice Prize Competition, showcasing the best applications of rigor and relevance by marketing scientists in working with practical problems. The winning paper is by a team who developed an innovative pricing tool to enable an electric utility operating in Germany to acquire customers on online price comparison sites while optimizing various metrics of interest. The other two finalists comprise a team that developed an integrated marketing model to address multiple business objectives of the Georgia Aquarium and a team that developed a marketing science model of evaluation and purchase intentions incorporating customer emotions to test advertisement effectiveness for Kmart Australia.
Market liberalization of the German household electricity market has led to an excessive number of competitors (1,150 electricity providers) and volatile price dynamics on price comparison sites. To date, providers that are struggling to achieve a top ranking on price comparison sites do not appear to implement a consistent or elaborate strategy for attracting customers. We developed a pricing tool, Electricity Contract Optimization (ECO), that addresses this highly competitive market situation by integrating various available data sources, such as data from price comparison sites, demographic data, and regional sales or cost data. ECO sets regionally varying one-time bonuses to attract new customers on price comparison sites with the goal of optimizing sales and profit targets or optimally allocating sales budgets. Based on two field experiments, we demonstrate that ECO’s optimization procedure reduces ENTEGA yearly sales costs for new customer business, on average, by 35% relative to previously used pricing heuristics. ENTEGA uses ECO monthly to analyze different scenarios or to set prices and one-time bonuses on price comparison sites.Data, as supplemental material, are available at http://dx.doi.org/10.1287/mksc.2015.0943.
Georgia Aquarium (GA), a non-profit organization, is the world’s second largest aquarium (by water volume) and is among the most popular tourist destinations in the United States. Recently GA management has observed that the organization’s growth trajectory is stagnating. While other aquariums face a similar trend, GA wants to be proactive and reverse this direction. They face a multifaceted business challenge involving four conflicting objectives: (1) How to increase revenues without increasing ticket prices; (2) how to increase attendance without compromising visitor satisfaction; (3) how to increase the impact of media investments without spending more; (4) how to attract customers with long-term value potential. To address these challenges, we developed an integrated approach consisting of multiple marketing science models including Data Envelopment Analysis, Competition Analysis, Spatial Analysis, Media Optimization Analysis, and Pass Holder Lifetime Net Revenue Analysis. Based on the findings of our analyses and the parameter estimates of our models, GA proceeded with a field implementation to validate our suggestions. As a result, they realized a 10% increase in attendance and a 12% increase in revenue in 2013, thereby enhancing their bottom line and growth.Data, as supplemental material, are available at http://dx.doi.org/10.1287/mksc.2015.0932.
This paper addresses the repositioning of Kmart Australia in 2011. It shows how by calibrating emotional as well as cognitive reactions and estimating their impact on purchase intentions, Kmart was able to focus its communications, improving market share. We measure nine key emotions, ranging from surprise to anger. Including these emotions significantly improves our model for likelihood to choose a store. Measuring emotions enabled Kmart’s advertising agency to create a television commercial that tapped into the specific emotions that most strongly predict the likelihood to choose a store; that is, the model drove the development of the advertising creative. The resulting television commercial tested well and was effective when launched. At the individual level, cognitions and emotions changed dramatically. At the aggregate level, an econometric model showed that store visits were significantly enhanced. Kmart’s EBIT (earnings before interest and tax) increased by 30%, whereas Kmart’s main rival had almost no EBIT growth, despite vigorous attempts to counter Kmart’s campaign. One of our key contributions is to incorporate emotions into marketing science models of evaluation and purchase intentions. We also provide a new methodology to measure emotions. The approach enables marketing science to participate in the design of marketing stimuli, rather than just testing preexisting ones.Data, as supplemental material, are available at http://dx.doi.org/10.1287/mksc.2015.0954.
This paper investigates early stage “modern” grocery retail adoption in an emerging market using primary household-level panel data on grocery purchases in India’s largest city, Mumbai. Specifically, we seek insight on which socioeconomic class is more likely to adopt, and why. We model adoption as a two-stage process of modern retail choice followed by category expenditures within a shopping trip. We find a nonmonotonic (V-shaped) relationship between socioeconomic class and preferences for modern retail; specifically, modern retail spending and relative preference are greater among the upper and lower middle classes, relative to the middle middle class. Upper middle class preference of modern retail is driven by credit card acceptance, shorter store distance (relative to other segments), and higher vehicle ownership; whereas lower prices and low travel costs drive the preferences of the lower middle class. Modern retail is preferred more for branded and less for perishable categories. Interestingly, the lower middle class share of modern grocery retail’s revenues is largest, and this share is projected to grow as prices fall and store density increases. To address concerns of endogeneity and generalizability, we replicate the key results with a “conjoint” type study with exogenous variation in price and distance in two cities—Mumbai and Bangalore. We discuss implications for targeting and public policy in emerging markets.
A vexing challenge when using the utility-maximization framework to estimate consumers’ decisions on which set of goods to purchase and how much quantity to buy is obtaining a functional form of the utility that satisfies three criteria: tractability, flexibility, and global regularity. Flexibility refers to the ability of a utility function to impose minimal prior restrictions on demand elasticities. Global regularity refers to the ability of a utility function to satisfy regularity properties required by economic theory in the entire feasible space of variables. The tractable utility functions used so far are either inflexible, which could yield inaccurate estimates of underlying elasticities, or do not satisfy global regularity, which can result in invalid expressions of likelihood and invalid policy simulations. I tackle this problem by deriving necessary and sufficient conditions for global regularity of Basic Translog utility. Using simulated and scanner data, I show that the proposed demand system yields better model fit, more accurately captures underlying elasticities, and yields substantially different results in counterfactuals compared to alternatives used in prior literature. Specifically, unlike the alternatives used so far, the proposed demand system allows for complementarities between goods, and more accurately captures the extent of their inferiority, the extent of their substitutability, and asymmetries in cross price effects.
We study the pricing decision for a monopolist launching a new innovation. At the time of launch, we assume that the monopolist has incomplete information about the true demand curve. Despite the lack of objective information the firm must set a retail price to maximize total profits. To model this environment, we develop a novel two-period non-Bayesian framework where the monopolist sets the price in each period based only on a nonparametric set of all feasible demand curves. Optimal prices are dynamic as prices in any period allow the firm to learn about demand and improve future pricing decisions. Our main results show that the direction of dynamic introductory prices (versus static prices) depends on the type of heterogeneity in the market. We find that (1) when consumers have homogeneous preferences, introductory dynamic price is higher than the static price; (2) when consumers have heterogeneous preferences and the monopolist has no ex ante information, the introductory dynamic price is the same as the static price; and (3) when consumers have heterogeneous preferences and the monopolist has ex ante information, the introductory dynamic price is lower than the static price. Furthermore, the degree of this initial reduction increases with the amount of heterogeneity in the ex ante information.
In keyword search advertising, many advertisers operate on a limited budget. Yet how limited budgets affect keyword search advertising has not been extensively studied. This paper offers an analysis of the generalized second-price auction with budget constraints. We find that the budget constraint may induce advertisers to raise their bids to the highest possible amount for two different reasons, i.e., to accelerate the elimination of the budget-constrained competitor, and to reduce their own advertising cost. Thus, in contrast to the current literature, our analysis shows that both budget-constrained and unconstrained advertisers could bid more than their own valuation. We further extend the model to consider dynamic bidding and budget-setting decisions.
This paper examines position auctions with budget-constrained advertisers, a dominant bidding environment used by publishers to allocate positions in online advertising. Budget constraints play a crucial role in equilibrium bidding by inducing advertisers to strategically deplete a higher-ranked advertiser’s budget to gain in rank. This strategic consideration has consequences for the advertisers’ profits and the publisher’s revenue. An advertiser’s profit can strictly decrease with her budget when competition for an advertising space (e.g., a keyword) is intense. The publisher’s revenue can also strictly decrease when an increase in the higher-ranked advertiser’s budget induces the lower-ranked rival to reduce her bid, due to her inability to deplete the higher-ranked advertiser’s budget. Several managerial implications for advertisers and publishers are discussed.
Advertising networks have recently played an increasingly important role in the online advertising market. Critical to the success of an advertising network are two mechanisms: an allocation mechanism that efficiently matches advertisers with publishers and a pricing scheme that maximally extracts surplus from the matches. In this paper, we quantify the value and investigate the determinants of a successful advertiser-publisher match, using data from Taobao’s advertising network. A counterfactual experiment reveals that the platform’s profit under a decentralized allocation mechanism is close to the profit level when the platform centrally assigns the matching under perfect platform knowledge of matching values. In another counterfactual experiment, we explore the effect of platform technology and revenue model on the strategic choice of the pricing schemes of list price versus generalized second price (GSP) auction pricing. We find that platforms that profit from the advertiser side may have less incentive to adopt GSP auction than platforms that profit from the publisher side.
Marketing Science is in a very healthy state as the premier journal for quantitative research in marketing. Since its inception, it has led the way in bringing novel and innovative methodologies and expanding into new substantive areas of inquiry. The journal is now at the cusp of its next stage of creativity and innovation. I outline new research possibilities due to big data, behavioral field studies, and managerial interest in substantive areas such as health, sustainability, emerging markets, innovation, and entrepreneurship. As quantitative marketing’s leading journal, Marketing Science should aid the field in the efficient production of in-depth, valid, current, and relevant knowledge across the breadth of the discipline. To this end, I will actively manage incentives for exploitation and deepening of existing competencies in established areas while supporting exploration and broadening into newer, riskier topics at Marketing Science. To increase the field’s overall efficiency of knowledge production, I suggest a lexicographic approach to reviewing where the incremental contribution threshold is primary and demands on quality of execution be driven by what is needed for proving the validity of the incremental contribution claims.
Of the many proposals to reverse the obesity epidemic, the most contentious is the use of price-based interventions such as the fat tax. Previous investigations of the efficacy of such initiatives in altering consumption behavior yielded contradictory findings. In this article, we use six years of point-of-sale scanner data for milk from a sample of over 1,700 supermarkets across the United States to investigate the potential of small price incentives for inducing substitution of healthier alternatives. We exploit a pricing pattern particular to milk in the United States, whereby prices in some geographical regions are flat across whole, 2%, 1%, and skim milk; whereas in other regions they are decreasing with the fat content level. The prevailing price structure is determined at a chain and regional level, and is independent of local demand conditions. This exogenous variation in price structure provides a quasi-experimental set-up to analyze the impact of small price differences on substitution across fat content. We use detailed demographics to evaluate price sensitivity and substitution patterns for different socioeconomic groups. Results show that small price differences are highly effective in inducing substitution to lower calorie options. The impact is highest for low-income households who are also most at risk for obesity. Our results suggest that a selective taxation mechanism that lowers the relative prices of healthier options, such that those price changes are reflected in shelf prices at the point-of-purchase, can serve as an effective health policy tool in the efforts to control obesity.Data, as supplemental material, are available at http://dx.doi.org/10.1287/mksc.2015.0917.Press Release
Beauty contests are auction mechanisms used to buy or sell differentiated products where the auctioneer does not specify a decision rule to pick the winning bidder. Beauty contests are widely used in procuring welfare-to-work projects, freelance services, selling online ads, real estate transactions, and hiring, dating/marriage decisions. Unlike price-only auctions, beauty contests have no closed-form bidding strategies and suffer from nonmultiplicatively separable unobserved auction heterogeneity, which makes their estimation challenging. To address these challenges, we formulate beauty contests as incomplete information games and present a two-step estimator. A key contribution of our method is its ability to account for common-knowledge auction-specific unobservables using finite unobserved types. We show that unobserved auction types and distributions of bids are nonparametrically identified and recoverable in the first step using a nonparametric Expectation-Maximization (EM)-like algorithm, and that these can then be used in the second step to recover cost distributions. We present an application of our method in the online freelancing context. We find that seller margins in this marketplace are around 15% of the bid, and that not accounting for unobserved heterogeneity can significantly bias cost estimates in this setting. Based on our estimates, we run counterfactual simulations and provide guidelines to managers of freelance firms.Data, as supplemental material, are available at http://dx.doi.org/10.1287/mksc.2015.0929.
Countermarketing, or efforts to reduce consumption of certain products, has become common in categories such as tobacco, junk food, fossil fuels, and furs. Countermarketing has a particularly long history in the tobacco industry. Efforts to reduce smoking have included excise taxes that increase the cost of consumption, smoke-free restrictions that make consumption less convenient, and antismoking advertising campaigns that highlight the dangers of tobacco use. This article presents an analysis of the relative effectiveness of these different strategies. We find that cigarette excise taxes are the most effective tool for reducing overall cigarette sales, followed by antismoking advertising. Smoke-free restrictions are not found to have a significant effect on cigarette sales. We also investigate how these various policy tools induce product substitution. This issue is of considerable importance because some countermarketing techniques may potentially shift consumers to more dangerous, higher nicotine and tar cigarettes. Specifically, we find that excise taxes levied on a per pack basis rather than based on nicotine levels often shift consumers to more dangerous products.
We empirically study the impact of the entry of a new theater on two important product decisions that incumbents in the movie exhibition industry face: (1) whether to invest in screening movies that are expected to be popular, and (2) when to adopt new releases. For theaters, both of these decisions feature a cost-demand trade-off inherent in quality decisions: Although screening popular and recent movies brings more patrons to the theater, distributors take a higher share of the revenue for such movies. The impact of competitive entry on the incumbent’s quality decisions is ambiguous, as it may simultaneously increase the competitive pressure to invest more in these dimensions of quality and also change the demand conditions that incumbents face. We find that incumbent theaters do not increase the provision of popular and recent movies in response to rival entry. To identify the role of competitive incentives, we study the differential impact of entry based on whether the entrant belongs to the same parent firm as the incumbent theaters. This comparison reveals that competitive incentives push incumbents to screen movies with high expected success more frequently and to adopt movies sooner. The product responses we document have important implications for the revenue impact of entry and the conclusions that researchers can draw from this impact. Ignoring the provision of these quality dimensions suggests cannibalization to exceed business stealing, a conclusion that is reversed when we account for endogenous product responses. We also show that our findings on popularity and recency cannot be explained by concomitant changes in theaters’ other product decisions, such as the variety of movies screened.
An exchange promotion allows consumers to turn in an old good and receive a discount toward the purchase of a new product. The old good that is turned in can either be within the same category as the new good or it may be in a different category. For example, one can turn in an old CD player to count toward a new CD player (a within-category exchange or traditional trade-in) or toward a new television (a cross-category exchange). This paper studies both within-category and multicategory exchange promotions and analyzes their similarities and differences. In a competitive setting with two firms, we model exchange promotions and establish the equilibrium outcomes. We find that categories in which consumers have a high level of waste aversion are more likely to have multicategory exchange promotions rather than within-category or no promotions. Multicategory exchange promotions can increase both consumers’ replacement purchases and their new purchases. Interestingly, we also find that strategic considerations can lead to a prisoner’s dilemma outcome in which neither firm offers any kind of exchange promotion. However, waste aversion and multicategory exchange promotions can give firms stronger incentives to get out of the prisoner’s dilemma outcome.
When match uncertainty is resolved via costly evaluation, the first product sampled by a customer is more likely to make the sale. This prompts firms to lower their products’ evaluation costs to attract customers to sample their products first. Such efforts by firms are called customer learning investment (CLI). When product quality is freely observable but horizontal match is not, we examine how CLI choices interact with quality and price competition in a duopoly. CLI has a competition effect in that a higher CLI of a firm increases its demand but decreases that of its rivals. In the market-covered duopoly, both qualities will decrease (increase) when the high-end (low-end) firm invests more in customer learning, and remain unchanged when they invest equally, relative to when neither firm invests. We further show that the firm with a higher relative production efficiency invests more in customer learning than the competitor. In the market-not-covered duopoly, CLI by the low-end firm also creates a market-expansion effect by inducing some additional low-end customers to sample and purchase its product. This may induce it to invest more than the high-end firm, even when the latter has a higher relative production efficiency.
In this paper we study product line scope and pricing decisions in a horizontally differentiated duopoly. Past research has shown that a firm may offer a broader product line to attract higher demand or charge a higher price (or both), and benefit at the expense of its competitor. We show that such outcomes may be reversed, especially when consumers have relatively high valuation and low heterogeneity in their preferences for the line extension. We find that an equilibrium exists such that only one firm prefers to expand scope but profits may be higher for both firms, even in the absence of market size expansion. This is because a broader scope permits that firm to effectively price discriminate by raising prices for its core customers. The competitor optimally responds by lowering prices to gain share and earn a higher profit. Thus, higher prices for the firm expanding its product line translate into higher demand for the competing firm, thus increasing profit for both. We show that our results hold when firms deploy generic, offensive or defensive strategies during product line expansion.
We study three ways firms can communicate about their brands—paid media (advertising), earned media (word of mouth and online social media), and owned media (brand websites and other owned content)—and the roles these media types play in reminding (i.e., activating memory), informing (i.e., learning their tastes for the brand), or enhancing enjoyment (e.g., gaining additional utility from socializing about the brand). We do this for a new TV show setting using a data set that contains reported viewing, exposures, expectations, and experiences. We present descriptive analyses and results from a new structural model, which indicate that earned media is more impactful than paid and owned media per exposure. However, paid media has far more exposures, so for a given percentage increase, paid media’s influence dominates earned and owned media. Earned media operate primarily through enhancing enjoyment, whereas paid media operate through reminding and owned media through reminding, but discourage live viewing. We find that media exposures help consumers learn about how well they will like the program. However, this learning can either increase or decrease the expected liking, and in our data the average audience effects are negligible. Overall, we find that earned and paid media play a central role in developing and maintaining entertainment brands.
Over the past 10 years there has been increased recognition of the importance of publicity as a means of generating product awareness. Despite this, previous research has seldom investigated the impact of publicity on demand. We contribute to the literature by (i) proposing a new method for the interpretation of publicity data, one that maps the information in news articles (or broadcasts) to a multidimensional attribute space; (ii) investigating how different types of publicity affect demand; and (iii) investigating how different types of publicity interact with firms’ own marketing communication efforts. We study these issues for statins. We find that publicity plays an important role both for expanding the market for statins and for determining which statins patients or physicians choose. We also find evidence that publicity can serve as either a substitute or a complement for traditional marketing channels depending on the complexity of the information type. We argue that the interaction results are driven by the relative strengths of the corroborative and rational inattention functions in publicity. These results suggest that managers should be aware of the interactions between publicity and traditional marketing channels to better determine how to allocate their marketing expenditures.
We introduce a method for identifying, analyzing, and visualizing submarkets in product categories. We give an overview of the market structure and competitive submarket literature and then describe a classic model for testing competitive submarkets along with associated extensions. In the era of big data and with the increasing availability of large-scale consumer purchase data, there is a need for techniques that can interpret these data and use them to help make managerial decisions. We introduce a statistical likelihood based technique for both identifying and testing market structure. We run a series of experiments on generated data and show that our method is better at identifying market structure from brand substitution data than a range of methods described in the marketing literature. We introduce tools for holdout validation, complexity control, and testing managerial hypotheses. We describe a method for visualization of submarket solutions, and we give several traditional consumer product examples and in addition give an example to show how market structure can be analyzed from online review data.Data, as supplemental material, are available at http://dx.doi.org/10.1287/mksc.2015.0958.
One of the most intriguing findings in the multichannel customer management literature is the positive association between multichannel purchasing and customer profitability. The question is whether this finding can be put into action. That is, can a firm develop a marketing campaign to increase multichannel purchasing and hence average customer profitability, and if so what are the key factors that enable success. We design and implement a randomized field experiment to investigate this question. The field experiment tests four marketing campaigns that vary in the communications message and the provision of financial incentives. We find that the multichannel/profitability relationship is actionable. A properly designed marketing campaign increases the number of multichannel customers and increases average customer profitability. That campaign’s message emphasizes the benefits of multichannel shopping but does not rely on financial incentives. Moreover, we use propensity score matching to show that, after accounting for self-selection, multichannel customers are more profitable than they would be if they were not multichannel. A post-test analysis suggests that the multichannel/nonfinancial incentive campaign succeeded in inducing customers to become multichannel because it decreased customer reactance and increased perceived behavioral control.Data, as supplemental material, are available at http://dx.doi.org/10.1287/mksc.2015.0923.
This research examines the effects of hyper-contextual targeting with physical crowdedness on consumer responses to mobile ads. It relies on rich field data from one of the world’s largest telecom providers who can gauge physical crowdedness in real-time in terms of the number of active mobile users in subway trains. The telecom provider randomly sent targeted mobile ads to individual users, measured purchase rates, and surveyed purchasers and nonpurchasers. Based on a sample of 14,972 mobile phone users, the results suggest that, counterintuitively, commuters in crowded subway trains are about twice as likely to respond to a mobile offer by making a purchase vis-à-vis those in noncrowded trains. On average, the purchase rates measured 2.1% with fewer than two people per square meter, and increased to 4.3% with five people per square meter, after controlling for peak and off-peak times, weekdays and weekends, mobile use behaviors, and randomly sending mobile ads to users. The effects are robust to exploiting sudden variations in crowdedness induced by unanticipated train delays underground and street closures aboveground. Follow-up surveys provide insights into the causal mechanism driving this result. A plausible explanation is mobile immersion: As increased crowding invades one’s physical space, people adaptively turn inwards and become more susceptible to mobile ads. Because crowding is often associated with negative emotions such as anxiety and risk-avoidance, the findings reveal an intriguing, positive aspect of crowding: Mobile ads can be a welcome relief in a crowded subway environment. The findings have economic significance because people living in cities commute 48 minutes each way on average, and global mobile ad spending is projected to exceed $100 billion. Marketers may consider the crowdedness of a consumer’s environment as a new way to boost the effectiveness of hyper-contextual mobile advertising.Data, as supplemental material, are available at http://dx.doi.org/10.1287/mksc.2015.0905.
Motivated by the growing practice of using social network data in credit scoring, we analyze the impact of using network-based measures on customer score accuracy and on tie formation among customers. We develop a series of models to compare the accuracy of customer scores obtained with and without network data. We also investigate how the accuracy of social network-based scores changes when consumers can strategically construct their social networks to attain higher scores. We find that those who are motivated to improve their scores may form fewer ties and focus more on similar partners. The impact of such endogenous tie formation on the accuracy of consumer scores is ambiguous. Scores can become more accurate as a result of modifications in social networks, but this accuracy improvement may come with greater network fragmentation. The threat of social exclusion in such endogenously formed networks provides incentives to low-type members to exert effort that improves everyone’s creditworthiness. We discuss implications for managers and public policy.
In sponsored search advertising, advertisers bid to be displayed in response to a keyword search. The operational activities associated with participating in an auction, i.e., submitting the bid and the ad copy, customizing bids and ad copies based on various factors (such as the geographical region from which the query originated, the time of day and the season, the characteristics of the searcher), and continuously measuring outcomes, involve considerable effort. We call the costs that arise from such activities keyword management costs. To reduce these costs and increase advertisers’ participation in keyword auctions, search engines offer an opt-in tool called broad match with automatic and flexible bidding, wherein the search engine automatically places bids on behalf of the advertisers and takes over the above activities as well. The bids are based on the search engine’s estimates of the advertisers’ valuations and, therefore, may be less accurate than the bids the advertisers would have turned in themselves. Using a game-theoretic model, we examine the strategic role of keyword management costs, and of broad match, in sponsored search advertising. We show that because these costs inhibit participation by advertisers in keyword auctions, the search engine has to reduce the reserve price, which reduces the search engine’s profits. This motivates the search engine to offer broad match as a tool to reduce keyword management costs. If the accuracy of broad match bids is sufficiently high, advertisers adopt broad match and benefit from the cost reduction, whereas if the accuracy is very low, advertisers do not use it. Interestingly, at moderate levels of bid accuracy, advertisers individually find it attractive to reduce costs by using broad match, but competing advertisers also adopt broad match and the increased competition hurts all advertisers’ profits, thus creating a “prisoner’s dilemma.” When advertisers adopt broad match, search engine profits increase. It therefore seems natural to expect that the search engine will be motivated to improve broad match accuracy. Our analysis shows that the search engine will increase broad match accuracy up to the point where advertisers choose broad match, but that increasing the accuracy any further reduces the search engine’s profits.
We provide a descriptive study of the cross-category effects of satisfaction with financial services on retention behavior. Behavioral contrast and learning theories provide the bases for our understanding of these effects. Our empirical results reveal the following: (i) Across banking and investment categories, when customers have different providers, satisfaction with one lowers the retention probability in the other service. (ii) A customer who is dissatisfied with the investment service is more likely to stay with the current banking service. (iii) Significantly, we find that when the same firm is involved in both categories, dissatisfaction with the firm in the investment category spills over into the banking category thereby lowering its retention probability. We also find that: (a) among customers who are satisfied with banking (investment), more exposure to media increases retention probability; (b) although switching costs and order of acquisition affect retention, they do not show cross-category interactions with satisfaction. We then obtain implications for customer lifetime value (CLV) and show that it can increase satisfaction by leveraging both the within and across category effects. Bottom line: It is important for a company providing multiple services to measure satisfaction at the category level but to manage customers across categories.Data, as supplemental material, are available at http://dx.doi.org/10.1287/mksc.2015.0941.
This paper considers the creation and consumption of content on user-generated content platforms, e.g., reviews, articles, chat, videos, etc. On these platforms, users’ expectations regarding the amount and timing of participation by others becomes germane to their own involvement levels. Accordingly, we develop a dynamic rational expectations equilibrium model of joint consumption and generation of information. We estimate the model on a novel data set from a large Internet forum site and offer recommendations regarding strategies of managing sponsored content and content quality.Data, as supplemental material, are available at http://dx.doi.org/10.1287/mksc.2015.0937.
In this paper, I investigate the question of how a firm producing substitutes should coordinate price promotions of these products. I model price competition between two firms, each producing two products that are horizontally differentiated with respect to some characteristic. Consumers are divided into loyals, who always purchase their preferred product, and switchers, who have heterogeneous preferences for the four products. If consumers substitute easily between the products produced by one firm, the firms promote one product at a time to avoid cannibalization. If consumers mainly substitute between the products with the same characteristic, the firms often employ joint promotions with at least one product at a deep discount. If, at the same time, consumers easily substitute to the products in other categories, the firms use joint promotions less often and avoid simultaneous deep discounts.Data, as supplemental material, are available at http://dx.doi.org/10.1287/mksc.2015.0938.
Promotions are used in marketing to increase sales and drive profits by temporarily decreasing the price per unit of a good. Some price promotions apply to all quantities (20% off), some have limits on the number of units that can be purchased at a reduced price, and others only offer the discount if the volume purchased is sufficiently high. We develop a model of price promotions in the context of a direct utility model where its effects are incorporated through the budget constraint. Price promotions complicate the estimation and analysis of direct utility models because they induce kinks and points of discontinuity in the budget set. We propose a Bayesian approach to addressing these irregularities and demonstrate the ability of the direct utility model to be used in counterfactual analyses of price promotions. We investigate the stability of utility function estimates for consumers under alternative price promotions, and find that the majority of the effect of a price promotion is through the budget set, not through changes in the utility function. We also investigate the economic value of customized price promotions where the customization includes the value and format of the offer.Data, as supplemental material, are available at http://dx.doi.org/10.1287/mksc.2015.0948.
Marketing Science greatly benefited from the admirable and fastidious efforts of more than 200 different individuals who provided manuscript reviews last year. Beyond those individuals already recognized on the editorial board, the editor-in-chief and senior/guest editors of Marketing Science are indebted to the many guest editors-in-chief, guest associate editors, and ad hoc reviewers who provided expert counsel and guidance on a voluntary basis. The following list acknowledges the contribution of guest editors-in-chief, guest associate editors, and ad hoc reviewers who served from January 1, 2015 to December 31, 2015. Finally, let us not forget to thank the authors. Marketing Science requires and receives outstanding submissions from many leading researchers and prestigious organizations.K. SudhirYale University
Consumer perceptions are important components of brand equity and therefore marketing strategy. Segmenting these perceptions into attributes such as eco-friendliness, nutrition, and luxury enable a fine-grained understanding of the brand’s strengths and weaknesses. Traditional approaches towards monitoring such perceptions (e.g., surveys) are costly and time consuming, and their results may quickly become outdated. Extant data mining methods are unsuitable for this goal, and generally require extensive hand-annotated data or context customization, which leads to many of the same limitations as direct elicitation. Here, we investigate a novel, general, and fully automated method for inferring attribute-specific brand perception ratings by mining the brand’s social connections on Twitter. Using a set of over 200 brands and three perceptual attributes, we compare the method’s automatic ratings estimates with directly-elicited survey data, finding a consistently strong correlation. The approach provides a reliable, flexible, and scalable method for monitoring brand perceptions, and offers a foundation for future advances in understanding brand-consumer social media relationships.Data, as supplemental material, are available at http://dx.doi.org/10.1287/mksc.2015.0968.
Accurate forecasting of sales/consumption is particularly important for marketing because this information can be used to adjust marketing budget allocations and overall marketing strategies. Recently, online social platforms have produced an unparalleled amount of data on consumer behavior. However, two challenges have limited the use of these data in obtaining meaningful business marketing insights. First, the data are typically in an unstructured format, such as texts, images, audio, and video. Second, the sheer volume of the data makes standard analysis procedures computationally unworkable. In this study, we combine methods from cloud computing, machine learning, and text mining to illustrate how online platform content, such as Twitter, can be effectively used for forecasting. We conduct our analysis on a significant volume of nearly two billion Tweets and 400 billion Wikipedia pages. Our main findings emphasize that, by contrast to basic surface-level measures such as the volume of or sentiments in Tweets, the information content of Tweets and their timeliness significantly improve forecasting accuracy. Our method endogenously summarizes the information in Tweets. The advantage of our method is that the classification of the Tweets is based on what is in the Tweets rather than preconceived topics that may not be relevant. We also find that, by contrast to Twitter, other online data (e.g., Google Trends, Wikipedia views, IMDB reviews, and Huffington Post news) are very weak predictors of TV show demand because users tweet about TV shows before, during, and after a TV show, whereas Google searches, Wikipedia views, IMDB reviews, and news posts typically lag behind the show.Data, as supplemental material, are available at http://dx.doi.org/10.1287/mksc.2015.0972.
An accurate prediction of what a customer will purchase next is of paramount importance to successful online retailing. In practice, customer purchase history data is readily available to make such predictions, sometimes complemented with customer characteristics. Given the large product assortments maintained by online retailers, scalability of the prediction method is just as important as its accuracy. We study two classes of models that use such data to predict what a customer will buy next, i.e., a novel approach that uses latent Dirichlet allocation (LDA), and mixtures of Dirichlet-Multinomials (MDM). A key benefit of a model-based approach is the potential to accommodate observed customer heterogeneity through the inclusion of predictor variables. We show that LDA can be extended in this direction while retaining its scalability. We apply the models to purchase data from an online retailer and contrast their predictive performance with that of a collaborative filter and a discrete choice model. Both LDA and MDM outperform the other methods. Moreover, LDA attains performance similar to that of MDM while being far more scalable, rendering it a promising approach to purchase prediction in large product assortments.Data, as supplemental material, are available at http://dx.doi.org/10.1287/mksc.2016.0985.
User profile is a summary of a consumer’s interests and preferences revealed through the consumer’s online activity. It is a fundamental component of numerous applications in digital marketing. McKinsey & Company view online user profiling as one of the promising opportunities companies should take advantage of to unlock “big data’s” potential. This paper proposes a modeling approach that uncovers individual user profiles from online surfing data and allows online businesses to make profile predictions when limited information is available. The approach is easily parallelized and scales well for processing massive records of user online activity. We demonstrate application of our approach to customer-base analysis and display advertising. Our empirical analysis uncovers easy-to-interpret behavior profiles and describes the distribution of such profiles. Furthermore, it reveals that even for information-rich online firms profile inference that is based solely on their internal data may produce biased results. We find that although search engines cover smaller portions of consumer Web visits than major advertising networks, their data is of higher quality. Thus, even with the smaller information set, search engines can effectively recover consumer behavioral profiles. We also show that temporal limitations imposed on individual-level tracking abilities are likely to have a differential impact across major online businesses, and that our approach is particularly effective for temporally limited data. Using economic simulation we demonstrate potential gains the proposed model may offer a firm if used in individual-level targeting of display ads.Data, as supplemental material, are available at http://dx.doi.org/10.1287/mksc.2015.0956.
Bayesian hierarchical modeling is a popular approach to capturing unobserved heterogeneity across individual units. However, standard estimation methods such as Markov chain Monte Carlo (MCMC) can be impracticable for modeling outcomes from a large number of units. We develop a new method to sample from posterior distributions of Bayesian models, without using MCMC. Samples are independent, so they can be collected in parallel, and we do not need to be concerned with issues like chain convergence and autocorrelation. The algorithm is scalable under the weak assumption that individual units are conditionally independent, making it applicable for large data sets. It can also be used to compute marginal likelihoods.Data, as supplemental material, are available at http://dx.doi.org/10.1287/mksc.2014.0901.
As technology advances, new products (e.g., digital cameras, computer tablets, etc.) have become increasingly more complex. Researchers often face considerable challenges in understanding consumers’ preferences for such products. This paper proposes an adaptive decompositional framework to elicit consumers’ preferences for complex products. The proposed method starts with collaborative-filtered initial part-worths, followed by an adaptive question selection process that uses a fuzzy support vector machine active learning algorithm to adaptively refine the individual-specific preference estimate after each question. Our empirical and synthetic studies suggest that the proposed method performs well for product categories equipped with as many as 70 to 100 attribute levels, which is typically considered prohibitive for decompositional preference elicitation methods. In addition, we demonstrate that the proposed method provides a natural remedy for a long-standing challenge in adaptive question design by gauging the possibility of response errors on the fly and incorporating the results into the survey design. This research also explores in a live setting how responses from previous respondents may be used to facilitate active learning of the focal respondent’s product preferences. Overall, the proposed approach offers new capabilities that complement existing preference elicitation methods, particularly in the context of complex products.Data, as supplemental material, are available at http://dx.doi.org/10.1287/mksc.2015.0946.
Online Display Advertising’s importance as a marketing channel is partially due to its ability to attribute conversions to campaigns. Current industry practice to measure ad effectiveness is to run randomized experiments using placebo ads, assuming external validity for future exposures. We identify two different effects, i.e., a strategic effect of the campaign presence in marketplaces, and a selection effect due to user targeting; these are confounded in current practices. We propose two novel randomized designs to: (1) estimate the overall campaign attribution without placebo ads, (2) disaggregate the campaign presence and ad effects. Using the Potential Outcomes Causal Model, we address the selection effect by estimating the probability of selecting influenceable users. We show the ex-ante value of continuing evaluation to enhance the user selection for ad exposure mid-flight. We analyze two performance-based (CPA) and one Cost-Per-Impression (CPM) campaigns with 20 million users each. We estimate a negative CPM campaign presence effect due to cross product spillovers. Experimental evidence suggests that CPA campaigns incentivize selection of converting users regardless of the ad, up to 96% more than CPM campaigns, thus challenging the standard practice of targeting most likely converting users.Data, as supplemental material, are available at http://dx.doi.org/10.1287/mksc.2016.0982.
In this paper, we propose an automated and scalable garment recommender system using real-time in-store videos that can improve the experiences of garment shoppers and increase product sales. The video-based automated recommender (VAR) system is based on observations that garment shoppers tend to try on garments and evaluate themselves in front of store mirrors. Combining state-of-the-art computer vision techniques with marketing models of consumer preferences, the system automatically identifies shoppers’ preferences based on their reactions and uses that information to make meaningful personalized recommendations. First, the system uses a camera to capture a shopper’s behavior in front of the mirror to make inferences about her preferences based on her facial expressions and the part of the garment she is examining at each time point. Second, the system identifies shoppers with preferences similar to the focal customer from a database of shoppers whose preferences, purchasing, and/or consideration decisions are known. Finally, recommendations are made to the focal customer based on the preferences, purchasing, and/or consideration decisions of these like-minded shoppers. Each of the three steps can be implemented with several variations, and a retailing chain can choose the specific configuration that best serves its purpose. In this paper, we present an empirical test that compares one specific type of VAR system implementation against two alternative, nonautomated personal recommender systems: self-explicated conjoint (SEC) and self-evaluation after try-on (SET). The results show that VAR consistently outperforms SEC and SET. A second empirical study demonstrates the feasibility of VAR in real-time applications. Participants in the second study enjoyed the VAR experience, and almost all of them tried on the recommended garments. VAR should prove to be a valuable tool for both garment retailers and shoppers.Data, as supplemental material, are available at http://dx.doi.org/10.1287/mksc.2016.0984.
In large markets comprising hundreds of products, comprehensive visualization of competitive market structures can be cumbersome and complex. Yet, as we show empirically, reduction of the analysis to smaller representative product sets can obscure important information. Herein we use big search data from a product- and price-comparison site to derive consideration sets of consumers that reflect competition between products. We integrate these data into a new modeling and two-dimensional mapping approach that enables the user to visualize asymmetric competition in large markets (>1,000 products) and to identify distinct submarkets. An empirical application to the LED-TV market, comprising 1,124 products and 56 brands, leads to valid and useful insights and shows that our method outperforms traditional models such as multidimensional scaling. Likewise, we demonstrate that big search data from product- and price-comparison sites provide higher external validity than search data from Google and Amazon.Data, as supplemental material, are available at http://dx.doi.org/10.1287/mksc.2015.0950.
Manufacturers in many industries frequently use vertical price policies, such as minimum advertised price (MAP), to influence prices set by downstream retailers. Although manufacturers expect retail partners to comply with MAP policies, violations of MAP are common in practice. In this research, we document and explain both the extent and the depth of MAP policy violations. We also shed light on how retailers vary in their propensity to violate MAP policies, and the depth by which they do so.Our inductive research approach documents managerial wisdom about MAP practices. We confront these insights from practice with a large empirical study that includes hundreds of products sold through hundreds of retailers. Consistent with managerial wisdom, we find that authorized retailers are more likely to comply with MAP than are unauthorized partners. By contrast to managerial wisdom, we find that authorized and unauthorized markets are largely separate, and that violations in the authorized channel have a small association with violations in the unauthorized channel. Last, we link our results to the literatures on agency theory, transaction cost analysis, and theories of price obfuscation.Data, as supplemental material, are available at http://dx.doi.org/10.1287/mksc.2015.0933.
This research investigates whether there are enduring effects of goal achievement and failure within customer loyalty promotion programs. We collaborated with a major hotel chain to launch a large scale field experiment involving 95,532 existing loyalty customers. We observed customers’ hotel stays for eight months before the experiment, eight months during the experiment, and eight months after the experiment. Customers in the treatment group were asked to increase their hotel nights during the 8-month promotion by a set percentage relative to their baseline to receive a reward. Overall, the promotion led to increased purchasing in the post-promotion period. However, only 20% of customers successfully reached the goal whereas 80% missed the goal. We use a propensity score analysis to examine the distinct effects of goal achievement versus goal failure. Results show that goal attainment significantly increased post-promotion purchasing whereas goal failure significantly reduced post-promotion purchasing. Additionally, we use econometric methods to empirically test a behavioral theory of relationship-based reciprocity. We find that customers in a high status tier relationship, with the most invested in the firm, are most affected by goal failure whereas customers in a low status tier relationship, with the least invested in the firm, are most affected by goal success. Because the type of loyalty program described in this paper is widely used in a variety of industries the findings suggest that marketers should set reachable goals within loyalty promotion programs. Firms should be particularly cautious about the impact of goal failures for the firm’s most loyal customers.Data, as supplemental material, are available at http://dx.doi.org/10.1287/mksc.2015.0966.
This paper examines how firms should allocate their advertising budgets between consumers who have a high preference for their products (i.e., strong segment) and those who prefer competing products (i.e., weak segment). Targeted advertising transmits relevant information to otherwise uninformed consumers and it is used as a price discrimination device. With targeted advertising and price discrimination, we find that, when the attractiveness of the weak segment is low, each firm advertises more intensively in its strong segment. The same result arises when the attractiveness of the weak segment is high and advertising is sufficiently expensive. Interestingly, when the attractiveness of the weak segment is high but advertising costs are sufficiently low, it is optimal for each firm to advertise more intensively in its weak segment. The paper also investigates how advertising strategies and equilibrium profits are affected by price discrimination. Compared with uniform pricing, firms can increase or reduce the intensity of advertising targeted to each segment when price discrimination is allowed. Furthermore, when the attractiveness of the weak market is high, price discrimination boosts firms’ profits provided that advertising costs are sufficiently low. The reverse happens when advertising costs are high.
In this paper, we study the effect of a firm’s local channel exits on prices charged by incumbents remaining in the marketplace. Exits could result in higher prices due to tempered competition or lower prices due to reduced colocation or agglomeration benefits. The net effect of these two countervailing forces remains unknown. In addition, little is known about how this effect could change depending on incumbents’ geographic locations. We address this research gap by examining new car price reactions by incumbent multiproduct automobile dealerships who experience the exit of a Chrysler dealership in their local markets. We find evidence that the competition effect exceeds the colocation effect: prices increase by about 1% ($318) following an exit relative to the price change in the absence of an exit. More important, we find that the price increase is lower at dealerships more proximate to the exiting dealership than dealerships farther away for the same set of cars available across these locations. This finding suggests differences in the extent of the two forces (competition and agglomeration) at different distances from the closed dealership. We assess the generalizability of our results by looking at the impact of GM’s closure of Pontiac dealerships. Taken together, our results inform consumers, firms, and policymakers about possible implications of an exit.Data, as supplemental material, are available at http://dx.doi.org/10.1287/mksc.2015.0952.
A seller often needs to determine the amount of product information to provide to consumers. We model costly consumer information search in the presence of limited information. We derive the consumer’s optimal stopping rule for the search process. We find that, in general, there is an intermediate amount of information that maximizes the likelihood of purchase. If too much information is provided, some of it is not as useful for the purchase decision, the average informativeness per search occasion is too low, and consumers end up choosing not to purchase the product. If too little information is provided, consumers may end up not having sufficient information to decide to purchase the product. The optimal amount of information increases with the consumer’s ex ante valuation of the product, because with greater ex ante valuation by the consumer, the firm wants to offer sufficient information for the consumer to be less likely to run out of information to check. One can also show that there is an intermediate amount of information that maximizes the consumer’s expected utility from the search problem (social welfare under some assumptions). Furthermore, this amount may be smaller than that which maximizes the probability of purchase; that is, the market outcome may lead to information overload with respect to the social welfare optimum. This paper can be seen as providing conditions under which too much information may hurt consumer decision making. Numerical analysis shows also that if consumers can choose to some extent which attributes to search through (but not perfectly), or if the firm can structure the information searched by consumers, the amount of information that maximizes the probability of purchase increases, but is close to the amount of information that maximizes the probability of purchase when the consumer cannot costlessly choose which attributes to search through.
We study the pass-through of wholesale price changes onto regular retail prices using an unusually detailed data set obtained from a major retailer. We model pass-through as a two-stage decision process that reflects both whether as well as how much to change the regular retail price. We show that pass-through is strongly asymmetric with respect to wholesale price increases versus decreases. Wholesale price increases are passed through to regular retail prices 70% of the time while wholesale price decreases are passed through only 9% of the time. Pass-through is also asymmetric with respect to the magnitude of the wholesale price change, with the magnitude affecting the response to wholesale price increases but not decreases. Finally, we show that covariates such as private label versus national brand, 99-cent price endings, and the time since the last wholesale price change have a much stronger impact on the first stage of the decision process (i.e., whether to change the regular retail price) than on the second stage (i.e., how much to change the regular retail price).Data, as supplemental material, are available at http://dx.doi.org/10.1287/mksc.2015.0947.
The marketing and operations disciplines have increasingly accounted for the presence of strategic consumer behavior. Theory suggests that such behavior exists when consumers are able to consider future distribution of prices, and that this behavior exposes firms to intertemporal competition that results with a downward pressure on prices. However, deriving future distribution of prices is not a trivial task. Online decision support tools that provide consumers with information about future distributions of prices can facilitate strategic consumer behavior. This paper studies whether the availability of such information affects transacted prices by conducting an empirical analysis in the context of the airline industry. Studying the effect at the route level, we find significant price reduction effects as such information becomes available for a route, both in fixed-effects and difference-in-differences estimation models. This effect is consistent across the different fare percentiles and amounts to a reduction of approximately 4%–6% in transactions’ prices. Our results lend ample support to the notion that price prediction decision tools make a statistically significant economic impact. Presumably, consumers are able to exploit the information available online and exhibit strategic behavior.Data, as supplemental material, are available at http://dx.doi.org/10.1287/mksc.2015.0965.
Successful initial public offerings (IPOs) provide firms with access to valuable resources, but also put pressure on firms to impress potential investors with evidence of their current well-being and prospects for future growth. To impress investors IPO firms might curtail their marketing budgets, which appears to inflate current earnings and provide evidence of current well-being. However, curtailing marketing budgets unexpectedly during an IPO may be a myopic practice, in that the immediate benefits of these budget cuts are offset by their longer-term adverse consequences on financial well-being. The extent of these adverse consequences in turn may be moderated by external firm factors, including strategic alliances and key customer relationships, and internal firm factors, such as the strategic emphasis on value creation versus value appropriation. A sample of 1,095 IPOs during 2000–2011 reveals evidence of myopic marketing practices in more than 37% of the sample. Investors seem misled during IPOs, but they correct their beliefs in the three years following the IPO and penalize these firms. The penalty for myopic marketing budgeting practices also increases with more strategic alliances and a strategic emphasis on value creation versus value appropriation, but it decreases in the presence (versus absence) of key customer relationships.Data, as supplemental material, are available at http://dx.doi.org/10.1287/mksc.2015.0970.
To inform the design of sales force compensation plans when carryover effects exist, we propose a dynamic model where these effects, together with present selling efforts, drive sales. Our results show that a salesperson with low risk aversion exerts effort to decrease attrition from existing business, whereas a salesperson with high risk aversion does not. Why? Because carryover increases not only expected sales but also sales uncertainty. Consequently, the manager should incentivize the high risk-aversion salesperson with a concave compensation plan to counterbalance suboptimal customer attrition, and the low risk-aversion salesperson with a convex compensation plan that limits coasting on past efforts. We generalize our results to when the firm employs multiple salespeople, and when advertising and personal selling are budgeted together.
We describe online consumers’ search behavior for differentiated durable goods using a data set that captures a detailed level of consumer search and attribute information for digital cameras. Consumers search extensively, engaging in 14 searches on average prior to purchase. Individual level search is confined to a small part of the attribute space. Early search is highly predictive of the characteristics of the camera eventually purchased. Search paths through the attribute space are state dependent and display “lock-in” as the search unfolds. Finally, the first-time discovery of the chosen alternative usually takes place toward the end of the search sequence. We discuss these and other findings in the context of optimal search strategies and discuss the prospects for consumer learning during search.Data, as supplemental material, are available at http://dx.doi.org/10.1287/mksc.2016.0977.
Features involving the taste, smell, touch, and sight of products, as well as attributes such as safety and confidence, are not easily measured in product research without respondents actually experiencing them. Moreover, product researchers often evaluate a large number of these attributes (e.g., >50) in applied studies, making standard valuation techniques such as conjoint analysis difficult to implement. Product researchers instead rely on ratings data to assess features for which the respondent has had actual experience. In this paper we develop a method of monetizing rating data to standardize product evaluations among respondents. The adjusted data are shown to increase the accuracy of purchase predictions by about 20% relative to existing methods of scale adjustment, leading to better inference in models using ratings data. We demonstrate our method using data from a large scale product use study by a packaged goods manufacturer.Data, as supplemental material, are available at http://dx.doi.org/10.1287/mksc.2016.0980.
This paper examines the incentives of firms to invest in socially responsible product innovations. Our analysis connects the existence of socially responsible innovations to the presence of intrinsic and extrinsic social responsibility preferences. In addition to deriving economic value from the product, consumers have heterogeneous intrinsic needs to consume products that are socially responsible. They also have extrinsic social comparison preferences that are based on their meetings with others in social interactions. The frequency of these meetings are endogenous to the consumption choices of consumers. A consumer enjoys a social comparison benefit if her consumption decision is more socially responsible than the consumer that she meets in a social interaction and a social comparison cost if it is less socially responsible.The analysis reveals a nonmonotonic effect of social comparison effects on innovation incentives. When the economic value of a product is relatively small, the incentive to innovate decreases as social comparison effects increase. By contrast, when the economic value of a product is sufficiently large, increases in social comparison effects increase the incentive to innovate. Social comparison benefits and costs have different effects on competition between firms. In particular, social comparison benefits soften price competition, whereas social comparison costs tend to exacerbate price competition. We also identify market conditions where a monopoly invests more or less compared to a firm facing competition.
We present an analytical framework of multimarket competition and supporting empirical analysis to explain why and when competing firms in an existing market may prefer an alliance entry over independent entry into a new market. Our findings suggest that an alliance entry is more profitable than an independent entry (i) when the new market is larger relative to the existing market, and (ii) when the competition in the existing market is stronger relative to the new market. We compare these key predictions with archival data from the regional shopping center industry in the United States and find that instances of alliance formation in this industry are consistent with our model-based predictions.Data, as supplemental material, are available at http://dx.doi.org/10.1287/mksc.2016.0988.
In the last two decades, organized retailing has transformed the retailing landscape in emerging economies, where unorganized retailing has traditionally been dominant. In this paper, we build a theoretical model of unorganized and organized retailing in emerging economies by carefully modeling key characteristics of the retailing environment, the retailers, the consumers, and product categories. The primary insight that we obtain is that in a competitive market comprising of only unorganized retailers, the advent of organized retailing injects efficiency into the market leading to a reduction in the number of unorganized retailers. This, in turn, makes the market less competitive. Building on this basic insight, we obtain a number of counterintuitive results. For instance, (i) the presence of organized retailing may increase the prices charged by unorganized retailers; (ii) as the consumers’ transportation cost to the unorganized retailers increases, the market share of the unorganized retailing sector may increase; (iii) as the probability of bulk consumption increases and consumers prefer to purchase more from the organized retailer, prices and profits at the organized retailer may decrease; and (iv) the presence of organized retailing can lead to both consumer and social surplus being lower because consumers face higher prices at unorganized retailers and there is wastage in the economy due to bulk purchasing at organized retailers. Our model offers an explanation for certain surprising empirical observations related to retailing in emerging markets, such as why in the last few years in the Indian market the unorganized retailers who have survived the advent of organized retailing seem to be doing better. Implications from our research can provide guidance to policy makers grappling with issues related to the balanced growth of unorganized and organized retailing in emerging markets.
Accurate predictions of a customer’s activity status and future purchase propensities are crucial for managing customer relationships. This article extends the recency–frequency paradigm of customer-base analysis by integrating regularity in interpurchase timing in a modeling framework. By definition, regularity implies less variation in timing patterns and thus better predictability. Whereas most stochastic customer behavior models assume a Poisson process of “random” purchase occurrence, allowing for regularity in the purchase timings is beneficial in noncontractual settings because it improves inferences about customers’ latent activity status. This especially applies to those valuable customers who were previously very frequently active but have recently exhibited a longer purchase hiatus. A newly developed generalization of the well-known Pareto/NBD model accounts for varying degrees of regularity across customers by replacing the NBD component with a mixture of gamma distributions (labeled Pareto/GGG). The authors demonstrate the impact of incorporating regularity on forecasting accuracy using an extensive simulation study and a range of empirical applications. Even for mildly regular timing patterns, it is possible to improve customer-level predictions; the stronger the regularity, the greater the gain. Furthermore, the cost in terms of data requirements is marginal because only one additional summary statistic, in addition to recency and frequency, is needed that captures historical transaction timing.Data, as supplemental material, are available at http://dx.doi.org/10.1287/mksc.2015.0963.
This paper studies information sharing in a distribution channel where the manufacturer possesses better demand-forecast information than the downstream retailer. We examine three information-sharing formats: no information sharing (i.e., the manufacturer ex ante commits to not sharing its forecast), voluntary information sharing (i.e., the manufacturer makes the sharing decision ex post after receiving the forecast), and mandatory information sharing (i.e., the manufacturer is mandated to share its forecast). We characterize the equilibrium outcomes under the three sharing formats and investigate the firms’ preferences regarding these formats. It is shown that when the retailer is risk-neutral, both firms are indifferent between voluntary and mandatory sharing. Among the three formats, ex ante, the retailer prefers the no-sharing format whereas the manufacturer prefers the mandatory-sharing format. In addition, we find that a more accurate forecast benefits both firms under voluntary- and mandatory-sharing formats, but may hurt both firms under the no-sharing format. Finally, we show that risk aversion plays a critical role in the firms’ sharing decisions and the impact of forecast accuracy. Specifically, when the retailer is risk-averse, the manufacturer may prefer the no-sharing format over the voluntary-sharing format, and improving forecast accuracy may hurt both firms even under voluntary sharing.
The proliferation of free trials for high-tech services calls for a careful study of their effectiveness, and the drivers thereof. On one hand, free trials can generate new paying subscribers by allowing consumers to become acquainted with the service free of charge. On the other hand, a disappointing trial experience might alienate potential customers, when they decide not to adopt the system and are lost for good. This dilemma is particularly worrisome in early periods, when service quality has not been “tried and tested” in the field, and breakdowns occur. We accommodate these phenomena in a model of consumers’ free-trial and regular adoption decisions. Among other effects, it incorporates usage- and word-of-mouth-based learning about quality in a setting where quality itself is evolving. Consumers are forward-looking in that they account for changes in quality and anticipate uncertainty reduction due to trial usage. We estimate our model and run simulations on the basis of a rich and unique data set that incorporates customers’ trial subscription, adoption, and usage behavior for an interactive digital television service. The results underscore that free trials constitute a double-edged sword, and that timing and consumers’ usage intensity during the trial are key to the effectiveness of these promotions. Implications for managers are also discussed.Data, as supplemental material, are available at http://dx.doi.org/10.1287/mksc.2015.0973.
Firms use different attribution strategies such as last-click or first-click attribution to assign conversion credits to search keywords that appear in their consumers’ paths to purchase. These attributed credits impact a firm’s future bidding and budget allocations among keywords and, in turn, determine the overall return-on-investment of search campaigns. In this paper, we model the relationship among the advertiser’s bidding decision for keywords, the search engine’s ranking decision for these keywords, and the consumer’s click-through rate and conversion rate on each keyword, and analyze the impact of the attribution strategy on the overall return-on-investment of paid search advertising.We estimate our simultaneous equations model using a six-month panel data of several hundred keywords from an online jewelry retailer. The data comprises a quasi-experiment as the firm changed attribution strategy from last-click to first-click attribution halfway through the data window. Our results show that returns for keyword investments vary significantly under the different attribution strategies. For the focal firm, first-click attribution leads to lower revenue returns and a more pronounced decrease for more specific keywords. Our policy simulation exercise shows how the firm can increase its overall returns by better attributing the real contribution of keywords. We discuss how an appropriate attribution strategy can help firms to better target customers and lower acquisition costs in the context of paid search advertising.Data, as supplemental material, are available at https://doi.org/10.1287/mksc.2016.0987.
We randomize advertising content motivated by the psychology literature on sympathy generation and framing effects in mailings to about 185,000 prospective new donors in India. We find a significant impact on the number of donors and amounts donated consistent with sympathy biases such as the “identifiable victim,” “in-group,” and “reference dependence.” A monthly reframing of the ask amount increases donors and the amount donated relative to daily reframing. A second field experiment targeted to past donors, finds that the effect of sympathy bias on giving is smaller in percentage terms but statistically and economically highly significant in terms of the magnitude of additional dollars raised. Methodologically, the paper complements the work of behavioral scholars by adopting an empirical researchers’ lens of measuring relative effect sizes and economic relevance of multiple behavioral theoretical constructs in the sympathy bias and charity domain within one field setting. Beyond the benefit of conceptual replications, the effect sizes provide guidance to managers on which behavioral theories are most managerially and economically relevant when developing advertising content.Data, as supplemental material, are available at https://doi.org/10.1287/mksc.2016.0989.
Consumer-to-consumer (C2C) platforms have become a major engine of growth in Internet commerce. This is especially true in countries such as China, which are experiencing a big rush toward e-commerce. The emergence of such platforms gives researchers the unique opportunity to investigate the evolution of such platforms by focusing on the growth of both buyers and sellers. In this research, we build a utility-based model to quantify both cross and direct network effects on Alibaba Group’s Taobao.com, the world’s largest online C2C platform (based in China). Specifically, we investigate the relative contributions of different factors that affect the growth of buyers and sellers on the platform. Our results suggest that the direct network effects do not play a big role in the platform’s growth (we detect a small positive direct network effect on buyer growth and no direct network effect on seller growth). More importantly, we find a significant, large and positive cross-network effect on both sides of the platform. In other words, the installed base of either side of the platform has propelled the growth of the other side (and thus the overall growth). Interestingly, this cross-network effect is asymmetric with the installed base of sellers having a much larger effect on the growth of buyers than vice versa. The growth in the number of buyers is driven primarily by the seller’s installed base and product variety with increasing importance of product variety. The growth in the number of sellers is driven by buyer’s installed base, buyer quality, and product price with increasing importance of buyer quality. We also investigate the nature of these cross-network effects over time. We find that the cross-network effect of sellers on buyers increases and then decreases to reach a stable level. By contrast, the cross-network effect of buyers on sellers is relatively stable. We discuss the policy implications of these findings for C2C platforms in general and Taobao in particular.Data, as supplemental material, are available at https://doi.org/10.1287/mksc.2016.0976.
This research aims to understand and predict online customers’ store visit and purchase behaviors. To this end, we develop a model that accounts for different patterns of online store visits at the individual level. Given the latency of visit patterns, we employ a changepoint modeling framework and statistically infer them using a Bayesian approach. The inferences obtained are then used to examine the effects of visit patterns on purchase dynamics across store visits. Using Internet clickstream data at an online retailer, we find that online store visit patterns tend to be clustered with significant variation across customers in terms of the number and size of visit clusters as well as the visit frequencies, both within and between clusters. Furthermore, the conversion rates vary significantly, depending on store visit patterns, such that they tend to be higher at later visits within a visit cluster, compared with earlier visits. The proposed model thereby offers superior fit and predictive performance than benchmark models that ignore clustered visit patterns and their impact on purchase behavior. We demonstrate the model’s ability to better identify prospective customers by utilizing their visit patterns, which can assist marketers in scoring customers and making targeting decisions across individuals for marketing activity.Data, as supplemental material, are available at https://doi.org/10.1287/mksc.2016.0990.
We study the effects of information content in 59,814 pharmaceutical sales calls on doctors’ prescription decisions for statins, in the face of entry of competing brands and generics, using a hierarchical Bayesian distributed lag model. We conclude that adding information content to the prescription response model improves the in- and out-of-sample performance of the model. In the first six months following generic entry, it is more effective for incumbent brands to detail on drug contraindications and indications, compared to other periods, to positively differentiate from generics. In the first six months following branded entry, it is less effective for incumbent brands to detail on drug indications and costs, given increased competitive clutter. We also document substantial heterogeneity among doctors in their response to information content. Our model is helpful for analysts to more accurately assess the effectiveness of detailing. Our empirical results are also informative for drug manufacturers as they set or change their messaging policies in response to entry and help firms to tailor their message content at the doctor level.Data, as supplemental material, are available at https://doi.org/10.1287/mksc.2015.0971.
Past research has primarily focused on what happens after a merger. This research attempts to determine whether anticipated benefits from the merger actually accrue. We characterize the effects of observed variables on whether pairs of firms merge, vis-à-vis roommate matching, and then link these factors to post-merger innovation (i.e., number of patents). We jointly estimate the two models using Markov Chain Monte Carlo methods with a unique panel data set of 1,979 mergers between 4,444 firms across industries and countries from 1992 to 2008. We find that similarity in national culture and technical knowledge has a positive effect on partner selection and post-merger innovation. Anticipated synergy from subindustry similarity, however, is not realized in post-merger innovation. Furthermore, some key synergy sources are unanticipated when selecting a merger partner. For example, financial synergy from higher total assets and complementarity in total assets and debt leverage as well as knowledge synergy from breadth and depth of knowledge positively influence innovation but not partner selection. Furthermore, factors that dilute synergy (e.g., higher debt levels) are unanticipated, and firms merge with firms that detract from their innovation potential. Overall, the results reveal some incongruity between anticipated and realized synergy.Data, as supplemental material, are available at https://doi.org/10.1287/mksc.2016.0978.
Firms collect an increasing amount of consumer feedback in the form of unstructured consumer reviews. These reviews contain text about consumer experiences with products and services that are different from surveys that query consumers for specific information. A challenge in analyzing unstructured consumer reviews is in making sense of the topics that are expressed in the words used to describe these experiences. We propose a new model for text analysis that makes use of the sentence structure contained in the reviews and show that it leads to improved inference and prediction of consumer ratings relative to existing models using data from www.expedia.com and www.we8there.com. Sentence-based topics are found to be more distinguished and coherent than those identified from a word-based analysis.Data, as supplemental material, are available at https://doi.org/10.1287/mksc.2016.0993.
Experimental methods are critical tools in marketing, psychology, and economics to isolate the effects of key variables from vagaries intrinsic to field data. As such, they are often considered exempt from the sort of sample selectivity artifacts widely documented in empirical research, in part because participants are randomly assigned to experimental conditions. To conserve time and resources, experiments often focus on items participants have chosen or are familiar with, for example, postchoice satisfaction ratings, certain free recall tasks, or specifying consideration sets preceding brand choice. When consumer input even partially influences the items about which researchers request subsequent data, the potential for item selectivity arises. In such situations, analyses are contingent on both the choice context(s) of the experiment and the alternatives participants elect to evaluate, potentially leading to substantial item selectivity overall and to differing degrees across conditions. We examine situations in which a nonignorable “choose one of many” (polytomous) selection process limits which items offer up subsequent information, and develop methods to allow substantive results to pertain to the full set of items, not only those selected. The framework is illustrated via two experiments in which participants choose and then evaluate a frequently purchased consumer good as well as data first examined by Ratner et al. [Ratner RK, Kahn BE, Kahneman D (1999) Choosing less-preferred experiences for the sake of variety. J. Consumer Res. 26(1):1–15]. Results indicate substantial item selectivity that, when corrected for, can lead to markedly different interpretations of focal variable effects, such as large effect size changes and even sign reversal. Moreover, failing to flexibly account for item selectivity across experimental conditions, even in well-designed experimental settings, can lead to inaccurate substantive inferences about consumers’ evaluative criteria. We further demonstrate robustness to theoretically driven (but not overtly misspecified) selection rules and provide researchers with a simple, “two-step” exploratory procedure akin to a “control function” approach—involving just one additional variable added to standard models—to determine whether and to what degree item selectivity may be affecting their substantive results.Data, as supplemental material, are available at https://doi.org/10.1287/mksc.2016.0991.
We explore the use of big data tools to shed new light on the idea generation process, automatically “read” ideas to identify promising ones, and help people be more creative. The literature suggests that creativity results from the optimal balance between novelty and familiarity, which can be measured based on the combinations of words in an idea. We build semantic networks where nodes represent word stems in a particular idea generation topic, and edge weights capture the degree of novelty versus familiarity of word stem combinations (i.e., the weight of an edge that connects two word stems measures their scaled co-occurrence in the relevant language). Each idea contains a set of word stems, which form a semantic subnetwork. The edge weight distribution in that subnetwork reflects how the idea balances novelty with familiarity. Based on the “beauty in averageness” effect, we hypothesize that ideas with semantic subnetworks that have a more prototypical edge weight distribution are judged as more creative. We show this effect in eight studies involving over 4,000 ideas across multiple domains. Practically, we demonstrate how our research can be used to automatically identify promising ideas and recommend words to users on the fly to help them improve their ideas.Data, as supplemental material, are available at http://dx.doi.org/10.1287/mksc.2016.0994.
We study the identification of the search method consumers use when resolving uncertainty in the prices of alternatives. We show that the search method—simultaneous or sequential—is identified with data on consumers’ consideration sets (but not the sequence of searches), prices for the considered alternatives and marketwide price distributions. We show that identification comes from differences in the patterns of actual prices in consumers’ consideration sets across search methods. We also provide a new estimation approach for the sequential search model that uses such data. Using data on consumer shopping behavior in the U.S. auto insurance industry that contain information on consideration sets and choices, we find that the pattern of actual prices in consumers’ consideration sets is consistent with consumers searching simultaneously. Via counterfactuals we show that the consideration set and purchase market shares of the largest insurance companies are overpredicted under the incorrect assumption of sequential search. As the search method affects consumers’ consideration sets, which in turn influence brand choices, understanding the nature of consumer search and its implications for consideration and choice is important from a managerial perspective.Data, as supplemental material, are available at http://dx.doi.org/10.1287/mksc.2016.0995.
Yahoo! Research partnered with a nationwide retailer to study the effects of online display advertising on both online and in-store purchases. We use a randomized field experiment on 3 million Yahoo! users who are also past customers of the retailer. We find statistically significant evidence that the retailer ads increase sales 3.6% relative to the control group. We show that control ads boost measurement precision by identifying and removing the half of in-campaign sales data that are unaffected by the ads. Less data give us 31% more precision in our estimates—equivalent to increasing our sample to 5.3 million users. By contrast, we only improve precision by 5% when we include additional covariate data to reduce the residual variance in our experimental regression. The covariate-adjustment strategy disappoints despite exceptional consumer-level data including demographics, ad exposure levels, and two years’ worth of past purchase history.Data, as supplemental material, are available at http://dx.doi.org/10.1287/mksc.2016.0998.
Firms develop products by manipulating the attributes of offerings, and consumers derive utility from the benefits that the attributes afford. While the field of marketing has long been aware of the distinction between attributes and benefits, it has not developed methods for understanding how attributes and benefits are related. This paper develops a benefit-based model for conjoint analysis that assumes consumers satiate on attributes that are perceived to provide the same benefit. A latent-variable model is proposed that estimates the map between attributes and benefits, and is applied to data from two conjoint studies involving a durable product and a household consumable. The model is shown to fit the data better, provide improved predictions, and lead to different product design implications than the standard conjoint model.Data, as supplemental material, are available at http://dx.doi.org/10.1287/mksc.2016.1003.
This paper studies the ability of competing retailers to form a cartel by sharing information with their mutual manufacturer. In a market characterized by demand uncertainty, colluding retailers wish to share information about the potential market demand to coordinate on the optimal collusive retail price. However, in light of potential exposure to antitrust investigations and possible sanctions, the retailers search for mechanisms to exchange information while avoiding the risks of scrutiny by the antitrust authorities. This paper examines such a mechanism: each retailer shares his private information with the mutual manufacturer; the wholesale price set by the latter is thereafter used by the retailers to infer the market condition and coordinate on the cartel’s price. Although a cartel at the retail level limits the manufacturer’s sold quantity, under certain conditions the manufacturer is better off accepting the retailers’ private information, thereby assisting the cartel formation. Moreover, vertical information sharing between the retailers and their mutual manufacturer can result in lower consumer surplus than that would have occurred had the retailers been permitted to collude directly.
Seeded marketing campaigns (SMCs) involve firms sending products to selected customers and encouraging them to spread word of mouth (WOM). Prior research has examined certain aspects of this increasingly popular form of marketing communication, such as seeding strategies and their efficacy. Building on prior research, this study investigates the effects of SMCs that extend beyond the generation of WOM for a campaign’s focal product by considering how seeding can affect WOM spillover effects at the brand and category levels. The authors introduce a framework of SMC-related spillover effects, and empirically estimate these with a unique data set covering 390 SMCs for products from 192 different cosmetics brands. Multiple spillover effects are found, suggesting that while SMCs can be used primarily to stimulate WOM for a focal product, marketers must also account for brand- and category-level WOM spillover effects. Specifically, seeding increases conversations about that product among nonseed consumers, and, interestingly, decreases WOM about other products from the same brand and about competitors’ products in the same category as the focal product. These findings indicate that marketers can use SMCs to focus online WOM on a particular product by drawing consumers away from talking about other related, but off-topic, products.Data, as supplemental material, are available at http://dx.doi.org/10.1287/mksc.2016.1001.
In this research, we investigate the relationship between television advertising and online word-of-mouth (WOM) by examining the joint consumption of television programming and production of social media by television viewers, termed social TV. We explore how television advertising impacts the volume of online WOM about advertised brands and about the programs in which the advertisements air. We also examine what encourages or discourages viewers to engage in this particular social TV activity. Using data containing television advertising instances and the volume of minute-by-minute social media mentions, our analyses reveal that television advertising impacts the volume of online WOM for both the brand advertised and the program in which the advertisement airs. We additionally find that the programs that receive the most online WOM are not necessarily the best programs for advertisers interested in online engagement for their brands. Finally, our results highlight the brand, advertisement, and program characteristics that can encourage or discourage social TV activity. We discuss the implications of our findings for media planning strategies and advertisement design strategies.Data, as supplemental material, are available at http://dx.doi.org/10.1287/mksc.2016.1002.
Product lines are ubiquitous. For example, Marriott International manages high-end ultra-luxury hotels (e.g., Ritz-Carlton) and low-end economy hotels (e.g., Fairfield Inn). Firms often bundle core products with ancillary services (or add-ons). Interestingly, empirical observations reveal that industries with ostensibly similar characteristics (e.g., customer types, costs, competition, distribution channels, etc.) employ different bundling strategies. For example, airlines bundle high-end first class with ancillary services (e.g., breakfast, entertainment) while hotel chains bundle ancillary services (e.g., breakfast, entertainment) at the low-end. We observe, unlike hotel lines that are highly differentiated at different geographic locations, airlines suffer low core differentiation because all passengers (first-class and economy) are at the same location (i.e., same plane, weather, delays, cancellations, etc.). In general, we find product lines with low core differentiation (e.g., airlines, amusement parks) routinely bundle high-end while product lines with highly differentiated cores (e.g., hotels, restaurants) routinely bundle low-end. High-end bundling makes the high-end more attractive, increasing line differentiation (less intraline competition) while low-end bundling decreases line differentiation. Therefore, bundling allows optimal differentiation given a differentiation constraint (complex costs). Last, firms may use strategic bundling for targeting in their core products; e.g., low-end hotels bundle targeted add-ons unattractive to high-end consumers such as lower-quality breakfasts and slower Internet.Data, as supplemental material, are available at https://doi.org/10.1287/mksc.2016.1004.
Consumers’ preferences can often be represented using a multimodal continuous heterogeneity distribution. One explanation for such a preference distribution is that consumers belong to a few distinct segments, with preferences of consumers in each segment being heterogeneous and unimodal. We propose an innovative approach for modeling such multimodal distributions that builds on recent advances in sparse learning and optimization. We apply the model to conjoint analysis where consumer heterogeneity plays a critical role in determining optimal marketing decisions. Our approach uses a two-stage divide-and-conquer framework, where we first divide the consumer population into segments by recovering a set of candidate segmentations using sparsity modeling, and then use each candidate segmentation to develop a set of individual-level heterogeneity representations. We select the optimal individual-level heterogeneity representation using cross-validation. Using extensive simulation experiments and three field data sets, we show the superior performance of our sparse learning model compared to benchmark models including the finite mixture model and the Bayesian normal component mixture model.Data, as supplemental material, are available at http://dx.doi.org/10.1287/mksc.2016.0992.
We test an information theory of prosocial behavior whereby ego utility and self-signaling crowd out the effect of consumption utility on choice. The data come from two field experiments involving purchases of a consumer good bundled with a charitable donation. Across experimental cells, we randomize the price level and the donation level. A model-free analysis of the data reveals nonmonotonic regions of demand when the good is bundled with relatively large charitable donations. Subjects also self-report lower ratings of “feeling good about themselves” when offered bundles with large donations and price discounts. The evidence suggests that price discounts crowd out consumer self-inference of altruism. Alternative motivation-crowding theories are rejected due to their inability to explain the nonmonotonic data moments. The standard use of interaction effects and other falsification checks to explore the underlying choice mechanism in an experimental setting is complicated in our self-signaling context. Instead, a novel feature of our analysis consists of using the experimental data to estimate the structural form of a model of consumer demand with self-signaling. We specify a model in which consumers obtain both consumption and ego utility from their choices. Ego utility derives from a consumer’s posterior self-beliefs after making her choice. An estimator is proposed that handles the potential multiplicity of equilibria that can arise in the self-signaling model. The model estimates allow us to quantify the economic role of ego utility and to explore the underlying signaling mechanism. Nested tests reject the hypothesis of no self-signaling. Alternative model specifications that potentially allow for nonmontonic demand without the self-signaling structure exhibit an inferior fit to the data. The model estimates imply that consumer response to the donations are mainly driven by ego utility and not by consumption utility (i.e., not by altruistic motives). The findings from the combination of a field experiment and a structural model contribute to a growing literature on self-signaling and consumer behavior by quantifying the magnitude of self-signaling on preferences and choices. The results also have implications for the design of a cause marketing campaign and the potential negative synergies between price and nonprice promotions.Data are available at https://doi.org/10.1287/mksc.2016.1012.
Two field experiments examined generosity under consumer elective pricing. In shared social responsibility (SSR), consumers choose how much to pay, knowing that a percentage of their payment goes to support a charitable cause. Replicating past research, consumers in our experiments were sensitive to the presence of charitable giving, paying more when a portion of their payment went to charity. Notably, however, they were largely insensitive to the percentage of payment allocated to charity—customers paid little more when 99% of the payment went to charity than when only 1% went to charity. Neither self-selection nor social pressure fully explained higher payments under SSR.Data and the online appendix are available at https://doi.org/10.1287/mksc.2016.1018.
We introduce a new methodology that can capture and explain differences across a series of cohorts of new customers in a repeat-transaction setting. More specifically, this new framework, which we call a vector changepoint model, exploits the underlying regime structure in a sequence of acquired customer cohorts to make predictive statements about new cohorts for which the firm has little or no longitudinal transaction data. To accomplish this, we develop our model within a hierarchical Bayesian framework to uncover evidence of (latent) regime changes for each cohort-level parameter separately, while disentangling cross-cohort changes from calendar-time changes. Calibrating the model using multicohort donation data from a nonprofit organization, we find that holdout predictions for new cohorts using this model have greater accuracy—and greater diagnostic value—compared to a variety of strong benchmarks. Our modeling approach also highlights the perils of pooling data across cohorts without accounting for cross-cohort shifts, thus enabling managers to quantify their uncertainty about potential regime changes and avoid “old data” aggregation bias.
It has been shown that a monopolist can use advance selling to increase profits. This paper documents that this may not hold when a firm faces competition. With advance selling a firm offers its service in an advance period, before consumers know their valuations for the firms’ services, or later on in a spot period, when consumers know their valuations. We identify two ways in which competition limits the effectiveness of advance selling. First, while a monopolist can sell to consumers with homogeneous preferences at a high price, this homogeneity intensifies price competition, which lowers profits. However, the firms may nevertheless find themselves in an equilibrium with advance selling. In this sense, advance selling is better described as a competitive necessity rather than as an advantageous tool to raise profits. Second, competition in the spot period is likely to lower spot period prices, thereby forcing firms to lower advance period prices, which is also not favorable to profits. Rational firms anticipate this and curtail or eliminate the use of advance selling. Thus, even though a monopolist fully exploits the practice of advance selling, rational firms facing competition either mitigate it or avoid it completely.The online appendix is available at https://doi.org/10.1287/mksc.2016.1006.
For many years, marketing managers have used dynamic sales response models to compute expected sales conditional on the available information. These models fail to recognize that the volatility (conditional variance) of sales can vary over time. Moreover, the covolatilities (conditional covariances) between sales and marketing-mix variables can be time varying. Both concepts introduce a new range of strategic and tactical considerations for product and brand managers. Using a multivariate volatility model, we investigate the covolatility of sales and the marketing mix of a focal brand and competing brands in the market. We also examine carryover effects from a volatility perspective. The methodology is applied to six product categories sold by Dominick’s Finer Foods. The results reveal valuable implications for marketing managers.Data and the online appendix are available at https://doi.org/10.1287/mksc.2016.1013.
Using a detailed data set from the U.S. automotive industry, we enrich the existing literature on product line breadth with new results that highlight previously unexplored operational aspects of its benefits and costs. We find that expanding product line breadth has a significant effect on increasing mismatch costs arising from the increased demand uncertainty associated with product proliferation. These mismatch costs are manifested through additional discounts and inventories. The effect of product line breadth on mismatch costs is comparable in magnitude to the effect on production costs, suggesting that the operational benefits of inventory pooling achievable by rationalizing product lines can be very substantial. Furthermore, we quantify the benefit of using a platform strategy to mitigate the effects of a broad product line on production costs. Finally, we propose an additional, attribute-based measure of product line breadth and find that product line breadth can work as a hedge against changes in demand conditions. For example, automakers that offer a broader range of fuel economy levels increase their market share and reduce their average discounts as gas prices become more volatile.
We develop a game-theoretic model to examine the entry of copycats and its implications by incorporating two salient features; these features are two product attributes, i.e., physical resemblance and product quality, and two consumer utilities, i.e., consumption utility and status utility. Our equilibrium analysis suggests that copycats with a high physical resemblance but low product quality are more likely to successfully enter the market by defying the deterrence of the incumbent. Furthermore, we show that higher quality can prevent the copycat from successfully entering the market. Finally, we show that the entry of copycats does not always improve consumer surplus and social welfare. In particular, when the quality of the copycat is sufficiently low, the loss in status utility from consumers of the incumbent product overshadows the small gain in consumption utility from buyers of the copycat, leading to an overall decrease in consumer surplus and social welfare.
Most new-product frameworks in marketing and economics, as well as lay beliefs and practices, hold that the larger the stock of adoption of a new product, the greater the likelihood of additional adoption. Less is known about the underlying mechanisms as well as the conditions under which this central assumption holds. We use a series of field and consequential choice experiments to demonstrate the existence of nonpositive and even negative effects of large adoption stock information on the likelihood of subsequent adoption. The results highlight the degree of homophily with the adopting stock as well as the level of customer uncertainty as key characteristics determining the nature of the effect of stock information. In particular, information about a large existing adoption stock generates a positive effect on adoption only under moderate customer uncertainty combined with sufficient homophily; in other levels of uncertainty and/or homophily we find effects ranging from null to negative. This is the first direct test and demonstration of the intricate role of information about a large stock of adoption in the new product diffusion process, and it carries direct implications for marketers.Data and the online appendix are available at https://doi.org/10.1287/mksc.2016.1011
The recent financial crisis led to the expansion of deposit-insurance coverage in many countries. We develop a structural model of the banking market in which banks act as financial intermediaries between consumers who have funds and businesses that seek loans, and explore the implications of such policies for banks and depositors. Our results indicate the policy could erode market discipline and increase banks’ moral hazard. As a result, banks extend their lending to riskier loans than they would have in the absence of the policy. We find this policy may even harm consumers. Moreover, market competition magnifies the lack of market discipline and induces additional moral hazard for excessive risk taking. Counterfactuals indicate banks may reduce their deposit interest rates by 2.7% in a duopoly market and almost triple their risk caps under the new policy. The estimated losses of depositors’ welfare are equivalent to at least a 3.27% drop in deposit interest rates.Data are available at https://doi.org/10.1287/mksc.2016.1009.
Marketing Science greatly benefited from the admirable and fastidious efforts of more than 200 different individuals who provided manuscript reviews last year. Beyond those individuals already recognized on the editorial board, the editor-in-chief and guest editors of Marketing Science are indebted to the many guest editors, guest associate editors, and ad hoc reviewers who provided expert counsel and guidance on a voluntary basis. The following list acknowledges the contribution of guest editors, guest associate editors, and ad hoc reviewers who served from January 1, 2016 to December 31, 2016. Finally, our sincere appreciation to the authors, whose outstanding submissions and careful revisions make the journal the go-to resource for leading edge knowledge in quantitative marketing.Marketing Science mourns the loss of Professor Frankel ter Hofstede, University of Texas at Austin, who passed away in an untimely accident in December 2016. The marketing science community has lost a wonderful scholar, colleague, and friend.K. SudhirYale University
The creation and sharing of user-generated content such as product reviews has become increasingly “social,” particularly in online communities where members are connected. While some online communities have used monetary rewards to motivate product review contributions, empirical evidence regarding the effectiveness of such rewards remains limited. We examine the possible moderating effect of social connectedness (measured as the number of friends) on publicly offered monetary rewards using field data from an online review community. This community saw an (unexpected) overall decrease in total contributions after introducing monetary rewards for posting reviews. Further examination across members finds a strong moderating effect of social connectedness. Specifically, contributions from less-connected members increased by 1,400%, while contributions from more-connected members declined by 90%. To corroborate this effect, we rule out multiple alternative explanations and conduct robustness checks. Our findings suggest that token-sized monetary rewards, when offered publicly, can undermine contribution rates among the most connected community members.Data and the online appendix are available at https://doi.org/10.1287/mksc.2016.1022
Chronic diseases, which account for 75% of healthcare expenditure, are of particular importance in trying to understand the rapid growth of healthcare costs over the last few decades. Individuals suffering from chronic diseases can consume three types of services: secondary preventive care, which includes diagnostic tests; primary preventive care, which consists of drugs that help prevent the illness from getting worse; and curative care, which includes surgeries and expensive drugs that provide a quantum boost to the patient’s health. Although the majority of cases can be managed by preventive care, most consumers opt for more expensive curative care that leads to a substantial increase in overall costs. To examine these inefficiencies, we build a model of consumers’ annual medical insurance plan decisions and periodic consumption decisions and apply it to a panel data set. Our results indicate that there exists a sizable segment of consumers who purchase more comprehensive plans than needed because of high uncertainty vis-à-vis their health status, and that once in the plan, they opt for curative care even when their illness could be managed through preventive care. We examine how changing cost-sharing characteristics of insurance plans and providing more accurate information to consumers via secondary preventive care can reduce these inefficiencies.Data and the Web appendix are available at https://doi.org/10.1287/mksc.2016.1021.
Firms seeking business opportunities often face corruptible agents in many markets. This paper investigates the marketing strategy implications for firms competing for business, and for the buyer in a corruptible market. We consider a setting in which a buyer (a firm or government) seeks to purchase a good through a corruptible agent. Supplier firms that may or may not be a good fit compete to be selected by the agent. Only the agent observes whether a firm is a good fit. Corruption arises due to the agent’s incentive to select a nondeserving firm in exchange for bribes. Intuitively and as expected, a sufficiently large monitoring of the agent eradicates corruption. Interestingly, however, increasing the monitoring from an initial low level can backfire, making the agent more likely to select a nondeserving firm. This nonmonotonic agent behavior makes it difficult for the buyer to reduce corruption. The implication is that the buyer should choose either to be ignorant or to take drastic measures to limit corruption. Furthermore, we show that unilateral anticorruption controls, such as the Foreign Corrupt Practices Act of 1977, on a U.S. firm seeking business in a corrupt foreign market can actually increase the firm’s profits.
This paper examines the spillover effects of promotions when products from different firms are consumed in a bundle. Using data from the HIV/AIDS category, a canonical example of combination therapy, we estimate a hierarchical Bayesian logit model across treatment regimens and show that detailing for one drug can increase demand for other drugs that are often combined with the focal drug. Such spillover effects could lead to free riding by the drugs benefitting from the spillover. We investigate the managerial and policy implications of detailing spillover effects via counterfactual policy simulations based on a dynamic oligopoly game of detailing. For managers, we show how firms can internalize the spillover effects and reduce the incentive for free riding. For policy makers, the implications of our findings relate to detailing restrictions that are often proposed on branded drugs. These restrictions aim to increase social welfare by encouraging the use of generic drugs. However, in combination therapies generic drugs may actually benefit from the detailing of complementary branded products. If detailing was curtailed, this could adversely affect generic drug prescriptions as well, which runs counter to policy makers’ objectives of encouraging use of generic drugs.Data and the online appendix are available at https://doi.org/10.1287/mksc.2016.1014.
Conventional advice to firms in competitive markets is to raise barriers against competitive poaching of their customers. However, we see instances where a firm enables competitor advertising to its customers. For example, Walmart hosts banner ads for TVs from Sears to customers searching for TVs on Walmart.com, risking a loss of customers in exchange for a commission. This paper explores whether and under what conditions allowing competitor advertising in one’s store may be a beneficial strategy. We analyze a duopoly market where customers are heterogeneous in search costs, information, and preferences. We find that hosting a competitor ad for an undifferentiated product can mitigate price competition and boost profits of both firms if the advertising commission is high enough. Otherwise, hosting competitor advertising may decrease the profits of both firms. Thus, there is no conflict of interest between firms in advertising and in setting the ad commission level. Yet the host prefers more efficient ads while the advertiser does not. Furthermore, the equilibrium outcome is asymmetric, with only one store featuring ads of the other. If stores are sufficiently differentiated in marginal costs of the product, the cost disadvantaged store will be the host. We show that the results are robust to displaying price in the ad, to different commission structures, and to customer uncertainty about the commission rate.The online appendix is available at https://doi.org/10.1287/mksc.2016.1015.
We study the strategic impacts of behavioral price discrimination (BPD) on manufacturers and retailers in a distribution channel when there are switching costs in consumer demand. Unlike previous empirical studies of behavioral price discrimination, which rely only on differences in price elasticity across customers, our pricing model allows the firm strategies to additionally account for differences in price elasticity across time (due to switching costs). We estimate a dynamic pricing model using empirical data from the cola category and, through a series of counterfactuals, we find that the retailer should simply outsource the data analytics and customization of coupons to manufacturers and improve its profit beyond what it can achieve by proactively couponing on its own. We further find that serving as an information broker to sell its customer database to manufacturers can be a vital source of profit to the retailer. By contrast, manufacturers end up worse off, illustrating that customer information is a potent source of channel power to the retailer. Finally, we show that simply using customers’ most recent purchase information can significantly impact firms’ profits. BPD based on this information is easy to implement and of low cost to manufacturers and retailers.Data are available at https://doi.org/10.1287/mksc.2016.1024.
Existing research on the compromise effect has focused exclusively on the individual. This paper investigates compromise effects in a setting that involves multiple individuals making a choice. We study whether the dyadic compromise effect (DCE) exists, the association between dyadic and individual compromise effects, and strategies to mitigate the DCE. We build a statistical model of dyadic choice that formally incorporates DCE. We conduct two studies to test our proposed models empirically. In Study 1, we begin with an investigation of the DCE with student subjects. In Study 2, we test for the presence of DCE among married couples when making retirement investment choices. In both studies, model-free and model-based evidence provides strong support for the presence of DCE. A model that incorporates DCE provides a better fit than models that do not. Evidence in support of DCE is shown to be robust to alternative compromise effect model specifications and utility aggregation methods. We find that the individual compromise effect tendency of a group member with a greater stake in the decision is likely to persist as a DCE in the joint choice setting. Our findings suggest that education of segments vulnerable to compromise effects reduces the DCE.Data and the web appendix are available at https://doi.org/10.1287/mksc.2016.1019.
Managing marketing resources over time requires dynamic model estimation, which necessitates specifying some parametric or nonparametric probability distribution. When the data generating process differs from the assumed distribution, the resulting model is misspecified. To hedge against such a misspecification risk, the extant theory recommends using the sandwich estimator. This approach, however, only corrects the variance of estimated parameters, but not their values. Consequently, the sandwich estimator does not affect any managerial outcomes such as marketing budgeting and allocation decisions. To overcome this drawback, we present the minimax framework that does not necessitate distributional assumptions to estimate dynamic models. Applying minimax control theory, we derive an optimal robust filter, illustrate its application to a unique advertising data set from the Canadian Blood Services, and contribute several novel findings. We discover the compensatory effect: Advertising effectiveness increases and the carryover effect decreases as robustness increases. We also find that the robust filter uniformly outperforms the Kalman filter on the out-of-sample predictions. Furthermore, we uncover the existence of a profit-volatility trade-off, similar to the returns-risk trade-off in finance, whereby the volatility of profit stream decreases at the expense of reduced total profit as robustness increases. Finally, we prove that, unlike for-profit companies, managers of nonprofit organizations should optimally allocate budgets opposite the advertising-to-sales ratio heuristic; that is, advertise more (less) when sales are low (high).Data and the web appendix are available at https://doi.org/10.1287/mksc.2016.1010.
We develop a structural model of brand management to estimate the value of a brand to a firm. In our framework, a brand’s value is the expected net present value of future cash flows accruing to a firm due to its brand. Our brand value measure recognizes that a firm can change its brand equity by investing in advertising. We estimate quarterly brand values in the stacked chips category for the period 2001–2006 and explore how those values change over time. Comparing our brand value measure to its static counterpart, we find that a static measure, which ignores advertising and its ability to affect brand equity dynamics, yields brand values that are artificially high and that fluctuate too much over time. We also explore how changing the ability to build and sustain brand equity affects brand value. At our estimated parameterization, if brand equity were to depreciate more slowly, or if advertising were more effective at building brand equity, then brand value would increase. However, counterintuitively, we find that when the effectiveness of advertising is sufficiently high, increasing the rate at which brand equity depreciates can increase the value of a firm’s brand, even as it reduces the value of the firm overall.Data and the online appendix are available at https://doi.org/10.1287/mksc.2016.1020.
Firms using online advertising regularly run experiments with multiple versions of their ads since they are uncertain about which ones are most effective. During a campaign, firms try to adapt to intermediate results of their tests, optimizing what they earn while learning about their ads. Yet how should they decide what percentage of impressions to allocate to each ad? This paper answers that question, resolving the well-known “learn-and-earn” trade-off using multi-armed bandit (MAB) methods. The online advertiser’s MAB problem, however, contains particular challenges, such as a hierarchical structure (ads within a website), attributes of actions (creative elements of an ad), and batched decisions (millions of impressions at a time), that are not fully accommodated by existing MAB methods. Our approach captures how the impact of observable ad attributes on ad effectiveness differs by website in unobserved ways, and our policy generates allocations of impressions that can be used in practice. We implemented this policy in a live field experiment delivering over 750 million ad impressions in an online display campaign with a large retail bank. Over the course of two months, our policy achieved an 8% improvement in the customer acquisition rate, relative to a control policy, without any additional costs to the bank. Beyond the actual experiment, we performed counterfactual simulations to evaluate a range of alternative model specifications and allocation rules in MAB policies. Finally, we show that customer acquisition would decrease by about 10% if the firm were to optimize click-through rates instead of conversion directly, a finding that has implications for understanding the marketing funnel.Data is available at https://doi.org/10.1287/mksc.2016.1023.
Outlet stores offer attractive prices at locations far from central shopping districts. They form a large and growing component of many firms’ retailing strategies, particularly in the fashion industry. I use a structural demand model to show that consumers are segmented according to their sensitivity to travel distance and taste for product newness. I then develop a supply model to predict product development responses to changes in store locations. Through policy simulations, I discover that the firm uses outlet stores to serve lower-value consumers who self-select by traveling to outlet stores from central shopping districts. The firm sells older, less desirable merchandise through outlet stores to prevent cannibalization of regular store revenues by means of exploiting the positive correlation between consumers’ travel sensitivity and taste for new products. I find that the rate of new product introduction in regular stores would fall by 16% if outlet stores were closed down, while variable profits would decline by 23%. These results imply that the existence of outlet stores may enable firms to improve quality in their regular channels, thus counteracting brand dilution effects.Data are available at https://doi.org/10.1287/mksc.2017.1031.
Consumers engage in costly searches to evaluate the increasing number of product options available from online retailers. Presenting the best alternatives at the beginning reduces search costs associated with a consumer finding the right product. We use rich data on consumer click-stream behavior from a major web-based hotel comparison platform to estimate a model of search and click. We propose a method of determining the ranking of search results that maximizes consumers’ click-through rates (CTRs) based on partial information available to the platform at the time of the consumer request, its assessment of consumers’ preferences, and the expected consumer type based on request parameters from the current visit. Our method has two distinct advantages. First, we endogenize a consumer response to the ranking using search refinement tools, such as sorting and filtering of product options. Accounting for these search refinement actions is important since the ranking and consumer search actions together shape the consideration set from which clicks are made. Second, rankings are targeted to anonymous consumers by relating price sensitivity to request parameters, such as the length of stay, number of guests, and day of the week of the stay. We find that predicted CTRs under our proposed ranking are almost double those of the platform’s default ranking.Data and the online appendix are available at https://doi.org/10.1287/mksc.2017.1036.
We study consumer search behavior in a brick-and-mortar store environment, using a unique data set obtained from radio-frequency identification tags, which are attached to supermarket shopping carts. This technology allows us to record consumers’ purchases as well as the time they spent in front of the shelf when contemplating which product to buy, giving us a direct measure of search effort. We estimate a linear regression of price paid on search duration, in which search duration is instrumented with a search-cost shifter. We show that this regression allows us to recover the marginal return from search in terms of price at the optimal stopping point for the average consumer. Our identification strategy and coefficient interpretation are valid for a broad class of search models, and we are hence able to remain agnostic about the details of the search process, such as search order and search protocol. We estimate an average return from search of $2.10 per minute and explore heterogeneity across consumer types, product categories, and category location in the store. We find little difference in the returns from search across product categories, but large differences across consumer types and locations. Our findings suggest that situational factors, such as the location of the category or the timing of the search within the shopping trip, are more important determinants of search behavior than category characteristics such as the number of available products.Data are available at https://doi.org/10.1287/mksc.2017.1026.
Whereas the extant literature on entry-order effects establishes that first entrants often earn higher market shares (“market-share advantage”), the literature on distribution suggests that increased distribution has a positive effect on sales. Can distribution help us better understand entry-order effects on market shares? This paper examines how the first entrant in a geographical market achieves a market-share advantage through distribution. For this purpose, I propose a simple method of decomposing sales into physical distribution and sales performance. The data come from a manually collected panel on six major Japanese convenience-store chains from 47 geographical markets between 1991 and 2007. Using an instrumental variable approach to address the potential endogeneity of entry order, I find first entrants have a positive market-share advantage over later entrants. Specifically, the physical distribution, measured by the number of outlets in a market, drives most of the advantage. Meanwhile, the positive effect on sales performance for the first chain brand becomes nonexistent when I control for the outlet density. This paper further finds that the density of own outlets is nonmonotonically (inverted U) related to sales performance per outlet, suggesting dynamic outlet expansion faces a trade-off between the business-stealing effect in a chain (“cannibalization”) and the advertising effect through repetition.Data and the online appendix are available at https://doi.org/10.1287/mksc.2017.1029.
This paper examines firms’ product policies when they sell an add-on (e.g., Internet service) in addition to a base product (e.g., hotel rooms) under vertical differentiation (e.g., four- versus three-star hotels). I show that the role of an add-on differs; higher-quality firms prefer to sell it as optional to discriminate consumers, and lower-quality firms trade off discrimination and differentiation, trying to lure consumers from higher-quality rivals with a lower-price add-on. Equilibrium policies of lower-quality firms are more sensitive to the cost-to-value ratio of an add-on. If the ratio is sufficiently small, then they sell it to all consumers, potentially explaining why lower-end hotels are more likely than higher-end ones to offer free Internet service. Contrary to consensus in the literature, optional add-ons can intensify price competition over consumers who trade off a higher-quality base product versus a lower-quality base including an add-on. Hence, higher-quality firms are incentivized to commit to bundling, while lower-quality firms prefer to commit to not selling it. Add-ons can further reduce lower-quality firms’ profits if consumers cannot observe the prices, because holding up consumers ex post encourages them to switch to higher-quality rivals, which then become better off. Therefore, lower-quality firms are incentivized to advertise add-on prices, and higher-quality firms are not.The online appendix is available at https://doi.org/10.1287/mksc.2017.1028.
We use monthly sales of all wines, beer, and spirits sold between 2006 and 2011 by Sweden’s retail monopoly on alcohol to estimate the causal effect of retail distribution on market share by volume at the product level. Products are defined at the level of the stock-keeping unit. Two institutional features are key to identifying the causal effect: First, the monopolist uses four levels of retail distribution; a change in retail distribution is therefore associated with a discrete shift in the number of stores that carry a product in a given month. Second, the retailer is legally bound and monitored by the European Union to ensure that it acts in a non-discriminatory manner with all its suppliers. These features allow us to rule out many possible confounding factors in estimating the effect of distribution on sales volume. We find large and statistically significant effects from changes in retail distribution on market share by volume across all levels of retail distribution. The associated volume elasticity of retail distribution is convex; the wider the retail distribution the greater the marginal volume increase from further widening. In this market, wider distribution means reaching stores with successively smaller assortment. Our results indicate that the smaller assortment in smaller stores, coupled with a low resistance to compromise, is the main reason for the convex pattern. In other words, convexity appears to be generated by products achieving “a larger share of a smaller pie” as retail distribution expands.Data are available at https://doi.org/10.1287/mksc.2017.1038.
We investigate the relationship between a firm’s use of management responses and its online reputation. We focus on the hotel industry and present several findings. First, hotels are likely to start responding following a negative shock to their ratings. Second, hotels respond to positive, negative, and neutral reviews at roughly the same rate. Third, by exploiting variation in the rate with which hotels respond on different review platforms and variation in the likelihood with which consumers are exposed to management responses, we find a 0.12-star increase in ratings and a 12% increase in review volume for responding hotels. Interestingly, when hotels start responding, they receive fewer but longer negative reviews. To explain this finding, we argue that unsatisfied consumers become less likely to leave short indefensible reviews when hotels are likely to scrutinize them. Our results highlight an interesting trade-off for managers considering responding: fewer negative ratings at the cost of longer and more detailed negative feedback.Data and the online appendix are available at https://doi.org/10.1287/mksc.2017.1043.
We investigate in a competitive setting the consequences of mobile geo targeting, the practice of firms targeting consumers based on their real-time locations. A distinct market feature of mobile geo targeting is that a consumer could travel across different locations for an offer that maximizes his total utility. This mobile-deal seeking opportunity motivates firms to carefully balance prices across locations to avoid intrafirm cannibalization, which in turn mitigates interfirm price competition and prevents firms from going into a prisoner’s dilemma. As a result, a firm’s profit can be higher under mobile geo targeting than under uniform or traditional targeted pricing. We extend our model in three different directions: (a) a fraction of consumers are not aware of mobile offers outside of their permanent locations, (b) mobile offers can be collected when consumers travel for other reasons, and (c) firms use both permanent and real-time locations when setting prices. Our findings have important managerial implications for marketers who are interested in optimizing their mobile geo-targeting strategies.The online appendix is available at https://doi.org/10.1287/mksc.2017.1030.
Evidence shows that marketers can direct consumers’ limited attention to specific product attributes by making them “prominent.” This research asks: How should firms decide which attribute to make prominent in competitive environments? A key feature of this setting is that consumers’ preferences are context-dependent and that a firm’s choice of an attribute affects the evaluation of all products in the category. We develop a model in which firms selectively promote one of two attributes (e.g., image or performance) before competing in price. We find when consumers evaluate both attributes, perceived differentiation within an attribute can become diluted; we call this the dilution effect. This implies that making the same attribute prominent can arise in equilibrium. Only if there is a sufficient quality advantage in an attribute do we find equilibria with firms making different attributes prominent. We also show how the dilution effect can be a disincentive for investments in quality improvements.Data and the online appendix are available at https://doi.org/10.1287/mksc.2017.1037.
Efforts on developing, implementing, and evaluating a marketing analytics framework at a real-world company are described. The framework uses individual-level transaction data to fit empirical models of consumer response to marketing efforts and uses these estimates to optimize segmentation and targeting. The models feature themes emphasized in the academic marketing science literature, including incorporation of consumer heterogeneity and state dependence into choice, and controls for the endogeneity of the firm’s historical targeting rule in estimation. To control for the endogeneity, we present an approach that involves conducting estimation separately across fixed partitions of the score variable that targeting is based on, which may be useful in other behavioral targeting settings. The models are customized to facilitate casino operations and are implemented at the MGM Resorts International’s group of companies. The framework is evaluated using a randomized trial implemented at MGM involving about 1.5 million consumers. Using the new model produces about $1 million to $5 million in incremental profits per campaign, translating to about 20¢ in incremental profit per dollar spent relative to the status quo. At current levels of marketing spending, this implies between $10 million and $15 million in incremental annual profit for the firm. The case study underscores the value of using empirically relevant marketing analytics solutions for improving outcomes for firms in real-world settings.Data are available at https://doi.org/10.1287/mksc.2017.1039.
In 2008, New York City mandated that all chain restaurants post calorie information on their menus. For managers of chain and standalone restaurants, as well as for policy makers, a pertinent goal might be to monitor the impact of this regulation on consumer conversations. We propose a scalable Bayesian topic model to measure and understand changes in consumer opinion about health (and other topics). We calibrate the model on 761,962 online reviews of restaurants posted over eight years. Our model allows managers to specify prior topics of interest such as “health” for a calorie posting regulation. It also allows the distribution of topic proportions within a review to be affected by its length, valence, and the experience level of its author. Using a difference-in-differences estimation approach, we isolate the potentially causal effect of the regulation on consumer opinion. Following the regulation, there was a statistically small but significant increase in the proportion of discussion of the health topic. This increase can be attributed largely to authors who did not post reviews before the regulation, suggesting that the regulation prompted several consumers to discuss health in online restaurant reviews.Data and the online appendix are available at https://doi.org/10.1287/mksc.2017.1048.
In many industries, product design and manufacturing lead times are sufficiently long that both the quality level of a product and the amount of inventory produced must be determined before a firm knows what the actual demand will be. In this paper, we conduct a theoretical analysis of such a setting. We first consider a centralized channel and characterize the optimal decisions by establishing relationships that must hold between the elasticity of cost of quality and the elasticity of revenue and show that quality and inventory are strategic substitutes. Next, we consider a decentralized channel with a wholesale price contract, in which a manufacturer determines quality and wholesale price, while a retailer determines inventory and retail price. We find that, different from the case without endogenous inventory, product quality can be higher in a decentralized channel compared to a centralized channel, and this is because a wholesale price contract shields the manufacturer from inventory risk. For both centralized and decentralized channels, we find that as demand uncertainty increases, quality decreases, while, different from the case without endogenous quality, inventory can be U-shaped. Interestingly, to mitigate the impact of demand uncertainty on profit, quality can be a more effective lever than inventory in a centralized channel; however, in a decentralized channel, quality is less responsive and inventory is more responsive to demand uncertainty than in a centralized channel.The online appendix is available at https://doi.org/10.1287/mksc.2017.1041.
Although firms are leveraging weather conditions in promotions, they struggle to quantify the impact. This study exploits field experiment data on weather-based mobile promotions with over six million users. Results find that sunny and rainy weather have first-order main effects. Purchase responses to promotions are higher and faster in sunny weather relative to cloudy weather, whereas purchase responses to promotions are lower and slower in rainy weather. These findings are robust across different measures of weather changes with both backward-looking historical weather and forward-looking forecasts, as well as deviations from normal weather. Also, sunny and rainy weather have second-order interactive effects with ad copies of mobile promotions. Compared with the neutral ad copy, the prevention frame ad copy hurts the initial promotion boost induced by sunshine, but improves the initial promotion drop induced by rainfall. For marketers, these findings imply new opportunities in customer data analytics for more effective weather-based mobile targeting.Data and the online appendix are available at https://doi.org/10.1287/mksc.2017.1044.
Some firms use a curious pricing mechanism called “pay as you wish” pricing (PAYW). When PAYW is used, a firm lets consumers decide what a product is worth to them and how much they want to pay to get the product. This practice has been observed in a number of industries. In this paper, we theoretically investigate why and where PAYW can be a profitable pricing strategy relative to the conventional “pay as asked” pricing (PAAP) strategy. We show that PAYW has a number of advantages over PAAP such that it is well suited for some industries but not for others. These advantages are as follows: (1) PAYW helps a firm to maximally penetrate a market; (2) it allows a firm to price discriminate among heterogenous consumers; (3) it helps to moderate price competition. We derive conditions under which PAYW dominates PAAP and discuss ways to improve the profitability of PAYW.
A key conundrum facing organizations is how to adjust marketing budgets in response to the business cycle. While most firms use procyclical spending (spending less during economic contractions), academic studies often recommend countercyclical spending (spending more during contractions), which begs the following question: What is the right thing to do? The spending problem is compounded further when demand is not just driven by one country’s business cycle, but by the (nonsynchronized) business cycles of multiple countries, as is the case for tourism marketing aiming to attract tourists originating from different countries. We derive insights into the best way to allocate marketing budgets across countries under varying economic conditions. We show that the allocation decisions are driven by the procyclical versus countercyclical nature of three factors: unit sales, marketing effectiveness, and per-unit profit contribution. To study how unit sales and marketing effectiveness respond to the business cycle, we develop a transfer function dynamic hierarchical linear model. We also model the responsiveness of the profit contribution to the business cycle. In an application to New Zealand tourism marketing, we find that a reallocation of the government’s marketing budget could yield an increase in tourist revenues of NZD $121 million.Data and the online appendix are available at https://doi.org/10.1287/mksc.2017.1046.
In developing countries, mobile telecom networks have emerged as major providers of financial services, bypassing the sparse retail networks of traditional banks. We analyze a large individual-level data set of mobile money transactions in Tanzania to provide evidence of the impact of mobile money on alleviating financial exclusion in developing countries. We identify three types of transactions: (i) money transfers to others, (ii) short-distance money self-transportation, and (iii) money storage for short to medium periods of time. We utilize a natural experiment of an unanticipated increase in transaction fees to identify the demand for these transactions. Using the demand estimates, we find that the willingness to pay to avoid walking with cash an extra kilometer (short-distance self-transportation) and to avoid storing money at home (money storage) for an extra day are 1.25% and 0.8% of an average transaction, respectively, which demonstrates that mobile money ameliorates significant amounts of crime-related risk. We explore the implications of these estimates for pricing and demonstrate the profitability of incentive-compatible price discrimination based on type of service, consumer location, and distance between transaction origin and destination. We show that differential pricing based on the features of a transaction delivers a Pareto improvement.Data and the online appendix are available at https://doi.org/10.1287/mksc.2017.1027.
We leverage a temporary block of the Chinese microblogging platform Sina Weibo due to political events to estimate the causal effect of online word-of-mouth content on product demand in the context of TV show viewership. Based on this source of exogenous variation, we estimate an elasticity of TV show ratings (market share in terms of viewership) with respect to the number of relevant comments (comments were disabled during the block) of 0.016. We find that more postshow microblogging activity increases demand, whereas comments posted prior to the show airing do not affect viewership. These patterns are inconsistent with informative or persuasive effects and suggest complementarity between TV consumption and anticipated postshow microblogging activity.Data and the online appendix are available at https://doi.org/10.1287/mksc.2017.1045.
This study analyzes the impact of offline television advertising on multiple metrics of online chatter or user-generated content. The context is a quasi experiment in which a focal brand undertakes a massive advertising campaign for a short period of time. The authors estimate multiple dimensions of chatter (popularity, negativity, visibility, and virality) from numerous raw metrics using the content and the hyperlink structure of consumer reviews and blogs. The authors use the method of synthetic control to construct a counterfactual (synthetic) brand as a convex combination of the rivals during the preadvertising period. The gap in the dimensions of chatter between the focal brand and the synthetic brand in the test versus advertising periods assesses the influence of advertising. Offline television advertising causes a short but significant positive effect on online chatter. This effect is stronger on information-spread dimensions (visibility and virality) than on content-based dimensions (popularity and negativity). Importantly, advertising has a small short-term effect in decreasing negativity in online chatter.  Data and the online appendix are available at https://doi.org/10.1287/mksc.2017.1040
Increased competition from the Internet has raised concerns about the quality of prescription drugs sold online. Given the pressure from the Department of Justice, Google agreed to ban pharmacies not certified by the National Association of Boards of Pharmacy (NABP) from sponsored search listings. Using comScore click-through data originated from health-related queries, we study how the ban affects consumer search and click behavior in a difference-in-differences framework using the synthetic control method. We find that non-NABP-certified pharmacies received fewer clicks after the ban, and this effect is heterogeneous. In particular, pharmacies not certified by the NABP but certified by other sources (other-certified websites), experienced an increase in organic clicks that partially offset the loss in paid clicks after the ban. By contrast, pharmacies not certified by any certification agency experience much lower rates of substitution in organic clicks. These results suggest that the ban has increased the search cost for other-certified websites, but at least some consumers overcome the search cost by switching from sponsored to organic links. The lower substitution for uncertified websites may be explained by the rising consumer concerns about the quality of drugs sold on uncertified websites after the ban.Data and the online appendix are available at https://doi.org/10.1287/mksc.2017.1058.
Multichannel retailing has created several new strategic choices for retailers. With respect to pricing, an important decision is whether to offer a “self-matching policy,” which allows a multichannel retailer to offer the lowest of its online and store prices to consumers. In practice, we observe considerable heterogeneity in self-matching policies: There are retailers who offer to self-match and retailers who explicitly state that they will not match prices across channels. Using a game-theoretic model, we investigate the strategic forces behind the adoption (or non-adoption) of self-matching across a range of competitive scenarios, including a monopolist, two competing multichannel retailers, as well as a mixed duopoly. Though self-matching can negatively impact a retailer when consumers pay the lower price, we uncover two novel mechanisms that can make self-matching profitable in a duopoly setting. Specifically, self-matching can dampen competition online and enable price discrimination in-store. Its effectiveness in these respects depends on the decision-making stage of consumers and the heterogeneity of their preference for the online versus store channels. Surprisingly, self-matching strategies can also be profitable when consumers use “smart” devices to discover online prices in stores. Our findings provide insights for managers on how and when self-matching can be an effective pricing strategy.The online appendix is available at https://doi.org/10.1287/mksc.2017.1035.
In the pharmaceutical industry, a product recall financially impacts not only the firm undertaking the recall but also other competitors in the category since it affects physician and consumer perception of the category as a whole. Often, such competitors have to engage in defensive marketing at the category level without complete certainty about whether a recall will occur or not. Such defensive effort could then lead to a change in postrecall sales effort directed at capturing market share in that category. This decision is affected by the probability of the recall and the size of the loyal segment in the category facing the recall, i.e., physicians who will continue to prescribe the category even without marketing effort. We focus on competitor reaction to product recalls where the competitor participates in multiple product categories that exhibit (dis)economies of scope in sales effort across them. Equilibrium analysis of our game-theoretic model uncovers several managerial insights that illustrate the importance of scale (dis)economies on the competitor’s promotional strategy in the wake of a recall. First, economies of scope across the two products leads to either an increase or decrease in postrecall sales effort for both products simultaneously depending on the loyal market size for the category. Second, diseconomies of scope can lead to a complete withdrawal of postrecall sales effort from one of the two products depending on the size of the loyal market, the cross-category price, and the recall probability. Third, as the recall probability increases, category-defense effort and postrecall sales effort are unequivocally complementary given economies of scope across the two products but may be substitutes given diseconomies of scope.The online appendix is available at https://doi.org/10.1287/mksc.2017.1054.
With the cooperation of a large mobile service provider, we conduct a novel field experiment that simultaneously randomizes the prices of two competing movie theaters using mobile coupons. Unlike studies that vary only one firm’s prices, our experiment allows us to account for competitor response. We test mobile targeting based on consumers’ real-time and historic locations, allowing us to evaluate popular mobile coupon strategies in a competitive market. The experiment reveals substantial profit gains from mobile discounts during an off-peak period. Both firms could create incremental profits by targeting their competitor’s location. However, the returns to such “geoconquesting” are reduced when the competitor also launches its own targeting campaign. We combine our experimentally generated data with a demand model to analyze optimal pricing in a static Bertrand–Nash equilibrium. Interestingly, competitive responses raise the profitability of behavioral targeting where symmetric pricing incentives soften price competition. By contrast, competitive responses lower the profitability of geographic targeting, where asymmetric pricing incentives toughen price competition. If we endogenize targeting choice, both firms would choose behavioral targeting in equilibrium, even though more granular geobehavioral targeting combining both real-time and historic locations is possible. These findings demonstrate the importance of considering competitor response when piloting novel price-targeting mechanisms.Data are available at https://doi.org/10.1287/mksc.2017.1042.
As Internet advertising infomediaries nowadays provide rich competition information, sponsored search advertisers are becoming more strategic when selecting keywords. This paper empirically examines the spillover effects in advertisers’ keyword market entry decisions, that is, how an advertiser’s likelihood of using a keyword is affected by competitors’ keyword entry decisions. We develop a structural model to characterize advertisers’ keyword market entry decisions. We apply the model to a panel data set of 1,252 laptop-related keywords mainly used by 28 manufacturers, retailers, and comparison websites that advertise on Google. Our analysis leads to several interesting findings. First, an advertiser’s expected position affects the nature of the competition. In particular, the spillover effect from below-ranked competitors is always positive, while the spillover effect from above-ranked competitors is either positive or negative. Second, the spillover effect from above-ranked ads is directionally affected by firms’ product-line characteristics: the effect among firms offering homogenous products (e.g., comparison sites) is negative, whereas the effect among firms with more differentiated products (e.g., manufacturers and retailers) is positive. Third, the spillover effect from above-ranked ads is directionally affected by firms’ positions in a distribution channel: the effect from upstream (downstream) on downstream (upstream) firms tends to be negative (positive). Finally, a downstream firm is more likely to learn new keywords from an upstream firm but not vice versa. Our counterfactual simulations demonstrate that the keyword-specific competition information provided by infomediaries can improve the search engine’s revenue by about 5.7%.Data are available at https://doi.org/10.1287/mksc.2017.1053.
In this paper, I introduce a framework of price promotions by firms that preschedule their sale dates. I set up a dynamic model of demand accumulation in which high- and low-valuation consumers enter the market every period. The high-valuation consumers buy immediately and leave the market; the low-valuation consumers accumulate while waiting for the sale price. The firms schedule the dates of their promotions in advance. I find that often they use mixed strategies, choosing the future promotion period according to a probability distribution function. If the firms can cancel their prescheduled sales, typically they can wait longer until holding sales by shifting the probability distribution towards later periods. Scheduling promotions in advance increases the firms’ profits in comparison to the case when the promotion decision is made in the period when the promotion is offered.Data and the online appendix are available at https://doi.org/10.1287/mksc.2017.1052.
Marketing Science has introduced a new section “Marketing Science: Frontiers,” focused on publishing timely research with high potential for impact. The section is positioned as “different, but equal” relative to regular Marketing Science with the same high quality standards, but differentiated contribution criteria and a shorter Science–like format to highlight the core contribution and maximize readability and impact. The section will encourage competition among authors for publishing timely and contemporaneously relevant research—undervalued attributes in traditional contribution evaluation—on topics with high impact potential. In exchange, it will accept papers that make major contributions on one “primary” dimension (methodological, modeling or substantive), with more relaxed thresholds on the non-primary dimensions compared to traditional top journals, and offer faster reviews and time to print. Authors benefit from the promise of first-mover impact rewards, while the field benefits from faster entry and larger volume of novel, timely and relevant ideas. The section will have a distinct editorial structure and a one round conditional accept/out review process. The editorial elaborates on the purpose of the section, its editorial structure and publication process.
Instead of purchasing individual content, streaming adopters rent access to libraries from which they can consume content at no additional cost. In this paper, we study how the adoption of music streaming affects listening behavior. Using a unique panel data set of individual consumers’ listening histories across many digital music platforms, adoption of streaming leads to very large increases in the quantity and diversity of consumption in the first months after adoption. Although the effects attenuate over time, even after half a year, adopters play substantially more, and more diverse, music. Relative to music ownership, where experimentation is expensive, adoption of streaming increases new music discovery. While repeat listening to new music decreases, users’ best discoveries have higher play rates. We discuss the implications for consumers and producers of music.Data and the online appendix are available at https://doi.org/10.1287/mksc.2017.1051.
We measure the causal effects of income and wealth on the demand for private-label products. Prior research suggests that these effects are large and, in particular, that private-label demand rises during recessions. Our empirical analysis is based on a comprehensive household-level transactions database matched with price information from store-level scanner data and wealth data based on local house value indices. The Great Recession provides a key source of the variation in our data, showing a large and geographically diverse impact on household incomes over time. We estimate income and wealth effects using “within” variation of income, at the household level, and wealth, at the zip code level. Our estimates can be interpreted as income and wealth effects consistent with a consumer demand model based on utility maximization. We establish a precisely measured negative effect of income on private-label shares. The effect of wealth is negative but not precisely measured. However, the estimated effect sizes are small, by contrast to prior academic work and industry views. An examination of the possible supply-side response to the recession shows only small changes in the relative price of national-brand and private-label products. Our estimates also reveal a large positive trend in private-label shares that predates the Great Recession. We examine some possible factors underlying this trend, but find no evidence that it is systematically related to specific private-label quality tiers or to the overall rate of private-label versus national-brand product introductions.Data and the online appendix are available at https://doi.org/10.1287/mksc.2017.1047.
We investigate the increasingly common business setting in which companies face the possibility of both observed and unobserved customer attrition (i.e., “overt” and “silent” churn) in the same pool of customers. This is the case for many online-based services where customers have the choice to stop interacting with the firm either by formally terminating the relationship (e.g., canceling their account) or by simply ignoring all communications coming from the firm. The standard contractual versus noncontractual categorization of customer–firm relationships does not apply in such hybrid settings, which means the standard models for analyzing customer attrition do not apply. We propose a hidden Markov model (HMM)-based framework to capture silent and overt churn. We apply our modeling framework to two different contexts—a daily deal website and a performing arts organization. In contrast to previous studies that have not separated the two types of churn, we find that overt churners in these hybrid settings tend to interact more, rather than less, with the firm prior to churning; that is, in settings where both types of churn are present, a high level of activity—such as customers actively opening emails received from the firm—is not necessarily a good indicator of future engagement; rather it is associated with higher risk of overt churn. We also identify a large number of “silent churners” in both empirical applications—customers who disengage with the company very early on, rarely exhibit any type of activity, and almost never churn overtly. Furthermore, we show how the two types of churners respond very differently to the firm’s communications, implying that a common retention strategy for proactive churn management is not appropriate in these hybrid settings.Data and the online appendix are available at https://doi.org/10.1287/mksc.2017.1057.
We explore the effects of television advertising in the setting of the National Football League’s Super Bowl telecast. The Super Bowl is well suited for evaluating advertising because viewers pay attention to the ads, more than 40% of households watch the game, and variation in ad exposures is exogenous because a brand cannot choose how many impressions it receives in each market. Viewership is primarily determined based on local preferences for watching the two competing teams. We combine Super Bowl ratings data with weekly sales data in the beer and soda categories to document three primary findings about advertising. First, the relationship between Super Bowl viewership and sales in the week leading up to the game reveals the brands customers buy to consume during the game. We find some brands are consumed while watching the game while others are not, but this is unrelated to whether a brand ever advertised during the Super Bowl or advertises in a specific year. This rejects the theory that advertising works by serving as a complement to brand consumption. Second, we find that post–Super Bowl sales effects of ad viewership are concentrated in weeks with subsequent sporting events. This suggests Super Bowl advertising builds a complementarity between the brand and sports viewership more broadly. Finally, we collect data on National Collegiate Athletic Association basketball tournament viewership to test this theory and find that the complementarity between a brand’s sales and viewership of the tournament is enhanced by Super Bowl ad viewership. Together, these findings identify advertising as a determinant of why some brands outperform others for particular consumption occasions such as sports viewership.Data are available at https://doi.org/10.1287/mksc.2017.1055.
Between 2006 and 2011, daily print newspapers in the United States lost 20% of their paid subscribers, partly because of the increasing availability of alternative sources of news, such as free content provided on newspaper websites and by news aggregators such as Yahoo. However, contrary to the expectation that firms respond to softening demand by lowering prices, newspapers increased subscription prices by 40%–60% during this period. In this paper, we explain and quantify the factors responsible for these price increases. We calibrate models of readership and advertising demand using data from a top-50 U.S. regional print newspaper. Conditional on these demand models, we calibrate the newspaper’s optimal pricing equations and assess whether the increases in subscription prices are mainly rationalized by (a) the decline in overall reader willingness to pay (WTP) in the presence of heterogeneity among subscribers, which rendered it optimal for the newspaper to focus on the high WTP readers, or (b) the newspaper’s reduced incentive to subsidize readers at the expense of advertisers, because of softening demand for newspaper advertising. We find that the decline in the ability of the newspaper to subsidize readers by extracting surplus from advertisers explains most of the increase in subscription prices. Of the three available subscription options (daily, weekend, and Sunday only), subscription prices increased more steeply for the daily option, a pattern consistent with the view that newspapers are driving away low valuation weekday readers while preserving Sunday readership and the corresponding ad revenues. Thus, our research augments theoretical propositions in two-sided markets by providing a formal empirical approach to unraveling the relative importance of the roles played by agents on the subsidy and demand sides in determining prices.Data and the online appendix are available at https://doi.org/10.1287/mksc.2017.1060.
Microentrepreneurs in emerging markets often rely on informal lenders for their routine borrowing needs. This paper investigates informal lenders’ and microentrepreneurs’ incentives to participate in a lender–borrower relationship in a market in which repayments are neither law protected nor asset secured. We consider a borrower who seeks a short-term loan, invests in a project, and repays in full using her project earnings if the project is successful. If the project fails, the borrower uses her outside option to repay over a period of time. The analysis uncovers an interesting effect of the borrower’s outside option on the loan rate offered by the lender—the loan rate first increases and then decreases with the borrower’s outside option. An important policy implication is that an increase in the outside options of the poor microentrepreneurs might actually reduce their surplus. Finally, we find that lenders in emerging markets may be more likely to engage in informal lending compared to those in developed or poorer markets.
This paper develops a new rationale for decentralization in distribution channels: providing a one-stop comparison shopping experience for consumers. In our duopoly model, when consumers are knowledgeable about their brand preferences, each manufacturer would distribute through its own vertically integrated retail outlets only. When some consumers are unsure about their brand preferences, however, it may be optimal for one of the manufacturers to also distribute through its competitor’s outlets. The resulting equilibrium has several interesting properties. First, only one of the manufacturers chooses to add competitor-outlet distribution, not both—even when the manufacturers are symmetric. Second, the manufacturer distributing through its competitor’s outlets also distributes through its own outlets, i.e., its distribution strategy is a hybrid strategy, combining vertical integration and decentralization. Third, when the manufacturers’ brands are asymmetric, it is the weaker brand that has a stronger incentive to pursue hybrid distribution. Fourth, the competitor’s outlets in question welcome the new brand, even when no consumer would actually buy the new brand—a case of pure showrooming. These results highlight the linkages between distribution strategy, shopping efficiency, and retail formats. Shopping costs and consumers’ uncertainty about their own brand preferences create a demand for multibrand retailing, and in pursuing this demand, manufacturers may eschew the efficiency advantages of vertical integration in favor of hybrid distribution. However, the fact that only one of the manufacturers chooses to do so suggests that this strategy also has weaknesses, which we discuss in the paper.
We develop a flexible methodology to protect marketing data in the context of a business ecosystem in which data providers seek to meet the information needs of data users, but wish to deter invalid use of the data by potential intruders. In this context we propose a Bayesian probability model that produces protected synthetic data. A key feature of our proposed method is that the data provider can balance the trade-off between information loss resulting from data protection and risk of disclosure to intruders. We apply our methodology to the problem facing a vendor of retail point-of-sale data whose customers use the data to estimate price elasticities and promotion effects. At the same time, the data provider wishes to protect the identities of sample stores from possible intrusion. We define metrics to measure the average and maximum loss of protection implied by a data protection method. We show that, by enabling the data provider to choose the degree of protection to infuse into the synthetic data, our method performs well relative to seven benchmark data protection methods, including the extant approach of aggregating data across stores.Data are available at https://doi.org/10.1287/mksc.2017.1064.
In the digital economy, influencing and controlling the spread of information is a key concern for firms. One way firms try to achieve this is to target firm communications to consumers who embrace and propagate the spread of new information on emerging and “trending” topics on social media. However, little is known about whether early trend propagators are indeed responsive to firm-sponsored messages. To explore whether early propagators of trending topics respond to advertising messages, we use data from two field tests conducted by a charity and an emerging fashion firm on the microblogging service Twitter. On Twitter, “promoted tweets” allow advertisers to target individuals based on the content of their recent postings. Twitter continuously identifies in real time which topics are newly popular among Twitter users. In the field tests, we collaborated with a charity and a fashion firm to target ads at consumers who embraced a Twitter trend early in its life cycle by posting about it, and compared their behavior to that of consumers who posted about the same topic later on. Throughout both field tests, we consistently find that early propagators of trends are less responsive to advertising than consumers who embrace trends later.Data and the online appendix are available at https://doi.org/10.1287/mksc.2017.1062.
On search keywords with trademarked terms, the brand owner (“focal brand”) and other relevant firms compete for consumers. For the focal brand, paid clicks have a direct substitute in the organic links below the paid ad(s). The proximity of this substitute depends on whether competing firms are aggressively bidding to siphon off traffic. We study the returns to focal brands and competitors using large-scale experiments on Bing with data from thousands of brands. When no competitors are present, we find a positive, statistically significant impact of brand ads of 1%–4%, with larger brands having a smaller causal effect. In this case, the effective “cost per incremental click” is significantly higher than what focal brands typically pay on other keywords. When the focal brand ad is present, competitors in paid positions 2–4 can “steal” 1%–5% of the focal brand’s clicks and raise its costs by shifting traffic to the paid link. Finally, for a set of brands that face competition on their brand search but choose not to advertise, competitors “steal” 18%–42% of clicks, suggesting a strong causal effect of position. Under such position effects, we find the return on investment on defensive advertising to be strongly positive.Data and the online appendix are available at https://doi.org/10.1287/mksc.2017.1065.
Marketing managers are responsible for understanding and predicting customer purchasing activity. This task is complicated by a lack of knowledge of all of the calendar time events that influence purchase timing. Yet, isolating calendar time variability from the natural ebb and flow of purchasing is important for accurately assessing the influence of calendar time shocks to the spending process, and for uncovering the customer-level purchasing patterns that robustly predict future spending. A comprehensive understanding of purchasing dynamics therefore requires a model that flexibly integrates known and unknown calendar time determinants of purchasing with individual-level predictors such as interpurchase time, customer lifetime, and number of past purchases. In this paper, we develop a Bayesian nonparametric framework based on Gaussian process priors, which integrates these two sets of predictors by modeling both through latent functions that jointly determine purchase propensity. The estimates of these latent functions yield a visual representation of purchasing dynamics, which we call the model-based dashboard, that provides a nuanced decomposition of spending patterns. We show the utility of this framework through an application to purchasing in free-to-play mobile video games. Moreover, we show that in forecasting future spending, our model outperforms existing benchmarks.Data and the online appendix are available at https://doi.org/10.1287/mksc.2017.1050.
In collaboration with three companies selling a diverse set of products, we conducted randomized field experiments in which experimentally tailored email ads were sent to millions of individuals. We found consistently that personalizing the emails by adding consumer-specific information (e.g., recipient’s name) benefited the advertisers. Importantly, such content is not likely to be informative about the advertised product or the company. In our main experiment, we found that adding the name of the message recipient to the email’s subject line increased the probability of the recipient opening it by 20% (from 9.05% to 10.80%), which translated to an increase in sales’ leads by 31% (from 0.39% to 0.51%) and a reduction in the number of individuals unsubscribing from the email campaign by 17% (from 1.2% to 1.0%). We present similar experiments conducted with other companies, which show that the effects we document extend from objectives ranging from acquiring new customers to retaining customers who have purchased from the company in the past. Our investigation of several possible mechanisms suggests that such content increases the effort consumers make in processing the other content in the rest of the advertising message. Our paper quantifies the benefits from personalization and sheds light on the role of noninformative advertising content by analyzing several detailed measures of recipient’s interaction with the message. It provides external validity to psychological mechanisms and has clear implications for the firms that are designing their advertising campaigns.Data and the online appendix are available at https://doi.org/10.1287/mksc.2017.1066.
Marketing researchers and practitioners are interested in targeting individuals in social networks who may have disproportionately higher levels of influence over others in their network. While the extant literature suggests individual characteristics or network position as proxies for relative influence, our study bridges these two streams by investigating the endogenous acquisition of network position as a function of exogenous individual characteristics. Specifically, do those with higher expertise achieve higher influence when people endogenously choose those to whom they listen? Using an agent-based modeling simulation framework, we model the dynamics of two types of individuals, i.e., independents with exogenous information and imitators. Over the course of multiple diffusions, agents choose whom to “listen to” for information; dropping less useful ties and adding new ones. We find that independents can have less influence (out-degree) than imitators who collect information from multiple sources. Furthermore, this effect is exacerbated by homophily. Noise in communication channels, on the other hand, moderates these effects, yet can increase penetration rates. We show that our results are robust to alternative dynamic network structures. Our research suggests that marketers should consider the environment, community characteristics, communication medium, and product domains when assessing the relative influence of individuals.Data and the online appendix are available at https://doi.org/10.1287/mksc.2017.1077.
Consumer preferences change through time and firms must adjust their product positioning for their products to continue to be appealing to consumers. These changes in product positioning require fixed investments such that firms reposition only occasionally. I construct a model that can include predictable and unpredictable consumer preference changes, and where a firm optimally repositions its product given the current market conditions, and expected future repositionings. When unpredictable consumer preferences evolve away from a current firm’s positioning, the decision to reposition is like exercising an option to be closer to current consumer preferences, or waiting to reposition later or for those preferences to return so as to be closer to the firm’s current position. We characterize this optimal repositioning strategy, how it depends on the discount factor, variance of preferences, and repositioning costs. I compare the optimal policy of the firm with what could be optimal from a social welfare point of view, and find that the firm repositions more frequently than is efficient when there is full market coverage. With predictable changes in consumer preferences, the optimal repositioning strategy involves overshooting and asymmetric repositioning thresholds.The online appendix is available at https://doi.org/10.1287/mksc.2017.1075.
Firms often learn about their own capabilities through their products’ successes and failures. This paper explores the interaction between learning about capabilities and product strategy in a formal model. We consider a firm that can launch a sequence of products, where each product’s success probability depends on the fit between the firm’s capabilities and the product. A successful new product always causes the firm to become more optimistic about the capability most relevant for that product; however, it can also cause the firm to become less optimistic about some of its other capabilities, including capabilities the new product does not use. The firm’s optimal forward-looking product strategy accounts for short-run expected profits as well as for the information value of learning for future decisions. We find that a product sharing few or even no capabilities with potential future products can have a greater information value than a product that shares more capabilities with future products and that learning about capabilities can affect the optimal sequence of product launch decisions.Data and the online appendix are available at https://doi.org/10.1287/mksc.2017.1068.
With behavior-based pricing (BBP), firms use customers’ purchase history data to price discriminate between past and new customers. Prior research has examined BBP in a non-channel setting. In this paper, we investigate BBP in a channel setting in which manufacturers sell to customers through exclusive retailers. We examine how channel members’ adoption of BBP affects wholesale and retail prices, profits, consumer surplus, and social welfare. We find that BBP decreases channel members’ profits when retailers use BBP and manufacturers use uniform pricing. However, BBP increases channel members’ profits when manufacturers and retailers use BBP. In addition, BBP by retailers alone increases consumer surplus, whereas BBP by manufacturers and retailers decreases consumer surplus. When manufacturers also use BBP, BBP decreases social welfare to a greater degree than when only retailers use BBP. Furthermore, when manufacturers cannot use BBP, their profits are higher with long-term wholesale price contracts. When manufacturers can use BBP, short-term wholesale price contracts yield higher profits for manufacturers and retailers.The online appendix is available at https://doi.org/10.1287/mksc.2017.1070.
Marketing Science greatly benefited from the admirable and fastidious efforts of more than 200 different individuals who provided manuscript reviews last year. Beyond those individuals already recognized on the editorial board, the editor-in-chief and guest editors of Marketing Science are indebted to the many guest editors, guest associate editors, and ad hoc reviewers who provided expert counsel and guidance on a voluntary basis. The following list acknowledges the contribution of guest editors, guest associate editors, and ad hoc reviewers who served from January 1, 2017 to December 31, 2017. Finally, our sincere appreciation to the authors, whose outstanding submissions and careful revisions make the journal the go-to resource for leading edge knowledge in quantitative marketing.K. SudhirYale University
Political advertising is controversial, as there is widespread concern about money from political action committees (PACs and super PACs) distorting the democratic process. Studying advertising effectiveness is, however, a challenging topic for several reasons, including the endogenous nature of fundraising and ad spending rates. However, the extensive use of targeting based on designated marketing areas (DMAs) creates a setting in which neighboring counties with comparable demographics receive different levels of advertising exposure. In this paper, we leverage these advertising discontinuities along DMA borders to study the relative effectiveness of political advertising on vote shares and turnout rates in 2010 and 2012 senatorial elections. We find that negative advertising sponsored by PACs is significantly less effective than that sponsored by candidates in affecting two-party vote shares and voter turnout. A 1% increase in negative advertising by the candidate produces a significant 0.015% lift in the candidate’s unconditional vote shares. By contrast, negative advertising from PACs is ineffective in increasing its supported candidate’s unconditional vote share. Further analysis reveals that the competitiveness of races moderates the effectiveness of political advertising, providing implications for those managing candidates’ campaigns, PACs, and super PACs.Data and the online appendix are available at https://doi.org/10.1287/mksc.2017.1079.
In various cultural and behavioral respects, emerging market consumers differ significantly from their counterparts of developed markets. They may thus derive consumption utility from different aspects of product meaning and functionality. Based on this premise, we investigate whether the economic rise of emerging markets may have begun to impact the typical one-size-fits-all design of many international product categories. Focusing on Hollywood films, and exploiting a recent relaxation of China’s foreign film importation policy, we provide evidence suggesting that these impacts may exist and be nonnegligible. In particular, we show that the Chinese society’s aesthetic preference for lighter skin can be linked to the more frequent casting of pale-skinned stars in films targeting the Chinese market. Implications for the design of international products are drawn.Data and the online appendix are available at https://doi.org/10.1287/mksc.2018.1089.
When a firm introduces a radical innovation, consumers are unaware of the product’s uses and benefits. Moreover, consumers are unsure of whether they even need the product. In this situation, we consider the role of marketing communication as generating consumers’ need recognition and thus market demand for a novel product. In particular, we model marketing communication as a two-sided process that involves both firms’ and consumers’ costly efforts to transmit and assimilate a novel product concept. When the marketing communication takes on a two-sided process, we study a firm’s different information disclosure strategies for its radical innovation. We find that sharing innovation, instead of extracting a higher rent by keeping the idea secret, can be optimal. A firm may benefit from the presence of a competitor and its communication effort. The innovator can share its innovation so that competitors can also benefit, which encourages rivals to enter the market. The presence of such competition guarantees a higher surplus for consumers, which can induce greater consumer effort in a two-sided communication process. Moreover, the increased consumer effort, in turn, prompts complementarity in the communication process and lessens the potential free-riding effect in communication between firms. Additionally, it encourages the rival firm to exert more effort, especially when the role of consumers becomes more important. Sharing innovation with a rival serves as a mechanism to induce more efforts in a two-sided communication process.The online appendix is available at https://doi.org/10.1287/mksc.2017.1071.
Many pharmaceutical companies use overt anti-counterfeiting technologies (OACTs), such as holograms, to fight counterfeiters. An OACT is typically implemented on the drug packaging, which makes it difficult for counterfeiters to produce convincing copies and easy for patients to tell the difference between authentic and counterfeit medicines. I consider a model in which an authentic firm sells its drug at a reliable source and counterfeiters and illegitimate genuine sellers sell their drugs at a dubious source. The authentic firm chooses an OACT to combat counterfeiters. I show that there may be an inverted U-shaped relationship between the complexity of the OACT and the magnitude of counterfeit medicine purchases. The nature of this relationship is a consequence of an OACT engendering two opposing effects. On one hand, adopting an OACT imposes an entry cost on counterfeiters, causing fewer counterfeiters to enter the dubious source; as a result, the drugs at this source have a greater chance of being genuine (a counterfeiters’ entry-dampening effect). On the other hand, more patients head to this dubious source rather than the reliable source owing to the increased chance of obtaining a genuine drug (a patients’ demand-enhancing effect). When the selected OACT is sufficiently complex to replicate, the former effect overrides the latter and thus the problem of counterfeit purchasing is relieved. However, when the OACT is not adequately sophisticated, the latter effect more than offsets the former. This leads to an anti-counterfeiting trap: the use of a rudimentary OACT may beget more counterfeit purchases. This result offers an understanding to the phenomenon that despite enormous spending on the upgrading of OACTs in recent years, annual global sales of counterfeit drugs have actually risen. I also find that using an OACT may result in higher prices for both counterfeit and authentic drugs. Furthermore, I demonstrate that, at the optimum, an authentic firm may find it more profitable to employ a mediocre OACT, whereas it may not use any OACTs if its price is regulated.
We ask whether online opinions impact consumers’ decision quality and assess whether this impact occurs immediately or requires one to undergo learning first. We focus on a setting where consumers have multiple learning experiences using opinions from both uni- and bidirectional network ties. This allows us to investigate the impact of learning from both weak and strong ties. We find that, with sufficient experience, having more ties may lead to better decisions. However, the dynamic effects are dependent on the strength of the tie. Additional strong ties (operationalized as bidirectional links) lead to immediate positive effects on decision quality. However, additional weak ties (unidirectional, follower relationships) initially lead to lower decision quality. We find beneficial learning effects, however: adding more weak ties improves decision quality once one has sufficient experience in the community. Indeed, more-experienced consumers receive, ultimately, higher positive effects on decision quality from weak ties compared with strong ties. We interpret this as demonstrating that one needs to learn the norms of a new community before using the available information to improve decisions.Data and the online appendix are available at https://doi.org/10.1287/mksc.2017.1076.
Facing purchase choice involving ambiguity in product quality, consumers behave in a boundedly rational manner. Consumers also exhibit varying degrees of predisposition toward a product. We present a simple model of boundedly rational choice under ambiguity. The model’s key feature is that it captures the interaction between predisposition and ambiguity. We build on the choice model to derive demand curves and the unique equilibrium market outcomes (regarding prices, profits, and market shares) under duopolistic competition. In equilibrium, market shares are proportional to prices. In symmetric competition, higher equilibrium prices obtain when the ambiguity in product quality is high or when the customer base is partisan. For vertically differentiated products, the strategy of a higher-quality firm to marginally reduce ambiguity depends on the ambiguity level inherent in the product–market environment. The presence of informed customers may increase the equilibrium prices and profits of both firms. An understanding of the predisposition–ambiguity interaction may improve the firm’s information and brand management strategy.The online appendix is available at https://doi.org/10.1287/mksc.2017.1069.
In a product market where consumers are open to uninformed purchases, we study competition between a traditional and an online retailer in the presence of showrooming. Several results are obtained. First, showrooming intensifies competition and lowers both firms’ profits, thus supporting traditional and online retailers’ recent strategy of carrying more exclusive varieties. Second, lowering consumer search costs may aggravate showrooming and decrease the traditional retailer’s profits for intermediate search costs. Third, opening an online store expands the demand of the traditional retailer but intensifies competition, thus lowering its profits under certain conditions. Fourth, a return policy by the online retailer alleviates showrooming and relaxes competition but weakly reduces its demand, increasing its profits only for intermediate search costs. The return policy (weakly) increases the traditional retailer’s profits. Fifth, when search cost is not high enough, price matching by the traditional retailer may also intensify competition and hurt its profits. We then examine how webrooming interacts with showrooming. When webrooming resolves partial match uncertainty, it may increase both firms’ profits by inducing more consumers to participate.The online appendix is available at https://doi.org/10.1287/mksc.2018.1084.
Accounting-based approximations of Tobin’s q (AATQ) are increasingly popular in marketing. AATQ differ from Tobin’s original conception in that they use accounting data to assess the replacement cost of a firm’s assets; the core problem with this is that valuable assets go unrecorded in external reports, including systematic underrecording of market-based assets. This research examines the extensive erroneous claims made about AATQ in marketing studies. We note the widespread use of the metrics and demonstrate that the AATQ used in marketing (1) are not comparable across industries, (2) do not use only tangible assets in their denominator, and (3) should not find equilibrium at 1. AATQ are often described as performance metrics and can respond appropriately to certain types of positive performance. Unfortunately, they also respond positively to performance-neutral strategic choices. Furthermore, whenever AATQ exceed 1, as is typical, they increase even with completely wasted investments. We note that AATQ are especially problematic measures of performance for marketers because they are biased toward reporting that investments in market-based assets (e.g., brand equity and customer satisfaction) are effective. The misuse of AATQ we document suggests the need for marketing scholars to pay greater attention to the theoretical underpinnings of their metrics.
The interplay between innovation and the stock market has been extensively studied by scholars across all business disciplines. However, one phenomenon remains understudied: the association between innovation and stock market bubbles. Bubbles—defined as rapid increases and subsequent declines in stock prices—have been primarily examined by economists who generally do not focus on individual characteristics of innovations or on the consequences of bubbles for their parent firms. We set out to fill this gap in our paper. Using a sample of 51 major innovations introduced between 1825 and 2000, we test for bubbles in the stock prices of parent firms subsequent to the commercialization of these innovations. We identify bubbles in 73% of the cases. The magnitude of these bubbles increases with the radicalness of innovations, with their potential to generate indirect network effects, and with their public visibility at the time of commercialization. Moreover, we find that parent firms typically raise new equity capital during bubble periods and that the amount of equity raised is proportional to the magnitude of the bubble. Finally, we show that the buy-and-hold abnormal returns of parent firms are significantly positive between the beginning and the end of the bubble, suggesting that these innovations add value to their firm and to the economy, in spite of the bubble. Our findings have important implications for managers interested in commercializing innovations and for policy makers concerned with the stability of the financial system.Data and the online appendix are available at https://doi.org/10.1287/mksc.2018.1095.
Online search intermediaries, such as Amazon or Expedia, use rankings (ordered lists) to present third-party sellers’ products to consumers. These rankings decrease consumer search costs and increase the probability of a match with a seller, ultimately increasing consumer welfare. Constructing relevant rankings requires understanding their causal effect on consumer choices. However, this is challenging because rankings are endogenous: consumers pay more attention to highly ranked products, and intermediaries rank the most relevant products at the top. In this paper, I use the first data set with experimental variation in the ranking from a field experiment at Expedia to make three contributions. First, I identify the causal effect of rankings and show that they affect what consumers search, but conditional on search, do not affect purchases. Second, I quantify the effect of rankings using a sequential search model and find an average position effect of $1.92, which is lower than literature estimates obtained without experimental variation. I also use model predictions, data patterns, and a feature of the data set (opaque offers) to show rankings lower search costs, instead of affecting consumer expectations or utility. Finally, I show a utility-based ranking built on this model’s estimates benefits consumers and the search intermediary.Data and the online appendix are available at https://doi.org/10.1287/mksc.2017.1072.
Display advertising is a major source of revenue for many online publishers and content providers. Historically, display advertising impressions have been sold through prenegotiated contracts, known as reservation contracts, between publishers and advertisers. In recent years, a growing number of impressions are being sold in real-time bidding (RTB), where advertisers bid for impressions in real time, as consumers visit publishers’ websites. RTB allows advertisers to target consumers at an individual level using browser cookie information, and enables them to customize their ads for each individual. The rapid growth of RTB has created new challenges for advertisers and publishers on how much budget and ad inventory to allocate to RTB. In this paper, we use a game theory model with two advertisers and a publisher to study the effects of RTB on advertisers’ and publishers’ strategies and their profits. We show that symmetric advertisers use asymmetric strategies where one advertiser buys all of his impressions in RTB, whereas the other advertiser focuses on reservation contracts. Interestingly, we find that while both advertisers benefit from the existence of RTB, the advertiser that focuses on reservation contracts benefits more than the advertiser that focuses on RTB. We show that while RTB lowers the equilibrium price of impressions in reservation contracts, it increases the publisher’s total revenue. Despite many analysts’ belief that, because of being more efficient, RTB will replace reservation contracts in the future, we show that publishers have to sell a sufficiently large fraction of their impressions in reservation contracts to maximize their revenue. We extend our model to consider premium consumers, publisher’s uncertainty about the number of future visitors, and effectiveness of ad customization.The online appendix is available at https://doi.org/10.1287/mksc.2017.1083.
Despite the common practice of multiple standards in the high-technology product industry, there is a lack of knowledge on how compatibility between base products and add-ons affects consumer purchase decisions at the brand and/or standard level. We recognize the existence of compatibility constraints and develop a dynamic model in which a consumer makes periodic purchase decisions on whether to adopt/replace a base and/or an add-on product under the expectation of future price, quality, and compatibility. Dynamic and interactive inventory effects are included by allowing consumers to account for the long-term financial implications when planning to switch to a base product that is incompatible with their inventory of add-ons. Applying the model to the consumer purchase history of digital cameras and memory cards from 1998 to 2004, we demonstrate that the inventory of add-ons significantly affects the purchase of base products. This “lock-in” effect is enhanced when future prices of add-ons increase. Interestingly, it is more costly for consumers to switch from Sony to other brands than vice versa. In two policy simulations, we explore the impact of alternative compatibility policies. For example, if Sony had not created its proprietary Memory Stick, the market share of its cameras would have been reduced by 6 percentage points. This result provides important insights that leading brands and early movers should implement a proprietary standard.Data and the online appendix are available at https://doi.org/10.1287/mksc.2017.1080.
The rising obesity epidemic is a worldwide concern for consumers, firms, and policy makers. One reason for the rise in obesity is consumers’ over-consumption of vice goods such as cookies, crackers, and soft drinks. Some authors have suggested that firms have incentives to make vice goods unhealthier and to encourage over-consumption. There are calls for regulations to ensure that firms make such products healthier by reducing harmful ingredients and provide nutritional information. Furthermore, public policy makers have begun to educate consumers to avoid over-consumption by using strategies such as pre-purchase planning. In this paper, we investigate how firms selling vice goods should respond to the growing concerns about obesity. We analyze how firms should adjust prices and product design to cater to consumers with self-control problems and obesity concerns. We use the literature on hyperbolic discounting to model consumers with self-control problems. In this framework, we examine how the unhealthiness of vice goods affects prices, firm’s profits, consumer surplus, and public health. In addition, we study how public policy efforts to encourage pre-purchase planning impact firm’s profits and consumers. Our results show that unlike standard goods, for vice goods a decrease in quality (i.e., increase in unhealthiness) and an increase in price can serve as a self-control device and increase demand. Therefore, firms sometimes can charge higher prices and make more profits by producing unhealthier products. Interestingly, producing unhealthier products can sometimes increase consumer surplus and improve public health. We also show that as the proportion of consumers who use pre-purchase planning increases, firms should respond by raising prices. In such situations, consumer surplus and public health improve but firm’s profits decline. These results have important implications for restaurants and firms that sell vice goods and for public policy makers who aim to combat obesity.The online appendix is available at https://doi.org/10.1287/mksc.2017.1082.
This paper proposes a method that makes use of firms’ mass store closures to measure the store network effects of cannibalization and density economies. I calculate each store’s contribution to chain-level profits via one-store perturbations on the set of retained stores, and map these onto the firm’s closure choices. To separate the demand- and supply-side store network effects, I exploit the fact that the business-stealing effect intensifies with local network density, whereas the supply-side disadvantage prevails at sparse regions of the network. I apply the method to study the Starbucks chain. The average rate of cannibalization imposed by a neighbor outlet is 1.2% within one mile and 0.4% within one to three miles. For remote outlets, operation costs increase by 0.3% of revenues for each mile of distance from the network. Counterfactual analyses suggest that income level is a more important determinant of demand than population count at low levels of store penetration, whereas high-population regions can sustain denser store networks because of the softening of the cannibalization effect.Data are available at https://doi.org/10.1287/mksc.2017.1078.
We develop a hierarchical choice model to account for the choice utility heterogeneity of individual shoppers that belong to the same household. Our model allows us to measure how much variability in purchase behavior exists among individuals in a household, and to compare this to the variability that exists across households. Because of the presence of multiple shoppers from the same household, we also extend the concept of household-level state dependence to consider state dependence at the individual level. We apply our model to five different grocery categories. We find that the intrahousehold heterogeneity in estimated brand intercepts and (to a lesser extent) price sensitivities is about 20%–30% of the interhousehold heterogeneity in these parameters. However, with promotion sensitivities, we find intrahousehold heterogeneity, in most cases, to be as large as interhousehold heterogeneity. Our state dependence results show that past brands purchased by an individual have a much stronger influence on subsequent purchases than those purchased by anyone in the household. We use our estimated utility parameters to compare the expected profitability of promotions targeted at the individual rather than at the household and find substantial (more than 50%) improvements in the incremental revenue of supermarket promotions.Data and the online appendix are available at https://doi.org/10.1287/mksc.2018.1088.
Quota-based and partial-refund return policies abound in practice between manufacturers and their resellers. While the literature has provided insights into the design of the partial-refund policy, little attention has been directed at the design of the quota-based return policy. Accordingly, this paper explores the relative preference of a quota-based policy vis-à-vis a partial-refund policy. We do this, first, in the context of risk-neutral channel partners to identify the strategic decisions of each party and the effect of demand uncertainty on the variation of their respective profits. Our results reveal that the manufacturer faces higher profit variation (between the different demand realizations) under the quota policy. The variance in profits for the reseller is, however, higher under the partial-refund policy. We explain the source of profit variations by comparing it across different channel structures (centralized and decentralized). Next, we formally extend the model to include a disutility associated with profit variation and show that when the manufacturer has a variation-induced disutility, the partial-refund contract should be used, as it is the dominating contract. Similarly, when the retailer has a variation-induced disutility, the quota contract should be used. This is consistent with the pattern of profit variations in the risk-neutral case where the manufacturer has lower variation with the partial-refund contract while the reseller has lower variation with the quota contract. Finally, our analysis also shows how the manufacturer may employ a combination policy to better manage its own profit variation while providing adequate overstocking protection for the reseller.The online appendix is available at https://doi.org/10.1287/mksc.2018.1094.
Many sellers bundle add-ons (e.g., in-flight entertainment, hotel amenities) with core services (e.g., transportation, lodging). One surprising empirical finding is that consumers often believe bundle frames provide greater value than equivalent unbundle frames ($10 > $9 + $1) despite equal all-inclusive prices. Although these context or framing effects appear irrational in isolation, the bundle-framing effect might reflect market relationships caused by underlying seller motives. We show that bundling can signal information about product appeal, that is, popularity. Specifically, only sellers of wide-appeal (popular) add-ons (e.g., well-liked entertainment, sought-after hotel amenities, standard side salads, popular excursions) have an incentive to bundle their add-ons with their core products (e.g., flights, hotel rooms, restaurant entrées, cruise trips). By contrast, sellers of narrow-appeal (niche) add-ons (e.g., unorthodox entertainment, unpopular amenities, exotic side salads, unusual excursions) find that bundling is undesirable because they lose core revenue. Consequently, bundling can convey information about horizontally differentiated markets even when the total all-inclusive price equals that of unbundling. Perhaps some presumed consumer biases can reveal market relationships. Frames provide information about the framer.The online appendix is available at https://doi.org/10.1287/mksc.2018.1097.
This report describes entrants in the 2016 ISMS Gary Lilien Marketing Science Practice Prize Competition, representing the best examples of rigor plus relevance that our profession produces. The winner, describing a collaboration between the World Bank and a team based at the London Business School, involved a randomized control experiment to calibrate the relative effectiveness of business training on business performance of microentrepreneurs in South Africa. The other four finalists include a method to estimate the value of key word searches that allowed for cannibalization of organic search at eBay; a methodology to model and manage customer satisfaction at the National Dutch Railways; a stock-carrying algorithm to assist a fashion department store manage inventory on a store-by-store basis, implemented by Celect, an inventory-management consultant based in Boston; and an integrated marketing communications-optimization tool used by Mercedes-Benz to increase advertising effectiveness.
We examine the effect of managerial response on consumer voice in a dynamic quality environment. We argue that the consumer is motivated to write reviews not only because reviews may impact other consumers, but because reviews may impact the management and the quality of the service. We examine this empirically in a scenario in which reviewers receive a credible signal that the service provider is listening. Specifically, we examine the “managerial response” feature allowed by many review platforms. We hypothesize that managerial responses will stimulate reviewing activity and, in particular, will stimulate negative reviews that are seen as more impactful. This effect is further heightened because managers respond more and in more detail to negative reviews. Using a multiple-differences specification, we show that reviewing activity and particularly negative reviewing is indeed stimulated by managerial response. Our specification exploits comparison of the same hotel immediately before and after response initiation and compares a given hotel’s reviewing activity on sites with review response initiation to that on sites that do not allow managerial response. We also explore the mechanism behind the effect using an online experiment.Data and the online appendix are available at https://doi.org/10.1287/mksc.2018.1090.
This paper investigates a manufacturer’s ability to influence compliance rates among its authorized online retailers by exploiting changes in the minimum advertised price (MAP) policy and in dealer agreements. MAP is a pricing policy widely used by manufacturers to influence prices set by their downstream partners. A MAP policy imposes a lower bound on advertised prices, subjecting violating retailers to punishments such as termination of distribution agreements. Despite this threat, violations are common. I uncover two key elements to improve compliance: customization to the online environment and credible monitoring and punishments. I analyze the pricing, enforcement, and channel management policies of a manufacturer over several years. During this period, new channel policies take effect, providing a quasi-experiment. The new policies lead to substantially fewer violations. With improved compliance, channel prices increase by 2% without loss in volume. The reduction in violations is particularly stark among authorized retailers with lower sales volume, those that previously operated unapproved websites, and those that have received violation notifications for the specific product before. Moreover, low service providers improve their service. At the same time, there is an increase in opportunistic behavior among top retailers, or retailers that received notifications for other products, and for less popular products via deep discounting.Data and the online appendix are available at https://doi.org/10.1287/mksc.2018.1092.
We consider a persuasion setting in which the sender of a message tries to elicit a desired action from a receiver by means of a compelling argument. To understand which arguments may indeed be compelling, the sender can use information about the receiver’s preferences prior to the communication stage. We find that when the sender’s motives are transparent to the receiver, communication is influential only if the sender is not well informed about the receiver’s preferences. The sender prefers an interior level of information quality, whereas the receiver prefers complete privacy unless disclosure is necessary to induce communication. We also find that the parties may fail to trade at intermediate communication cost levels. In other cases, the content and cost of communication can affect market outcomes simultaneously. Finally, in general, the sender’s first-best outcome involves pooling with unattractive sender types: the sender prefers to stay relatively guarded about which aspects he is knowledgeable about in order to hinder the receiver’s discernment when topics he does not master are touched on. Our results are discussed in the contexts of matching markets, including online advertising, sales, expert advice, dating searches, and job searches.
We quantify the effect of consumers’ price uncertainty on gasoline prices and margins on an Italian highway. We observe the change in prices triggered by a longitudinal policy-based change in consumers’ price information from one in which drivers on the highway have no information on the prices of stations they encounter to one that allows consumers to observe the prices of four upcoming stations on a single price sign by the side of the highway. Using these data, we estimate a model of consumer search and purchase behavior and a corresponding model of gas station pricing. We then measure the impact of varying degrees of price information on equilibrium prices, including (i) no price information, (ii) the current policy, and (iii) full price information. We also compare the current policy with an alternative policy in which stations’ prices are advertised with individual price signs. We find that when consumers do not have price information, gas stations are able to charge 31% more, in terms of higher price-cost margins, than when prices are known. Our welfare analysis suggests that price information is worth €0.57 to consumers every time they take the highway. Relative to the current mandatory policy, advertising price on individual signs is worth €0.19 more to consumers.The e-companion is available at https://doi.org/10.1287/mksc.2018.1105.
Online advertisers often utilize multiple publishers to deliver ads to multihoming consumers. These ads often generate externalities and their exposure is uncertain, impacting advertising effectiveness across publishers. We analyze the inefficiencies created by externalities and uncertainty when information is symmetric between advertisers and publishers, in contrast to most previous research that assumes information asymmetry. Although these inefficiencies cannot be resolved through publisher-side actions, attribution methods that measure campaign uncertainty can serve as alternative solutions to help advertisers adjust their strategies. Attribution creates a virtual competition between publishers, resulting in a team compensation problem. The equilibrium may potentially increase the aggressiveness of advertiser bidding, leading to increased advertiser profits. The popular last-touch method is shown to overincentivize ad exposures, often resulting in lower advertiser profits. The Shapley value achieves an increase in profits compared with the last-touch method. Popular publishers and those that appear early in the conversion funnel benefit the most from advertisers using last-touch attribution. The increase in advertiser profits comes at the expense of total publisher profits and often results in decreased ad allocation efficiency. We also find that the prices paid in the market will decrease when more sophisticated attribution methods are adopted.The online appendix is available at https://doi.org/10.1287/mksc.2018.1104.
We propose a new strategy for proactive churn management that actively uses social network information to help retain consumers. We collaborate with a major telecommunications provider to design, deploy, and analyze the outcomes of a randomized control trial at the household level to evaluate the effectiveness of this strategy. A random subset of likely churners were selected to be called by the firm. We also randomly selected whether their friends would be called. We find that listing likely churners to be called reduced their propensity to churn by 1.9 percentage points from a baseline of 17.2%. When their friends were also listed to be called, their likelihood of churn reduced an additional 1.3 percentage points. The client lifetime value of likely churners increased 2.1% with traditional proactive churn management, and this statistic becomes 6.4% when their friends were also listed to be called by the firm. We show that, in our setting, likely churners receive a signal from their friends that reduces churn among the former. We also discuss how this signal may trigger mechanisms akin to both financial comparisons and conformity that may explain our findings.Data and the online appendices are available at https://doi.org/10.1287/mksc.2018.1099.
Geographic price discrimination is generally considered beneficial to firm profitability. However, theoretical results point to conditions under which firms might prefer to price across markets uniformly in oligopolistic settings. This paper provides an empirical analysis of competitive price discrimination and quantitatively assesses the profitability of national pricing relative to store-level pricing policies under different market conditions. Specifically, we construct and estimate a model of retail competition using extensive data from the digital camera market. A series of counterfactuals show that, under reasonable commitment mechanisms, two leading chains would benefit from employing national pricing policies, whereas a discount retailer should target prices in each local market. Additional results explore the boundary conditions of these findings and evaluate hybrid pricing policies.The online appendices are available at https://doi.org/10.1287/mksc.2018.1100.
The main objective in this paper is to study the effect of reviews by top- and bottom-ranked reviewers on product sales. We use designated market area sales data for 182 new music albums released over an approximately three-month period along with user review data from Amazon.com. Our estimation accounts for confounding factors in the effects of online word-of-mouth measures via the use of instrumental variables. There are several key insights. Overall, we find that bottom-ranked reviewers have a greater effect on sales than top-ranked reviewers. Top-ranked reviewers can be opinion leaders, but their influence is largely limited to special cases like very new products or products with high variance in existing reviews. Additional analysis reveals that the differences in the influence of top- and bottom-ranked reviewers is driven by both what they write (content) and who they are (identity). The results are robust across multiple product categories (music and cameras) and multiple dependent variables (sales and sales rank).Data are available at https://doi.org/10.1287/mksc.2018.1101.
In 2012, consumers paid $32 billion in overdraft fees, representing the single largest source of revenue for banks from demand deposit accounts during this period. Owing to consumer attrition caused by overdraft fees and potential government regulations to reform these fees, financial institutions have become motivated to investigate their overdraft fee structures. Banks need to balance the revenue generated from overdraft fees with consumer dissatisfaction and potential churn caused by these fees. However, no empirical research has been conducted to explain consumer responses to overdraft fees or to evaluate alternative pricing strategies associated with these fees. In this research, we propose a dynamic structural model with consumer monitoring costs and dissatisfaction associated with overdraft fees. We apply the model to an enterprise-level data set of more than 500,000 accounts with a history of 450 days, providing a total of 200 million transactions. We find that consumers heavily discount the future and potentially overdraw because of impulsive spending. However, we also find that high monitoring costs hinder consumers’ effort to track their balance accurately; consequently, consumers may overdraw because of rational inattention. The large data set is necessary because of the infrequent nature of overdrafts; however, it also engenders computational challenges, which we address by using parallel computing techniques. Our policy simulations show that alternative pricing strategies may increase bank revenue and improve consumer welfare. Fixed bill schedules and overdraft waiver programs may also enhance social welfare. This paper explains consumer responses to overdraft fees and evaluates alternative pricing strategies associated with these fees.The online appendices are available at https://doi.org/10.1287/mksc.2018.1106.
We develop a structural model of demand and supply for tied goods, which we estimate using aggregate data from the single-serve coffee system industry. We use the parameter estimates to quantify the impact of licensing on equilibrium prices and profits for firms in the industry. In particular, we look at the decision to allow other firms to sell components (coffee pods) that are compatible with a firm’s primary good (coffee machines) by licensing the use of its patents. We solve for the counterfactual market equilibrium in which one of the market leaders enters a licensing agreement with one of the competitor brands—with the latter brand only selling compatible coffee pods and not the machines. We show the existence of a range of royalty rates under which firms could potentially reach a beneficial licensing agreement. In addition, we find that the relationship between the licensee’s profits and the royalty rate is not always decreasing. Finally, we find that, within the set of royalty rates in which licensing benefits both brands, the licensing agreement is associated with less price dispersion in the aftermarket (coffee pods), and with lower prices of the primary good (coffee machines) relative to the nonlicensing scenario.Data and the online appendices are available at https://doi.org/10.1287/mksc.2018.1114.
How does a health club or credit counseling service market itself when its consumer becomes demotivated after a minor slipup? To examine this issue, we utilize a self-signaling model that accounts for the complex process in which a resolution seeker manages his self-control perceptions. Specifically, we employ a planner–doer model wherein a consumer oscillates between long-term resolution planning and short-term implementation: during each implementation juncture, the consumer must determine whether to lapse or use the program as planned, a decision that affects his self-control perceptions in subsequent periods of long-term resolution planning. Using this framework, we derive many significant marketing insights for self-improvement programs, products which assist the pursuit of long-term resolutions. First, we demonstrate that the seller tailors its contract strategy because of self-signaling, the process whereby the decision maker manages his self-control perceptions. Furthermore, we determine that the seller’s program contract depends on the level of noise in self-signaling: when the consumer’s program-use decisions reveal his general level of self-restraint, the seller imposes relatively high per-usage rates; on the other hand, the firm levies low usage fees when implementation decisions depend on short-term fluctuations in self-control. Additionally, we examine program quality as a strategic decision. We determine that the firm offers additional frills when self-signaling is noisy and provides minimal benefits when self-signaling is more informative. Finally, we analyze program length as a marketing strategy and show that lengthy contracts transpire when usage decisions do not sufficiently reveal self-control.
We extend latent Dirichlet allocation by introducing a topic model, hierarchically dual latent Dirichlet allocation (HDLDA), for contexts in which one type of document (e.g., search queries) are semantically related to another type of document (e.g., search results). In the context of online search engines, HDLDA identifies not only topics in short search queries and web pages, but also how the topics in search queries relate to the topics in the corresponding top search results. The output of HDLDA provides a basis for estimating consumers’ content preferences on the fly from their search queries given a set of assumptions on how consumers translate their content preferences into search queries. We apply HDLDA and explore its use in the estimation of content preferences in two studies. The first is a lab experiment in which we manipulate participants’ content preferences and observe the queries they formulate and their browsing behavior across different product categories. The second is a field study, which allows us to explore whether the content preferences estimated based on HDLDA may be used to explain and predict click-through rates in online search advertising.
Telecommunications service is an important and growing market, with worldwide revenue exceeding $2.2 trillion in 2016. In the U.S. market, the total number of mobile wireless connections has grown from 279.6 million in 2008 to 396 million in 2016. All the firms in this market offer consumers an option of purchasing either an individual plan or a family plan. Whereas a menu of individual plans can be thought of as a means to segment the market, the theoretical challenge is to understand how a firm stands to benefit from adding family plans to its product mix. In this paper, we use a game-theoretic framework to explore the role of family plans. Interestingly, we find that even when a family plan does not draw in any new consumers, a firm can still benefit from offering these plans. This occurs primarily because a family plan enables the firm to price discriminate more effectively. In particular, because some consumers can bundle themselves and join a family plan, the firm is able to charge a higher price to single high-valuation consumers who are unable to be part of a family. Furthermore, the presence of a family plan can have a negative impact on the plan offered to single low-valuation consumers who now have to pay a higher overage price. We also show that not all family plans are profitable and that the profitability depends on the sizes of different types of families.The online appendices are available at https://doi.org/mksc.2018.1121.
A recent development in online advertising has been the ability of advertisers to have their ads displayed exclusively on (a part of) a web page. We study this phenomenon in the context of both sponsored search advertising and display advertising. Ads are sold through auctions, and when exclusivity is allowed, the seller accepts two bids from advertisers, where one bid is for the standard display format in which multiple advertisers are displayed, and the other bid is for being shown exclusively (therefore they are called two-dimensional, or 2D, auctions). We identify two opposing forces at play in an auction that provides the exclusive placement option—allowing more flexible expression of preferences through bidding for exclusivity increases competition among advertisers, leading to higher bids, which increases the seller’s revenue (between-advertiser competition effect), but it also gives advertisers the incentive to shade their bids for their nonpreferred outcomes, which decreases the seller’s revenue (within-advertiser competition effect). Depending on which effect is stronger, the revenue may increase or decrease. We find that the 2D generalized second price (GSP2D) auction, which is an extension of the widely used generalized second price (GSP) auction and on which currently used auctions for exclusive placement are based, may lead to higher or lower revenue under different parametric conditions. Paradoxically, the revenue from allowing exclusive placement decreases as bidders have higher valuations for exclusive placement. We verify several key implications from our analysis of GSP2D using data from Bing for over 100,000 auctions. As a possible solution (applicable to both sponsored search and display advertising), we show that using VCG2D, which is the adaptation of the Vickrey–Clarke–Groves (VCG) auction for the 2D setting, guarantees weakly higher revenue when exclusive display is allowed. This is because it induces truthful bidding, which alleviates the problem of bid shading due to the within-advertiser competition effect.The online appendix is available at https://doi.org/10.1287/mksc.2018.1098.
Internet recommender systems are popular in contexts that include heterogeneous consumers and numerous products. In such contexts, product features that adequately describe all the products are often not readily available. Content-based systems therefore rely on user-generated content such as product reviews or textual product tags to make recommendations. In this paper, we develop a novel covariate-guided, heterogeneous supervised topic model that uses product covariates, user ratings, and product tags to succinctly characterize products in terms of latent topics and specifies consumer preferences via these topics. Recommendation contexts also generate big-data problems stemming from data volume, variety, and veracity, as in our setting, which includes massive textual and numerical data. We therefore develop a novel stochastic variational Bayesian framework to achieve fast, scalable, and accurate estimation in such big-data settings and apply it to a MovieLens data set of movie ratings and semantic tags. We show that our model yields interesting insights about movie preferences and makes much better predictions than a benchmark model that uses only product covariates. We show how our model can be used to target recommendations to particular users and illustrate its use in generating personalized search rankings of relevant products.Data are available at https://doi.org/10.1287/mksc.2018.1113.
Individual demand for consumer packaged goods shows discrete jumps between zero and large quantities, under a marginal change in price. Ruling out multiple alternative explanations, this paper provides evidence from microdata in the yogurt category that these jumps are caused by consumer fixed purchasing costs per product. We formulate and estimate a model in which (1) such fixed costs limit the number of different products considered and (2) consumers use prices to screen a product in and out of their consideration set. Our structural estimation finds that the consumer incurs fixed costs of $0.81 to consider a product. These costs are increased by 280% if she has not purchased the product for a year and are decreased by 59% when the product is featured in the store; the dependence of fixed costs on information shifters suggests that these costs are incurred because of consideration. Consideration being scarce at the shelf, firms compete fiercely for customers: We simulate counterfactual markups in a world full of feature advertising and find that firms enjoy higher equilibrium markups because the provision of information softens competition for consideration.Data and the online appendix are available at https://doi.org/10.1287/mksc.2018.1108.
I develop a simple dynamic oligopoly model for empirical work. A unique feature of the model is that any Markov-perfect equilibrium that survives some intuitive refinements can be quickly computed from low-dimensional contraction mappings in seconds. After computation, it is easy to check the uniqueness of the refined equilibrium. These results facilitate fast estimation using a full-solution approach and produce reliable counterfactual analysis. Model estimation at its minimum only requires panel data on firm presence, yet it quantifies important market primitives, such as toughness of competition and entry costs. I provide a step-by-step illustration of the estimation approach by applying it to the Dutch retail grocery industry, in which chain stores slowly replace local stores. A counterfactual simulation computing equilibrium under a large number of different primitive values shows that relaxing restrictions on chain-store entry will not only quicken the destruction of local stores, but also hurt chain stores’ profits.Data and the online appendix are available at https://doi.org/10.1287/mksc.2018.1117.
Firms traditionally rely on interviews and focus groups to identify customer needs for marketing strategy and product development. User-generated content (UGC) is a promising alternative source for identifying customer needs. However, established methods are neither efficient nor effective for large UGC corpora because much content is noninformative or repetitive. We propose a machine-learning approach to facilitate qualitative analysis by selecting content for efficient review. We use a convolutional neural network to filter out noninformative content and cluster dense sentence embeddings to avoid sampling repetitive content. We further address two key questions: Are UGC-based customer needs comparable to interview-based customer needs? Do the machine-learning methods improve customer-need identification? These comparisons are enabled by a custom data set of customer needs for oral care products identified by professional analysts using industry-standard experiential interviews. The analysts also coded 12,000 UGC sentences to identify which previously identified customer needs and/or new customer needs were articulated in each sentence. We show that (1) UGC is at least as valuable as a source of customer needs for product development, likely more valuable, compared with conventional methods, and (2) machine-learning methods improve efficiency of identifying customer needs from UGC (unique customer needs per unit of professional services cost).Data are available at https://doi.org/10.1287/mksc.2018.1123.
Usage-based insurance (UBI) is a recent auto insurance innovation that enables insurers to collect individual-level driving data, provide feedback on driving performance, and offer individually targeted price discounts based on each consumer’s driving behavior. Using individual driving behavior (from sensor data) and other information for UBI adopters, we estimate the relationship from being enrolled and monitored (for up to 26 weeks) in the UBI program and changes in the driving behavior of UBI customers. The key results of our analysis show that after UBI adoption, the UBI users improve the safety of their driving, providing a meaningful benefit for the individual driver, the insurer, and society as a whole. While UBI customers decrease their daily average hard-brake frequency by an average of 21% after six months, their mileage driven does not decrease comparing week 26 to week 1. We also find heterogeneous effects across different demographic groups. For example, younger drivers improve their UBI scores more than older drivers after UBI adoption, and females show more improvement than males. Furthermore, we find evidence that negative feedback and economic incentives correlate with greater improvement in driving behavior. Our results suggest that by sharing private consumer information with the insurer, UBI can benefit consumers who become better drivers, as well as the entire society from improved road safety.Data and the online appendix are available at https://doi.org/10.1287/mksc.2018.1126.
Channel coordination in search advertising is an important but complicated managerial decision for both manufacturers and retailers. Because of the highly concentrated market of search advertising, a manufacturer’s and its retailers’ ads can compete instead of complementing each other. We consider a manufacturer, who coordinates with its retailers by sharing a fixed percentage of each retailer’s advertising cost and, at the same time, competes with its retailers and outside advertisers in search ad position auctions. Our model prescribes the optimal cooperative advertising strategies from the manufacturer’s perspective. We find that different from cooperative advertising in traditional media, it can be optimal for a manufacturer to cooperate with only a subset of its retailers even if they are ex ante the same. This reflects the manufacturer’s trade-off between higher demand and higher bidding cost caused by more intense competition. We also find that with two asymmetric retailers, the manufacturer should support the retailer with higher channel profit per click to get a higher position than the other retailer, which demonstrates the effectiveness of the participation-rate mechanism. The manufacturer should take a higher position than a retailer when its profit per click via direct sales exceeds the channel profit per click of the retailer. The main results still hold when we endogenize retail price competition or wholesale contracts.The online appendix is available at https://doi.org/10.1287/mksc.2018.1111.
This paper investigates the effect of performance measurement on the optimal effort allocation by salespeople when firms are concerned about retention of salespeople with higher abilities. It shows that introducing a salesperson performance measurement may result in productivity, profit, and welfare losses when all market participants optimally respond to the expected information provided by the measurement and the (ex post) optimal retention efforts of the firm cannot be (ex ante) contractually prohibited. In other words, the dynamic inconsistency of the management problems of inducing the desired effort allocation by the salespeople and the subsequent firm objective to retain high-ability salespeople may result in performance measurement yielding an inferior outcome.The online appendix is available at https://doi.org/10.1287/mksc.2018.1122.
Despite the growth of online retail, the majority of products are still sold offline, and the “touch-and-feel” aspect of physically examining a product before purchase remains important to many consumers. In this paper, we demonstrate that large discrepancies can exist between how consumers evaluate products when examining them “live” versus based on online descriptions, even for a relatively familiar product (messenger bags) and for utilitarian features. Therefore, the use of online evaluations in market research may result in inaccurate predictions and potentially suboptimal decisions by the firm. Because eliciting preferences by conducting large-scale offline market research is costly, we propose fusing data from a large online study with data from a smaller set of participants who complete both an online and an offline study. We demonstrate our approach using conjoint studies on two sets of participants. The group who completed both online and offline studies allows us to calibrate the relationship between online and offline partworths. To obtain reliable parameter estimates, we propose two statistical methods: a hierarchical Bayesian approach and a k-nearest-neighbors approach. We demonstrate that the proposed approach achieves better out-of-sample predictive performance on individual choices (up to 25% improvement), as well as aggregate market shares (up to 33% improvement).Data are available at https://doi.org/10.1287/mksc.2018.1124.
How much does consumer learning by doing affect the demand for advanced products? In the context of digital cameras, I use detailed picture-level data to directly measure changes in picture quality as a result of learning by doing or product switching. Although learning by doing builds up consumer human capital, a fraction of this human capital is product specific, creating consumer switching costs. To quantify the role of consumer human capital, I structurally estimate the demand for digital cameras with consumer learning by doing. The evolution of consumer human capital explains 23% of the sales of advanced digital cameras, whereas brand-specific human capital—arising from incompatibility in product design—explains 15% of consumer brand-choice inertia.Data and the online appendix are available at https://doi.org/10.1287/mksc.2018.1118.
We analyze multimarket interactions between firms that must invest limited budgets in value (surplus) creation and in competitive rent-seeking activities. Firms are horizontally differentiated on a line segment and compete for multiple markets and prizes that differ in the relative effectiveness of each firm’s competitive rent-seeking spending. Each firm faces a dual trade-off: First, they must choose how much to invest in value creation versus how much to spend in rent-seeking competition. Second, they must decide on how to allocate resources across the different markets. When the market values are exogenous (and identical across markets), the intensity of competition is highest for the market in the middle, rather than in (advantaged) markets that are close or in (disadvantaged) markets that are closer to the rival. Counter to what one would expect, greater firm differentiation actually intensifies the competition in the middle markets. When firms endogenously invest in value creation, they invest more in value creation in closer markets and the investments decline toward the middle. This results in the most intense competition moving away from the middle to a market in each firm’s turf. The analysis also provides a competitive perspective on the home-turf bias phenomenon.
Despite its immense popularity, the freemium business model remains a complex strategy to master and often a topic of heated debate. Adopting a generalized version of the screening framework, we ask when and why a firm should endogenously offer a zero price on its low-end product when users’ product usages generate network externalities on each other. In the standard screening framework without network effects, freemium never emerges as optimal, and the firm always chooses the efficient price point for its low-end product. We show that even with network effects, freemium is typically not optimal. When network effects are identical across products (“symmetric”), the firm has greater incentive to expand its network size and may find it profitable to sell to the low-end customers. However, this does not lead to freemium as an equilibrium strategy. Instead, the firm should offer a low-end product to attract customers, while keeping its price positive. Freemium can only emerge if the high- and low-end products provide different levels of (“asymmetric”) marginal network effects. In other words, the firm would set a zero price for its low-end product only if the high-end product provided larger utility gain from an expansion of the firm’s user base. In contrast to conventional beliefs, a firm pursuing the freemium strategy might increase the baseline quality on its low-end product above the “efficient” level, which seemingly reduces differentiation.The online appendices are available at https://doi.org/10.1287/mksc.2018.1109.
Consumers may incur deliberation costs in learning about their valuations for new products. When the deliberation cost is not trivial, the retailer may set a low price to inhibit deliberation (regressive pricing) or choose a high price to induce deliberation (transgressive pricing). In a decentralized channel, we find that, first, the retailer is more likely to adopt the regressive pricing (versus transgressive pricing) when the wholesale price is lower. In response, the manufacturer sets a high (low) wholesale price to induce the transgressive (regressive) pricing when the deliberation cost is intermediate (high). Second, channel members can be misaligned in the incentive in investing in consumer empowerment. The ability to empower consumers and reduce their deliberation costs enhances the retailer’s channel power and its share of channel profit. Finally, the manufacturer may offer a socially suboptimal product quality because a high quality can lead to excessive deliberation. These nontrivial effects of the deliberation cost underscore the importance of considering consumer deliberations in channel management. The insights are robust under a positive production cost, heterogeneous deliberation costs, continuous deliberation efforts, and a channel structure with multiple layers.The online appendix is available at https://doi.org/10.1287/mksc.2018.1120.
Measuring the causal effects of digital advertising remains challenging despite the availability of granular data. Unobservable factors make exposure endogenous, and advertising’s effect on outcomes tends to be small. In principle, these concerns could be addressed using randomized controlled trials (RCTs). In practice, few online ad campaigns rely on RCTs and instead use observational methods to estimate ad effects. We assess empirically whether the variation in data typically available in the advertising industry enables observational methods to recover the causal effects of online advertising. Using data from 15 U.S. advertising experiments at Facebook comprising 500 million user-experiment observations and 1.6 billion ad impressions, we contrast the experimental results to those obtained from multiple observational models. The observational methods often fail to produce the same effects as the randomized experiments, even after conditioning on extensive demographic and behavioral variables. In our setting, advances in causal inference methods do not allow us to isolate the exogenous variation needed to estimate the treatment effects. We also characterize the incremental explanatory power our data would require to enable observational methods to successfully measure advertising effects. Our findings suggest that commonly used observational approaches based on the data usually available in the industry often fail to accurately measure the true effect of advertising.The online appendix and data files are available at https://doi.org/10.1287/mksc.2018.1135.
Pricing managers at online retailers face a unique challenge. They must decide on real-time prices for a large number of products with incomplete demand information. The manager runs price experiments to learn about each product’s demand curve and the profit-maximizing price. In practice, balanced field price experiments can create high opportunity costs, because a large number of customers are presented with suboptimal prices. In this paper, we propose an alternative dynamic price experimentation policy. The proposed approach extends multiarmed bandit (MAB) algorithms from statistical machine learning to include microeconomic choice theory. Our automated pricing policy solves this MAB problem using a scalable distribution-free algorithm. We prove analytically that our method is asymptotically optimal for any weakly downward sloping demand curve. In a series of Monte Carlo simulations, we show that the proposed approach performs favorably compared with balanced field experiments and standard methods in dynamic pricing from computer science. In a calibrated simulation based on an existing pricing field experiment, we find that our algorithm can increase profits by 43% during the month of testing and 4% annually.Data files and the online appendix are available at https://doi.org/10.1287/mksc.2018.1129.
Understanding the effects of contextual factors is crucial in designing context-based marketing. This paper focuses on product recommendations and studies how time and crowd pressures—two prominent contextual effects in the consumer behavior literature—can impact the effectiveness of recommendations. Measuring these effects is not straightforward because the joint distribution of consumer choice, time, and crowd pressures is rarely observed outside the laboratory and recommendations are often endogenously determined. We overcome these issues using data from an experiment conducted with vending machines in railway stations across Tokyo. The machines are equipped with a facial recognition system to make recommendations, and recommendations are changed exogenously in the experiment. This setup provides us with well-measured variables of the time and crowd pressures that affect the effectiveness of recommendations. After showing that recommendations increase the sales of both the recommended and nonrecommended products, we show that time pressures moderate the effectiveness of product recommendations for both recommended products directly and nonrecommended products indirectly. Crowd pressures weaken the direct effect on the recommended products, although its impact on the nonrecommended products is small and not robust in some cases. These results indicate that, when marketers make context-based recommendations, they should be mindful of the consumers under time pressure.Data files and the online appendix are available at https://doi.org/10.1287/mksc.2018.1132.
Television viewers are increasingly engaging in media-multitasking while watching programming. One prevalent multiscreen activity is the simultaneous consumption of television alongside social media chatter about the programming, an activity referred to as “social TV.” Although online interactions with programming can result in a more engaged and committed audience, social TV activities may distract media multitaskers from advertisements. These competing outcomes of social TV raise the question: are programs with high online social TV activity, so called “social shows,” good for advertisers? In this research, we empirically examine this question by exploring the relationship among television advertising, social TV, online traffic, and online sales. Specifically, we investigate how the volume of program-related online chatter is related to online shopping behavior at retailers that advertise during the programs. We find that advertisements that air in programs with more social TV activity see increased ad responsiveness in terms of subsequent online shopping behavior. This result varies with the mood of the advertisement, with more affective advertisements—in particular, funny and emotional advertisements—seeing the largest increases in online shopping activity. Our results shed light on how advertisers can encourage online shopping activity on their websites in the age of multiscreen consumers.Data and the online appendix are available at https://doi.org/10.1287/mksc.2018.1139.
When financially distressed firms have overwhelming debts, a prominent option for survival is to file for Chapter 11 bankruptcy protection. We empirically study the effect of Chrysler’s Chapter 11 bankruptcy filing on the quantity sold by its competitors in the U.S. auto industry. The demand for competitors could increase because they may benefit from the distress of the bankrupt firm (competitive effect). By contrast, competitors could experience lower sales if the bankruptcy increases consumer uncertainty about their own viability (contagion effect). A challenge to measuring the impact of bankruptcies is the coincident decline in economic conditions stemming from the Great Recession and the potential effect of the “cash for clunkers” program (among other confounding factors). To identify the effect of the bankruptcy filing, we employ a regression-discontinuity-in-time design based on a temporal discontinuity in treatment (i.e., bankruptcy filing), along with an extensive set of control variables. Such a design is facilitated by a unique data set at the dealer–model–day level that allows us to compare changes in unit sales in close temporal vicinity of the filing. We find that unit sales for an average competitor decrease by 28% following Chrysler’s bankruptcy filing. Several types of evidence suggest that this negative demand spillover effect is driven by a heightened consumer uncertainty about the viability of the bankrupt firm’s rivals. For example, we show that the sales of competitors’ vehicles that compete within the same segments as the bankrupt firm’s vehicles or that provide lower value for money are affected more negatively in response to the Chrysler filing. We also observe more web search activity for Chrysler’s competitors after the filing. Our findings are robust to different estimation strategies (global versus local), different functional forms, different estimation windows, the inclusion of various controls (e.g., “cash for clunkers,” incentives, advertising, inventory, recalls, price, and consumer confidence), the donut regression discontinuity approach, a potential serial correlation issue, a falsification exercise, and the inclusion of differential trends at various levels. Our study aims to inform policymakers and managers about unintended short-term demand consequences of Chapter 11 bankruptcy.The online appendices and data files are available at https://doi.org/10.1287/mksc.2018.1138.
Previous research demonstrates that stacked discounts increase retail revenue. For instance, a retailer should sell more products by offering “20% off, plus an extra 25% off” than by offering an economically equivalent single discount of “40% off.” We conduct multimethodology research to investigate a potential downside of offering stacked discounts: Can stacked discounts disproportionately increase retailer costs from product returns? We incorporate insights from prior behavioral work and develop an analytical model to generate predictions about how stacked discounts affect retail sales and return performance. Next, we conduct a laboratory experiment to provide evidence for our theory in a controlled environment. Subsequently, we empirically test our predictions under real market conditions using six-year transactional data from promotional events at a national jewelry retailer. Finally, drawing upon the empirical estimates, we conduct a numerical study to assess the impact of stacked discounts on retail profitability. Our analytical model and the empirical results identify the inherent tradeoffs associated with stacked discounts and demonstrate the cost structures under which stacked discounts will decrease firm profitability despite an increase in initial sales. We conclude by discussing implications for retailers in assessing the impact of how they frame their price discounts.Data files are available at https://doi.org/10.1287/mksc.2018.1137.
The launch of a new product is one of the most critical activities that product and brand managers are faced with. It requires a substantial communications budget to introduce the new product to the market. As the number of media channels proliferates, however, managers are increasingly held accountable to demonstrate the efficient use of resources. This article introduces a new decision support tool to optimize advertising campaigns for new product launches based on lessons learned from an ex post analysis of prior campaigns. The tool builds on a distinct data collection approach combined with econometric modeling to produce advertising elasticities, which is the key information in the media mix optimization. The approach was implemented at Mercedes-Benz and applied to four major new car launches in Germany in 2012 and 2013. It revealed estimated savings of 15%–30% or EUR 2 million per campaign from a more efficient use of resources.Data files and the online appendix are available at https://doi.org/10.1287/mksc.2018.1136.
Marketing Science greatly benefited from the admirable and fastidious efforts of more than 200 different individuals who provided manuscript reviews last year. Beyond those individuals already recognized on the editorial board, the editor-in-chief and senior editors of Marketing Science are indebted to the many guest editors, guest associate editors, and ad hoc reviewers who provided expert counsel and guidance on a voluntary basis. The following list acknowledges the contribution of guest editors, guest associate editors, and ad hoc reviewers who served from January 1, 2018, to December 31, 2018. Finally, our sincere appreciation to the authors, whose outstanding submissions and careful revisions make the journal the go-to resource for leading edge knowledge in quantitative marketing.K. SudhirYale University
The entry of flexible-capacity sharing economy platforms (e.g., Airbnb and Uber) has potentially changed the competitive landscape in traditional industries with fixed-capacity incumbents and volatile demand. Leveraging panel data on hotels and Airbnb, we study how the sharing economy fundamentally changes the way the industry accommodates demand fluctuations and how incumbent firms should strategically respond. The demand estimates suggest that Airbnb’s flexible supply helps recover the lost underlying demand due to hotel seasonal pricing (i.e., higher prices during high-demand seasons) and even stimulates more demand in some cities. The counterfactual results suggest that some hotel types in some cities may benefit from conducting less seasonal pricing and even considering counter-seasonal pricing. Market conditions (e.g., seasonality patterns, hotel prices and quality, consumer composition, and Airbnb supply elasticity) play a crucial role in determining the impact of Airbnb on hotel sales and hotels’ strategic response. Finally, recent Airbnb and policy changes (e.g., higher Airbnb hosting costs due to hotel taxes or lower Airbnb hosting costs due to third-party services and the “professionalism” of hosts) affect the competitive dynamics. The profits of high-end hotels are the most sensitive to the changes in Airbnb hosting costs. Airbnb’s recent attempt to behave more like hotels can increase hotels’ vulnerability to lower Airbnb hosting costs.Data files and the online appendix are available at https://doi.org/10.1287/mksc.2018.1143.
For information/digital products, the used goods market has been viewed as a threat by producers. However, it is not clear whether this view is justified because the used goods market also provides owners with an opportunity to sell their products. To investigate the impact of the used goods market on new goods sales, we collect a unique data set from the Japanese video game market. On the basis of the data, we develop and estimate a new dynamic structural model of consumers’ buying and selling decisions. The estimation results show that potential buyers’ consumption value from a game deteriorates by 50% from the release week to the second week, and game owners’ consumption value deteriorates by 23%–58% after the first week of ownership, and the rate depends on game characteristics. Examination of the cross-price elasticities suggests that the elasticities tend to be high especially when the used-game inventory at retailers is low, but they quickly decrease as the inventory is accumulated. Using the estimates, we quantify the impact of eliminating the used game market on publishers’ profits and consumer welfare. We find that holding the new-copy price at the observed level, this policy would increase publishers’ profits by 7.3% but reduce the consumer surplus by 0.9%, resulting in an overall decrease in social surplus by 0.3%. However, if firms adjust prices optimally, it would increase the profits by 26.8% and also increase the consumer surplus by 1.4% owing to lower new game’s prices. Overall, the social surplus increases by 2.7%.
Consumers often learn the weights they ascribe to product attributes (“preference weights”) as they search. For example, after test driving cars, a consumer might find that he or she undervalued trunk space and overvalued sunroofs. Preference-weight learning makes optimal search complex because each time a product is searched, updated preference weights affect the expected utility of all products and the value of subsequent optimal search. Product recommendations, which take preference-weight learning into account, help consumers search. We motivate a model in which consumers learn (update) their preference weights. When consumers learn preference weights, it may not be optimal to recommend the product with the highest option value, as in most search models, or the product most likely to be chosen, as in traditional recommendation systems. Recommendations are improved if consumers are encouraged to search products with diverse attribute levels, products that are undervalued, or products for which recommendation-system priors differ from consumers’ priors. Synthetic data experiments demonstrate that proposed recommendation systems outperform benchmark recommendation systems, especially when consumers are novices and when recommendation systems have good priors. We demonstrate empirically that consumers learn preference weights during search, that recommendation systems can predict changes, and that a proposed recommendation system encourages learning.The data files and online appendix are available at https://doi.org/10.1287/mksc.2018.1144.
This paper studies probabilistic selling for vertically differentiated products, whereby consumers do not know the exact identity of a product until after making the purchase. An important feature of probabilistic selling overlooked by previous literature is that it changes the product line, which often determines consumers’ choice context. Our work discovers the crucial role of context effects taking into account consumers’ salient thinking behavior: consumers focus their limited attention on and hence overweight the salient attribute of a product in their perception, leading to context-dependent preferences. We show that probabilistic selling can improve the seller’s profit with salient thinkers even when this strategy does not emerge with rational consumers. With salient thinking, the probabilistic product enables the seller to transform the consumers’ choice context favorably and direct their attention to quality. Our findings demonstrate the importance of exploiting consumers’ salient thinking behavior and suggest that probabilistic selling, as a context management tool, can be more beneficial than previously shown.
We explore how people balance their needs to belong and to be different from their friends by studying their choices of wall color in a virtual house on a leading Chinese social-networking site. The setting enables us to randomize both the popular color and the adoption rate at the individual level so that our experimental design minimizes informational social influence, homophily, and group-identity signaling to the general public. We find that there exists significant social influence within a user’s friend circle. While learning about the most popular color among a user’s friends generally increases the likelihood for the user to adopt that color, conformity first decreases and then increases with the adoption rate of that choice, which ranges from 50% to 100%. In addition, users who are of a minority or lower socioeconomic status or are newer are more likely to conform upon learning about the popular choice. Our findings are consistent with optimal distinctiveness and middle-status conformity theories and have implications for designing normative marketing campaigns.Simulated data and the online appendix are available at https://doi.org/10.1287/mksc.2018.1133.
Advertisers are growing increasingly concerned about the ease with which traditional television advertising can be avoided. Product placement activities, where brands are visually and/or verbally incorporated into television and movies, have continued to grow. In contrast to television commercials that can be avoided by viewers, product placement is embedded in the programming itself and is more difficult to avoid. Despite its popularity, there is limited research in marketing that has investigated the impact of product placement. In this research, the authors investigate the relationship between product placement in television programs and the volume of social media activity and website traffic for the featured brand. Using data on nearly 3,000 product placements for 99 brands from the fall 2015 television season, the authors find that prominent product placement activities—especially verbal placements—are associated with increases in both online conversations and web traffic for the brand, with some evidence of decreasing returns at high levels of prominence. The authors also find that, for most placement modalities, television advertising occurring in close proximity to placement activities does not enhance these increases in online viewer engagement.Data files and the online appendix are available at https://doi.org/10.1287/mksc.2018.1147.
Many customer service organizations (CSOs) reflect a tiered, or multilevel, organizational structure, which we argue imposes hassle costs for dissatisfied customers seeking high levels of redress. The tiered structure specifies that first-level CSO agents (e.g., call center operators) be restricted in their payout authority. Only by escalating a claim to a higher level (e.g., a manager), and incurring extra hassles, can a dissatisfied customer obtain more redress from the firm. We argue that the tiered structure helps the firm to control redress costs by (1) screening less severe claims so that such customers do not escalate their claims to a manager and (2) screening illegitimate claims. Our main result is that a firm can be more profitable if it uses a tiered CSO.The online appendix is available at https://doi.org/10.1287/mksc.2019.1149.
This article proposes that managers may use local consumer culture (LCC), or the culture of one’s home country, in their brand-building activities by adapting the brand’s positioning to the country image the brand targets. It introduces the concept of brand image–country image (BICI) fit, which measures the extent to which consumers in a specific country perceive a brand image as being congruent with their home country’s image. Using more than 350,000 brand-respondent observations across three countries, we develop and empirically illustrate a multiattribute methodology for operationalizing BICI fit and provide robust evidence that BICI fit is positively associated with consumers’ brand evaluations. A large number of validity and robustness tests support the proposed BICI fit metric and the findings derived from it. For example, we find that age, education, being female, and need for structure enhance the BICI fit effect, whereas materialism diminishes it. Furthermore, BICI fit matters more in categories that are closely tied to a local cultural context or that are characterized by high purchase risk. Given its multiattribute nature, the proposed BICI fit metric identifies concrete image attributes and thereby provides managers with an effective way to develop or revise LCC positioning plans for their brands.Data and the online appendix are available at https://doi.org/10.1287/mksc.2019.1151.
This paper studies implications of competitive customer poaching in markets with heterogeneous and privately known costs to serve. Using individual-level driving records from a large car insurer in Portugal, we show that poached customers generate a 21% higher cost to serve than observationally equivalent own customers. Screening on all available consumer characteristics and behavioral variables, with the exception of switching behavior, can alleviate only 50% of adverse selection. We develop and estimate an empirical framework based on a dynamic churn model that rationalizes this adverse selection. Our estimates imply that risky customers have more incentive to search and switch, and that the population of switchers is itself heterogeneous in riskiness. We propose a new consumer lifetime value measure that accounts for switchers’ risk endogeneity. We apply this measure to study actuarial pricing and insurance contract design.
We quantify the effects of others’ adoptions and word of mouth (volume and valence) on consumers’ product adoption decisions. We differentiate between the effects of word of mouth and observed adoptions from friends (personal network) and the effects of word of mouth and observed adoptions from the whole community (community network). Understanding the relative importance of word of mouth and observed adoptions at each network level provides crucial guidance for companies regarding their information provision and platform design strategies. Our unique data come from an online anime (Japanese cartoon) platform containing individual-level data on users’ networks, anime adoptions, forum posts, and ratings of anime series. Our results reveal that both word of mouth (volume and valence) and observed adoptions from the community network have significant positive effects on individual users’ anime-watching decisions. Furthermore, this finding also holds true for word of mouth and observed adoptions coming from the personal network. Comparing the magnitudes of the effects of word of mouth and observed adoptions across both network levels, we find that word-of-mouth valence from the community network is the largest driver among the social learning forces we study. Thus our results show that word of mouth and observed adoptions provide unique and different information that individuals use in their anime-watching decisions and that the community network is the primary source of information driving anime adoptions.
Prior literature on pay-per-click advertising assumes that publishers know advertisers’ click-through rates (CTRs). This information, however, is not available when a new advertiser first joins a publisher. The new advertiser’s CTR can be learned only if its ad is shown to enough consumers, that is, the advertiser wins enough auctions. Because publishers use CTRs to calculate payments and allocations, the lack of information about a new advertiser can affect the advertisers’ bids. Using a game-theory model, we analyze advertisers’ strategies, their payoffs, and the publisher’s revenue in a learning environment. Our results indicate that a new advertiser always bids higher (sometimes above valuation) in the beginning. The incumbent advertiser’s strategy depends on its valuation and CTR. A strong incumbent increases its bid to deter the publisher from learning the new advertiser’s CTR, whereas a weak incumbent decreases its bid to facilitate learning. Interestingly, the publisher may benefit from not knowing the new advertiser’s CTR because its ignorance could induce advertisers to bid more aggressively. Nonetheless, the publisher’s revenue sometimes decreases because of this lack of information. The publisher can mitigate this loss by lowering the reserve price of, offering advertising credit to, or boosting the bids of new advertisers.
We study consumers’ purchase behavior on daily deal websites (e.g., Groupon promotions) using individual clickstream data on the browsing history of new subscribers to Groupon between January and March 2011. We observe two patterns in the data. First, the probability that a given consumer clicks on a merchant in the emailed newsletter declines over time, which seems to be consistent with the notion of consumer “fatigue”—a phenomenon highlighted by the popular press. Second, the probability that the consumer makes a purchase conditional on clicking increases over time, which seems contrary to the notion of “fatigue.” To reconcile these two observations, we propose a model that rationalizes these patterns and then use it to provide insights for companies in the daily deal industry. When consumers first subscribe to a daily deal website, they are unlikely to be fully informed about the quality of the deals offered on that site. The daily newsletter provides only the price and some limited information about that day’s featured deal. To learn more about quality, consumers need to click on the emailed newsletter; this takes them to the deal’s website, where they invest time and effort to learn about the deal’s quality. Such a search for information is costly. Furthermore, consumers do not know about the quality of deals they may receive in the future. Given the cost of searching and the uncertainty about the quality of future deals, consumers are more likely to search early on (i.e., click on the newsletter) in their tenure. As they learn about the distribution of the quality of deals on Groupon, they require less searching, resulting in a decline in clicks over time. As learning accumulates, consumers are better at recognizing the position of a deal in the quality distribution of Groupon deals and are therefore more likely to purchase the clicked deals. This results in an increase in the conditional probability of purchasing. We formulate a dynamic model of search and Dirichlet learning based on the above characterization of consumer behavior. We show that the model is able to replicate patterns in the data. Next, we estimate the parameters of the model and provide insights for managers of daily deal websites based on our findings and policy simulations.
Price promotions are typically offered in groups on websites, mailings, and circulars, but little is known about how promotional offers in near proximity affect each other. Across two large-scale field experiments (N = 66,184) conducted on a multibrand coupon website, we find that when lead promotions offer high-value deals, consumers are more likely to print subsequent offers, a finding we call “lead offer spillover.” In the first field experiment, doubling the value of three lead offers increased the printing of subsequent offers by 18% and redemptions by 12%. In the second, doubling the value of a single lead offer increased subsequent offer prints by 12%. Additional analyses and experiments indicate that larger lead offers increase consumer search for subsequent offers and are not primarily driven by changes in evaluative judgments or complementarities between lead and subsequent offers.
This research studies the product line design problem when consumers are subject to perceptual errors in assessing their intrinsic preferences. If perceptual errors are driven by common variables, then a firm may use aggregate consumer data (e.g., conjoint studies or anonymous usage data) to deduce the errors and infer the consumer preferences. In this way, we develop microfoundations necessary to show when and how the firm can understand consumer preferences better than consumers themselves, a situation we call superior knowledge. But is superior knowledge ever unprofitable? How should the firm with superior knowledge design its product line? Do consumers receive more-relevant products or simply have more surplus extracted? Can data collection help consumers make better choices? Our results suggest that consumers’ rational suspicions may prevent the firm from exploiting its superior knowledge. In addition, the burden of signaling may force the firm to offer efficient quality for its products. Therefore, allowing the firm to collect aggregate consumer data may be strictly Pareto improving.
This paper presents a methodology for identifying groups of products that exhibit similar patterns in demand and responsiveness to changes in price using store-level sales data. We use the concept of economic separability as the basis for establishing similarity between products and build a weakly separable model of aggregate demand. A common issue with separable demand models is that the partition of products into separable groups must be known a priori, which severely shrinks the set of admissible substitution patterns. We develop a methodology that allows the partition to be an estimated model parameter. In particular, we specify a log-linear demand system in which weak separability induces equality restrictions on a subset of cross-price elasticity parameters. An advantage of our approach is that we are able to find groups of separable products rather than just test whether a given set of groups is separable. Our method is applied to two aggregate, store-level data sets. We find evidence that the separable structure of demand can be inconsistent with category labels, which has implications for optimal category marketing strategies.
Marketers have recognized that the probability of a consumer’s (or household’s) purchase in a particular product category may be influenced by past purchases in the same category and also, purchases in other related categories. Past studies of crosscategory effects have focused on a limited number of product categories, and they have often ignored intertemporal effects in their analyses. Those studies have generally used multivariate logit or probit models, which are limited in their ability to analyze enormous data sets that contain consumer purchase records across a large number of categories and time periods. The availability of such enormous consumer shopping data sets and the value of analyzing the complex relationships across categories and over time (for example, for personalized promotions) suggest the need for computationally efficient modeling and estimation methods. Such models can capture associations among buying decisions across all product categories and over all time periods for which data are available, but they must also have a tractable and clear model structure that permits meaningful interpretation of the results. We explore the nature of intertemporal crossproduct patterns in an enormous consumer purchase data set using a model that adopts the structure of conditional restricted Boltzmann machines (CRBMs). Our empirical results demonstrate that our proposed approach using the efficient estimation algorithm embodied in the CRBM enables us to process very large data sets and capture the consumer decision patterns for both predictive and descriptive purposes that might not otherwise be apparent. In addition to persistent intertemporal within-category effects, we find that there are also significant intertemporal cross effects between product categories.
This paper investigates the impact of mobile hailing technology on taxi driving behaviors. A controversial feature of mobile hailing applications in China is the disclosure of not only pickup locations but also drop-off destinations before drivers accept offers. It provides taxi drivers two different mechanisms to improve their hourly earnings: reducing cruising time and selecting more profitable trips. We examine 3.6-terabyte minute-by-minute geolocation data of 2,106 single-shift drivers in Beijing. A modified change-point model is proposed to infer the adoption decisions and estimate the changes in driving behaviors. We show that mobile hailing technology adoption is associated with an average increase of 6.8% in hourly earnings, equivalent to an extra CNY 750 monthly income. A typical taxi driver greatly improves hourly earnings through trip selection in favor of longer trips rather than aiming for cruising-time reduction. We find that the relative importance of cruising-time reduction and trip selection depends on driver skills and market conditions. We do not find market expansions on the number of trips or working hours, but rather a redistribution of realized trips toward long distances.
How do a retailer’s mobile app adopters differ from nonadopters in their shopping outcomes, such as online and offline purchases and product returns? In this paper, we model the relationship between a retailer’s mobile app launch and the frequency, quantity, and monetary value of purchases in its online and offline channels, as well as product returns. We leverage data on a large retailer’s launch of a mobile app and use a difference-in-differences approach. Our results show that app adopters buy 33% more frequently, buy 34% more items, and spend 37% more than non-adopters in the period after app introduction. At the same time, they return 35% more frequently, 35% more items, and 41% more in dollar value. Combined, app adopters spend 36% more in net monetary value. Furthermore, app adopters’ purchases in both the online and offline channels increase after app launch. The time, location, and features of app use provide descriptive evidence of how the app aids shopping in other channels. App-linked shoppers (those who make a purchase within 48 hours of app use) use the app when they are close to the store of purchase and access the app for loyalty rewards, product details, and notifications. These insights offer important substantive implications.
Mobile users can create word of mouth (WOM) wherever they are and whenever they want to do so. This real-time creation process may be associated with differences in the content and consumption value of mobile versus nonmobile word of mouth. We analyze 275,362 reviews from 117,827 reviewers describing their experiences at 134,976 restaurants as well as a dual platform subsample of 21,026 reviews written by 673 reviewers who wrote at least four mobile and four nonmobile reviews. We also examine how the introduction of the mobile platform affected WOM consumption. We find that WOM content is more affective, more concrete, and less extreme when created on mobile devices. These differences in content (more affective, more concrete, and less extreme) vary in their relationships with the perceived consumption value of mobile content. Beyond the indirect relationship between platform and consumption value through content, reviews created on mobile devices are associated with lower consumption value. This direct relationship grows stronger over time. Although consumers initially value both real-time mobile content and nonmobile content, even after controlling for a large set of content and contextual variables, over time consumers value mobile reviews less than they do nonmobile reviews.
We study the relationship between online reviews and advertising spending in the hotel industry. Combining a data set of TripAdvisor reviews with other data sets describing these hotels’ advertising expenditures, we show, first, that online ratings have a causal demand-side effect on ad spending. Second, this effect is negative: hotels with higher ratings spend less on advertising than hotels with lower ratings. This suggests that hotels treat TripAdvisor ratings and advertising spending as substitutes, not complements. Third, the relationship is stronger for independent hotels than for chains, and stronger in less differentiated markets than in more differentiated markets. The former suggests that a strong brand name continues to provide some immunity to reviews and the latter that the advertising response is stronger when ratings are more likely to be pivotal. Finally, we show that the relationship between online ratings and advertising has strengthened over time, just as TripAdvisor has become more popular, implying that firms respond to online reviews if and only if consumers respond to them.
To form expectations of product quality, consumers frequently rely on opinion leaders who presumably have better expertise in the category. But opinion leader recommendations are influenced by both the product quality and the idiosyncratic preferences of the opinion leader. Consequently, the followers need to form expectations of how much a recommendation is driven by the product quality versus by the product’s idiosyncratic fit to the opinion leader. Since the opinion leader is likely able to select a better fitting version from a larger variety, the opinion leader’s idiosyncratic fit depends on the product variety. We analyze how the product variety affects consumer inference of quality, and consequently, formulate recommendations of how the firm should adjust its product variety in the presence of opinion leaders. We find that the adjustment may be either upward or downward. Generally, it is upward if opinion leaders are more difficult to satisfy. However, when the optimal variety absent consideration of opinion leaders is infinite, the optimal variety given this consideration may be finite. Moreover, the firm’s knowledge of the true quality may further increase the distortion of the optimal variety even when the equilibrium variety is pooling across the product qualities.
Many new products have several features that are not used by most consumers. One explanation for consumers buying products with features that they do not use is that consumers value features and ease of use differently at the point of purchase and at the time of usage. This behavior can be explained by consumer biases such as hyperbolic discounting. Some researchers claim that such consumer biases encourage firms to offer too many features at the cost of making products less user friendly. This paper examines how consumer biases such as hyperbolic discounting and naive expectations affect pricing, product design, firm profits, and welfare. We build a game theoretic model in which consumers need to invest in learning in order to use the additional features of a product. In our model, consumers use quasi-hyperbolic discounting, and some consumers have naive beliefs about their future learning behavior. Contrary to intuition, we show that hyperbolic discounting encourages firms to focus more on ease of learning rather than on investing in additional features. We also find that consumer biases do not always hurt consumers but can sometimes lead to improvement in consumer welfare. From a public policy perspective, our results suggest that educating consumers to reduce the number of overly optimistic consumers could be welfare reducing. We also show that firms can improve learning by making features complementary or by having a consistent design to encourage sequential learning of features. Our analysis also reveals that one approach to make consumers learn existing features may be to add more complementary features. We also investigate whether allowing consumers to choose between simple and feature-rich products would enhance welfare. We find that providing consumers with more choice can lead to reduced consumer welfare.
We consider the compensation design problem of a firm that hires a salesperson to exert effort to increase demand. We assume both demand and supply to be uncertain with sales being the smaller of demand and supply and assume that, if demand exceeds supply, then unmet demand is unobservable (demand censoring). Under single moral hazard (i.e., when the salesperson’s effort is unobservable to the firm), we show that the optimal contract has an extreme convex form in which a bonus is provided only for achieving the highest sales outcome even if low realized sales are due to low realized supply (on which the salesperson has no influence). Under double moral hazard (i.e., when the firm can also take supply-related actions that are unobservable to the salesperson), we show that the optimal contract is smoother as it involves positive compensation for intermediate sales outcomes to assure the salesperson that the firm does not have an incentive to deviate to an action that hurts the agent; in fact, under certain conditions, the contract is concave in sales. We also determine conditions under which, if possible, the firm should postpone contracting until after supply is realized.
This paper studies how salespeople affect the choices of which products consumers choose, and from that, how a firm should set optimal commissions as a function of the appeal, substitutability, and profit margins of different products. We also examine whether firms are better off promoting products through sales incentives or price discounts. To achieve these goals, we develop a salesforce-driven consumer choice model to study how performance-based commissions incentivize a salesperson’s service effort toward heterogeneous, substitutable products carried by a firm. The model treats the selling process as a joint decision by the salesperson and the consumer. It allows the salesperson’s efforts to vary across different transactions, depending on the unique preferences of each consumer, and incorporates the effects of commissions and other marketing mix elements on the selling outcome in a unified framework. We estimate the model using data from a car dealership. We find that the optimal commissions should be lower for popular items and for items that are closer substitutes with other products. We also find that for the car industry we study, the cost of selling more cars using sales incentives is cheaper than the cost of selling the same number of cars using price discounts.
We develop a new approach using market-level data to model, identify, and estimate a dynamic discrete choice demand model for durable goods with continuous unobserved product-specific state variables. They are specified as serially correlated and correlated with the observed product characteristics, particularly price. We provide a method to estimate all model primitives, including the consumer’s discount factor and the state transition distributions of unobserved product characteristics without the need to reduce the dimension of the state space or by other approximation techniques, such as discretizing state variables. We prove the identification of model primitives and provide an estimation algorithm in which the most computationally demanding step is a linear regression. Finally, we show how it can be implemented in an application in which we estimate the demand for smartphones.
Frontiers is a new section positioned as a prestigious subbrand under the Marketing Science umbrella. Its purpose is to encourage and nurture timely research with potentially high impact in quantitative marketing with a differentiated format and review process. In this issue, we publish the first set of three papers in Frontiers. This editorial describes (i) the papers and how they fit the goals of Frontiers and (ii) the minor changes in the Frontiers’ review process to improve efficiency and consistency for both authors and the journal.
Data brokers often use online browsing records to create digital consumer profiles that they sell to marketers as predefined audiences for ad targeting. However, this process is a “black box”—little is known about the reliability of the digital profiles that are created or of the audience identification provided by buying platforms. In this paper, we investigate using three field tests the accuracy of a variety of demographic and audience-interest segments. We examine the accuracy of more than 90 third-party audiences across 19 data brokers. Audience segments vary greatly in quality and are often inaccurate across leading data brokers. In comparison with random audience selection, the use of black box data profiles, on average, increased identification of a user with a desired single attribute by 0%–77%. Audience identification can be improved, on average, by 123% when combined with optimization software. However, given the high extra costs of targeting solutions and the relative inaccuracy, we find that third-party audiences are often economically unattractive except for higher-priced media placements.
Recently, as cannabis was legalized for recreational use in an increasing number of states, it has become more important to understand the effects of cannabis policies, especially on youth. Marketers of other recreational substances are also paying close attention to cannabis policy changes. Alcohol and tobacco companies typically view the cannabis industry as a potential threat and are often found among the opponents to its legalization. However, based on extant research, the treatment effects of recreational cannabis legalization (RCL) and its cross-commodity effects on the alcohol and tobacco industries remain inconclusive. Analyzing large-scale web-based behavioral data, we find that although RCL significantly increases cannabis search, the increase comes from adults only, but not youth. RCL also influences alcohol and tobacco industries asymmetrically: it reduces search volume and advertising effectiveness for alcohol but increases those for tobacco. Hence, cannabis appears to be a substitute for alcohol but not tobacco.
Empowered by artificial intelligence (AI), chatbots are surging as new technologies with both business potential and customer pushback. This study exploits field experiment data on more than 6,200 customers who are randomized to receive highly structured outbound sales calls from chatbots or human workers. Results suggest that undisclosed chatbots are as effective as proficient workers and four times more effective than inexperienced workers in engendering customer purchases. However, a disclosure of chatbot identity before the machine–customer conversation reduces purchase rates by more than 79.7%. Additional analyses find that these results are robust to nonresponse bias and hang-ups, and the chatbot disclosure substantially decreases call length. Exploration of the mechanisms reveals that when customers know the conversational partner is not a human, they are curt and purchase less because they perceive the disclosed bot as less knowledgeable and less empathetic. The negative disclosure effect seems to be driven by a subjective human perception against machines, despite the objective competence of AI chatbots. Fortunately, such negative impact can be mitigated by a late disclosure timing strategy and customer prior AI experience. These findings offer useful implications for chatbot applications, customer targeting, and advertising in conversational commerce.
This paper considers the monetization of online marketplaces. These platforms trade off fees from advertising with commissions from product sales. Although featuring advertised products can make search less efficient (lowering transaction commissions), it incentivizes sellers to compete for better placements via advertising (increasing advertising fees). We consider this trade-off by modeling both sides of the platform. On the demand side, we develop a joint model of browsing (impressions), clicking, and purchase. On the supply side, we consider sellers’ valuations and advertising competition under various fee structures (cost-per-mille, cost-per-click (CPC), and cost-per-action) and ranking algorithms. Using buyer, seller, and platform data from an online marketplace where advertising dollars affect the order of seller items listed, we explore various product-ranking and ad-pricing mechanisms. We find that sorting items below the fifth position by expected sales revenue while conducting a CPC auction in the top 5 positions yields the greatest improvement in profits (181%) because this approach balances the highest valuations from advertising in the top positions with the transaction revenues in the lower positions.
Sports franchises derive significant portions of their revenues from season ticket holders. A development that may affect season ticket management is the growth of legal secondary markets. We develop a structural model that integrates both the supply and demand sides of the secondary market into season ticket buyers’ ticket purchase and usage choices. We use a panel data set that combines season and single ticket purchase records with ticket usage data to investigate the value of secondary markets. We estimate that the secondary market increases the team’s season ticket revenues by about $1 million per season. At the level of the individual season ticket customer, we estimate an increase in customer lifetime value ranging from $1,327 in the lowest quality seat tier to $2,553 in the highest. In terms of value to the customer, the average dollar value of having a secondary market is $138 per season ticket. Across segments, the secondary market provides the equivalent of a 4% discount in the premium seat tier versus an 11% discount in the economy seat tier. Whereas the secondary market creates more value in the premium-ticket tier segments, the secondary market has the most impact on behavior in the low price oriented segment.
Although TV advertising for traditional cigarettes has been banned since 1971, advertising for e-cigarettes remains unregulated. The effects of e-cigarette ads have been heavily debated, but empirical analysis of the market has been limited. Analyzing both individual and aggregate data, I present descriptive evidence showing that e-cigarette advertising reduces demand for traditional cigarettes and that individuals treat e-cigarettes and traditional cigarettes as substitutes. I then specify a structural model of demand for cigarettes that incorporates addiction and allows for heterogeneity across households. The model enables me to leverage the information content of both data sets to identify variation in tastes across markets and the state dependence induced on choice by addiction. Using the demand model estimates, I evaluate the impact of a proposed ban on e-cigarette television advertising. I find that in the absence of e-cigarette advertising, demand for traditional cigarettes would increase, suggesting that a ban on e-cigarette advertising may have unintended consequences.
In this paper, we investigate advertisers’ budgeting and bidding strategies across multiple search platforms. We develop a model with two platforms and budget-limited advertisers that compete for advertising slots across platforms. When platform reserve prices are low and exogenous, we find that symmetric advertisers pursue asymmetric budget allocation strategies and partially differentiate: one advertiser allocates a share of its budget to platform A higher than A’s share of user traffic and a share of its budget to platform B lower than B’s share of user traffic, whereas the second advertiser does the reverse. This partial differentiation balances two forces: a demand force arising from a desire to be present on both platforms to obtain more clicks and a strategic force driven by a desire to be budget dominant on at least one platform to obtain clicks at a lower cost. We then show that the benefit from differentiation for advertisers diminishes if platforms strategically increase their reserve prices. At reserve prices that maximize platform revenues, advertisers allocate their budgets proportional to each platform’s share of user traffic, and platforms fully appropriate these budgets.
Marketers often use A/B testing as a tool to compare marketing treatments in a test stage and then deploy the better-performing treatment to the remainder of the consumer population. Whereas these tests have traditionally been analyzed using hypothesis testing, we reframe them as an explicit trade-off between the opportunity cost of the test (where some customers receive a suboptimal treatment) and the potential losses associated with deploying a suboptimal treatment to the remainder of the population. We derive a closed-form expression for the profit-maximizing test size and show that it is substantially smaller than typically recommended for a hypothesis test, particularly when the response is noisy or when the total population is small. The common practice of using small holdout groups can be rationalized by asymmetric priors. The proposed test design achieves nearly the same expected regret as the flexible yet harder-to-implement multi-armed bandit under a wide range of conditions. We demonstrate the benefits of the method in three different marketing contexts—website design, display advertising, and catalog tests—in which we estimate priors from past data. In all three cases, the optimal sample sizes are substantially smaller than for a traditional hypothesis test, resulting in higher profit.
Choice-based conjoint (CBC) studies have begun to rely on simulators to forecast equilibrium prices for pricing, strategic product positioning, and patent/copyright valuations. Whereas CBC research has long focused on the accuracy of estimated relative partworths of attribute levels, predicted equilibrium prices and strategic positioning are surprisingly and dramatically dependent on scale: the magnitude of the partworths (including the price coefficient) relative to the magnitude of the error term. Although the impact of scale on the ability to estimate heterogeneous partworths is well known, neither the literature nor current practice address the sensitivity of pricing and positioning to scale. This sensitivity is important because (estimated) scale depends on seemingly innocuous market-research decisions such as whether attributes are described by text or by realistic images. We demonstrate the strategic implications of scale using a stylized model in which heterogeneity is modeled explicitly. If a firm shirks on the quality of a CBC study and acts on incorrectly observed scale, a follower, but not an innovator, can make costly strategic errors. Externally valid estimates of scale are extremely important. We demonstrate empirically that image realism and incentive alignment affect scale sufficiently to change strategic decisions and affect patent/copyright valuations by hundreds of millions of dollars.
This article introduces the Marketing Science Special Issue on Consumer Protection. This special issue and an accompanying conference were conceived as a partnership with the U.S. Federal Trade Commission. We outline the potential areas and opportunities for academic scholarship in marketing to inform regulation on consumer protection. We group the areas of potential research and the papers in the special issue into three broad buckets: (1) what consumers need protection from, especially the need for regulations in new industries; (2) the impact of existing regulations; and (3) the distributional impact of regulations. The article concludes with a call for ongoing policy-relevant research on consumer protection.
Recent advances in advertising technology have lead to the development of “native advertising,” which is a format of advertising that mimics the other nonsponsored content on the medium. Whereas advertisers have rapidly embraced the format on a variety of digital media, regulators have expressed serious concerns about whether this format materially deceives consumers because the advertising disclosure is incomplete or inappropriate. This has reignited a longstanding debate about the distinction between advertising and content in media markets, and how it affects consumers. This paper contributes to this debate by providing empirical evidence from a randomized experiment conducted on native advertising at a mobile restaurant-search platform. We experimentally vary the format of paid-search advertising, the extent to which ads are disclosed to over 200,000 users, and track their anonymized browsing behavior including clicks and conversions. The research design we propose uses comparisons of revealed preferences under experimentally manipulated treatment and control conditions to assess the potential for consumer confusion and deception. A design based on revealed preference speaks to the “material” standard of regulators, helps assess confusion while avoiding directly questioning consumers, and may be useful in other settings. Implementing the design, we find that native advertising benefits advertisers and detect no evidence of deception under typically used formats of disclosure currently used in the paid-search marketplace. Further investigation shows that the incremental conversions due to advertising are not driven by users clicking on the native ads. Rather, the benefits from advertising are driven by users seeing the ads and later clicking on the advertiser’s “organic” listings. Thus, we find little support of native advertising tricking users into clicking and driving them to advertisers as typically feared; instead, users seem to view ads and deliberately evaluate the advertisers. Furthermore, mere exposure seems sufficient to produce most of the incremental effect of advertising.
We study consumer privacy choice in the context of online display advertising, where advertisers track consumers’ browsing to improve ad targeting. In 2010, the American ad industry self-regulated by implementing the AdChoices program: consumers could opt out of online behavioral advertising via a dedicated website, which can be reached by clicking the overlaid AdChoices icons on ads. We examine the real-world uptake of AdChoices using transaction data from an ad exchange. Though consumers express strong privacy concerns in surveys, we find that only 0.23% of American ad impressions arise from users who opted out of online behavioral advertising. We also find that opt-out user ads fetch 52% less revenue on the exchange than comparable ads for users who allow behavioral targeting. These findings are broadly consistent with evidence from the European Union and Canada, where industry subsequently implemented the AdChoices program. We calculate that the inability to behaviorally target opt-out users results in a loss of about $8.58 in ad spending per American opt-out consumer, which is borne by publishers and the exchange. We find that opt-out users tend to be more technologically sophisticated, though opt-out rates are also higher in older and wealthier American cities. These results inform the privacy policy discussion by illuminating the real-world consequences of an opt-out privacy mechanism.
We study the price effects of consolidation in the car rental industry using three cross-sections of price data from U.S. airport markets spanning the years 2005 to 2016. The auto rental industry went through a series of mergers during this period, leading to a significant increase in market concentration. We find that the concentration of ownership affects the business (weekday) and leisure (weekend) segments differently. Average weekday prices rose by 2.1% and weekend prices fell by 3.3% with the increase in market concentration. Given the periodic differences in demand from business and leisure travelers, we explain this finding with a model of horizontal product differentiation that allows for heterogeneity in customer types and firms’ marginal costs. Consolidation leads to marginal cost savings, but the extent to which these savings are passed onto different customer types depends on the magnitude of switching costs. In particular, weekday customers with high switching costs are charged higher prices because of suppliers’ augmented market power whereas the more price-sensitive weekend segment enjoys the lower prices facilitated by efficiency gains. Our findings highlight that consolidation can have differential welfare effects on different customer groups and merger analyses should account for the heterogeneous impact based on firms’ price discrimination practices rather than just considering average effects.
Governments often regulate marketing activities to ensure marketers do not misinform consumers and obtain “unfair” advantages. Yet, ample research finds such regulations may be ineffective since marketers are able to circumvent them. We examine if umbrella branding, a marketing strategy of multiple products sharing a common brand, can be used to circumvent marketing regulations on a given product. Specifically, in the asset management industry, we examine if hedge funds, faced with a comprehensive marketing ban, benefited from the advertising by their umbrella brand mutual fund affiliates and, if so, whether the hedge funds exploited this effect. We find that higher advertising by mutual fund affiliates leads to a significant increase in sales of umbrella brand hedge funds and that hedge funds’ circumstances in a trailing period impact the likelihood of advertising by their umbrella brand mutual fund affiliates. More importantly, using the 2012 JOBS Act that removed hedge funds’ marketing restrictions as a natural experiment, we find that hedge funds’ trailing circumstances had significantly less impact on umbrella branded mutual fund advertising after the passage of the JOBS Act. These findings are consistent with hedge funds using umbrella branding to circumvent the marketing ban.
We investigate whether online travel agents (OTAs) assign hotels worse positions in their search results if these set lower hotel prices at other OTAs or on their own websites. We formally characterize how an OTA can use such a strategy to reduce price differentiation across distribution channels. Our empirical analysis shows that the position of a hotel in the search results of OTAs is better when the prices charged by the hotel on other channels are higher. This is consistent with the hypothesis that OTAs alter their search results to discipline hotels for aggressive prices on competing channels, thereby reducing the search quality for consumers.
Given the rise of the online gaming industry, consumer protection laws have been implemented to restrict excessive gaming on the internet. This research evaluates one such consumer protection policy and its effectiveness from both marketing and public policy perspectives. Specifically, we investigate the impact of usage restriction in South Korea on both gamers and the industry using individual-level game usage and spending data. Combining the difference-in-differences approach with a regression discontinuity design and propensity score matching, we show that although the regulation reduces overall game usage, the effects vary depending on past behaviors; that is, counter to expectations, the regulation effect, although negative for average gamers, is less so for heavy gamers and in fact directionally flips for the heaviest gamers. Furthermore, we find that its revenue impact is negligible, suggesting that gamers do not change spending patterns because of the intervention. Thus, usage restriction laws may deter light gamers from potentially excessive gaming but do not work well to dissuade heavy gamers, suggesting that a more nuanced approach (as opposed to a blanket usage restriction law) may be called for. Finally, we discuss the implications of such usage restriction laws as a vehicle to control overconsumption and protect consumers.
The impact of aggregators on news outlets is ambiguous. In particular, the existing theoretical literature highlights that, although aggregators create a market expansion effect when they bring visitors to news outlets, they also generate a substitution effect if some visitors switch from the news outlets to the aggregators. Using the shutdown of the Spanish edition of Google News in December of 2014 and difference-in-differences methodology, this paper empirically examines the relevance of these two effects. We show that the shutdown of Google News in Spain decreased the number of daily visits to Spanish news outlets between 8% and 14% and that this effect was larger in outlets with fewer overall daily visits and a lower share of international visitors. We also find evidence suggesting that the shutdown decreased online advertisement revenues and advertising intensity at news outlets. We then analyze the effect of the opt-in policy adopted by the German edition of Google News in October of 2014. Although such policy did not significantly affect the daily visits of all outlets that opted out, it reduced by 8% the number of visits of the outlets controlled by the publisher Axel Springer. Our results show the existence of a net market expansion effect through which news aggregators increase consumers' awareness of news outlets' contents, thereby increasing their number of visits.
Consumer voice has increasingly become a major factor in the marketplace through consumer complaints, but little is known about who chooses to complain and how complainants compare with consumers of the product. Any differences in complaint rates across groups can reflect either different propensities to complain or different consumer experiences, making it difficult to assess the degree of self-selection. I utilize a set of law enforcement actions to separate these two explanations by comparing characteristics of complaining consumers to those of victims, and I find much lower complaint rates in heavily minority areas compared with nonminority areas, relative to their respective victimization rates. I find evidence against information-based accounts for why victims from minority areas are less likely to complain and in favor of explanations related to lower levels of trust or general social capital. I then provide a statistical weighting approach to remedy the problem of self-selection and apply it to develop an implied victimization rate using complaints from the Consumer Sentinel database.
This research examines how drip pricing—a strategy whereby a firm advertises only part of a product’s price up front and then reveals additional mandatory or optional fees/surcharges as the consumer proceeds through the buying process—affects consumer choice and satisfaction. Across six studies, we find that when optional surcharges are dripped (versus revealed up front) consumers are more likely to initially select a lower base priced option which, after surcharges are included, is often more expensive than the alternative. Moreover, consumers exposed to drip pricing tend to ultimately select this lower base price but higher total price option, even after being exposed to the total price and given the opportunity to change their selection and even though they are relatively dissatisfied with it. We explore why drip pricing has these effects and find that they are driven by consumers’ perceptions regarding the costs and benefits of starting over and switching. Specifically, we find that high perceived search costs (study 2), self-justification (study 3), and mistaken perceptions regarding the potential gains of switching because of inaccurate beliefs that all firms charge similar additional fees/surcharges (study 4) all play roles. We discuss the implications of these findings for marketers, consumers, and policy makers.
This article investigates consumer protection on Kickstarter—a popular and sizeable, yet largely unregulated reward-based crowdfunding platform. Specifically, the article focuses on Kickstarter campaigns’ use of price advertising claims (PACs) and their failure to honor the promised discounts. Analyses show that between 2009 and 2016, more than 500,000 consumers who backed a wide variety of game or technology campaigns lost on average $45.72 because of broken PAC promises. Whereas 75% of PAC campaigns did not provide the promised discounts, in almost 50% of all cases backers who were promised a discount paid more, not less, than the retail price. In contrast, backers of campaigns that did not promise a discount received larger effective discounts. Analyzing an extensive data set comprising 34,745 Kickstarter campaigns, complete backing histories of more than 400,000 backers, and more than 4 million consumer comments, complaints, and reviews, we show that broken PAC promises pose a substantial problem to consumers, that the problem is persistent across more than 6 years, and that it has not been resolved through self-regulation by market participants thus far.
We assess the impact of legislation mandating the plain packaging of cigarettes in 2012 in Australia on both primary and secondary demand. We first examine the causal impact of the legislation at the cigarette category level by comparing the changes in sales before and after legislation with the corresponding changes in sales in a comparable market, New Zealand, where the plain packaging mandate (PPM) was not imposed. Our results suggest a decline in sales due to the PPM of around 67 million units (sticks) per month, representing around 7.5% of the market. Our results on the mechanism using brand-level sales data from Australia suggest reduced differentiation after the PPM, with higher price sensitivity. Premium and mainstream brands’ price sensitivities are most affected after the PPM, but we also find channel-specific differences, with grocery (convenience) channels showing an increase (a decline) in post-PPM short-term price sensitivity. Because the government has some control over price through excise taxes, understanding changes in price sensitivities provides guidance to health authorities on the relative impacts of price- and non-price-related policy on cigarettes sales. We also explore other public policy implications of our results, such as the expected reduction in sales per month we might see in New Zealand due to their instituting a PPM.
This paper estimates an individual-level demand model for eggs differentiated by animal welfare. Typically, after minimum quality standards for eggs are raised, the price of higher-quality eggs falls. As a result, consumer welfare is redistributed from households that do not value animal welfare to households that are willing to pay a premium for animal welfare. In our analysis of German household data, we find that, on average, households with higher income are willing to pay more for eggs that provide higher animal welfare. This provides evidence that higher minimum quality standards have a regressive impact. In counterfactual scenarios, we estimate the cost reduction that would be needed to offset the regressive effect and find that as retailers’ pricing power increases, the cost reduction must be higher. Finally, we consider hypothetical future scenarios that continue to increase the minimum quality standard until only the highest-quality eggs remain on the market.
Consumption of entertainment products such as movies, video games, and sports events often lasts a nontrivial time period. During these experiences, consumers are likely to encounter temporal variations in the content of consumption, to which they may react in real time. Compared with existing in-consumption analysis (e.g., eye tracking and neural activity analysis), listening to in-consumption consumers’ voices on social media has great potential. Our paper proposes a new approach for in-consumption social listening and demonstrates its value in the context of online movie watching wherein viewers can react to movie content with live comments. Specifically, we propose to listen to the live comments through a novel measure, moment-to-moment synchronicity (MTMS), to capture viewers’ in-consumption engagement. MTMS refers to the synchronicity between temporal variations in the volume of live comments and those in movie content mined from unstructured video, audio, and text data (i.e., camera motion, shot length, sound loudness, pitch, and spoken lines). We demonstrate that MTMS significantly predicts viewers’ postconsumption appreciation of movies and that it can be evaluated at a finer level to identify engaging content. Finally, we discuss the information value of MTMS with the presence of measures used in the previous literature and the value of integrating supply-side content information into in-consumption analysis.
Social platforms often use curation algorithms to match content to user tastes. Although designed to improve user experience, these algorithms have been blamed for increased polarization of consumed content. We analyze how curation algorithms impact the number of friends users follow and the quality of content generated on the network, taking into account horizontal and vertical differentiation. Although algorithms increase polarization for fixed networks, when they indirectly influence network connectivity and content quality their impact on polarization and segregation is less clear. We find that network connectivity and content quality are strategic complements, and that introducing curation makes consumers less selective and increases connectivity. In equilibrium, content creators receive lower payoffs because the competition leads to a prisoner’s dilemma. Filter bubbles are not always a consequence of curation algorithms. A perfect filtering algorithm increases content polarization and creates a filter bubble when the marginal cost of quality is low, and an algorithm focused on vertical content quality increases connectivity and lowers polarization and does not create a filter bubble. Consequently, although user surplus can increase through curating and encouraging high-quality content, the type of algorithm used matters for the unintended consequence of creating a filter bubble.
An increasingly common practice among media platforms is to provide premium content versions with fewer or even no ads. This practice leads to an intriguing question: how should ad-financed media price discriminate through versioning? I develop a two-sided media model and illustrate that price discrimination on one side can strengthen the incentive to discriminate on the other. Under this self-reinforcing mechanism, the ad allocations across different consumer types depend crucially on how much nuisance of an ad “costs” consumers relative to the value it brings to them. Interestingly, higher-type consumers, who value content and advertising quality highly, may see more ads than lower-type consumers if the nuisance cost is relatively low. Furthermore, the standard downward quality distortion generally fails to materialize in a two-sided market and may even be reversed: higher-type consumers may be exposed to too few ads that result in a lower total quality than the socially efficient level, whereas lower-type consumers may receive a socially excessive quality. The circumstances under which the self-reinforcing mechanism may be weakened and the implications for media platform design are explored.
We investigate various dynamics characterizing the crowdfunding process: stagnation after friend-funding, gradual increase through crowd participation, and acceleration in the last phase. We propose three mechanisms as major drivers of the crowdfunding dynamics: forward-looking delaying investment behavior, contemporaneous social interactions, and forward-looking social interactions. We apply the rational expectations equilibrium of the approximate aggregation approach to model the underlying mechanisms. Using the Bayesian IJC method, we analyze individual-level investment data from a crowdfunding platform, Sellaband. We find strong evidence for the three mechanisms and confirm that they contribute to the contrasting dynamic patterns observed in our data. We also simulate counterfactuals to derive optimal policy decisions for both fundraisers and platforms. For fundraisers, we infer the optimal goals that ensure goal completion while raising the maximum capital. For platforms, we suggest an optimal targeting strategy that identifies those crowdfunders who contribute the most to the crowding process and, ultimately, goal success. Also, we provide critical input for various resource allocation decisions by accurately predicting whether the project will succeed and when it will succeed at the time when 50% of the goal has been achieved.
Customers often exhibit considerable uncertainty in their service valuations. In response, firms may tailor their products and allow service cancellations. We consider the joint product customization and refund policy decisions of a monopolistic firm selling to a heterogeneous customer population with imperfect signals on their valuations. Our results shed light on how customers’ valuation uncertainty, characterized by the valuation heterogeneity and signal quality, drives the interaction between product line and refund policy designs. In particular, when the valuation heterogeneity is high, the firm may choose to offer a single quality level with a full refund, leading to a variety reduction in the product line. In contrast, when the valuation heterogeneity is low, the firm will always offer a full product line without any refund. At moderate valuation heterogeneity, both qualities and refunds are subject to more customization, and a partial refund can be optimal when the signal quality is high, even though our setup does not involve aggregate demand uncertainty, capacity limitations, competition, or channel conflicts. Interestingly, despite its appeal, generous refund terms do not increase aggregate customer surplus. Furthermore, the firm may not have incentives to reduce customers’ valuation uncertainty even if doing so is costless. We verify the robustness of our results and discuss their practical implications.
To study consumer brand misinformation, we run in-store blind taste tests with a retailer’s private label food brands and the leading national brand counterparts in three large consumer packaged goods categories. Subjects self-report very high expectations about the quality of the private labels relative to national brands. However, they predict a relatively low probability of choosing them in a blind taste test. An overwhelming majority systematically choose the private label in the blinded test. Using program evaluation methods, we find that the causal effect of this intervention on treated consumers increases their market share for the tested private label product by 15 share points during the week after the intervention, on top of a base share of 8 share points. However, the effect diminishes to 8 share points during the second to fourth weeks after the test and to 2 share points during the second to fifth months after the test. Using a structural model of demand that controls for the self-selected participation and allows for heterogeneous treatment effects, we show that these effects survive controls for point-of-purchase prices, purchase incidence, and the feedback effects of brand loyalty. We also find that the intervention increases the preference for the private label brands, and that it decreases the preference for the national brands, relative to the outside good. Interpreting the intervention as an information treatment about the product, we find evidence consistent with an economically large informational barrier on demand for the private label product relative to an established national brand.
Understanding the drivers of preferences requires models and covariates that explain their cross-sectional variation in models of demand. In this paper, we develop an integrated model of choice and covariates where parameters of the covariate model serve as regressors to explain preference heterogeneity in the choice model. We investigate alternative models to deal with many potential covariates and find that a grade of membership model provides empirical and conceptual advantages in explaining preferences. Our proposed model is illustrated with two conjoint data sets.
Choice environments in practice are often more complicated than the well-studied case of choice between perfect substitutes. Consumers choosing from menus or configuring products face choice sets that consist of substitutes, complements, and independent items, and the utility-maximizing choice corresponds to a particular item combination out of a potentially huge number of possible combinations. This reality is mirrored in menu-based choice experiments. The inferential challenge posed by data from such choices is in the calibration of utility functions that accommodate a mix of substitutes, complements, and independent items. We develop a model that not only accounts for heterogeneity in preferences, but also in what consumers perceive to be substitutes and complements and show how to perform Bayesian inference for this model based on the exact likelihood, despite its practically intractable normalizing constant. We characterize the model from first principles and show how it structurally improves on the multivariate probit model and on models that include cross-price effects in the utility function. We find empirical support for our model in a menu-based discrete choice experiment investigating demand for game consoles and accessories. Finally, we illustrate substantial implications from modeling substitution and complementarity for optimal pricing.
This report summarizes the entrants in the 2018 ISMS Gary Lilien Marketing Science Practice Prize Competition, designed to identify, encourage, recognize, and reward the application of impactful marketing science to industry and noncommercial settings. These applications aim to showcase innovative and impactful examples of applications demonstrating the best of rigor and relevance that our profession produces. The winner described an application of econometric and experimental techniques at the French-based retailer of body, face, fragrances, and home products, L’Occitane, aimed to optimize budget allocation between online and off-line marketing expenditure across six countries. The other three finalists include a decision support aid to assist members of the United Services Automobile Association (a not-for-profit organization to assist military service personnel select and finance their automobile purchases) choose vehicles that balance their individual tastes and preferences with their financial circumstances, a business-to-business pricing decision support aid for use by the sales force of Hadco Metal Trading to avoid loss of margin by undercharging and loss of business by overcharging, and a methodology to test online advertisements cost-effectively by generating “ghost ads” as a way of minimizing control group costs in a reliable and valid way.
Marketing Science greatly benefited from the admirable and fastidious efforts of more than 200 different individuals who provided manuscript reviews last year. Beyond those individuals already recognized on the editorial board, the editor-in-chief and senior editors of Marketing Science are indebted to the many guest editors, guest associate editors, and ad hoc reviewers who provided expert counsel and guidance on a voluntary basis. The following list acknowledges the contribution of guest editors, guest associate editors, and ad hoc reviewers who served from January 1, 2019, to December 31, 2019. Finally, our sincere appreciation to the authors, whose outstanding submissions and careful revisions make the journal the go-to resource for leading-edge knowledge in quantitative marketing.K. SudhirYale University
This editorial introduces the special issue on marketing science and health. We begin by describing the healthcare ecosystem and its many distinguishing features relative to other markets. With its large share of U.S. and world gross domestic product; rapid changes on the demand, supply, and regulatory sides; and a complex ecosystem with many types of participants, healthcare markets provide a rich canvas of novel research opportunities for marketing scholars. We then describe the special issue process and the papers published in the special issue. We summarize key themes that emerge from these papers and conclude with a discussion of future research opportunities in the area.
We examine the effectiveness in field settings of seven healthy eating nudges, classified according to whether they are (1) cognitively oriented, such as “descriptive nutritional labeling,” “evaluative nutritional labeling,” or “visibility enhancements”; (2) affectively oriented, such as “hedonic enhancements or “healthy eating calls”; or (3) behaviorally oriented, such as “convenience enhancements” or “size enhancements.” Our multivariate, three-level meta-analysis of 299 effect sizes, controlling for eating behavior, population, and study characteristics, yields a standardized mean difference (Cohen’s d) of 0.23 (equivalent to −124 kcal/day). Effect sizes increase as the focus of the nudges shifts from cognition (d = 0.12, −64 kcal) to affect (d = 0.24, −129 kcal) to behavior (d = 0.39, −209 kcal). Interventions are more effective at reducing unhealthy eating than increasing healthy eating or reducing total eating. Effect sizes are larger in the United States than in other countries, in restaurants or cafeterias than in grocery stores, and in studies including a control group. Effect sizes are similar for food selection versus consumption and for children versus adults and are independent of study duration. Compared with the typical nudge study (d = 0.12), one implementing the best nudge scenario can expect a sixfold increase in effectiveness (to d = 0.74) with half the result of switching from cognitively oriented to behaviorally oriented nudges.
We investigate the role of heterogeneous peer effects in encouraging healthy lifestyles. Our analysis revolves around one of the largest and most extensive databases about weight loss that track individual participants’ meeting attendance and progress in a large national weight loss program. The main finding is that, although weight loss among average-performing peers has a negative effect on an individual’s weight loss, the corresponding effect for the top performer among peers is positive. Furthermore, we show that our results are robust to potential issues related to selection into meetings, endogenous peer outcomes, individual unobserved heterogeneity, lagged dependent variables, and contextual effects. Ultimately, these results provide guidance about how the weight loss program should identify role models.
In an effort to reduce sexually transmitted infections (STI) in developing countries, health organizations often recruit former sex workers as “peer educators” to counsel current sex workers. Although peer educator outreach (PEO) programs have generally been shown to reduce STI, it is not clear whether such efficacy is primarily driven by “prevention” (reducing the infection rate of STI through safe-sex education) or “detection” (educating sex workers about STI symptoms so that they will seek prompt treatment if/when infected). Such differentiation is not only of academic interest, but also has important practical implications on resource management. We develop an integrated Bayesian model to disentangle the role of prevention versus detection in PEO programs. Our results show that PEO programs appear to be not effective in preventing STI, but they do facilitate earlier detection by enhancing sex workers’ knowledge and ability to recognize STI symptoms. Simulations based on our model suggest that increasing PEO efforts by 10% from the current level would increase clinic visits by 1.0%, thereby reducing STI prevalence by around 3.0%. Further, we conducted a randomized controlled field experiment that provides directionally consistent evidence that PEO visits are effective in increasing clinic visits among sex workers.
U.S. pharmaceutical companies frequently pay doctors to promote their drugs. This has raised concerns about conflict of interest, which policy makers have attempted to address by introducing payment disclosure laws. However, it is unclear if such disclosure has an effect on physician prescription behavior. We use individual-level claims data from a major provider of health insurance in the United States and employ a difference-in-differences research design to study the effect of the payment disclosure law introduced in Massachusetts in June 2009. The research design exploits the fact that, although physicians operating in Massachusetts were impacted by the legislation, their counterparts in the neighboring states of Connecticut, New York, New Hampshire, and Rhode Island were not. In order to keep the groups of physicians comparable, we restrict our analysis to physicians in the counties that are on the border of these states. We find that the Massachusetts disclosure law resulted in a decline in prescriptions in all three drug classes studied: statins, antidepressants, and antipsychotics. Our findings are robust to alternative control groups, time periods and estimation methods. We also show that the effect is highly heterogeneous across physician groups. Finally, we explore potential mechanisms driving these results.
We study the problem a diagnostic expert (e.g., a physician) faces when offering a diagnosis to a client (e.g., a patient) that may be based only on the expert’s own diagnostic ability or supplemented by a diagnostic test—conventional and artificial intelligence (AI) tools alike—revealing the client’s true condition. The expert’s diagnostic ability (or type) is private information. The expert is impurely altruistic in that the expert cares about both the client’s utility and the expert’s own reputational payoff that depends on the peer perception of the expert’s diagnostic ability. The decision of whether to perform the test, which is costly for the client, provides the expert with an opportunity to influence that perception. We show a unique separating equilibrium exists in which the high-type expert does not resort to diagnostic testing and offers a diagnosis based only on the expert’s own diagnostic ability, whereas the low-type expert performs the test. Furthermore, we establish that high-type expert may skip necessary diagnostic tests to separate them from the low-type expert. Interestingly, the effect of reputational payoff on undertesting is nonmonotonic, and the desire to appear of high type leads to undertesting only when the reputational payoff is intermediate. Our results also suggest a more altruistic expert may be more likely to engage in undertesting. Furthermore, efforts to encourage testing by providing financial incentives or by raising malpractice lawsuit concerns may, surprisingly, help fuel undertesting in the equilibrium. Our paper sheds new light on barriers to the adoption of AI tools aimed at enhancing physicians’ diagnostic decision making.
Do pharmaceutical firms respond to the actions of their competitors in research and development, and if so, how much? Answering this question has implications for policies aimed at incentivizing drug development, such as greater exclusivity protections and a faster Food and Drug Administration approval process. Although such policies lead to quicker realization of profits and/or more time to earn profits, they also intensify competition, thereby reducing per-firm profits. Which effect dominates depends on the degree of competition. To this end, I estimate a dynamic investment model using Phase 3 data. Solving the new equilibrium, I find that even though an expedited process and longer periods of market exclusivity increase competitive intensity, it could prompt increased entry into Phase 3, thereby encouraging innovation.
The effects of television advertising in the market for health insurance are of distinct interest to both firms and regulators. Regulators are concerned about firms potentially using ads to “cream skim,” or attract an advantageous risk pool, as well as the potential for firms to use misinformation to take advantage of the elderly. Firms are interested in using advertising to acquire potentially highly profitable seniors. Meanwhile, health insurance is a useful setting to study the mechanisms through which advertising could work. Using the discontinuity in advertising exposure created by the borders of television markets, this study estimates the effects of advertising on consumer choice in health insurance. Television advertising has a small effect on brand enrollments, making advertising a relatively expensive means of acquiring customers. Heterogeneous effects point to advertising being more effective in less healthy counties, which runs opposite to the concern of cream skimming. Leveraging the unilateral cessation of advertising by United-Healthcare, evidence is provided that the small advertising effect is not explained by a prisoner’s dilemma equilibrium. An analysis of longer-run effects of advertising shows that advertising effects are short lived, further decreasing the potential of advertising to create long-run value to the firm.
Does hospital advertising influence patient choice and health outcomes? We examine more than 220,000 individual patient-level visits over 24 months in Massachusetts to answer this question. We find that patients are positively influenced by hospital advertising; seeing a television advertisement for a given hospital makes a patient more likely to select that hospital. We also observe significant heterogeneity in patient response depending on insurance status, medical conditions, and demographic factors, like age, gender, and race. For example, patients with more restrictive forms of insurance are less sensitive to advertisements. Our demand model allows us to study the impact of a ban on hospital advertising, which has been recently considered by policy makers. We find that banning hospital advertising can hurt patient health outcomes through increased hospital readmissions. This is because hospital advertisements drive patients to higher-quality hospitals, which tend to advertise more and have lower readmission rates. However, we do not find a significant change in the overall mortality rate.
In a healthcare industry with capacity constraints, the best healthcare providers are often congested after quality information disclosure. This congestion can lead to the reallocation of urgent patients to low-quality healthcare providers. The reallocation can have a detrimental impact on the overall patient survival rate if sicker patients benefit more from the best providers. This paper provides the first empirical evidence regarding this problem in the context of the publication of cardiac surgery report cards. I find that these report cards can have a negative impact on positive assortative matching between patients and surgeons because of a reallocation of high-risk patients to low-quality surgeons. Despite the quality improvement in response to these report cards, such patient reallocation can still be a problem, conditional on the improved quality, and, thus, should not be ignored.
Images are close to surpassing text as the medium of choice for online conversations. They convey rich information about the consumption experience, attitudes, and feelings of the user. In this paper, we propose a “visual listening in” approach (i.e., mining visual content posted by users) to measure how brands are portrayed on social media. We develop BrandImageNet, a multi-label deep convolutional neural network model, to predict the presence of perceptual brand attributes in the images consumers post online. We validate BrandImageNet model performance using human judges and find a high degree of agreement between our model and human evaluations of images. We apply the BrandImageNet model to brand-related images posted on social media to extract brand portrayal based on model predictions for 56 national brands in the apparel and beverages categories. We find a strong link between brand portrayal in consumer-created images and consumer brand perceptions collected through traditional survey tools. Firms can use the BrandImageNet model to automatically monitor their brand portrayal in real time and better understand consumer brand perceptions and attitudes toward their and competitors’ brands.
The prevalence of online platforms opens new doors to traditional businesses for customer reach and revenue growth. This research investigates platform competition in a setting in which prices are determined by negotiations between platforms (specifically, their salespeople) and businesses. We compile a unique and comprehensive data set from the U.S. daily deal market, where merchants offer deals to generate revenues and attract new customers. We specify and estimate a two-stage supply-side model in which platforms and merchants bargain on the wholesale price of deals. Based on Nash bargaining solutions, our model generates insights into how bargaining power and bargaining position jointly determine price and firm profits. By working with a bigger platform, merchants enjoy a larger customer base, but they are subject to lower margins because of less bargaining power. Counterfactual results reveal that, in the absence of platform competition, merchants are worse off owing to their weaker bargaining position, but consumers experience lower prices, thus leading to an increase in total demand.
Understanding how forward-looking consumers respond to price promotions in storable goods markets is an important area of research in empirical marketing and industrial organization. In prior work, researchers have assumed that consumers in these markets are very forward-looking, and calibrated their weekly discount factors to levels around 0.9995. This calibration has been used because earlier research has assumed that a consumer’s storage cost is a continuous function of inventory, which rules out exclusion restrictions that can be used to identify the discount factor. We show that by properly modeling storage cost as a step function of inventory (because storage cost depends on the number of packages stored, instead of the actual amount of inventory), natural exclusion restrictions arise that allow for the discount factor to be point identified. In an application to a storable good category, we find that weekly discount factors are very heterogeneous across consumers, and are on average 0.71. We show through a counterfactual exercise that if one used a model that fixed the discount factor to be consistent with the standard calibrated value, one would overpredict the effect of increased promotional depth for a product on its quantity sold by 18% in the short term, and 15% in the long term.
User-generated content in the form of customer reviews, blogs, and tweets is an emerging and rich source of data for marketers. Topic models have been successfully applied to such data, demonstrating that empirical text analysis benefits greatly from a latent variable approach that summarizes high-level interactions among words. We propose a new topic model that allows for serial dependency of topics in text. That is, topics may carry over from word to word in a document, violating the bag-of-words assumption in traditional topic models. In the proposed model, topic carryover is informed by sentence conjunctions and punctuation. Typically, such observed information is eliminated prior to analyzing text data (i.e., preprocessing) because words such as “and” and “but” do not differentiate topics. We find that these elements of grammar contain information relevant to topic changes. We examine the performance of our models using multiple data sets and establish boundary conditions for when our model leads to improved inference about customer evaluations. Implications and opportunities for future research are discussed.
Cash back sites are referral intermediaries that help retailers attract consumers and serve consumers through rebate offers. What exactly is the strategic impact of cash back sites on retailer pricing? We examine this by analyzing two competing retailers that use a cash back site to serve consumers, some of whom are loyal, some of whom are switchers, and some of whom are searchers. We formulate a multistage game by innovating on extant models of consumer price search and solve for the subgame perfect Nash equilibrium prices. We find that the cash back site can allow retailers to profitably eliminate consumer search and that makes retailer sites more sticky. Thus, cash back sites can act as strategic partners of retailers. Surprisingly, even consumers that use the cash back site can be worse off in the presence of cash back sites under some conditions. In particular, if search prevention is profitable even without a cash back site, then cash back sites result in higher prices to consumers. We also offer practical guidance through our finding that the optimal discount offer should be proportional to price rather than a flat sum.
According to household production theory, consumers buy inputs and combine them to produce final goods from which they derive utility. We use this idea to build a micro-level model for the quantity demanded by a consumer across product categories. Our model proposes an intuitive explanation for the existence of negative cross-price effects across categories and can be estimated on purchase data in the presence of corner solutions and indivisible packages. We find that, even when reusing the same functional form as some previous models of demand for substitutes, our model can accommodate very different patterns of consumer preferences from perfect complementarity to no complementarity between goods. We estimate the model on purchase data from a panel of consumers and find that it yields a better fit than a set of benchmark models. We then show how the demand system estimated can be used to increase the profitability of couponing strategies by taking into account the spillover effect of coupons on demand for complementary categories and by manufacturers to make decisions regarding the size of packages by taking into account cross-category consumption. We also use the model to simulate demand under a shift in the proportions used in joint consumption, which could be stimulated via marketing efforts.
Nowadays firms often claim that their products are superior, but product statements may not be truthful. Knowing firms’ potential dishonesty, consumers are skeptical about these possibly false statements and may investigate. To protect consumers, regulators can penalize firms who deceive consumers. In response to consumers and regulators, firms can make their false claims deceptive to impede investigation. We develop a game theoretical model to study interactions between dishonest firms, skeptical consumers, and regulations. We show that increasing the penalty for false statements can surprisingly reduce consumer surplus, firm profits, and social welfare. The welfare reduction is due to higher spending on deceptiveness, which hinders consumers from investigating potentially false claims. The lack of information discourages consumers from identifying product quality, thus decreasing welfare. Furthermore, when it is costless to adjust the penalty, the optimal penalty that maximizes both consumer surplus and welfare is the minimum penalty that ensures truthful claims, and it increases with firms’ quality difference and the probability of encountering a high-quality firm. In an extension, we allow regulators to detect false claims through consumer complaints. We find that higher penalty leads to lower consumer surplus if and only if the average product value is sufficiently high.
Increasingly, firms have the ability to make high-quality, microlevel predictions of demand for their products, which improves their ability to target advertising. In spite of this, firms may choose to target advertising at a higher level of aggregation than their predictions allow to benefit from the significant discounts that often accompany mass advertising purchases. We argue that firms making such a choice generate “advertising spillovers” that are quasi-random and can be used to identify the response to advertising. These advertising spillovers occur when local levels of advertising are higher or lower than locally optimal because of the influence of other markets or individuals on the mass advertising decision. We formalize the supply-side conditions that incentivize firms to generate these spillovers as part of their optimization strategy, present an empirical strategy for exploiting these conditions, and apply the strategy to multiple product categories and brands. Estimates from this “spillover strategy” agree with recent literature that suggests many standard approaches to estimating the response to advertising may produce biased results because of unobservables; our estimates also suggest that some recent empirical strategies, such as the DMA-border strategy, can produce biased estimates for seasonal products.
Although social media has emerged as a popular source of insights for both researchers and practitioners, much of the work on the dynamics in social media has focused on common metrics such as volume and sentiment. In this research, we develop a changepoint model to capture the underlying shifts in social media content. We extend latent Dirichlet allocation (LDA), a topic modeling approach, by incorporating multiple latent changepoints through a Dirichlet process hidden Markov model that allows for the prevalence of topics to differ before and after each changepoint without requiring prior knowledge about the number of changepoints. We demonstrate our modeling framework using social media posts from brand crises (Volkswagen’s 2015 emissions testing scandal and Under Armour’s 2018 data breach) and a new product launch (Burger King’s 2016 launch of the Angriest Whopper). We show that our model identifies shifts in the conversation surrounding each of these events and outperforms both static and other dynamic topic models. We demonstrate how the model may be used by marketers to actively monitor conversations around their brands, including distinguishing between changes in the conversation arising from a shift in the contributor base and underlying changes in the topics discussed by contributors.
In studying consumer search behavior, researchers typically focus on which products consumers add to their consideration sets (the extensive margin of search). In this article, we attempt to additionally study how much consumers search individual products (the intensive margin of search) by analyzing the time they spend searching (search duration). We develop a sequential search model by which consumers who are uncertain (and have prior beliefs) about their match value for a product search to reveal (noisy) signals about it that they then use to update their beliefs in a Bayesian fashion. Search duration, in this context, is an outcome of the decision by a consumer to seek information on the same product multiple times; with a unit of time corresponding to one signal, the more the number of signals sought greater is the search duration. We also show how the model can be used to study revisits, a feature not easily accommodated in Weitzman’s sequential search model. We build on the framework by Chick and Frazier for describing the optimal search rules for the full set of decisions consumers make (which products to search, for how long, in what order, and whether to purchase) and develop the model’s empirical counterpart. We estimate the proposed model using data on consumers searching for restaurants online. We document that search duration is considerable, even when consumers search few restaurants, and that restaurants that are searched longer are more likely to be purchased. Using our model, we quantify preferences and search costs, as well as consumer prior beliefs, providing additional insights into consumers’ search process. Finally, we develop managerial implications related to the amount of information companies should provide to consumers, given that this will affect search duration and thus search and purchase decisions.
This study jointly examines the effects of television advertising and field operations in U.S. presidential elections, with the former referred to as the “air war” and the latter as the “ground game.” Specifically, the study focuses on how different campaign activities—personal selling in the form of field operations and mass media advertising by the candidates and by outside sources—vary in their effectiveness with voters who have different political predispositions. The voting choice model takes into consideration voter heterogeneity and analyzes comprehensive data that include voting outcomes, detailed campaign activities, and voters’ party affiliation for three presidential elections (2004–2012). The results reveal that different campaign activities have heterogeneous effects depending on voters’ party affiliation. Field operations and political advertising from outside groups are more effective with partisans, whereas a candidate’s advertising is more effective with nonpartisans. These findings can help strategists better allocate resources across and within channels to design an effective political marketing campaign.
Aggregators are facing increased scrutiny by regulatory authorities, suggesting these sites have considerable market power. On the other extreme, firms are bypassing aggregators, choosing instead to sell directly to consumers. This raises the question as to which party has more market power: the aggregator or the individual firm. Focusing on the airline industry, we investigate who benefits most in the airline-aggregator relationship. Specifically, we ask what would happen to airline and aggregator site visits and purchases in the absence of a comprehensive aggregator. We first explore consumers’ search patterns on Southwest, an airline that has never been part of any aggregator. In a descriptive exercise, we find that consumers who book on Southwest are the least likely to visit aggregator sites. Second, we use the 2011 American dispute with Orbitz as an exogenous event, which led to American fares no longer being displayed on Orbitz for five months. We use this dispute to identify who was hurt the most—the aggregator or the airline—in the months following the dispute. Our findings indicate the aggregator loses the most when it is not comprehensive.
Firms can improve market demand by disclosing privately known information on their advantages (e.g., quality) to potential buyers. The conventional prediction in the literature on voluntary disclosure is that, because of rational buyer expectation, any private information would be perfectly unravelled in equilibrium. However, concealments can be seen in practice for both low- and high-quality firms. This paper proposes a new explanation for this puzzle, based on downstream manufacturers’ incentive to mitigate upstream exploitation by input suppliers. We highlight two natural consequences of increasing quality: higher product value and less elastic demand. The latter force would push up the wholesale price to yield a nonmonotonic impact of quality on equilibrium manufacturer payoff. Therefore, there may exist intermediate-disclosure equilibria where the manufacturer withholds both low- and high quality levels, even when disclosure is costless. In addition, partial disclosure can benefit not only channel members but also buyers. The intermediate-disclosure equilibria can survive even when supplier actions to influence the disclosure outcome are endogenized. Moreover, strategic concealment may undermine incentives for vertical integration.
We model how quality concerns affect the relationship between a firm and its supplier. A firm concerned about uncontractible quality for a customizable good has to pay higher prices to sustain a relationship with the supplier. If the customizable good has sufficiently volatile demand, then a contract that includes a constant unit price premium only for this good cannot be sustained. Instead, the downstream firm pays a premium both for the customizable good and also for a good with more stable demand that is correlated with the demand for the customizable good. Our results imply that a supplier of customized goods should also supply other products, which can include goods that do not require customization, and both the supplier and buyer benefit from the greater pricing flexibility they achieve by trading multiple goods.
Customer defection threatens many industries, prompting companies to deploy targeted, proactive customer retention programs and offers. A conventional approach has been to target customers either based on their predicted churn probability, or their responsiveness to a retention offer. However, both approaches ignore that some customers contribute more to the profitability of retention campaigns than others. This study addresses this problem by defining a profit-based loss function to predict, for each customer, the financial impact of a retention intervention. This profit-based loss function aligns the objective of the estimation algorithm with the managerial goal of maximizing the campaign profit. It ensures (1) that customers are ranked based on the incremental impact of the intervention on churn and postcampaign cash flows, after accounting for the cost of the intervention, and (2) that the model minimizes the cost of prediction errors by penalizing customers based on their expected profit lift. Finally, it provides a method to optimize the size of the retention campaign. Two field experiments affirm that this approach leads to significantly more profitable campaigns than competing models.
The decoy effect (DE) has been robustly documented across dozens of product categories and choice settings using laboratory experiments. However, it has never been verified in a real product market in the literature. In this paper, we empirically test and quantify the DE in the diamond sales of a leading online jewelry retailer. We develop a diamond-level proportional hazard framework by jointly modeling market-level decoy–dominant detection probabilities and the boost in sales upon detection of dominants. Results suggest that decoy–dominant detection probabilities are low (11%–25%) in the diamond market; however, upon detection, the DE increases dominant diamonds’ sale hazards significantly (1.8–3.2 times). In terms of the managerial significance, we find that the DE substantially increases the diamond retailer’s gross profit by 14.3%. We further conduct simulation studies to understand the DE’s profit impact under various dominance scenarios.
Two duopolists compete on price in the market for a homogeneous product. They can “profile” consumers, that is, identify their valuations with some probability. If both firms can profile consumers but with different abilities, then they achieve positive expected profits at equilibrium. This provides a rationale for firms to (partially and unequally) share data about consumers or for data brokers to sell different customer analytics to competing firms. Consumers prefer that both firms profile exactly the same set of consumers or that only one firm profiles consumers as this entails marginal cost pricing (so does a policy requiring list prices to be public). Otherwise, more protective privacy regulations have ambiguous effects on consumer surplus.
In common parlance, luxury and markdowns are, in many respects, contradictory concepts. Markdowns decrease product exclusivity and hence consumers’ willingness to pay (i.e., snob effect) because most consumers purchasing luxury desire uniqueness. Markdowns also encourage strategic (forward-looking) consumers to wait for lower prices (i.e., strategic effect). Yet, luxury retailers frequently adopt markdowns in practice to stimulate the demand for their seasonal products (i.e., sales effect). To study the impact of these three countervailing effects on a luxury retailer’s markdown policy and rationing strategy, this paper develops a game-theoretic model with strategic and exclusivity-seeking consumers who have heterogeneous (high and low) valuations. We characterize a luxury retailer’s equilibrium markdown and rationing strategies and find that the retailer induces a buying frenzy (i.e., selling deliberately less than the demand) to increase consumers’ willingness to pay when they are sufficiently exclusivity seeking. We show that the retailer’s markdown policy depends on consumers’ desire for exclusivity when the proportion of consumers with high valuation is not too high or too low. Interestingly, we find that, in such cases, consumers’ higher desire for exclusivity does not motivate the retailer to increase exclusivity and to adopt uniform pricing. To the contrary, it motivates the retailer to decrease the exclusivity and to adopt markdowns. By doing so, we identify exclusivity-seeking consumer behavior as another rationale behind markdown pricing. Last, we find that, when selling to exclusivity-seeking consumers, the negative impact of strategic consumer behavior is lower; however, ignoring it can be more costly.
This editorial introduces the special issue on marketing science and field experiments. We compare the characteristics of the papers that were submitted and accepted for the special issue and provide several recommendations for researchers. In general, we find field experiment research is greater in the areas of advertising and pricing with digital being the most common channel. We suggest that, beyond the estimation of effects and tests of hypotheses, field experiments can complement structural models; help train targeting policies; and also contribute to the nascent area of real-time, adaptive experimentation. We also discuss how field experiment research with a marketing science orientation can enhance and contribute in the areas of behavioral research and marketing strategy.
A growing number of American workers are now freelancers and thus, responsible for their own retirement savings, yet they face psychological hurdles that hamper them from saving enough money for the long term. Although prior theory-derived interventions have been successful in addressing some of these obstacles, encouraging participation in saving programs is a challenging endeavor for policy makers and consumers alike. In a field setting, we test whether framing savings in more or less granular formats (for example, saving daily versus monthly) can encourage continued saving behavior through increasing the take up of a recurring deposit program. Among thousands of new users of a financial technology app, we find that framing deposits in daily amounts as opposed to monthly amounts quadruples the number of consumers who enroll. Furthermore, framing deposits in more granular terms reduced the participation gap between lower- and higher-income consumers: three times as many consumers in the highest rather than lowest income bracket participated in the program when it was framed as a $150 monthly deposit, but this difference in participation was eliminated when deposits were framed as $5 per day.
We present a complete empirical case study of fundraising campaign decisions that demonstrates the unique importance of in-context field experiments. We first design novel matching-based fundraising appeals. We then discuss the assumptions needed to derive theory-based predictions from the standard impure altruism model and solicit expert opinion about the potential performance of our interventions. Both theory-based and experts’ predictions suggested improved fundraising performance from framing a matching intervention as crediting donors for the matched funds, whereas predictions for the other appeals were more ambiguous. However, the results of a natural field experiment with prior donors of a nonprofit instead showed a significantly poorer performance from employing the giving-credit framing. This surprising finding was replicated in a second natural field experiment to confirm the ground truth, at least within a specific context. Experts also lacked consensus about a conditional matching scheme, which, in fact, did not improve fundraising. Theoretically, our results highlight the limitations of both impure altruism models and expert opinion in predicting complex warm-glow motivation. More practically, our results question the availability of useful guidance and suggest the indispensability of field testing for behavioral interventions in fundraising.
In a randomized field experiment with the education charitable giving platform DonorsChoose.org (N = 30,297), we examined email personalization using a potential donor’s name. We measured the effectiveness of matching potential donors to specific teachers in need based on surname, surname initial letters, gender, ethnicity, and surname country of origin. Full surname matching was most effective, with potential donors being more likely to open an email, click on a link in the email, and donate to teachers who shared their own surname. They also donated more money overall. Our results suggest that uniting people with shared names is an effective individual-level approach to email personalization. Potential donors who shared a surname first letter but not an entire name with teachers also behaved more generously. We discuss how using a person’s name in marketing communications may capture attention and bridge social distance.
Why have companies faced a backlash for running experiments? Academics and pundits have argued that people find corporate experimentation intrinsically objectionable. Here we investigate “experiment aversion,” finding evidence that, if anything, experiments are more acceptable than the worst policies they contain. In six studies, participants evaluated the acceptability of either corporate policy changes or of experiments testing them. When all policy changes were deemed acceptable, so was the experiment even when it involved deception, unequal outcomes, and lack of consent. When a policy change was deemed unacceptable, so was the experiment but less so. The acceptability of an experiment hinges on its critical condition—its least acceptable policy. Experiments are not unpopular; unpopular policies are unpopular.
Firms do not typically disclose information on their costs to produce a good to consumers. However, we provide evidence of when and why doing so can increase consumers’ purchase interest. Specifically, building on the psychology of disclosure and trust, we posit that cost transparency, insofar as it represents an act of sensitive disclosure, fosters trust. In turn, this heightened trust enhances consumers’ willingness to purchase from that firm. In support of this account, we present six studies conducted in the field and in the laboratory. A preregistered field experiment indicated that diners were 21.1% more likely to buy a bowl of chicken noodle soup when a sign revealing its ingredients also included the cafeteria’s costs to make it. Five subsequent online experiments replicated and extended this basic effect, providing evidence of when and why it occurs. Taken together, these studies imply that the proactive revelation of costs can improve a firm’s bottom line.
When salespeople with heterogeneous sales abilities are assigned into teams, how do they adjust effort as the abilities of their coworkers change? We investigate this question using a field experiment that spans 29 retail booths and 116 salespeople at a major department store in China. Each booth compensates salespeople using either an individual-based commission (IB) or a revenue-sharing (RS) incentive and employs four salespeople, with two salespeople per shift. Our field experiment randomly assigns salespeople to work shifts, thus exogenously varying the ability of a salesperson’s coworker. The results show that under the IB incentive, the lower-ability salesperson will strategically decrease effort as the ability of the coworker rises; correspondingly, the higher-ability salesperson reduces effort as the coworker’s ability decreases. In contrast, under the RS incentive, the sales pattern suggests that the lower-ability (higher-ability) salesperson increases effort when the coworker’s ability increases (decreases). These empirical results provide broad support to our theory that accounts for the effect of social preferences on salespeople’s effort decisions. We also examine the revenue implications of team composition on firm performance under different sales incentives.
Most of the empirical evidence on social advertising effectiveness focuses on a single product at a time. As a result, little is known about how the effectiveness of social advertising varies across product categories or product characteristics. We therefore collaborated with a large online social network to conduct a randomized field experiment measuring social ad effectiveness across 71 products in 25 categories among more than 37 million users. We found some product categories, like clothing, cars, and food, exhibited significantly stronger social advertising effectiveness than other categories, like financial services, electrical appliances, and mobile games. More generally, we found that status goods, which rely on status-driven consumption, displayed strong social advertising effectiveness. Meanwhile, social ads for experience goods, which rely on informational social influence, did not perform any better or worse than social ads for their theoretical counterparts, search goods. Social advertising effectiveness also significantly varied across the relative characteristics of ad viewers and their friends shown in ads. Understanding the heterogeneous effects of social advertising across products can help marketers differentiate their social advertising strategies and lead researchers to more nuanced theories of social influence in product evaluation.
Managers use referral reward programs to stimulate positive word of mouth by rewarding existing customers for every new customer they successfully refer. A key decision variable in these programs is the referral reward size—but what are the effects of offering smaller versus larger rewards? Whereas previous research has studied the impact of referral reward size on the number of referred new customers, we, for the first time, investigate its effect on the profitability of referred new customers. We analyze a field experiment involving more than 160,000 bank customers and test the generalizability of the results with archival data from approximately 270,000 telecommunication customers. In both studies, we find that even though larger referral rewards lead to the acquisition of more new customers, they considerably decrease the profitability of referred new customers. Managers need to take both of these effects into account when deciding about their program design.
Subscription services typically offer duration discounts, rewarding longer commitments with lower per-period costs. The “menu” of contract plan prices must be balanced to encourage potential customers to select into subscription overall and to nudge those that do to more profitable contracts. Because subscription menu prices change infrequently, they are difficult to optimize using historical pricing data alone. We propose that firms solve this problem via an experiment and a model that jointly accounts for whether to opt in and, conditionally, which plan to choose. To that end, we conduct a randomized online pricing experiment that orthogonalizes the “elevation” and “steepness” of price menus for a major dating pay site. Users’ opt-in and plan choice decisions are analyzed using a novel model for correlated binary selection and multinomial conditional choice, estimated via Hamiltonian Monte Carlo. Benchmark comparisons suggest that common models of consumer choice may systematically misestimate price substitution patterns, and that a key consideration is the distinctiveness of the opt-out (e.g., nonsubscription) option relative to others available. Our model confirms several anticipated pricing effects (e.g., on the margin, raising prices discourages both opt-in overall and choice of any higher-priced plans), but also some that alternative models fail to capture, most notably that across-the-board pricing increases have a far lower negative impact than standard random-utility models would imply. Joint optimization of the menu’s component prices suggests the firm has set them too low overall, particularly so for its longest-duration plan.
Motivated by their increasing prevalence, we study outcomes when competing sellers use machine learning algorithms to run real-time dynamic price experiments. These algorithms are often misspecified, ignoring the effect of factors outside their control, for example, competitors’ prices. We show that the long-run prices depend on the informational value (or signal-to-noise ratio) of price experiments: if low, the long-run prices are consistent with the static Nash equilibrium of the corresponding full information setting. However, if high, the long-run prices are supra-competitive—the full information joint monopoly outcome is possible. We show that this occurs via a novel channel: competitors’ algorithms’ prices end up running correlated experiments. Therefore, sellers’ misspecified models overestimate the own price sensitivity, resulting in higher prices. We discuss the implications on competition policy.
Moment marketing is a new strategy that entails the ability to synchronize online advertising (e.g., sponsored search) in real time with relevant offline events such as TV ads. More and more practitioners are employing this strategy, given the increasing availability of technologies that enable coordination across advertising channels in real time. However, very little is known about the instant impact of TV advertising on the effectiveness of search advertising. We take advantage of a unique opportunity for causal estimation in this research area by leveraging large exogenous variation in TV advertising expenditure over a long period, while at the same time having access to granular consumer search data under relatively stable sponsored search advertising strategies. Utilizing this novel setup, we provide the first empirical evidence that TV-moment-based search advertising could be effective for optimizing sponsored search advertising for both TV-advertised brands and their competitors. We also document the mechanisms driving such cross-channel advertising effects. Specifically, TV advertising can change the quality of online search traffic (e.g., who searches, where they search, and how they search) in the moments following a TV ad, so that an average searcher responds differently to subsequent search results.
We assess the impact of home-sharing on residential house prices and rents. Using a data set of Airbnb listings from the entire United States and an instrumental variables estimation strategy, we show that Airbnb has a positive impact on house prices and rents. This effect is stronger in zip codes with a lower share of owner-occupiers, consistent with non-owner-occupiers being more likely to reallocate their homes from the long- to the short-term rental market. At the median owner-occupancy rate zip code, we find that a 1% increase in Airbnb listings leads to a 0.018% increase in rents and a 0.026% increase in house prices. Considering the median annual Airbnb growth in each zip code, these results translate to an annual increase of $9 in monthly rent and $1,800 in house prices for the median zip code in our data, which accounts for about one-fifth of actual rent growth and about one-seventh of actual price growth. Finally, we formally test whether the Airbnb effect is due to the reallocation of the housing supply. Consistent with this hypothesis, we find that although the total supply of housing is not affected by the entry of Airbnb, Airbnb listings increase the supply of short-term rental units and decrease the supply of long-term rental units.
Firms use coupons to stimulate demand. Although couponing is popular in practice, limited research has examined the causal effects of coupons on visit, search, and purchase behaviors among heterogeneous customers. In this paper, we examine coupon effects using data from a randomized field experiment with an online retailer in which customers were divided into two heterogeneous customer segments (low value and high value) with two types of coupon discounts (base value and better value). We find couponing is effective in increasing revenue, primarily by attracting customers who purchase without coupon redemption, and the lift in revenue per customer is larger for the high-value segment. Using clickstream data of customer visit and search behavior, we find most of the revenue lift arises from a corresponding lift in the likelihood of visiting the website under couponing. Though the lift in visit likelihood is relatively homogeneous across customer segments under the base coupon, the high-value segment has a higher purchase conversion rate than the low-value segment, leading to an amplified revenue lift. We also find a deeper discount leads to higher redemption and purchase conversion for the high-value segment but does not change visit likelihood. Finally, most of the search behaviors are unchanged under couponing, suggesting the mix of customers brought in under couponing are similar to those who visit without receiving coupons.
Understanding consumer preferences is important for new product management, but is famously challenging in the absence of actual sales data. Stated-preference data are relatively cheap but less reliable, whereas revealed-preference data based on actual choices are reliable but expensive to obtain prior to product launch. We develop a cost-effective solution. We argue that people do not automatically know their preferences, but can make an effort to acquire such knowledge when given sufficient incentives. The method we develop regulates people’s preference-learning incentives using a single parameter, realization probability, meaning the probability with which an individual has to actually purchase the product she says she is willing to buy. We derive a theoretical relationship between realization probability and elicited preferences. This allows us to forecast demand in real purchase settings using inexpensive choice data with small to moderate realization probabilities. Data from a large-scale field experiment support the theory and demonstrate the predictive validity and cost-effectiveness of the proposed method.
In response to growing environmental concerns, governments have promoted products that are less harmful to the environment—green products—through various incentives. We empirically study the impact of a commonly used nonmonetary incentive—namely, the single-occupancy permission to high-occupancy vehicle (HOV) lanes—on green and non‐green product demand in the U.S. automobile industry. The HOV incentive could increase unit sales of green vehicles by enhancing their functional value through time saving. On the other hand, the incentive may prove counterproductive if it reduces the symbolic value (i.e., signaling a proenvironmental image) consumers derive from green vehicles. Assessing the effectiveness of green-product incentives is challenging, given the endogenous nature of governments’ incentive provisions. To identify the effect of the HOV incentive on unit sales of green and non‐green vehicles, we take advantage of HOV-incentive changes in two states, and we employ a multitude of quasi‐experimental methods using a data set at the county–model–month level. Unlike previous studies that only examine the launch of the HOV incentive and find an insignificant association between incentive launch and green-vehicle demand, we concentrate on its termination. We find that the termination of the HOV incentive decreases unit sales of vehicles covered by the incentive by 14.4%. We provide suggestive evidence that this significant negative effect of HOV-incentive termination is due to the elimination of the functional value the incentive provides: time saving. Specifically, we find that the negative effect is more pronounced in counties where consumers value time saving more (i.e., counties with a longer commute to work and higher income). Additionally, in line with prior literature, the launch of the HOV incentive is not found to have a significant effect on green-vehicle sales. Combined, our findings reveal that the effect of termination is not simply the opposite of that of launch, implying that governments’ green-product incentives could backfire.
It is well understood that a downstream firm’s service can impact the performance of vertical channels. Although many academic works address the service provision of the downstream firm, empirically quantifying the impact has been challenging because the downstream firm’s service is often unobservable to the researcher. I propose a new empirical framework that incorporates the downstream firm’s unobserved endogenous service provision by modifying the standard demand model. I apply this empirical framework to proprietary data from a franchise network in the car radiator market to quantify the downstream firms’ (e.g., franchisees’) endogenous service. Counterfactuals under maximum resale price maintenance (RPM) policies show that the standard demand model ignoring the franchisees’ endogenous service reduction (i.e., service externality) results in more optimistic counterfactual predictions than the developed framework does. Such service externality can be mitigated if the service provision cost is lower for franchisees. Last, I examine boundary conditions: under the extreme regime of maximum RPM aiming to fully extract franchisees’ profit, I find that information asymmetry is a greater concern for the upstream firm within the focal industry. Additionally, when service externality is combined with channel information asymmetry, maximum RPM at such extremes may no longer increase the franchisor’s profit.
Firms are increasingly using technology to enable targeted, or “personalized,” pricing strategies. In settings where prices are transparent to all consumers, however, there is the potential for interpersonal price differences to be perceived as inherently unfair. In response, firms may strategically obfuscate their prices so that direct interpersonal comparisons are more difficult. The feasibility of such a pricing strategy is not well understood. In this paper, we investigate the conditions under which it is profitable for firms to engage in price obfuscation, given the potential fairness concerns of consumers. We study how price obfuscation affects consumer fairness concerns, consumer demand, and equilibrium pricing strategies. The findings suggest that if obfuscation mitigates fairness concerns, it can arise as an equilibrium outcome, even if consumers are aware of the seller’s strategic behavior and are able to update their beliefs and expectations about the prices offered to their peers accordingly. To test the theoretical predictions, an experiment is conducted in which price obfuscation is varied both exogenously and endogenously. The results confirm that buyers have intrinsic distributional (based on the seller’s margins) and peer-induced fairness (due to others being charged different prices) concerns when prices are transparent. In particular, disadvantaged peer-induced fairness concerns enter utility as an intrinsic cost that the seller has to compensate for through lower prices. Obfuscation effectively reduces peer-induced fairness concerns and increases sellers’ pricing power. However, this pricing power is constrained by distributive inequity becoming more salient when prices are obfuscated.
Some firms that operate in multiple product markets use the same brand in different markets, whereas others use different brands in different markets. This research investigates in which product markets a firm should use the same or different brands and how this decision depends on the relatedness of product markets. To answer this question, I propose a framework of market relatedness that characterizes the relationships among distinct product markets from the supply side (e.g., shared production technology) and demand side (e.g., correlated customer preferences). This framework is applied to a model of reputation in which a multiproduct firm’s product quality is jointly determined by its hidden capability type (i.e., adverse selection) and hidden choice of effort level (i.e., moral hazard) in each product market. Consumers obtain noisy information about the firm by observing its track record, that is, product quality produced in the past. Umbrella branding allows consumers to pool the firm’s track record across different product markets and form expectations about the product quality based on market relatedness. The analysis shows that umbrella branding is optimal if supply-side relatedness is high and demand-side relatedness is not too high. However, if the product markets are closely related in both dimensions, then independent branding may be optimal because, as an umbrella brand, the firm faces a temptation to exploit positive information spillover across product markets through its shared brand name. By using different brand names, a firm can credibly commit to investing in all product markets and thereby earn higher profits. Finally, this paper provides implications for an umbrella brand’s customer relationship management strategy whether to serve the same or distinct customer segments with its products.
This research analyzes a firm’s investment in advertising that signals quality when consumers learn about quality not only from such advertising but also from interactions with other consumers in the form of observational learning or word of mouth. Further, word-of-mouth interactions may involve underreporting (not everyone shares experiences), positivity (positive experiences are communicated more widely than negative ones), or negativity (negative experiences are communicated more widely than positive ones). The analysis focuses on whether a firm should advertise more or less aggressively in the presence of such consumer interactions compared with their absence and offers four key insights. First, consumer interactions can amplify the signaling effect of advertising, and as a consequence, to prevent mimicking it may be optimal for a high-quality firm to become more aggressive and spend more on advertising to signal quality in the presence of such interactions than without. Second, as underreporting increases, it can be optimal to reduce advertising, sometimes significantly. Third, with increasing positivity, it can be optimal to increase advertising. Fourth, even with increasing negativity, under certain conditions it may still be optimal to increase advertising rather than decrease it.
Mobile in-app advertising is now the dominant form of digital advertising. Although these ads have excellent user-tracking properties, they have raised concerns among privacy advocates. This has resulted in an ongoing debate on the value of different types of targeting information, the incentives of ad networks to engage in behavioral targeting, and the role of regulation. To answer these questions, we propose a unified modeling framework that consists of two components—a machine learning framework for targeting and an analytical auction model for examining market outcomes under counterfactual targeting regimes. We apply our framework to large-scale data from the leading in-app ad network of an Asian country. We find that an efficient targeting policy based on our machine learning framework improves the average click-through rate by 66.80% over the current system. These gains mainly stem from behavioral information compared with contextual information. Theoretical and empirical counterfactuals show that although total surplus grows with more granular targeting, the ad network’s revenues are nonmonotonic; that is, the most efficient targeting does not maximize ad network revenues. Rather, it is maximized when the ad network does not allow advertisers to engage in behavioral targeting. Our results suggest that ad networks may have economic incentives to preserve users’ privacy without external regulation.
This paper empirically investigates how marketers can retarget consumers who have searched online but did not purchase, based on their search behaviors. To infer the relationship between search activities and preferences, we estimate a structural search model that characterizes the consumer search process. We propose an estimator similar to the Geweke-Hajivassiliou-Keane estimator to evaluate the likelihood function. The proposed estimator makes recursive draws from truncated distributions that arise because of the observed search and choice behaviors in an optimal sequential search model. The recovered preferences are used to improve retargeting strategies demonstrated through a series of counterfactuals. Results show a substantial heterogeneity in responses to retargeting among consumers who exhibited different search behaviors. By contrast, the heterogeneity among consumers based on other characteristics (e.g., age, gender) is moderate. We consider two counterfactual marketing strategies: sending out coupons redeemed upon purchasing and sending seller recommendations that reveal the offering of recommended sellers. We find that although both strategies help increase the conversion rate, seller recommendations are more effective than coupons, suggesting the importance of providing consumers with the sellers’ information for retargeting. We also show that a pricing mechanism such as an auction that makes the seller self-select to participate will improve the effectiveness of retargeting. Finally, online retail platforms can benefit both sellers and consumers by providing sellers with the information on consumers’ search behaviors.
We study product adoption in the context of a cryptocurrency market. Cryptocurrencies are subject to network effects and speculative investments, which are not part of standard models of product diffusion. To explore this unique setting, we marry models of stochastic bubbles and the standard model of product diffusion. A rational bubble is raised by speculative investors seeking short-term gains. We find that a bubble accelerates the adoption, which can help explain the fast diffusion of bitcoin. There are reinforcing interactions between the speculative investors and regular users of currency, which can make it easier to form a bubble (compared with a setting without regular users). Our findings suggest how bubbles may help to market products. We also provide conditions under which bubbles may unravel.
We empirically investigate the impact of category captaincy, an arrangement where the retailer works exclusively with a manufacturer to manage both the manufacturer’s and his rivals’ products. Using a unique data set that contains information on category captaincy as well as SKU-store-level sales and price across 24 retail chains and eight local markets in the United States for a frozen food category, we quantify the impact of captaincy on prices, assortments, profits, and consumer welfare. Interestingly, our estimates suggest that captaincy can lead to welfare gains for consumers, which argues against a purely negative view of captaincy by policy makers.
A central challenge in estimating the causal effect of television (TV) advertising on demand is isolating quasirandom variation in advertising. Political advertising, which topped $14 billion in expenditures in 2016, has been proposed as a plausible source of such variation and thus, a candidate for an instrumental variable (IV). We provide a critical evaluation of how and where this instrument is valid and useful across categories. We characterize the conditions under which political cycles theoretically identify the causal effect of TV advertising on demand, highlight threats to the exclusion restriction and monotonicity condition, and suggest a specification to address the most serious concerns. We test the strength of the first-stage category by category for 274 product categories. For most categories, weak-instrument robust inference is recommended, as first-stage F statistics are less than 10 for 221 of 274 product categories in our benchmark specification. The largest first-stage F statistics occur in categories that typically advertise locally, such as automobile dealerships and restaurants. Failure to use the suggested specification leads to results that suggest violations of exclusion and monotonicity in a significant number of categories. Finally, we conduct a case study of the auto industry. Despite a very strong first stage, the IV estimate for this category is imprecise.
Political advertisements on television can affect viewers and may, consequently, influence the effectiveness of subsequent ads. Such ad-to-ad spillover effects—where one ad influences how viewers respond to a subsequent ad—have drawn concerns from nonpolitical advertisers, raising the question: how do political ads on television impact viewers’ response to subsequent ads? We empirically investigate this question using two outcomes of ad response: ad viewership and online conversations about ads. We use data on 849 national political television ads from the 2016 election and leverage a quasi-experimental design to delineate the effect that a political ad has on the subsequent ad. We show that, counterintuitively, political ads spur positive spillover effects. Specifically, ads that follow a political ad, compared with ads that follow a nonpolitical ad, experience an 89% reduction in audience decline and thus air to larger audiences. Additionally, we find evidence that viewers engage in more positive online ad chatter about ads that air after political ads, with these ads experiencing a 3% increase in positive chatter after the ad. Our investigation contributes to research on advertising that has yet to explore ad-to-ad spillover effects and, more broadly, provides insights into how political messages influence consumers.
Access to a platform’s services often requires consumers to use a complementary hardware product or service, for example, internet service is needed to access the YouTube platform. Typically, such access products are provided by third-party firms. More recently, however, some major platforms such as Google have themselves ventured into providing these access products. For example, Google Fiber provides access to YouTube. In this paper, we examine the effect of a platform’s entry into an access product market when the profits from the platform’s advertising business depend upon the quality of the access products. We develop a theoretical model to study this context and find that such an entry by the platform (i) can lead the platform to provide a higher quality access product than the third-party firms at a lower price, (ii) may, in contrast to the entry by a third-party firm, lead to a higher quality access product by both the platform as well as the firms, (iii) improves the platform’s profits because of increased advertising revenue and/or additional profits from the access product sales, and (iv) increases consumer surplus even though the platform becomes more dominant because of its entry into the access product market. All the results are driven by the positive association between the platform’s advertising profits and the quality of access products in the market.
The multibillion-dollar search engine marketing (SEM) industry’s central objective is to gain visibility for businesses on search engine results pages (SERPs) to bring customers to a firm’s website. This paper sheds light on a foundational element of SEM, namely, consumers’ interactions with SERPs. Using eye-tracking equipment and a custom-built Google-like search engine, we conducted a laboratory experiment in which participants performed a series of online searches for a set of consumer goods while their eye movements and interactions with the results page (i.e., scrolling and clicks) were recorded. We provide model-free and model-based analyses of the inspection process, which encompasses a series of microdecisions, including what part of the page to look at, whether to scroll to bring additional results into view, what listings to explore and for how long, and ultimately which listing to click on. The results suggest that search goals (navigational, transactional, and informational), semantic context, spatial characteristics, viewing centrality, and prior inspection path are all predictive of both the flow of the inspection process and the ultimate listing choice. Managers should account for the significant variation in inspection scope across search tasks and SERP compositions when assessing a listing’s performance and deciding on a desirable position on the SERP.
From B2B sales to AI-powered ecommerce, one common pricing mechanism is “list price–discount”: The seller first publishes a (committed) list price, then during interactions with a buyer, offers (noncommitted) discounts off of the list price. Some B2B sellers never sell at their list prices. In such cases, what role does a list price play and how to choose the optimal list price and discounts? In this paper, I study a stochastic sales process in which a buyer and a seller discover their match value sequentially. The seller can adjust its price offers over time, and the buyer decides whether to accept each offer. I show that this discovery process creates a hold-up problem for the buyer that results in inefficient no-trades. The seller alleviates this problem by committing to an upper bound in the form of a list price. But in equilibrium players always reach agreement at a discount. I show that the seller prefers this mechanism to having no list price or committing not to discount. When the cost of selling is high, the seller’s ability to offer discount is necessary for trade to happen. There exists reverse price discrimination when buyers are heterogeneous, and list price can serve as a signaling device when sellers are heterogeneous. Extensions with alternative pricing or matching mechanisms are discussed.
Marketing Science greatly benefited from the admirable and fastidious efforts of more than 300 different individuals who provided manuscript reviews last year. Beyond those individuals already recognized on the editorial board, the editor-in-chief and senior editors of Marketing Science are indebted to the many guest editors, guest associate editors, and ad hoc reviewers who provided expert counsel and guidance on a voluntary basis. The following list acknowledges the contribution of guest editors, guest associate editors, and ad hoc reviewers who served from January 1, 2020, to December 31, 2020. Finally, our sincere appreciation to the authors, whose outstanding submissions and careful revisions make the journal the go-to resource for leading-edge knowledge in quantitative marketing.K. SudhirYale University
The lean start-up method (LSM) advocates an iterative and adaptive product development and testing approach to innovation. It recommends firms to build test products, use them to learn about consumer preferences, and modify (or “pivot”) the product design accordingly. It is less straightforward to understand how effective LSM can be, however, not least because consumers’ responses to the test product are influenced by its quality, price, and design—that is, learning is endogenous to the features of the test product. This paper analyzes the build-test-learn cycle of LSM using an analytical model to understand its microfoundation and how best to implement it. We find that an optimal test product that maximizes learning should aim either to confirm a more likely product design or to rule out a less likely product design as being the most desired by consumers, have a vertical quality that is neither too high nor too low, and have a higher quality when aiming to confirm than to rule out. We also identify the product-market conditions for which the LSM is more effective. Conceptualizing the LSM via a formal model may help to improve its implementation in practice and to advance further academic research.
Consumers may not be perfectly informed of the availability and attributes of competitive offerings (price, match value). We examine equilibrium outcomes when consumers can search among potential options and invest evaluation efforts to resolve product value uncertainty by endogenizing the information structure in the canonical model of sequential search with horizontal differentiation. We show that consumers’ joint decisions on search and evaluation interact in an asymmetric manner. That is, greater evaluation pushes up the search-terminating threshold, whereas a higher stopping threshold first increases and then decreases the incentive for evaluation. As a result, changes in the search/evaluation cost may exert unusual influences on equilibrium behaviors and payoffs. Both the impact of the search cost on optimal evaluation and that of the evaluation cost on expected number of searched products exhibit an inverted U pattern. Because of endogenous evaluation, the equilibrium price and profit can vary nonmonotonically with the search/evaluation cost. This implies that we may observe the coexistence of consumers searching fewer products and firms charging lower prices as sampling becomes more costly. Moreover, consumers can benefit from more costly sampling. We discuss how these findings can shed light on strategic design of shopping environment, user acquisition/retention, and empirical research.
A consumer searching for information on a product may be indicative that the consumer has some interest in that product but is still undecided about whether to purchase it. Some of this consumer search for information is not observable to firms, but some may be observable. Once a firm observes a consumer searching for information on its product, the firm may then want to try to provide further information about the product to that consumer, a phenomenon that has been known in electronic commerce as retargeting. Firms may not observe all activities by a consumer in searching for information, may not be able to observe the information gained by consumers, and may not be able to observe whether a consumer stopped searching for information. A consumer could stop searching either because he received information of poor fit with the product, because he bought the product (which may be unobservable to the firm), or because he exogenously lost interest in the product. This paper presents a dynamic model with these features characterizing the optimal advertising retargeting strategy by the firm. We find that a forward-looking firm can advertise more or less than a myopic firm to gain more information about whether the consumer is searching for information, advertising more if the effect of advertising is relatively high. We characterize how the optimal advertising retargeting strategy is affected by the ability of the firm to observe when the consumer purchases the product, when the firm is better able to observe the consumer search behavior, and by the informativeness of the signal received by the consumer. We find that better tracking of consumer search behavior could be beneficial for consumers, because it may reduce the length of time when a consumer receives retargeting, but that it also enlarges the region of firm’s beliefs where retargeting is optimal. Finally, we also find that the value of retargeting is highest for an intermediate value of the likelihood of the consumer receiving an informative signal and that retargeting may allow the firm to charge higher prices if consumers are forward-looking.
Increasingly, applied researchers study problems for which multiple sources of data are available. These sources may come with varying degrees of aggregation, and some of them may not be representative of the population of interest. Using multiple data sources could lead to richer insights. However, existing data fusion approaches do not correct for selection bias in data sources that may not be representative and either do not scale to large populations or are statistically inefficient. We propose an aggregate-disaggregate data fusion method that corrects for selection bias and is both computationally scalable and statistically efficient. We apply the method to estimate a model of customer acquisition and churn at subscription-based firms. We bring the model to life using a large credit card panel and public data from Spotify, the music streaming service. This application and supporting simulations show that incorporating the granular data through our data fusion method enhances identification and offers richer insights than extant approaches. We find, for example, that previously churned customers remain with Spotify longer than newly adopted subscribers do, implying a more sanguine view of Spotify’s future retention profile than previous approaches that do not use multiple data sources.
This paper uncovers the striking power of a product’s first consumer review. Our analytical model suggests that two key metrics of online consumer reviews, valence and volume, are not independent, but instead evolve interdependently. This interdependence forms a mechanism to transfer a (dis)advantage from a product’s first review to both a long-lasting (dis)advantage in future word-of-mouth (WOM) valence and an increasing (dis)advantage in future WOM volume. As a result, a single consumer review can significantly influence the fate of a given product. These theoretical predictions, although seemingly unlikely, are supported by our empirical investigations. For example, more than 30% of vacuum cleaner models offered by both Amazon.com and BestBuy.com receive first reviews of opposite valence on the two platforms. Those with a negative first review subsequently suffer a loss in both valence and volume vis-à-vis their counterparts with a positive first review, even after 36 months. More strikingly, the first-review effect on WOM volume increases over time. Our findings reveal a crucial weakness in the user-generated information mechanism. As a consumption-based information source, it creates an information-availability bias such that when a product receives a negative first review, it not only suffers low initial sales, but also loses the opportunity to correct the possible negative bias via subsequent reviews. These findings have substantial implications for online sellers, e-commerce platform providers, and consumers.
We study the impact of launching a non-tiered customer loyalty program on consumers’ spending per visit, frequency of visits and attrition rates, and overall customer value. We demonstrate these results both through descriptive difference-in-difference regressions and a duration-dependent hidden Markov model we develop. We find that the program increases customer value by almost 30% over a five-year horizon, which is considerably larger than has been previously found for non-tiered loyalty programs. Most of the impact of the loyalty program comes through attrition: we show that the program’s reduction in attrition accounts for more than 80% of the program’s total lift, whereas increased frequency accounts for less than 20% of the program’s lift. The program’s lift is highest for least and most frequent automatic members, who experience reductions in attrition rates after joining the program. The impact of the loyalty program on spending per visit is negligible.
Some media platforms earn their profits from both consumers and advertisers (e.g., Spotify, Hulu), whereas others earn their profits from either advertisers only (e.g., Jango, Tubi) or consumers only (e.g., Tidal, Netflix). Thus, media platforms adopt divergent strategies depending on how they allocate the limited space or bandwidth between content and advertising. In this paper, we examine media platforms’ content provision strategies and their implications for the profits of media platforms as well as content suppliers, taking into account the cross-side effects of a multisided media market and the nature of competition in the content supplier market. To facilitate the analysis, we propose a model where media platforms interact with three sides: content suppliers, consumers, and advertisers. First, our analysis of a perfectly competitive content market shows that though consumers’ desire for content raises the willingness to pay, it can hurt platforms’ profits. Second, counter to our intuition, platforms’ profits can increase with the cost of procuring content. Third, advertisers’ desire for consumers reduces a monopoly content supplier’s profits under a paid-content-with-ads strategy. Fourth, a monopoly content supplier cannot extract all the profits from competing platforms. Furthermore, competing content suppliers may even charge higher prices than a monopoly content supplier. Finally, we highlight how the nature of competition in the content market shapes platforms’ choice of a no-ad strategy.
The rise of over-the-top (OTT) video streaming services has raised the question of how this new form of digital media affects consumer search for pirated content. We address this question by using Netflix’s unexpected announcement of a global market expansion in January 2016 and the subsequent block by the primary telecommunications firm in Indonesia as an exogenous shock to the supply of OTT services in that country. Using synthetic control methods, we compare the change in piracy search between Indonesia and 40 Asian countries where Netflix simultaneously entered and remained available. Netflix’s failure to launch in Indonesia leads to a 19.7% increase in search for pirated movies and TV shows in Indonesia, relative to the other countries, suggesting a net substitution of piracy for OTT services. Comparison of treatment effects between exclusive and nonexclusive content shows that the treatment effect is driven by both a combination of an expansion of the market for piracy and a substitution between piracy and OTT services. We also find that the treatment effect is stronger for less dialogue-oriented content, which is consistent with the greater appeal of dialogue-light content to non–English-speaking consumers.
One goal of promotions for frequently purchased products is increasing short-term sales. Increases could be at competitors’ expense, coming from consumers with relatively weak brand preferences. However, increased sales from brand-loyal consumers could well cannibalize future sales of the promoted brand. An unintended consequence of promotions is that loyal consumers otherwise willing to pay high prices may strategically stockpile at low prices. What is its impact on firms’ profits? Who benefits from stockpiling? How should firms adapt their pricing to accommodate consumer stockpiling? For answers, we analyze an infinite horizon dynamic model of competition and derive the Markov perfect equilibrium pricing strategies that yield several managerial insights.We find strategic stockpiling does not reduce firms’ long-run profits when managers adopt pricing strategies we identify. Turning to strategies, stockpiling causes firms to move away from frequently promoting below the stockpiling threshold, but it leads to mass points at reservation price and stockpiling threshold. Stockpiling is beneficial to consumers who stockpile but hurts those that do not stockpile, whereas switchers remain largely unaffected. State-dependent pricing as a result of stockpiling leads to positive intertemporal price correlation, implying, counterintuitively, that in equilibrium, deep promotions should be followed by deep promotions.
This paper studies reserve prices computed to maximize the expected profit of the seller based on historical observations of the top two bids from online auctions in an asymmetric, correlated private values environment. This direct approach to computing reserve prices circumvents the need to fully recover distributions of bidder valuations. We specify precise conditions under which this approach is valid and derive asymptotic properties of the estimators. We demonstrate in Monte Carlo simulations that directly estimating reserve prices is faster and, outside of independent private values settings, more accurate than fully estimating the distribution of valuations. We apply the approach to e-commerce auction data for used smartphones from eBay, where we examine empirically the benefit of the optimal reserve and the size of the data set required in practice to achieve that benefit. This simple approach to estimating reserves may be particularly useful for auction design in Big Data settings, where traditional empirical auctions methods may be costly to implement, whereas the approach we discuss is immediately scalable.
Online vendors often employ drip-pricing strategies, where mandatory fees are displayed at a later stage in the purchase process than base prices. We analyze a large-scale field experiment on StubHub.com and show that disclosing fees upfront reduces both the quantity and quality of purchases. The effect of salience on quality accounts for at least 28% of the overall revenue decline. Detailed click-stream data show that price shrouding makes price comparisons difficult and results in consumers spending more than they would otherwise. We also find that sellers respond to increased price obfuscation by listing higher-quality tickets.
Although typically overlooked, many purchase datasets exhibit a high incidence of products with zero sales. We propose a new estimator for the Random-Coefficients Logit demand system for purchase datasets with zero-valued market shares. The identification of the demand parameters is based on a pairwise-differencing approach that constructs moment conditions based on differences in demand between pairs of products. The corresponding estimator corrects nonparametrically for the potential selection of the incidence of zeros on unobserved aspects of demand. The estimator also corrects for the potential endogeneity of marketing variables both in demand and in the selection propensities. Monte Carlo simulations show that our proposed estimator provides reliable small-sample inference both with and without selection-on-unobservables. In an empirical case study, the proposed estimator not only generates different demand estimates than approaches that ignore selection in the incidence of zero shares, it also generates better out-of-sample fit of observed retail contribution margins.
The General Data Protection Regulation (GDPR) was enacted in the European Union in April 2016 and went into effect in May 2018. We study its impact on investment in new and emerging technology firms. Our findings indicate negative post-GDPR effects after its 2018 rollout on European ventures relative to their counterparts in the United States and the rest of the world, and considerably lesser effects after its 2016 enactment and before implementation. The negative effects manifest in the number of and amounts raised in financing deals, and are particularly pronounced for newer, data-related, and business-to-consumer ventures.
We empirically investigate how writers’ output is affected by copyright piracy using data from a Chinese digital publishing platform. We identify two measurements of writers’ output—creative productivity and customer care—which are also affected by readers’ feedback through purchasing, tipping, and commenting. We take advantage of an exogenous event—the termination of a free personal storage service and search function by a leading Chinese cloud storage provider in June 2016—to causally identify the effects of the resulting reduced copyright piracy on writers’ efforts. Using a difference-in-differences modeling approach, we compare the changes in average writer behavior before and after the event across two groups of writers: (1) writers who have profit-sharing contracts with the platform and (2) those who do not. We find that after the termination, contracted writers increased their creative productivity efforts in terms of quantity without sacrificing quality but reduced their customer care efforts. However, these effects are absent for noncontracted writers. Our study is among the first to provide empirical support for the positive effect of digital intellectual property rights infringement reduction on creative productivity.
Online product reviews constitute a powerful source of information for consumers. Past research has studied the effect of aggregate measures of reviews (such as average product rating and number of reviews) on consumer behavior. In this study, we investigate how individual reviews displayed on a product web page affect consumers’ purchase likelihood. Identifying this effect is challenging because retailers are free to select which reviews to display on the product page and in what order, making the display of reviews in particular positions potentially endogenous. We address this challenge by utilizing an empirical context in which the retailer displays reviews by recency and exploit the variation in review positions generated as newer reviews are added on top of older ones. We find that individual reviews have a strong effect on consumer purchase decisions even after accounting for a product’s average rating. These effects are particularly pronounced when individual reviews help consumers resolve uncertainty about the product or contrast with the aggregate information that is instantly available on the product page.
Firms often display product information on their front-of-package labels with some firms going as far as to make deceptive claims. We study the impact of the “Made in USA” claim—a disclosure not legally required on consumer-packaged goods and yet a claim highlighted by many firms, sometimes deceptively—on consumer demand. Leveraging the Federal Trade Commission’s investigation of four brands that resulted in removal of the claim from product packages, we study the impact such removal had on sales. We find a decline in demand following the removal of the “Made in USA” claim. Second, to ensure complete exogenous variation, we conduct a field experiment on eBay, on which we run more than 900 auctions, varying only whether a product contains this country-of-origin information. We find that, although products with the “Made in USA” claim have a slightly higher chance of drawing a zero valuation, such products obtain a 44% higher willingness-to-pay conditional on a positive valuation. However, this increased valuation is insufficient to economically justify firm relocation efforts. Auction transaction prices, on the other hand, are significantly and 28% higher with the claim, suggesting resellers and auctioneers have incentives to display the claim. The experiments alongside observational data allow us to rationalize firms’ incentives in making deceptive country-of-origin claims.
Manufacturers routinely rely on retailers to reach potential customers. Concurrently, they often offer low-price guarantees (LPGs) to customers who purchase through their direct channel; that is, should consumers find a lower price from distribution partners, manufacturers promise to match or even beat the lower price. Many manufacturers, such as Apple, Dell, Hewlett-Packard, Lenovo, and Goodyear, use price-matching guarantees (PMGs) against retailers. In the online travel industry, price-beating guarantees (PBGs) are prevalent among travel suppliers. In this paper, we develop a game-theoretic model to investigate the manufacturer’s optimal choice of LPG policies and its implications for the manufacturer, retailer, and channel. Our analysis demonstrates that no LPG, PMG, and PBG can each emerge in equilibrium depending on consumer characteristics. Although LPGs can improve channel profit, they may benefit the manufacturer at the expense of the retailer. As such, LPGs can intensify vertical channel conflict. However, both horizontal channel conflict and vertical channel conflict are present in dual channels. LPGs are not merely price discrimination devices; they mitigate horizontal channel conflict. The benefit of LPGs in reducing horizontal channel conflict outweighs the loss from intensified vertical channel conflict under a wide range of conditions. Therefore, LPGs serve as channel coordination devices.
Customer base analysis of noncontractual businesses builds on modeling purchases and latent attrition. With the Pareto/NBD model, this has become a straightforward exercise. However, this simplicity comes at a price. Customer-level predictions often lack precision. This issue can be addressed by acknowledging the importance of contextual factors for customer behavior. Considering contextual factors might contribute in two ways: (1) by increasing predictive accuracy and (2) by identifying the impact of these determinants on the purchase and attrition process. However, there is no generalization of the Pareto/NBD model that incorporates time-varying contextual factors. Preserving a closed-form maximum likelihood solution, this study proposes an extension that facilitates modeling time-invariant and time-varying contextual factors in continuous noncontractual settings. These contextual factors can influence the purchase process, the attrition process, or both. The authors further illustrate how to control for endogenous contextual factors. Benchmarking with three data sets from the retailing industry shows that explicitly modeling time-varying contextual factors significantly improves the accuracy of out-of-sample predictions for future purchases and latent attrition.
We study the effect of Airbnb’s smart-pricing algorithm on the racial disparity in the daily revenue earned by Airbnb hosts. Our empirical strategy exploits Airbnb’s introduction of the algorithm and its voluntary adoption by hosts as a quasinatural experiment. Among those who adopted the algorithm, the average nightly rate decreased by 5.7%, but average daily revenue increased by 8.6%. Before Airbnb introduced the algorithm, White hosts earned $12.16 more in daily revenue than Black hosts, controlling for observed characteristics of the hosts, properties, and locations. Conditional on its adoption, the revenue gap between White and Black hosts decreased by 71.3%. However, Black hosts were significantly less likely than White hosts to adopt the algorithm, so at the population level, the revenue gap increased after the introduction of the algorithm. We show that the algorithm’s price recommendations are not affected by the host’s race—but we argue that the algorithm’s race blindness may lead to pricing that is suboptimal and more so for Black hosts than for White hosts. We also show that the algorithm’s effectiveness at mitigating the Airbnb revenue gap is limited by the low rate of algorithm adoption among Black hosts. We offer recommendations with which policy makers and Airbnb may advance smart-pricing algorithms in mitigating racial economic disparities.
We examine how the introduction of digital cinema technologies in the South Korean movie industry created flexibility for theaters in movie showings. Using detailed data on theaters’ digital adoption and daily assortment decisions between 2006 and 2016, we show that, on average, digitization is associated with both increased variety of movies and increased showings of the most popular movies. But, delivering these benefits to consumers took at least four years to materialize and varied with the number of screens in a theater. During the early years of theater digitization, product variety declined in larger theaters. Yet, when digital movies became widely available, product variety increased. Once digital movies were broadly available, we show that theaters created increased product variety during less popular time slots and offered more showings of consumers’ favorite movies during peak demand on weekend evenings. Overall, we show that digitization of movies and projection technology creates flexibility in scheduling, which seems to allow theaters to better respond to consumer demand.
In modern retail contexts, retailers sell products from vast product assortments to a large and heterogeneous customer base. Understanding purchase behavior in such a context is very important. Standard models cannot be used because of the high dimensionality of the data. We propose a new model that creates an efficient dimension reduction through the idea of purchase motivations. We only require customer-level purchase history data, which is ubiquitous in modern retailing. The model handles large-scale data and even works in settings with shopping trips consisting of few purchases. Essential features of our model are that it accounts for the product, customer, and time dimensions present in purchase history data; relates the relevance of motivations to customer- and shopping-trip characteristics; captures interdependencies between motivations; and achieves superior predictive performance. Estimation results from this comprehensive model provide deep insights into purchase behavior. Such insights can be used by managers to create more intuitive, better informed, and more effective marketing actions. As scalability of the model is essential for practical applicability, we develop a fast, custom-made inference algorithm based on variational inference. We illustrate the model using purchase history data from a Fortune 500 retailer involving more than 4,000 unique products.
We study the estimation of preference heterogeneity in markets in which consumers engage in costly search to learn product characteristics. Costly search amplifies the way consumer preferences translate into purchase probabilities, generating a seemingly large degree of preference heterogeneity. We develop a search model that allows for flexible preference heterogeneity and estimate its parameters using a unique panel data set on consumers’ search and purchase behavior. The results reveal that when search costs are ignored, the model overestimates standard deviations of product intercepts by 53%. We show that the bias in heterogeneity estimates leads to incorrect inference about price elasticities and seller markups and has important consequences for personalized pricing.
The mere fact that consumers are targeted by advertisements can affect their inference about the expected utility of a product. We build a micromodel where multiple firms compete through targeted advertising. Consumers make inferences from targeted advertising about their potential match values for the product category, as well as the advertising firm’s unobserved quality. We show that in equilibrium, upon being targeted by a firm, consumers make more positive inferences about the product category and the firm’s quality. With such improved beliefs, a targeted consumer is more likely to engage in a costly search throughout the category. We find that the increase in consumer search creates an advertising spillover beyond the level of the mere awareness effects of advertising and that firms’ equilibrium level of targeted advertising can be nonmonotonic in targeting accuracy. Additionally, we show that sometimes it can be optimal for firms to relinquish customer data and instead engage in nontargeted advertising. The results provide insights into the tradeoffs between advertising reach and targeting accuracy.
We measure the effectiveness of competitive advertising on brand keywords in sponsored search (“brand search”) using a large-scale, quasiexperimental ad allocation on Bing. Competitors are able to steal traffic from the focal brand, and they steal an order of magnitude more clicks if the focal brand’s link is exogenously removed from the top paid position (6%–15% instead of 1%–2% of traffic). The traffic stealing is primarily done by a competitor in the top paid link (6%–9% of traffic) who benefits from the presence of other competitors below. However, the probability of an immediate conversion on these “stolen” clicks is low, with around 20%–47% of consumers returning to Bing in less than 30 seconds after the click (“quick back”) compared with 7% for consumers clicking on the focal brand’s link. More relevant competitors get more clicks with a lower quick-back probability. We discuss the managerial implications of our estimates and compute the quality-adjusted cost of competitors’ “offense” and focal brands’ “defense.”
In 1956, a group of trade associations representing publishers and independent advertising agencies signed a consent decree aimed at ending a set of trade practices that for half a century effectively precluded advertisers from owning and operating in-house agencies. Since then, large firms have internalized more and more of the services formerly performed by external agencies, perhaps as many as half. We use this phenomenon to test a theory of the firm, thereby simultaneously offering an explanation for it. The theory suggests that firms should internalize activities for which their competitive position implies (1) that it is more important for human capital to be firm specific as opposed to function specific and (2) that frequent modifications are desirable. It also predicts (3) that these two effects reinforce each other. This is the first paper to report on a test of the specialization hypothesis, and we find that it is robustly significant in a cross-sectional data set covering nine different agency activities in 79 firms. In addition to the cross-sectional test, we informally present some time-series data suggesting that both specialization and frequency have grown over time along with the level of internalization.
As live streaming of events gains traction, pay what you want (PWYW) pricing strategies are emerging as critical monetization tools. We assess the viability of PWYW by examining the relationship between popularity (i.e., audience size) of a live streaming event and the revenue it generates under a PWYW scheme. On the one hand, increasing audience size may enhance voluntary payment/tips if social image concerns are important because larger audiences amplify the utility pertaining to social image. On the other hand, increasing audience size may reduce tips if gaining the broadcaster’s reciprocal acts motivates tipping because larger audiences are associated with fiercer competition for reciprocity. To examine these trade-offs in the relationship between audience size and revenue under PWYW, we manipulate audience size by exogenously adding synthetic viewers in live streaming shows on a platform in China. The results reveal a mostly positive relationship between audience size and average tip per viewer, which suggests that social image concerns dominate seeking reciprocity. In support of herding, adding synthetic viewers also increases the number of real viewers. Social image concerns and herding together explain the finding that adding one additional viewer improves the tipping revenue per minute by approximately 0.01 yuan (1% of the mean level). Further, famous female broadcasters who use recognition-related words frequently during the event benefit the most from an increase in audience size. Overall, the results indicate that revenues under PWYW do not scale linearly and support the relevance of social image concerns in driving individual payment decisions under PWYW.
We study the impact of a mandated increase in minimum wages on consumer perceptions of multiple dimensions of service quality in the restaurant industry. When faced with higher minimum wages, firms might reduce the number of employees, resulting in poorer consumer service. Alternatively, higher-paid workers might be more motivated to improve consumer service. Using a combination of human annotation and several transformer models, we estimate the incidence of discussion of several service quality attributes (and their valence) in a textual data set of 97,242 online reviews of 1,752 restaurants posted over two years. We exploit a natural experiment in the County of Santa Clara, California, wherein only the city of San Jose legislated a 25% minimum wage increase in 2013. By comparing restaurant reviews in San Jose with those of synthetic controls, we find an improvement in the perceived service quality of San Jose restaurants. Specifically, we find reduced negative discussion of the courtesy and friendliness of workers. This decrease is present in independent restaurants and not in chains. This finding appears to be consistent with agency theory–based predictions of greater incentives to improve service in independent restaurants. We discuss alternative mechanisms for our results. We also discuss implications for consumers, restaurants, and policy makers.
Reputation systems are used by nearly every digital marketplace, but designs vary and the effects of these designs are not well understood. We use a large-scale experiment on Airbnb to study the causal effects of one particular design choice—the timing with which feedback by one user about another is revealed on the platform. Feedback was hidden until both parties submitted a review in the treatment group and was revealed immediately after submission in the control group. The treatment stimulated more reviewing in total. This is due to users’ curiosity about what their counterparty wrote and/or the desire to have feedback visible to other users. We also show that the treatment reduced retaliation and reciprocation in feedback and led to lower ratings as a result. The effects of the policy on feedback did not translate into reduced adverse selection on the platform.
We investigate the relationship between both advertising content and quantity and several stages of consumers’ decision making, namely, unaided and aided awareness, consideration, and purchase. Understanding how the amount and content of advertisements affect consumers’ decision making is crucial for companies to effectively and efficiently use their advertising budgets. Spanning a time period from 2010 to 2016, we combine a unique data set on TV advertising content and quantities with individual-level data containing information on purchases, consideration and awareness sets, demographic variables, and perceived prices. Our results reveal that advertising quantity significantly increases consumer (unaided and aided) awareness but has no effect on conditional consideration and conditional purchase. However, when investigating the relationship between different types of advertising content and purchase stages, we find a more nuanced set of results: advertising only containing noninformational content increases unaided awareness, whereas advertising only containing informational content increases aided awareness. Advertising with both informational and noninformational content affects shoppers’ but not nonshoppers’ awareness and the awareness of other groups of involved consumers.
In recent years, there has been significant interest in understanding users’ online content consumption patterns. But the unstructured, high-dimensional, and dynamic nature of such data makes extracting valuable insights challenging. Here we propose a model that combines the simplicity of matrix factorization with the flexibility of neural networks to efficiently extract nonlinear patterns from massive text data collections relevant to consumers’ online consumption patterns. Our model decomposes a user’s content consumption journey into nonlinear user and content factors that are used to model their dynamic interests. This natural decomposition allows us to summarize each user’s content consumption journey with a dynamic probabilistic weighting over a set of underlying content attributes. The model is fast to estimate, easy to interpret, and can harness external data sources as an empirical prior. These advantages make our method well suited to the challenges posed by modern data sets used by digital marketers. We use our model to understand the dynamic news consumption interests of Boston Globe readers over five years. Thorough qualitative studies, including a crowdsourced evaluation, highlight our model’s ability to accurately identify nuanced and coherent consumption patterns. These results are supported by our model’s superior and robust predictive performance over several competitive baseline methods.
Bargaining is an important pricing mechanism, prevalent in both online and offline markets. However, there is little empirical work documenting the costs and benefits of bargaining, primarily because of the lack of real-world bargaining data. We leverage rich, transaction-level bargaining data from a major online platform and supplement it with primary data to quantify the costs and benefits of bargaining for sellers, buyers, and the platform. We do this by building a structural model of buyer demand and seller pricing decisions while allowing for the existence of bargaining initiation cost, loss-of-face cost, and price discrimination. Using our results, we perform three policy simulations to quantify the importance of not distinguishing between no-bargain and failed-bargain transactions, ignoring the loss-of-face cost, and not allowing for bargaining. These simulations provide rich details on how the various costs of bargaining impact our understanding of buyer and seller behavior and transaction outcomes. Banning bargaining, in particular, benefits the buyer and the platform greatly but only has a modest benefit for sellers. Finally, we show that our results are robust to our assumptions and replicate in another product category.
At the “fuzzy front end” of an innovation process, organizations typically consider dozens, or even hundreds, of raw ideas. Selecting the best ones is a double challenge: evaluating so many ideas is a large undertaking, and the ideas in their raw form permit only noisy evaluations. In this paper, we demonstrate a further challenge to that large-scale evaluation of raw ideas. We show that verbosity raises the evaluation of ideas, that is, ideas expressed in more words are rated higher. This relationship is especially pronounced for ratings of creativity. Theory tells us that the effect of length on creativity is compounded because length cues both components of creativity—novelty and usefulness. We demonstrate how effort in reading (disfluency) and perceptions of complexity work together to explain the relationship between length and creativity. Our findings provide simple but important new directives for improving the use of crowdsourcing in the practice and study of innovation: either standardize the length of the ideas or control for length in their evaluation. Overall, we urge care with using measures of novelty or creativity when the idea descriptions vary in length.
This research aims to demonstrate that the abundant marketing data that companies are using to explore new business opportunities can be an equally fertile source for uncovering an undesirable social attitude or behavior that may be relevant to firms’ business. Companies may benefit from this knowledge when developing innovative new programs that aim to benefit society, such as corporate social responsibility initiatives. In this study, we examine boy-girl gender discrimination in China as manifested in parents’ purchase decisions on behalf of their children across different markets. Our study in itself is significant, because it is the first large-scale empirical work to clearly verify the phenomenon of boy-girl discrimination, taking advantage of e-commerce marketing data. Specifically, we compare the clothing expenditures on boys versus girls using a rich, household-specific data set obtained from two online retailers. We find that the patterns of gender inequality vary systematically across different geographic markets, as the relative expenditure difference on boys versus on girls is bigger in less developed areas as compared with metropolitan areas, and this relative expenditure difference is closely tied with socioeconomic conditions, education levels, and birth rates of a district. Managerial and social implications are discussed.
Despite increasing use of social media, little is known about user competition and its effect on social platforms. In this research, we propose a model where social media users supply content in return for user attention. Using Twitter data on soccer players from the National Women’s Soccer League, we estimate a demand model where users decide how to allocate their attention among players, based on their content posted on social media and their performance on the soccer field. We consider the amount of tweets mentioning a player’s account as a measure for the level of attention captured by the player. On the supply side, players decide the amount of social media content posted on the platform. We show that the attention substitution between players depends on their posting activity and soccer performance but also on personal characteristics, such as physical attractiveness and team affiliation. Our analysis suggests that the competitive pressure to capture user attention is responsible for about one out of three tweets posted by players. This additional content benefits the social network, increasing by 7% the users’ activity on the platform. We also quantify the effect on user activity of a revenue-sharing model in which the platform rewards players for posting tweets.
We measure the cross-category spillover effects of a retailer changing its assortment at the extensive margin (by dropping an entire category from its portfolio) on the outcomes for its rivals in the industry. By leveraging the quasi-experimental nature of the exit by a national pharmacy chain, hereafter referred to as the exiting chain (EC), from the tobacco category, we measure the effects of this event on the revenue generated by nontobacco products at rival pharmacy chains. We show, using Nielsen store-level aggregate (Retail Measurement Services) data, that for each 1% increase of cigarette sales in non-EC stores that are located near an “exiting” EC store relative to those non-EC stores that are not, the revenue generated by nontobacco products grows by 0.04%. Next, using Nielsen household panel data (Homescan), we try to uncover the mechanism underlying this spillover by looking at how the behavior of smokers and nonsmokers at EC stores is affected by the decision. We find that the frequency of trips to the exiting stores that included some tobacco product was negatively affected, suggesting that tobacco was one of the main drivers of store patronage for those trips. For nonsmokers, we find that they react to EC’s action by increasing the frequency of trips to the exiting chain. However, the gains from nonsmokers do not seem to outweigh losses caused by smokers. To assess the generalizability of our results, we analyze the impact of a set of tobacco bans imposed by municipalities in Massachusetts and find cross-category loss patterns for drugstores in those areas that are similar to the gains to rival stores from EC’s exit.
We study the effect of management responses on the reviewing behavior of self-identified female and male reviewers. Using data from Tripadvisor, we show that after hotels begin to respond to reviews, the probability that a negative review comes from a self-identified female reviewer decreases. To explain these findings, we use a survey to show that female reviewers, when writing a negative review, are more likely to perceive management responses as a source of conflict. To understand whether these concerns are well founded, we use Tripadvisor data to provide evidence of gender bias in the way hotel managers address reviewers writing negative reviews. We show that responses to self-identified female reviewers are more likely to be contentious, that is, confrontational, aggressive, or trying to discredit the reviewer. Finally, to confirm that gender bias directly affects reviewing behavior, we show that the probability that a negative review comes from a self-identified female reviewer is lower for hotels that write more contentious responses. Although the introduction of management responses created a new channel of communication between firms and consumers, our findings show that such a channel can be misused to discriminate and can lead to unexpected consequences such as a reduction of reviews by those users more likely to be discriminated against.
Marketing Science authors are facing challenging times, in which we have no monopoly over any topic. Yet we must produce original and relevant research in a timely manner. We are also living through an exciting era in which many constituencies in other academic fields, industry, and public policy are interested in our research and we have the opportunity to have significant impact. The continued success of Marketing Science over the next few years will be the result of the creativity and rigor of authors and the diligence and insight of reviewers, associate editors, and senior editors. The result will be a secured place both among the top business journals and the top social science journals. I plan to do my part by helping to reduce the “time to market” of our research (time from initial submission to acceptance) without compromising on quality and by continuing to invest in Frontiers in Marketing Science.
Digital advertising is on track to become the dominant form of advertising, but ad-blocking technologies have recently emerged, posing a potential threat to the online advertising ecosystem. A significant and increasing fraction of internet users has indeed already started employing ad-blockers. However, surprisingly little is known yet about the effects of ad-blockers on consumers. This paper investigates the impact of ad-blockers on online search and purchasing behaviors by empirically analyzing a consumer-level panel data set. Interestingly, the analyses reveal that ad-blockers have a significant effect on online purchasing behavior: online consumer spending decreases due to ad-blockers by approximately $14.2 billion per year in total. In examining the underlying mechanism of the ad-blocker effects, I find that ad-blockers significantly decrease spending for brands that consumers have not experienced before, partially shifting spending toward brands that they have experienced in the past. I also find that ad-blockers spur additional unintended consequences, as they reduce consumers’ search activities across information channels. The findings remain robust to different identifying assumptions and robustness checks. The analyses draw timely managerial and policy implications for the digital advertising industry, as well as additional insights into the role of online advertising.
Many have speculated that the recent outbreak of COVID-19 has led to a surge in the use of online streaming services. However, this assumption has not been closely examined for music streaming services, the consumption patterns of which can be different from video streaming services. To provide insights into this question, we analyze Spotify’s streaming data for the weekly top 200 songs for two years in 60 countries between June 2018 and May 2020, along with varying lockdown policies and detailed daily mobility information from Google. Empirical evidence shows that the COVID-19 outbreak significantly reduced music streaming consumption in many countries. We also find that countries with larger mobility decreases saw more notable downturns in streaming during the pandemic. Further, we reveal that the mobility effect was attributable to the complementarity of music consumption to other activities and likely to be transient rather than irreversible. Alternative mechanisms, such as unobservable Spotify-specific factors, a demand shift from top-selling songs to niche music, and supply-side effects, did not explain the decline in music consumption.
This paper explores the impact of information avoidance in the context of consumer finance. Specifically, under what circumstances do individuals avoid information about their credit, and how does avoiding this information affect their future credit scores? Using data from a consumer finance platform, we find that a decline in credit score decreases the likelihood that an individual views her credit report in the future. We then measure the impact of receiving information on future credit scores, especially for those likely to avoid information. To obtain a causal local average treatment effect, we use variation in whether an individual views her credit report induced by email campaign A/B tests on a subsample of users who do not opt out of email communication. We find heterogeneous effects of information on credit scores. For individuals who were more likely to avoid information (users whose credit scores were decreasing), viewing their credit reports further decreases credit scores, whereas information increases credit scores for individuals less likely to avoid information. This finding suggests that encouraging individuals to access information when they are more likely to avoid information may worsen their financial health. We discuss the implications for firms’ targeting strategies in retention efforts.
We study the dynamic information design problem of a firm seeking to influence consumer checking behavior by designing push notifications. Firm payoffs are increasing in the frequency of consumer checking. The consumer is uncertain about the arrival of information as well as its valuation. In addition to direct consumption utility, the consumer also has preferences over realized uncertainty: she experiences disutility (anxiety) from the variance of the unchecked information stock. We show that push notifications can lead to more frequent checking compared with no-push, even though it reduces the information variance. Whereas push notifications resolve the information arrival uncertainty, they also create an endogenous impulse to check the information immediately. They can allow the firm to create a more efficient spread in the consumer’s beliefs/anxiety between zero or a level enough to induce checking. We generalize push strategies in two directions: a noisy push strategy that allows the firm to add phantom notifications and a partial push strategy in which the firm can mute information arrivals. Despite consumers having rational expectations, we establish conditions under which both these strategies increase checking. We also extend the model to account for consumer self-control as well as the possibility of endogenous prices.
We examine the effect of user’s popularity information on their demand in a mobile dating platform. Knowing that a potential partner is popular can increase their appeal. However, popular people may be less likely to reciprocate. Hence, users may strategically shade down or lower their revealed preferences for popular people to avoid rejection. In our setting, users play a game where they rank-order members of the opposite sex and are then matched based on a stable matching algorithm. Users can message and chat with their matches after the game. We quantify the causal effect of a user’s popularity (star rating) on the rankings received during the game and the likelihood of receiving messages after the game. To overcome the endogeneity between a user’s star rating and her unobserved attractiveness, we employ nonlinear fixed-effects models. We find that popular users receive worse rankings during the game, but receive more messages after the game. We link the heterogeneity across outcomes to the perceived severity of rejection concerns and provide support for the strategic shading hypothesis. We find that popularity information can lead to strategic behavior even in centralized matching markets if users have postmatch rejection concerns.
What are the implications of adopting a proprietary standard in a durable goods market? This paper shows that proprietary lens mounts tie digital cameras and lenses to the same brand, creating high consumer brand-switching costs and hurting firm profits and consumer surplus. I estimate a structural model of dynamic consumer demand and forward-looking firms setting committed long-run equilibrium prices, using detailed individual-level product-adoption data. I find that lens investments create consumer switching costs on the same order of magnitude as a new camera’s price. These switching costs result in significant friction in consumer brand choice, give rise to market power in the secondary (lens) market, but lead to intense competition in the primary (camera) market. Introducing compatibility—a universal standard across firms—will benefit consumers and firms: firm profit increases by 13%–49%, favoring the smaller firm, and average consumer surplus increases by 12%.
E-commerce platforms, such as Amazon and Alibaba, enable hundreds of millions of consumers to search and purchase products offered by millions of independent sellers who can advertise their products on the platforms. In this paper, we study how to design such a marketplace when there are two types of asymmetric information—sellers have some private information about their products that the platform does not have, whereas the platform has private information about consumers that the sellers do not have. Using a game theory model, we formulate the marketplace design problem as maximizing marketplace profit (comprising ad revenues and sales commissions) by jointly considering the following elements: ensuring sellers join the platform, specifying the auction that enables sellers to advertise in the sponsored list, leveraging the information revealed in the ad auction to refine the organic list, and setting the commission rate on sales to consumers who are rational and heterogeneous in their preferences. We show that sellers’ bids in ad auctions, through which sponsored slots are allocated, can reveal the sellers’ private information to the platform (“information effect”), which it can optimally combine with information that it has about consumers to improve the placement of organic results, a practice we call “strategic listing.” However, by introducing an externality between the sponsored and organic sides, strategic listing also leads to more competition in the ad auction (“competition effect”), thus reducing the incentive of sellers to join the platform. The platform can incentivize sellers to join by reducing the commission rate on sales; however, under certain conditions, the platform must also reduce the degree of strategic listing (i.e., commit to limiting the influence of the sponsored ad auction outcomes on the placement of organic results). If the sellers’ participation is sufficiently difficult to induce, the platform obtains a larger proportion (under some conditions, all) of its revenue from advertising than from sales commissions. Our results shed light on the variation in practices across different platforms and provide timely guidance for platforms to refine their marketplace design.
Managers frequently explore new strategies, and exploit familiar ones, when making decisions on new product development, pricing, or advertising. Exploring for too long, or exploiting too soon, will generate inferior financial returns. Our research describes decision makers’ exploration/exploitation trade-offs and their link to psychometric traits. We conduct an incentive-aligned study in which subjects play a multiarmed bandit experiment and evaluate how subjects balance exploration and exploitation, linked to psychometric traits. To formally describe exploration/exploitation trade-offs, we develop a behavioral model that captures latent dynamics in learning behavior. Subjects transition between three unobserved states—exploration, exploitation, and inertia—updating their beliefs about expected payoffs. Our analysis suggests that decision makers overexplore low-performing options, forgoing over 30% of potential revenue. They heavily rely on recent experiences. Risk-averse decision makers spend more time exploring. Maximizers are more sensitive to payoffs than satisficers. Our research builds the groundwork needed to devise remedial actions aimed at helping managers find an optimal balance between exploration and exploitation. One way to achieve this goal is by carefully designing the learning environment. In two additional studies, we analyze the evolution of exploration/exploitation trade-offs across different learning environments. Offering decision makers repeated opportunities to learn and increasing the planning horizon appears beneficial.
The advance of ad-blocking technology is expected to have profound implications on the advertising industry. This paper makes the first attempt to understand the impacts of ad blocking on consumer’s ad avoidance and optimal reactions by advertiser and ad platform while advertising signals quality. We extend the standard models on ad signaling to the context of ad blocking. Our model incorporates both ad-production cost and ad-distribution cost, and allows ad quality (ad production) to impact consumers’ nuisance costs. We find that, counterintuitively, a lower ad-blocking cost may result in fewer consumers blocking ads and higher profit for the advertiser. This is driven by the signaling function of advertising. In particular, the ad platform reacts to lower ad-blocking cost by lowering the unit ad-distribution cost it charges, forcing the advertiser to spend more on ad production because ad-distribution cost alone is insufficient to signal product quality. The high ad-production cost may offset consumers’ disutility of ad viewing and result in fewer consumers blocking ads when ad blocking becomes less costly. We also confirm the robustness of this insight with various model extensions and discuss the implications of our findings.
The success, if not survival, of service businesses depends on their ability to satisfy their customers. Yet, businesses often recognize slumping customer satisfaction too late and ultimately fail. To prevent this, marketers require early warning tools. In this paper, we build upon online ratings as a direct measure of customer satisfaction and, based on this, predict business failures. Specifically, we develop a variable-duration hidden Markov model; it models the rating sequence of a service business in order to predict the likelihood of failure. Using 64,887 ratings from 921 restaurants, we find that our model detects business failures with a balanced accuracy of 78.02%, and this prediction is even possible several months in advance. In comparison, simple metrics from practice have limited ability in predicting business failures; for instance, the mean rating yields a balanced accuracy of only around 50%. Furthermore, our model recovers a latent state (“at risk”) with an elevated failure rate. Avoiding the at-risk state is associated with a reduction in the failure rate of more than 41.41%. Our research thus entails direct managerial implications: we assist marketers in monitoring customer satisfaction and, for this purpose, offer a data-driven tool that provides early warnings of impending business failures.
During the 2020 COVID-19 epidemic, the U.S. Congress passed the CARES Act that (among other measures) provides direct payments to households. Using a large debit cards database, we analyze consumer expenditures following the stimulus payments. We observe zip code level daily transactions (approximately 12 million cards) before and immediately following the disbursements of stimulus checks. Empirical analysis exploits geographical variation in timing of federal deposits to identify marginal propensity to consume (MPC) for stimulus payments. We estimate between 0.29 (excluding banking) and 0.51 (all spend) of the rebate is spent within a few days of receipt. We find large cross-sectional heterogeneity with MPC estimates that are three times higher in magnitude in the most densely populated urban areas with higher cost-of-living. In areas with more restricted movement during the pandemic (as measured by Google workplace mobility), MPC estimates are approximately 60% higher. We reanalyze data from previous fiscal initiatives (2001 tax rebates and the 2008 fiscal stimulus) and find similar geographical differences. Collectively our results highlight an important shortcoming in fiscal policies that ignore local environment, particularly cross-sectional differences in cost-of-living across the United States.
To what extent do mass media outlets influence viewers’ trust in scientific evidence and compliance with behavior recommended by scientific experts? Exploiting the U.S. lockdown period of the COVID-19 pandemic in early 2020, we analyze a large longitudinal database that combines daily stay-at-home behavior from approximately 8 million mobile phones and local viewership of cable news networks. Early in the pandemic, several of Fox News’ hosts downplayed the severity of the pandemic and the risks associated with the transmission of the virus. A combination of regression analysis and a natural experiment finds that a 10% increase in viewership of Fox News in a zip code causes a 0.76-percentage-point reduction in compliance with stay-at-home behavior. The results imply a media persuasion rate that is larger than typical advertising persuasion rates on consumer behavior. Similar analyses using viewership of MSNBC and CNN, which supported lockdown measures, were inconclusive but suggested a smaller, positive effect on compliance with social distancing regulations.
This paper studies the effect of nutrition warning labels and advertising restrictions on the breakfast cereal market in Chile. In June 2016, the Ministry of Health required food products that exceed thresholds for sugar (22.5 g) and calories (350 kcal) to carry conspicuous front-of-package warning labels. Furthermore, these products were barred from advertising on television programs with high child viewership. Early evidence suggests that the regulation induces consumers to switch to products without warning labels; we show that this change in demand elicits a supply response. In particular, we present evidence of bunching just below the cutoffs. Using a structural model of cereal demand, we find that reformulation tends to reinforce the intent of the reform, in particular, by lowering the calorie content of cereal purchases.
In 2014, Washington State used a lottery system to allocate licenses to firms in the newly legalized retail cannabis industry, generating random variation in how many stores entrepreneurs were able to own. We observe highly detailed data on all subsequent industry transactions, including prices, wholesale costs, markups, and product assortments. We find that entrepreneurs who are randomly allocated more store licenses ultimately earn substantially higher per store profits than do single-store firms, suggesting that the returns to scale in the mom-and-pop retail sector are quite large. Despite these firms having less local competition, this increase in profits does not come at the expense of consumers. Rather, retailers in multistore chains ultimately charge significantly lower prices and margins and offer greater product variety. This gap in prices is not initially present but grows substantially over time, as does the difference in assortment size and profits between stores in multistore chains and stores operating alone, consistent with firm learning. Using the full history of outcomes, we track the evolution of firms in this new market and show that multistore retailers use an initial advantage in offering larger assortments to position themselves as the low-price, large-assortment retail option and attract a larger but more price-sensitive set of customers. These results have implications for the study of retail concentration and mergers, countervailing buyer power, and consumer search. Our results suggest that policies to help entrepreneurs expand in retail may have large benefits to both firms and consumers.
Previous studies have found that increases in price promotions lead decreases in the average price of seasonal goods in high-demand seasons, countering basic supply and demand predictions. I explain this phenomenon by proposing that price-sensitive consumers are less likely to search in low-demand periods, changing consumer composition and decreasing aggregate price elasticity. Simultaneously, consumer searches allow the firm to use price promotions to attract price-sensitive consumers while maintaining high average prices. I test this and other explanations using a seasonal dynamic structural inventory model where consumers make decisions on whether to search, which reveals price promotions and allows consumers to purchase. I find that price-sensitive consumers make up 14.9% of searching consumers in the low-demand season versus 29% in the high-demand season, resulting in increased price elasticity. The results suggest that consumer composition is changing due to the different search incentives of different segments.
Retailers control what products are offered on shelves, which may allow them to capture preferential contracts from their suppliers. I analyze the determinants of retail profitability through the lens of replacement threats, in which a buyer may obtain favorable pricing by threatening to source an input from a competitor. A theoretical analysis shows that retailers are less likely to benefit from replacement threats if they are better positioned to use other negotiation levers. An empirical analysis of the U.S. yogurt market confirms that larger buyers and regional retailers, with many stores in the market, benefit relatively less from replacement threats.
Unlike random sampling, selective sampling draws units based on the outcome values, such as oversampling rare events in choice outcomes and extreme activities on continuous and count outcomes. Despite high cost-effectiveness for marketing research, such endogenously selected samples must be carefully analyzed to avoid selection bias. We introduce a unified and efficient approach based on semiparametric odds ratio (SOR) models applicable for categorical, continuous and count response data collected using selective sampling. Unlike extant sampling-adjusting methods and Heckman-type selection models, the proposed approach requires neither modeling selection mechanisms nor imposing parametric distributional assumptions on the response variables, eliminating both sources of mis-specification bias. Using this approach, one can quantify and test for the relationships among variables as if samples had been collected via random sampling, simplifying bias correction of endogenously selected samples. We evaluate and illustrate the method using extensive simulation studies and two real data examples: endogenously stratified sampling for linear/nonlinear regressions to identify drivers of the share-of-wallet outcome for cigarettes smokers and using truncated and on-site samples for count data models of store shopping demand. The evaluation shows that selective sampling followed by applying the SOR approach reduces required sample size by more than 70% compared with random sampling and that in a wide range of selective sampling scenarios SOR offers novel solutions outperforming extant methods for selective samples with opportunities to make better managerial decisions.
Many e-commerce platforms provide marketing tools to help their sellers attract customers and enhance user experience. However, there is virtually no theoretical framework or systematic evidence that provides insights to platforms on how their business customers use these marketing tools. In this paper, we develop a theoretical framework and apply it to an empirical setting to understand how business customers choose between two service offerings (paid search and hot shop) provided by an e-commerce platform. A unique aspect of our modeling framework is that we incorporate two types of heterogeneous strategic considerations in sellers’ choice decisions of marketing tools: competitor and consumer reactions. To capture seller consideration of competition, we adapt the cognitive hierarchy framework by modeling sellers’ differing abilities to predict how competition affects their decisions. To capture seller consideration of consumer response, we first specify a sales-response model in which sales are affected by the marketing tools used and then incorporate the response parameters in sellers’ payoff functions. Our empirical analysis indicates that these two types of strategic considerations are both important. Our estimation results show that, in making decisions on which marketing tool(s) to use, sellers tend to differentiate themselves from the competition. We also find that sellers with a higher rating tend to be more strategic. This finding provides a useful metric associated with firms’ strategic ability, which is often difficult to quantify, and helps researchers to test theoretical predictions related to firms’ strategic thinking using field data. We perform two comparative statics exercises to derive managerial insights. The first exercise offers a benchmark analysis to help platforms evaluate which targeted promotion strategies are most effective. The second exercise indicates that an increase in seller strategic ability would reduce the overall seller usage of marketing tools, and we offer specific suggestions to help platform managers increase the use of their marketing tools.
We examine a seller’s optimal bundling strategy when it sells two events offered over time. Given the temporal and perishability features of events, a bundle can be sold only in the first period, and the first-period event is unavailable later. We consider the following features: (i) events may differ in their popularity; (ii) the more popular event is offered first or later; (iii) consumers may be uncertain about their valuation of the later event; (iv) the seller may be unable to commit to the price of a future event; and (v) consumer valuations of events are independently distributed or positively correlated. We show how these market features determine the benefit of segmenting consumers with offers of bundle and individual events, the gains from the unconstrained optimization of the second-period event, and its cannibalization effect on first-period offerings. We demonstrate circumstances where the seller’s optimal strategy is selling only individual events, only the bundle of events, the bundle of events with only the less popular event offered outside the bundle, or the bundle of events and both events offered outside the bundle. We also show that the seller prefers to offer the more popular event later in all these market settings.
Logos serve a fundamental role as the visual figureheads of brands. Yet, because of the difficulty of using unstructured image data, prior research on logo design has largely been limited to nonquantitative studies. In this work, we explore the interplay between logo design and brand identity creation from a data-driven perspective. We develop both a novel logo feature extraction algorithm that uses modern image processing tools to decompose pixel-level image data into meaningful features and a multiview representation learning framework that links these visual features to textual descriptions, consumer ratings of brand personality, and other high-level tags describing firms. We apply this framework to a unique data set of brands to understand which brands use which logo features and how consumers evaluate these brands’ personalities. Moreover, we show that manipulating the model’s learned representations through what we term “brand arithmetic” yields new brand identities and can help with ideation. Finally, through an application to fast-food branding, we show how our model can be used as a decision support tool for suggesting typical logo features for a brand and for predicting consumers’ reactions to new brands or rebranding efforts.
Marketing Science greatly benefited from the admirable and fastidious efforts of more than 300 different individuals who provided manuscript reviews last year. Beyond those individuals already recognized on the editorial board, the editor-in-chief and senior editors of Marketing Science are indebted to the many guest editors, guest associate editors, and ad hoc reviewers who provided expert counsel and guidance. The following list acknowledges the contribution of guest editors, guest associate editors, and ad hoc reviewers who served from January 1, 2021, to December 31, 2021. Finally, our sincere appreciation to the authors, whose outstanding submissions and careful revisions make the journal the go-to resource for leading-edge knowledge in quantitative marketing.Olivier ToubiaColumbia University
Consumers interact with firms across multiple devices, browsers, and machines; these interactions are often recorded with different identifiers for the same consumer. The failure to correctly match different identities leads to a fragmented view of exposures and behaviors. This paper studies the identity fragmentation bias, referring to the estimation bias resulted from using fragmented data. Using a formal framework, we decompose the contributing factors of the estimation bias caused by data fragmentation and discuss the direction of bias. Contrary to conventional wisdom, this bias cannot be signed or bounded under standard assumptions. Instead, upward biases and sign reversals can occur even in experimental settings. We compare several corrective measures and discuss their advantages and caveats.
Advances in natural language generation (NLG) have facilitated technologies such as digital voice assistants and chatbots. In this research, we demonstrate how NLG can support content marketing by using it to draft content for the landing page of a website in search engine optimization (SEO). Traditional SEO projects rely on hand-crafted content that is both time consuming and costly to produce. To address the costs associated with producing SEO content, we propose a semiautomated methodology using state-of-the-art NLG and demonstrate that the content-writing machine can create unique, human-like SEO content. As part of our research, we demonstrate that although the machine-generated content is designed to perform well in search engines, the role of the human editor remains essential. Comparing the resulting content with human refinement to traditional human-written SEO texts, we find that the revised, machine-generated texts are virtually indistinguishable from those created by SEO experts along a number of human perceptual dimensions. We conduct field experiments in two industries to demonstrate our approach and show that the resulting SEO content outperforms that created by human writers (including SEO experts) in search engine rankings. Additionally, we illustrate how our approach can substantially reduce the production costs associated with content marketing, increasing their return on investment.
The underlying channels through which peer influence operates in durable good adoption can affect the ability of marketers to leverage them. In this paper, we assess whether the visibility of peers’ adoption decisions leads to greater peer influence. The context we study is residential rooftop solar panels. We exploit the plausibly exogenous location and orientation of peers’ rooftop solar panels relative to proximate roadways and visual obstructions, such as vegetation, in order to determine whether geographically proximate peer installations increase a household’s probability of solar adoption more if they are visible from the road. We find that the total angle of visibility of peer installations on the same street positively affects solar adoption decisions at distances of at least 500 meters (m). In contrast, we only find a positive effect of nonvisible solar arrays within 100 m, which may be due to causal peer influence via other channels, such as word of mouth, or very localized unobservable effects. The effect of peer visibility is moderated by the economic value that the peers receive from installing solar, providing suggestive evidence of social learning through visual information.
This research examines how network effects, a phenomenon whereby the larger the demand for a product or service, the higher the value of that product or service for each consumer, influence the impact of price personalization on firms and consumers. We study the impact of network effects on price personalization in terms of price, demand, profit, and consumer surplus. One belief commonly held by companies is that price personalization increases their profit by providing means for capturing heterogeneity in consumers’ willingness to pay and enticing demand from heterogeneous consumer segments. Using a game-theoretical model, we find conditions under which price personalization not only shrinks demand, but also decreases the profit of the firm and reduces surplus of all consumers. Specifically, our model shows, in markets with network effects, unless when the impact of expected consumer heterogeneity is higher than the impact of network effects, personalized pricing reduces demand, lowers profit, reduces surplus of all consumers, and thus causes a deficiency in the market.
This paper develops a model of vertical markets with multiple upstream and downstream firms. Networks of vertical relationships, negotiated contract terms for those relationships, and downstream prices charged to end customers all arise endogenously. In addition, I provide an estimation procedure to fully rationalize the observed network and contracts in a computationally scaleable manner. I empirically apply the framework to the market between hospitals and health insurers. I use the model to quantify the effects of “network adequacy regulations,” which are designed to broaden patients’ hospital choice sets but can backfire by increasing prices. I show that “tight” regulations have adverse effects. I also show market response is substantially impacted by complex mechanisms involving the interplay among multiple insurers and between upstream negotiation and downstream pricing.
Digital media platforms commonly use the skippable ad format, which gives a viewer the option to skip part of an advertisement after seeing some limited information and jump directly to the desired content. It also enables these platforms to charge advertisers only when viewers attend to the entire ad. We develop a dynamic model of a viewer receiving incremental information from the advertiser and embed it in a two-sided market setting with an advertising market. Our results show that skippable ads can be less effective overall in converting existing viewers to advertisers. However, skippable ads bring more viewers to the platform and, in turn, induce more advertisers. This suggests that a switch to the skippable format is a profitable strategy for an emerging and growing platform but not necessarily for one in a saturated market.
We study how frequent-flyer program members change their purchase behaviors as they progress toward achieving elite status. Using data from a leading U.S. airline, we empirically test the theoretical prediction that travelers’ switching costs vary dynamically with their progress toward attaining status. We show evidence for increased switching costs as the consumer approaches the target pace of point accumulation required to attain status. These switching costs reflect changes in booking behavior with the airline: Travelers become more likely to choose the airline even when it is less appealing than its competitors, and to pay higher prices than they otherwise would. These responses are reduced when travelers accumulate points at a rate substantially ahead of the target pace. The increase in switching costs is more pronounced for consumers at a hub of the airline and for business travelers. Moreover, we document a stronger willingness-to-pay response when consumers are less likely to shoulder the ticket costs themselves because they are traveling for business. This response suggests that asymmetric incentives induced by business travel explain much of the heterogeneity between business and leisure travelers, and moral hazard may be responsible for a large part of the profitability of frequent-flyer status incentives.
Jackpots are an important advertising and promotional tool in the casino gambling industry. In this paper, we use a unique data set to measure the impact of a slot machine jackpot event on subsequent gambling behavior. We use a difference in differences method to partition jackpot value into its impact on the post-jackpot behavior of three categories of players: (1) jackpot winners, (2) their peers or partners, and (3) bystanders who are in the proximity of the jackpot event. We find that jackpot events increase gambling expenditures (average slot machine bet amount) and frequency of plays by jackpot winners. The average impact on the jackpot winner is a $39 increase in bet amount per play and a 33% increase in the number of plays for the two-hour period after the jackpot event, whereas the impact on peers is a 21% increase in number of plays in the same period. For bystanders, effects are weaker and dissipate about an hour after the jackpot. Our study of jackpot return on investment shows that 49% of jackpots are profitable for the casino. Our study of the underlying mechanism of winners’ response favors the hot hand effect rather than the house money or gambler’s fallacy effects.
Social media influencers are category enthusiasts who often post product recommendations. Firms sometimes pay influencers to skew their product reviews in favor of the firm. We ask the following research questions. First, what is the optimal level of affiliation (if any) from the firm’s perspective? Affiliation introduces positive bias to the influencer’s review but also decreases the persuasiveness of the review. Second, because affiliated reviews are often biased in favor of the firm, what is the impact of affiliation on consumer welfare? We find that the affiliation decision depends on the cost of information acquisition, the consumer’s prior and awareness, and the disclosure regime. When the consumer’s prior belief is low, the firm needs to affiliate less closely or not at all in order to preserve the influencer’s persuasiveness, the change in the consumer’s belief following the influencer’s review. In contrast, when the consumer’s prior belief is high, the firm fully affiliates with the influencer to both maximize awareness and prevent a negative review. We also show that the firm’s involvement can be Pareto improving if the information acquisition cost is relatively high, and a partial disclosure rule may increase consumer welfare.
This study examines consumers’ time-inconsistent preferences in digital content consumption and their strategic self-control behaviors. We used a unique data set obtained from a major digital book platform in China, where consumers can pay either by chapter or a monthly subscription. A third of consumers consistently chose to pay by chapter, even though a monthly subscription would significantly reduce the monetary cost. We propose a dynamic structural model that incorporates time-inconsistent preferences and strategic self-control behaviors to rationalize such behavior. We first analytically demonstrate the existence of a unique equilibrium and show how, under steady states, overpaying for reading can be optimal for consumers. We then estimate the model from the data. The results show that there is a large segment of price-sensitive consumers who are willing to overpay to curb future consumption. Our counterfactuals show that eliminating the pay-per-chapter option would hurt consumer welfare and the platform’s profit. Eliminating the monthly subscription plan, however, would increase the platform’s profit but reduce consumer welfare. We introduce a novel nonlinear pricing plan with a volume surcharge and illustrate how it can benefit both consumers and the platform.
Industry sentiment links income and wealth to private-label demand. The intuition is that decreasing income and wealth increases the demand for (cheaper) private labels. Whereas plausible causality is harder to establish in aggregate time series analyses, such analyses suggest large effect sizes. An individual-level perspective greatly facilitates plausibly causal estimates but poses measurement challenges. We overcome these challenges by linking household scanner data to administrative data. We analyze individual-level private-label shares measured in household scanner data as a function of income and wealth, both from a linked administrative database in the Netherlands in the period from 2011 to 2018 and aggregated over all household members (rather than only from the main earner). We find that relying on within-household variation in surveyed income data significantly attenuates income effects relative to using that from administrative data. Yet, we still find an economically small effect. In addition, changes in wealth have at most an economically small effect on private-label shares.
I empirically separate two components in a consumer’s privacy preference. The intrinsic component is a “taste” for privacy, a utility primitive. The instrumental component comes from the consumer’s anticipated economic loss from revealing his private information to the firm and arises endogenously from a firm’s usage of consumer data. Combining an experiment and a structural model, I measure the revealed preferences separately for each component. Intrinsic preferences have seemingly small mean values, ranging from $0.14 to $2.37 per demographic variable. Meanwhile, they are highly heterogeneous across consumers and categories of data: The valuations of consumers at the right tail often exceed the firm’s valuation of consumer data. Consumers’ self-selection into data sharing depends on the respective magnitudes and correlation between the two preference components and often deviates from the “low types are more willing to hide” argument. Through counterfactual analysis, I show how this more nuanced selection pattern changes a firm’s inference from consumers’ privacy decisions and its data-buying strategy.
This paper investigates a contest in information revelation between firms that seek to persuade consumers by revealing positive own information and negative information about the rival. In the face of limited bandwidth, firms are forced to make a tradeoff between disclosing their own positive information and their rival’s negative information. A negative-communication equilibrium, in which firms disclose rival’s negative information whenever possible, exists when consumers have poor outside options or when firms are better informed. Bandwidth limitations make competitive firms more likely to disclose information compared with when they have no limitations. When firms strictly prefer consumers to choose the outside option over the rival (as in political contests), there is a greater prevalence of the negative-communication equilibrium while the incidence of positive communication is lowered. Competition in information disclosure leads to greater consumer surplus compared with the unilateral disclosure case, whereas bandwidth limitations reduce consumer surplus. Finally, when firms are asymmetric in their ex ante quality valuations, the higher quality firm is less likely to engage in negative communication.
We conduct an empirical case study of the U.S. beer industry to analyze the disruptive effects of locally manufactured craft brands on market structure, an increasingly common phenomenon in consumer packaged goods industries typically attributed to the emerging generation of adult millennial consumers. We document a generational share gap: millennials buy more craft beer than earlier generations. We test between two competing mechanisms: (i) persistent generational differences in tastes and (ii) differences in past experiences or consumption capital. Our test exploits a novel database tracking the geographic differences in the diffusion of craft breweries across the United States. Using a structural model of demand with endogenous consumption capital stock formation, we find that heterogeneous consumption capital accounts for 86% of the generational share gap between millennials and baby boomers with the remainder explained by intrinsic generational differences in preferences. We predict the beer market structure will continue to fragment over the next decade, overturning a nearly century-old structure dominated by a small number of national brands. The attribution of the share gap to consumption capital shaped through availability on the supply side of the market highlights how barriers to entry, such as regulation and high traditional marketing costs, sustained a concentrated market structure.
We show that average buyer ratings of sellers have grown substantially more positive over time in five online marketplaces. Although this increase could by explained by (i) marketplace improvements that increased rater satisfaction, it could also be caused by (ii) “reputation inflation,” with raters giving higher ratings without being more satisfied. We present a method to decompose the growth in average ratings into components attributable to these two reasons. Using this method in one marketplace where we have extensive transaction-level data, we find that much of the observed increase in ratings is attributable to reputation inflation. We discuss the negative informational implications of reputation inflation and consider the likely causes.
We document short-run changes in websites and the web technology industry with the introduction of the European General Data Protection Regulation (GDPR). We follow more than 110,000 websites and their third-party HTTP requests for 12 months before and 6 months after the GDPR became effective and show that websites substantially reduced their interactions with web technology providers. Importantly, this also holds for websites not legally bound by the GDPR. These changes are especially pronounced among less popular websites and regarding the collection of personal data. We document an increase in market concentration in web technology services after the introduction of the GDPR: Although all firms suffer losses, the largest vendor—Google—loses relatively less and significantly increases market share in important markets such as advertising and analytics. Our findings contribute to the discussion on how regulating privacy, artificial intelligence and other areas of data governance relate to data minimization, regulatory competition, and market structure.
Websites are created to help visitors take an action, such as making a purchase or a donation. As visitors browse various web pages, they may take rapid steps toward the action or may bounce away. Websites that can adapt to match such consumer dynamics perform better. However, assessing visitors’ changing distance to the action, at each click, and adapting to it in real time is challenging because of the sheer number of design elements that are found in websites, that combine exponentially. We solve this problem by matching latent states to web page designs, combining recent advances in multiarmed bandit (MAB), website morphing, and hidden Markov models (HMM) literature. We develop a novel dynamic program to explicitly model the trade-off firms face between nudging a visitor to later states along the funnel, and maximizing immediate reward given current estimates of purchase probabilities. We use an HMM to assess visitors’ states in real time, and couple it with an MAB model to learn the effectiveness of each design × state combination. We provide a proof of concept in two applications. First, we conduct a field study on the Master of Business Administration website of a major university. Second, we implement our algorithm on a cloud server and test it on an experimental online store.
What is the value of verified employment data in consumer lending? We study this question using a data set covering all employment verification inquiries to Equifax. Using a difference-in-differences approach, we analyze the changes in applicants’ loan outcomes after their employers join Equifax’s digital verification system, which provides lenders with an efficient way of accessing the (employer-) verified employment data in auto loan applications. Holding the employment status constant, we find that the availability of digitally verified data significantly expands credit access: the loan origination rate increases by 35.5% on average, and is more significant among deep subprime (146%) and subprime consumers (44%). The interest rates charged on these loans rise only slightly. The expanded credit access also benefits lenders, with an estimated 19.6% increase in profit. This is because the benefit of the market expansion effect dominates the cost of a higher delinquency risk among the expanded loan portfolio. Our results suggest that, besides seeking new data sources, managers and policy makers should also consider ways to extract more value from existing data.
This paper proposes a new approach to modeling heterogeneity in choice data that can accommodate fixed-point ratings data and text. Respondent choices, survey responses, and narratives are combined to form latent archetypes that provide an integrated description of respondents in terms of the objects and drivers of their wants. We propose a measure of coherence to assess the value of integrating these data elements and demonstrate the value of integrating text data into an analysis of choice and scaled response data. A conjoint data set is used to illustrate the model where we find that the text data helps clarify the origin of demand.
This paper develops a theoretical model to study the economic incentives for a social media platform to moderate user-generated content. We show that a self-interested platform can use content moderation as an effective marketing tool to expand its installed user base, to increase the utility of its users, and to achieve its positioning as a moderate or extreme content platform. For the purpose of maximizing its own profit, a platform balances pruning some extreme content, thus losing some users, with gaining new users because of more moderate content on the platform. This balancing act plays out differently depending on whether users have to pay to join (subscription versus advertising revenue models) and on whether the technology for content moderation is perfect. We show that, when conducting content moderation optimally, a platform under advertising is more likely to moderate its content than one under subscription but does it less aggressively compared with the latter when it does. This is because a platform under advertising is more concerned about expanding its user base, whereas a platform under subscription is also concerned with users’ willingness to pay. We also show a platform’s optimal content moderation strategy depends on its technical sophistication. Because of imperfect technology, a platform may optimally throw away the moderate content more than the extreme content. Therefore, one cannot judge how extreme a platform is by just looking at its content moderation strategy. Furthermore, we show that a platform under advertising does not necessarily benefit from a better technology for content moderation, but one under subscription does as the latter can always internalize the benefits of a better technology. This means that platforms under different revenue models can have different incentives to improve their content moderation technology. Finally, we draw managerial and policy implications from our insights.
Firms employ temporal data for predicting sales and making managerial decisions accordingly. To use such data appropriately, managers need to make two major analysis decisions: (a) the temporal granularity (e.g., weekly, monthly) and (b) an accompanying demand model. In most empirical contexts, however, model selection, sales forecasts, and managerial decisions are vulnerable to both of these choices. Whereas extant literature has proposed methods that can select the best-fitted model (e.g., Bayesian information criterion) or provide predictions robust to model misspecification (e.g., weighted likelihood), most methods assume that the granularity is either correctly specified or prespecify it. Our research fills this gap by proposing a method, the scaled power likelihood with multiple weights (SPLM), that not only identifies the best-fitted granularity-model combination jointly, but also conducts doubly (granularity and model) robust prediction against their potentially incorrect selection. An extensive set of simulations shows that SPLM has higher statistical power than extant approaches for selecting the best-fitted granularity-model combination and provides doubly robust prediction in a wide variety of misspecified conditions. We apply our framework to predict sales for a scanner data set and find that, similar to our simulations, SPLM improves sales forecasts due to its ability to select the best-fitted pair via SPLM’s dual weights.
New technology measures TV viewer tuning, presence, and attention, enabling the first distinctions between TV ad viewability and actual ad viewing. We compare new and traditional viewing metrics to evaluate the new metrics’ utility to advertisers. We find that 30% of TV ads play to empty rooms. We then use broadcast networks’ verifiably quasi-random ordering of ads within commercial breaks to estimate causal effects of ads on new viewing metrics among four million advertising exposures. We measure ad metadata and machine-code content features for 6,650 frequent ad videos. We find that recreational product ads preserve audience tuning and presence. Prescription drug advertisements decrease tuning and presence, more so for drugs that treat more prevalent and severe conditions. We also investigate whether new viewing data can inform advertiser objectives, finding that attention helps predict brand search lift after ads.
We study the market for fake product reviews on Amazon.com. Reviews are purchased in large private groups on Facebook and other sites. We hand-collect data on these markets and then collect a panel of data on these products’ ratings and reviews on Amazon, as well as their sales rank, advertising, and pricing policies. We find that a wide array of products purchase fake reviews, including products with many reviews and high average ratings. Buying fake reviews on Facebook is associated with a significant but short-term increase in average rating and number of reviews. We exploit a sharp but temporary policy shift by Amazon to show that rating manipulation has a large causal effect on sales. Finally, we examine whether rating manipulation harms consumers or whether it is mainly used by high-quality products in a manner like advertising or by new products trying to solve the cold-start problem. We find that after firms stop buying fake reviews, their average ratings fall and the share of one-star reviews increases significantly, particularly for young products, indicating rating manipulation is mostly used by low-quality products.
Sellers may display unreasonably high prices (in high-cost oriented contexts) that are never accepted or would surely be discounted through bilateral bargaining. Conversely, general or vague prices with incredible appeals may be advertised (in low-cost oriented settings) as a range or for a category but not tied to specific items. Nevertheless, these seemingly irrelevant or unbinding prices can affect buyer behavior and bargaining outcome. This paper presents a new explanation for these puzzling phenomena. We propose that advertised reference prices can be a strategic tool to communicate privately known seller costs to influence optimal buyer strategy to search value information. We show that cheap-talk communication via uncommitted prices can be endogenously credible, because optimal buyer search strategy can be imperfectly aligned with seller preference. In particular, a high-cost seller may prefer the buyer to acquire information in a more cautious manner (i.e., less willing to stop at good news but more eager to quit at bad news) than a low-cost seller, which may coincide with the buyer’s optimal search strategy. We demonstrate that endogenous buyer search can serve as a two-way discipline to regulate both high-cost and low-cost sellers’ incentives for deception, which thus can sustain credible communication for high-cost and/or low-cost oriented settings.
We present a deep learning algorithm to provide personalized feedback on creative appeals, written content intended to persuade readers to undertake some action. Such appeals are widespread in marketing, including advertising copy, RFP responses, call center scripts, product descriptions, and many others. Although marketing research has produced several tools to help managers glean insights from online word-of-mouth, less attention has been paid to creating tools to assist the innumerable marketers responsible for crafting effective marketing messages. Our approach leverages the hierarchical structure of written works, associating words with sentences and sentences with documents, and the linguistic relationships developed therein. We score each sentence in an appeal by its expected contribution to success accounting for its substance and persuasive impact. The sentences with the lowest scores make the appeal less compelling and are the most effective points to focus a revision. The approach has proved effective in a randomized control trial, with subjects rating essays revised with the aid of algorithmic feedback as being 4.5% more likely to achieve their objectives. In addition to providing automated feedback to authors, we leverage the model’s output to derive substantive insights into what makes an appeal compelling.
This paper develops an oligopoly model in which firms first choose capacity and then compete in prices in a series of advance-purchase markets. We show that the existence of multiple sales opportunities creates strong competitive forces that prevent firms from utilizing intertemporal price discrimination. We then show that intertemporal price discrimination is possible but only when firms adopt inventory controls (sales limit restrictions) and demand becomes more inelastic over time. Therefore, we show that in addition to being useful to manage demand uncertainty, inventory controls are also a tool to soften price competition. We discuss model extensions, including product differentiation, aggregate demand uncertainty, and longer sales horizons.
We examine a large-scale mandatory food labeling regulation to identify its effects on consumer behavior. We take advantage of exogenous variation in product-labeling status from the gradual and asynchronous introduction of labeled products on store shelves many weeks before the legal deadline. We combine individual-level scan data from a large retailer with on-the-shelf information on the actual warning label status for breakfast cereals, chocolates, and cookies. Warning labels decrease demand and purchase probabilities in the cereal category, and this effect is larger on medium-low socioeconomic groups. We find inconclusive results of the warning label on chocolates and cookies. Overall, results suggest that the warning label effect is consistent with information disclosure influencing consumers’ choices when the advertised information is unexpected.
In a series of 10 studies, we find that people are more likely to make virtuous decisions on paper than on a digital device because they perceive choices on paper as more real (i.e., tangible, actual, and belonging to the physical rather than the virtual world) and hence as more self-diagnostic (i.e., representative of who they are). We first show people express more interest in donating and volunteering (Studies 1a and 1b), are more likely to donate (Study 2), and put more effort into helping a charitable cause (Study 3) when these choices occur on paper (versus tablet)—a pattern of decision making we label the good-on-paper effect. Study 4 extends these findings to book choices (highbrow versus lowbrow) and to a device interaction that closely mimics writing on paper (i.e., tablet with digital pen). In the context of volunteering decisions, we then provide evidence for the sequential mediating roles of perceptions of realness and self-diagnosticity in the good-on-paper effect (Study 5 and Studies 6a and 6b). Finally, we show that chronic (Study 7) and situational (Study 8) perceptions of self-diagnosticity moderate this effect in the contexts of environmental protection and food choices (healthy versus indulgent), respectively. We discuss the theoretical and practical implications of these findings.
