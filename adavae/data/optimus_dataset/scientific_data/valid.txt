1	Crowd-feedback is receiving increasing attention in research and practice as a contemporary approach for involving users in information systems development. Thereby, feedback on various aspects of an information system is collected from a non-expert crowd using designated crowd-feedback systems that are able to collect comprehensive and reliable feedback at scale. However, the current body of knowledge on crowd-feedback is scattered and lacks a structured form in which research on crowd-feedback can be classified. To address this gap, this article provides a comprehensive overview of the state-of-the-art of existing crowd-feedback research by: (1) conducting a systematic literature review, (2) developing a morphological box to conceptualize crowd-feedback, and (3) performing a cluster analysis for identifying research streams on crowd-feedback. Analyzing 40 articles, our key contribution resides in the synopsis of the existing crowd-feedback literature. Based on our review, we suggest four research avenues to guide researchers in investigating crowd-feedback in the future.
1	Digital sensors capture a growing range of real-world phenomena with increasing robustness and precision. However, we observe that the overreliance on digital sensors comes with technical and economic feasibility constraints limiting which aspects of real-world phenomena information systems can represent. Using representation theory as a theoretical lens, we argue that real-world phenomena can be represented more faithfully in IS if digital sensors are complemented by human sensors working in concert as a hybrid sensing system. We use a mixed-method research approach to study the design, operation, and impact of hybrid sensing systems at a German bike sharing provider that relies on such a system to monitor the location and availability of its fleet. Through this research, we aim to bring the origin of data into focus of information systems design and present a sociotechnical solution to the fundamental problem of limited observability of user misbehavior in sharing economy business models.
1	Information Systems Development (ISD) depends on the availability of help among team members to create value for customers. Without helping, team members are blocked and wait time occurs leading to delays in the ISD deliveries. However, helping is costly because it disrupts already busy team members and takes their time and focus away from the current task. Consequently, a paradoxical tension emerges between ‘must help’ and ‘cannot help’ that when unresolved impairs the delivery of the software. The short paper draws on the theoretical framework of ‘help-seeking and help-giving’ to gain a deeper and more detailed understanding of the how and why ISD teams balance the helping paradox. Building on an exploratory, qualitative case study of ISD teams, we identify four helping balancing strategies that in concert ensure help is available to create software of customer value.
1	We study the spread of several leading machine learning frameworks underlying the current permeation of artificial intelligence in information systems through the lens of generativity. Empirical evidence indicates that machine learning frameworks do, indeed, exhibit generativity. We further identify three different types of generativity in the frameworks’ development process: evolutionary, combinatorial, and reciprocal generativity. We demonstrate predictive relations of those facets of generativity to related constructs like popularity, activity, and growth empirically, based on an analysis of longitudinal archival data. Empirical analysis further demonstrates that all three facets should be selected as part of a comprehensive predictive model of generativity. The research procedure follows an iterative methodology that facilitates the connection of this work's findings to the broader information systems field.
1	Maturity models (MM) enjoy great popularity among scholars and practitioners, particularly for addressing the novel challenges in IS design and management of IS projects. However, MM research is harshly criticized by rigor-centered academics because of methodological shortcomings. This is because existing design principles for MM development often are insufficiently applied. In this research, we conduct a scoping review of the academic MM literature. Informed by the analysis of MM literature, we pinpoint existing methodological weaknesses of MM research and offer suggestions on how to improve extant design principles for MM development. The presented guideline extensions apply to the complete MM design process from problem definition to results presentation and cover eight design principles. Our results contribute to increased relevance and rigor in MM research. Further, they motivate further research on empirically and longitudinally validating MM’s impact in practice and on the mechanisms of how MM supports organizational learning.
1	Communication cold-start problems are pervasive in privacy-sensitive settings. To mitigate those problems, our research examined ephemeral sharing as a privacy-preserving mechanism to navigate the balance between users’ privacy concerns and information sharing in the initial interaction stages in online dating. In partnership with Summer, a leading online dating platform, we report a large-scale randomized field experiment with over 70k users to understand how ephemeral sharing influences users’ information sharing behavior and subsequent match outcomes. We find that the subject in the ephemeral group achieves a significantly larger number of personal photos along with their matching request, a more significant number of initial matches, and higher conversational engagement from receivers. Further, our sequential mediation tests further show that the increased sharing of personal photos is the primary mechanism. Our study contributes to the literature on the design of matching platforms and provides actionable implications for the privacy-preserving design of matching platforms.
1	Digitalization permeates entire companies. Rethinking information systems (IS) success models developed in the 1980/1990s becomes obvious. But how does one define such a new model and implement it? Taking an SAP S/4HANA migration as our use case, the new triple “E” rating on hand focuses on the IS success of digital technologies by balancing efficiency, effectiveness, and experience criteria. Six takeaways emerge from our case study that should drive future discussion: (1) Evaluate digital technologies by adopting a simple but comprehensive approach including criteria beyond efficiency. (2) Consider diverse IS success perspectives consciously but never lose focus on the end-user. (3) A triad of efficiency, effectiveness, and experience encompasses the challenges of digital technologies. (4) Separate efficiency and effectiveness criteria, however, consider trade-offs between them. (5) The experience category highlights digital awareness and socio-technical changes. (6) Tailor your triple “E” rating individually to your project.
1	The traditional role of information technology project managers (IT PMs) is challenged significantly in agile information systems development (ISD) projects in which teams are self-managed. Thus far, research recognizes their contradictory role but overlooks to problematize the consequent identity tensions that IT PMs have to face. This is critical because IT PMs continue to play an essential role in ISD project success. Using identity theory as a lens, we conducted a qualitative interpretive study interviewing 26 IT PMs in agile ISD team settings. Our findings reveal five fundamental role identity tensions IT PMs experience during agile transformations. We provide a nuanced understanding of IT PMs’ role in agile ISD team settings to explain challenges in agile ISD. Moreover, we suggest moving from a narrow focus in IS research on agile ISD team-internal roles, such as developers, toward a broader focus that includes both team-internal and team-adjacent roles in agile ISD.
1	Although prior literature suggested that machine learning (ML) development can suffer strongly from uncertainty, it neglected to unveil the specific uncertainties arising in ML development projects and to understand their impact on the development process. To address this gap, we conduct an exploratory case study based on 62 interviews with ML experts from a multinational software provider. Our study reveals that uncertainty management strategies either target an uncertainty’s reducible or irreducible part and can thus be divided into reducing and handling strategies. We develop a model that shows at which stage of the ML development process each uncertainty is addressed and how as well as by whom the respective reducing and handling strategies are executed. The study mainly contributes to literature on ML development and on uncertainties in software development by unveiling the impact of uncertainty reducing and handling strategies triggered by ML specific uncertainties on the ML development project.
1	Depression has become a global medical crisis. Barriers for effective depression treatment include shortage of medical professionals, inaccurate assessment, and social stigma towards depression. Leveraging artificial intelligence (AI) system in depression treatment is a possible way to remove these barriers. However, AI system has its limitations such as assessment bias. Hence, in this short paper, we propose integrating AI and human intelligence to create a task assembly AI-human hybrid for depression treatment. We then develop a research model to assess users’ preference with three service agents (i.e., human physicians, AI system, and AI-human hybrid) in terms of privacy concern and trust. We also argue that social stigma plays a moderating role in users’ service agent preference. Further, we examine the underlying mechanisms that form the users’ intention to use a certain service agent. This paper can have significant theoretical and practical implications for AI implementation in mental healthcare setting.
1	Voice assistants are a novel class of information systems that fundamentally change human–computer interaction. Although these assistants are widespread, the utilization of these information systems is oftentimes only considered on a surface level by individuals. In addition, prior research has focused predominantly on initial use instead of looking deeper into post-adoption and habit formation. In consequence, this paper reviews how the notion of habit has been conceptualized in relation to biographical utilization of voice assistants and presents findings based on a qualitative study approach. From a perspective of post-adoption users, the study suggests that existing habits persist, and new habits hardly ever form in the context of voice assistant utilization. This paper outlines four key factors that help explain voice assistant utilization behavior and furthermore provides practical implications that help to ensure continued voice assistant use in the future.
1	Intelligent Personal Assistants (IPAs) are increasingly being integrated in many consumer products such as smartphones or cars. However, according to recent research only 50% of the population is using IPAs displaying a certain resistance behavior. Moreover, studies on the reasons for IPA resistance are not available. Consequently, our study strives to close this research gap by identifying key drivers of innovation resistance and adoption behavior of IPAs. Using a large-scale online survey (n=168) we find that individual differences, data privacy concerns, trust in AI and perceived creepiness play an important role for innovation resistance. More specifically, trust in AI applications partially mediates the relationship between individual differences and innovation resistance. Additionally, perceived creepiness fully mediates the relationship between AI data privacy concerns and innovation resistance. Our results provide valuable implications for the development and marketing of IPAs emphasizing the importance of trust and perceived creepiness for a customer’s innovation resistance.
1	In SMEs, the data usage has been rapidly increasing. Thus, the need to implement and use of the BI systems is crucial in order to enhance the organization performance. This study aims to examine the initial and post-adoption factors that influence owners /managers’ decision to adopt and use BI systems in SMEs in SA and to compare effects from these factors on both sides. To achieve this, an integrated model is proposed and empirically tested. This model has integrated established theories including TOE framework, IS adoption for small business and DOI theory. Our results show that, in initial adoption stage, all the proposed factors are significant predictors of BI system adoption, except the relative advantage. In contrast, in post-adoption stage, all those factors are significant predictors of the extent use of BI system, except the relative advantage and complexity. Also the comparison of the initial and post-adoption stages is highlighted.
1	The COVID-19 pandemic has forced the temporary closure of gyms and changed how people exercise. A growing number of people choose to exercise at home instead. Despite the rapid growth, little is known about the problems associated with this change and how information systems can be adopted to alleviate these problems. This paper identifies two problems, higher risk of injury and lower motivation levels, and proposes a set of design principles for the development of digital fitness trainer (DFT) for home gyms. Using a design science research approach, requirements are derived from interviews with practitioners and motivation theories. Design principles are then formulated based on these requirements. This work, when completed, will shed light on the challenges individuals face with exercising at home and how DFT can be designed to address these challenges.
1	Enterprise system use is critical for users to accomplish related tasks and achieve performance and innovation. However, users might choose workarounds not conducive to long-term goals for users, given that the complexity and uncertainty of ES features reduce the ES utilization. As a coping behavior of ES, workarounds are closely related to technology-driven stressors and personal traits, but there still exists limitations about the research. Anchoring on challenge-hindrance stressor framework and construal level theory, this study use transactional model of stress and coping to investigate how to reduce workarounds through technology-driven stressors and construal level. Specifically, we study the positive effect of technology-driven hindrance stressors and negative effect of technology-driven challenge stressors on workarounds, and whether technology-driven challenge stressors weaken the positive effect on workarounds. Further, we also investigate the moderating role of construal level on the relationships between technology-driven stressors and workarounds. Research methodology and expected contributions are discussed.
1	This paper explores and explains key factors affecting the adoption of Artificial Intelligence (AI) by auditing firms through the lens of the Technology-Organisation-Environment (TOE) framework. Using the multiple-case study method, the primary data collection was through semi-structured interviews with decision-makers of auditing firms, complemented by secondary data. The preliminary data analysis identified anomalies to existing theories, revealing the specificity of the AI adoption compared to other technology adoption processes. The findings show that firms’ adoption process is affected by the affordance of AI, barriers to adoption, communication process, staff’s AI-expertise, auditing regulation and client’s acceptance. This study contributes to the literature by providing a better understanding of AI adoption at the firm level, thus filling the gaps in the extant research. The findings strengthen the theories that underpin our understanding of the technology adoption by firms, revising, extending, and elaborating the TOE framework with more empirical evidence.
1	Owing to the necessity of effectively establishing enterprise architecture (EA) in an organizational context, there is a growing stream of research to examine the assimilation and institutionalization of EA in organizations. Our study aims to contribute to this stream by giving rise to the legitimacy of EA as the cornerstone of its institutionalization. To this end, we investigate four criteria of legitimacy, namely regulatory, pragmatic, normative, and cultural-cognitive legitimacy, in a case organization that lost legitimacy for its EA practices. We found criticality of regulatory and pragmatic legitimacy that need to be obtained effectively and promptly in order to grant sufficient time for normative and cultural-cognitive types of legitimacy to be attained.
1	This research investigates the chief executive offers’ (CEO) commitment to the status quo and its effects on adopting new technologies in agri-businesses. The study employed a mixed method approach, gathering data from two exploratory case studies and quantitative survey data from 128 agri-business CEOs. The findings identified that despite a myriad of advantages of modern technologies and the growing emphasis for global food security, there is still a strong element of resistance to adopt new technologies associated with status quo bias. The study also identified two new constructs of status quo bias that are unique to agri-businesses. Through the confirmatory survey data, the study established the relative importance of the constructs and how status quo influences the decision to adopt new technologies. Moreover, a series of multi-group analyses identified nuanced views of CEO status quo bias, in relation to firm size, years of experience, age and the gender.
1	This study examines the antecedents of user resistance to an information system (IS) in the context of Electronic Health Records (EHR), through a perceived threat perspective. Based on the threat appraisal process in Protective Motivation Theory and Rational Choice Theory, we examine the burdens (i.e., burden of ensuring patient privacy and the physical and mental burden) and benefits (i.e., perceived usefulness and mobility) in using EHR systems. We use surveys to collect responses from healthcare providers, including doctors and nurses. Our preliminary findings show that mobility, physical burden, and mental burden are associated with resistance behaviors. This study is one of the first efforts in uncovering the drivers of resistance from the perspective of an individual’s threat appraisal process.
1	The adoption of artificial intelligence (AI) within organizations is experiencing growing interest. Since AI has distinctive characteristics (automation – augmentation), it is unclear how both characteristics influence the adoption. Besides, current research calls for a configurational perspective within the adoption theory. Building on these research gaps, we use the technology-organization-environment framework (TOE) and combine the AI characteristics ‘automation – augmentation’ and ‘product or service – process’ with the organizational and environmental characteristics: differentiation strategy, resources, entrepreneurial orientation, and network support. We collected a sample of 104 questionnaires from top-level and middle-level managers as well as IT experts. Using fuzzy set Qualitative Comparative Analysis (fsQCA), we reveal four different solutions, namely internal integrator, supportive provider, powerful innovator, and rising enthusiast, that explain the adoption of AI. Moreover, we show that organizations adopting AI have an explicit focus on AI characteristics. Furthermore, we underline the importance of configurational thinking within the adoption.
1	Through the increasing diffusion of digital technology, also traditional areas such as the automotive context are expected to implement digital innovation. In these established ecosystems, demands for new digital capabilities increase, which could potentially be satisfied by actors from the IT ecosystem. However, it is unclear, which of these actors engage in foreign terrains and how they embed themselves in the established ecosystem. Analyzing rich data from both the IT and the automotive ecosystem, we find that migration of IT actors is associated with influence and embeddedness in the IT ecosystem and that migrating IT actors take essential positions in the automotive ecosystem subsequently. We complement IS ecosystem research by going beyond the dominant intra-ecosystem perspective. Furthermore, we contribute to digital transformation research by underscoring the increasing power of IT actors across contexts. Finally, we inform incumbent managers by revealing which IT actors might become valuable partners or hostile threats.
1	The logistics market is facing considerable changes driven by digitalization and the continuously growing e-commerce sector. This has prompted logistics companies to search for innovative solutions that complement the dominant forms of last-mile parcel delivery. Specifically, new drone-based delivery technologies have reached a late development stage and offer advantages over the current truck delivery such as lower emissions and faster services. However, research on drone delivery adoption by end consumers is still in its infancies. In response to this research void, we conducted two surveys on consumers’ drone delivery service adoption intention and related risk perceptions – one before and one during the COVID-19 pandemic. With the two studies, we find robust support for the influence of drone-related risks on technology adoption. We further unravel differences that can be attributed to COVID-19, which generally has risen the need for contactless services. We discuss implications for IS theory and management.
1	Digital transformation research and practice has received significant attention across the information systems (IS) field in recent years. However, this area of research is still in its infancy and in particular most studies focus on the initial adoption of digital technology rather than the subsequent long-term normalisation and sustaining of that digital transformation. Little is known about normalising a digital transformation (i.e., embedding and sustaining a digital transformation). This paper presents research-in-progress to theorise about the normalisation of a digital transformation. This research presents a longitudinal case study on Hewlett Packard Enterprise Finance Services (HPEFS) and their efforts to normalise a digital transformation over seven years (2013-2020). The contribution of this research is threefold. Firstly, this research demonstrates the novelty of applying normalisation process theory (NPT) to examine a digital transformation process. Secondly, NPT goes beyond traditional IS theoretical perspectives and draws attention towards embedding and sustaining digital transformations. Thirdly, we present a case study on how HPEFS normalised a digital transformation and we outline our future research avenues.
1	Organizations are increasingly embracing digital technologies to seize new business opportunities. Organizing visions help organizations to identify opportunities associated with new technologies. They come into being primarily because they serve three functions in the adoption and diffusion of innovations: interpretation, legitimation, and mobilization. There is limited work on organizing vision functions, despite their importance. Moreover, the functioning of organizing visions in the digital world warrants examination because the institutional production of organizing visions is undergoing fundamental changes due to core technology being digital and discourse and community formation taking place in digital media. We examined how organizing vision functions operate in the digital world. We examined the blockchain discourse on Twitter over seven years using computational and qualitative methods. We developed a theoretical framework for organizing vision functions and offer three conjectures on how they operate in the digital world. In doing so, we advanced the organizing vision theory.
1	Digital debt refers to the build-up of technical and informational obligations related to platform maintenance and evolvability that represent performance risks in an organization’s work processes, which may result in expensive maintenance cost and severe system failures in the future. While it is becoming increasingly prevalent as resource-constrained organizations across the globe chase the latest technology trends, little is currently known about its mechanisms through which it is accrued. Using a case study of a software vendor from South Korea who has served over 100 resource constrained clients, this study develops and presents a theoretical framework of digital debt accrual. More specifically, the framework suggests that digital debt accrual is a process of three stages: (1) Design Economization, (2) Piecemeal Development, and (3) Redundant Concurrency. Each of the stages results in a different form of digital debt that reinforces each other over time to form a vicious cycle.
1	Contact-tracing apps represent an algorithm-based technology with significant potential to help address the COVID-19 pandemic by facilitating swift case isolation. However, low adoption rates have prevented these apps from tapping their full potential. A key barrier to adoption has been citizens’ uncertainty surrounding contact-tracing apps. In this regard, algorithmic transparency may be a critical factor in fostering app adoption. In this study, we focus on one central aspect of algorithmic transparency — namely, transformation algorithmic transparency in terms of information disclosure on the app’s inner workings (TAT disclosure). Using an online experiment with 206 participants, we find no significant direct relationship between TAT disclosure and individuals’ installation decision; still, we do find a significant indirect relationship, mediated by actual comprehension of the app’s inner workings and trust into the app. Two moderating factors (benefit appeal and Coronavirus anxiety) did not show any significant effects. Theoretical and practical implications are discussed.
1	One of the central veins of the Information System (IS) discipline is to help practitioners in other fields to solve their real-world problems. This article focuses on building the decision support system for patients with Traumatic Brain Injuries (TBI) using Hidden Markov Models (HMM). Most of the existing literature focuses on the long-term outcomes arising from TBIs. Our development’s key distinguishing factor is that we predict the likelihood of stability within the next 24 hours to undergo a non-cranial surgery of polytrauma patients with TBI in acute settings. Our goal in this work is to build an HMM-based Bayesian optimization framework to help doctors to determine the optimal release schedule for TBI patients.
1	Personal healthcare information (PHI) disclosure is vital in leveraging artificial intelligence (AI) technology for depression treatment. Two challenges for PHI disclosure are high privacy concern and low trust. In this study, we integrate three theoretical lenses, i.e., information boundary theory, trust, and AI principles to investigate whether AI principles of empathy, accountability, and explainability can address these two challenges. We propose that AI empathy can increase depression patients’ privacy concern and trust simultaneously. This paradox of high privacy concern and high trust has to be addressed for successful AI deployment in depression treatment. The proxies of AI accountability such as AI company reputation and government regulation can help reduce this paradox. Further, we argue that explainability can moderate the relationships between this paradox (i.e., privacy concern and trust) and patient’s intention to disclose PHI. Overall, our expected results can provide significant implications to IS literature and practitioners.
1	Artificial intelligence (AI) applications are particularly promising in the field of medical imaging. Especially in radiology, research presents various AI uses cases, highlighting AI applications' potential to improve the quality and efficiency of healthcare. Further, despite numerous research projects of AI applications, an investigation from the real-world, practice-based point of view regarding AI applications is lacking. Consequently, little is known about medical imaging specialists’ perspective on AI applications. Following the Grounded Theory Methodology, we conducted 15 semi-structured interviews with medical imaging specialists. We derived four opportunities and five concerns representing medical imaging specialists’ perspective on AI applications.
1	To understand the HIT spillovers, we propose a healthcare referral network model that illustrates referral directions between and within different provider types. We model spillover effects of ambulatory EHR adoption on the inpatient cost of neighboring hospitals. Leveraging on a nationwide sample of 2,768 US hospitals across 13 years, matched with approximately 30,000 ambulatory care entities, we find that focal hospital's inpatient cost per discharge decreases as EMR adoption by neighboring ambulatory entities increases. Further, we observe that the effects are stronger in urban, densely populated regions with more ambulatory entities, and when the focal hospital and ambulatory entities are proximal and belong to the same health system. These findings reveal patient sharing and health information exchange as the underlying mechanisms. Our referral network model in conjunction with empirical evidence on the business value of information exchange can propagate a culture of sustained cooperation among providers.
1	In online healthcare communities, channel integration services have become the bridge between online and offline channels, enabling patients to easily migrate across channels. Different from pure online services, online-to-offline (On2Off) and offline-to-online (Off2On) channel integration services involve both channels. This study examines the interrelationships between pure online services and channel integration services. Using a panel dataset composed of data from an online healthcare community, we find that pure online services decrease patients’ demand for On2Off integration services but increase their use of Off2On integration services. Our findings suggest that providing healthcare services online can reduce online patients’ needs to visit physicians offline and convert physicians’ offline patients into online patients. We further confirm that the substitution effect of online services for offline visits is driven by physicians’ medical responses to patients’ online enquiries. Our work contributes to the literature on online healthcare communities and channel integration in delivering healthcare services.
1	In the unique context of an ongoing pandemic, this study aims to investigate the acceptance of mobile contact tracing applications (CTA) provided by the German government. Herein, the study develops and validates a research model based on established acceptance, privacy-, and health-related theories, which explains 80% of the variance of the intention to use CTA. The results of the structural equation model (n=656) indicate performance expectancy, effort expectancy, social influence, trust, privacy concerns, and anxiety to be relevant drivers of the intention to use CTA. This research contributes to theory and practical application by remarkable results, including the implementation of a collective-oriented task as an app performance, and identifying the effect of trust to be mediated via privacy concerns. Thereby, we offer valuable insights for research and institutions in charge of distributing the app.
1	Given a greater need for digital technologies that guide individual behavior during pandemics, this paper follows the Design Science Research (DSR) method to present a framework that conceptualizes viral transmission as a communication process occurring between human actors and introduces guided self-regulation as a possible means to disrupt this process. This paper contributes to DSR by providing a conceptual artifact that may be used to design, develop, and evaluate digital technologies in this important area of research.
1	Using hospital claims data, we study the effect of telehealth expansion on the disparities between care access in rural and urban areas during Covid-19. We use urban areas as the control group and compare the changes in patients' access to care before and after the telehealth expansion. We find that the rural-urban disparities in overall access to care (i.e., the total number of visits) remain unchanged after the policy. We further distinguish patients' visiting modalities and find enlarged disparities in patients' visiting modalities. In particular, urban patients substitute in-person visits with telehealth visits, yet rural patients have a much lower adoption rate of telehealth services and continued with the in-person visits. Finally, we perform visit-level analyses and identify patients' social determinants and physicians' characteristics associated with telehealth adoptions.
1	Team-based teleconsultation is recently embraced by countries worldwide to better serve patients and to reduce the burden of senior physicians. The success of this new practice hinges on effective physician-patient interaction. In this research, we conceptualize four types of the team leader’s actions toward patient interaction: direct and delegated informational support; self-oriented and team-oriented emotional support. We then incorporate lens of symbolic management to propose the influencing mechanisms of these actions on patients’ continuous engagement in teleconsultation. We utilize a rule-based recognizer to identify the leader’s actions from large unstructured text information generated in teleconsultation. This research is expected to contribute to the literature on online health IT, social support and symbolic management. It also offers practical insights to help better manage team-based teleconsultation.
1	To improve mobile health application (mHealth) use, developers often send push notifications to users. Messages that utilize dynamic user data can adapt to users’ changing behavior and have the potential to further improve mHealth use and behavior change by delivering the right support at the right time. However, existing behavior change theories are either static in nature or lack temporal specificity. We consider dynamic feedback loops proposed by Social Cognitive Theory to understand how goal and social feedback messages dynamically impact mHealth use during goal pursuit. Using a micro-randomized trial design (n=61) and a custom-developed mHealth application, our findings suggest that the impact of goal and social feedback messages on mHealth use varies based on if users are in the beginning, middle, or end stage of goal pursuit. Moreover, use of specific mHealth features subsequently impacts physical activity behavior. Theoretical reasons for these findings and future research opportunities are discussed.
1	As digital technologies are increasingly important in healthcare, it is important to determine whether and why potential users intend to use such health information systems (HIS). Several theories exist however mainly focusing on either healthcare or information systems aspects next to general psychological theories. We develop an Integrated Theoretical Model allowing to analyze the duality of adaptive and maladaptive appraisals and their influence on the intention to use HIS. We apply the Integrated Theoretical Model to the important domain of AI-based HIS for surgeries in order to gather empirical support. The results show that the model can be applied successfully and provide important insights which factors are relevant depending on the novelty of the AI-based HIS. We contribute to information systems literature by highlighting the importance to integrate aspects regarding disease and technology in a joint model. Practitioners can use the instrument to identify the most promising reasons for adoption.
1	Physician Review Websites (PRWs) help users select physicians by providing both structured and unstructured data on physicians and patients’ experiences with the physicians. In this paper, we study PRWs as an information system and investigate the adequacy of the available information. We first conduct an empirical study to understand the information that patients seek from PRWs through a survey instrument administered on 243 patients. We then use a topic modeling approach on approximately 81,000 text reviews of patients (unstructured data) to discover the information that is actually available on the PRWs. We then combine the findings from both the approaches and find that 9 out of the top 15 areas patients consider important are unavailable or partially available on PRWs, thus highlighting an information gap. Our work will help PRWs and physicians/hospitals restructure information on their websites and devise strategies to nudge patients to write reviews highlighting the desired information.
1	The healthcare sector has experienced a rapid accumulation of digital data recently, especially in the form of electronic health records (EHRs). EHRs constitute a precious resource that IS researchers could utilize for clinical applications (e.g., morbidity prediction). Deep learning seems like the obvious choice to exploit this surfeit of data. However, numerous studies have shown that deep learning does not enjoy the same kind of success on EHR data as it has in other domains; simple models like logistic regression are frequently as good as sophisticated deep learning ones. Inspired by this observation, we develop a novel model called rational logistic regression (RLR) that has standard logistic regression (LR) as its special case (and thus inherits LR's inductive bias that aligns with EHR data). RLR has rational series as its theoretical underpinnings, works on longitudinal time-series data, and learns interpretable patterns. Empirical comparisons on real-world clinical tasks demonstrate RLR's efficacy.
1	People are living much longer lives due to the advancement of medicine and healthy lifestyles. However, chronic conditions have become an increasing societal issue. Parkinson’s disease (PD), costing $15.5 billion per year for the U.S., is the second most common neurodegenerative disorder in the U.S. To manage PD, health profiling tools project a trajectory for senior citizen’s disease progression, which assist therapies and interventions. With the advancement of sensor technologies, motion sensors have emerged as an effective and efficient approach to collecting motion data from senior citizens. However, few studies have examined motion sensors in health profiling. In this study, we propose a novel deep learning model, the Adaptive Time-aware Convolutional Long Short-Term Memory (ATCLSTM), to accurately assess PD severity with walking experiments over time. We collected a publicly available dataset to test our proposed model. Results show that ATCLSTM significantly and consistently outperforms state-of-the-art benchmark models.
1	Online pharmacy becomes a convenient and efficient channel these days with greater access and lower product costs. Notwithstanding the fastest growing trend, low conversion rates have been formidable challenge to the platforms. To date, little is known about how platforms can scientifically track the health risk of online pharmacy consumers using drug consumption and leverage the predicted risk in the targeting strategies to provide business values for platforms. This paper adopted a novel Attention-based Graph Convolutional Networks to model patient’s future health risks based on drug consumption data. We further leverage the predicted health risk in the pharmacy targeting strategy. We found the effectiveness of a drug-refilling reminder is closely related to the predicted health risk. Moreover, we found for patients who undergo the health status change, reminder facilitates their information learning towards the new disease. For patients with stability, reminder facilitates their adherence to existing disease management.
1	Online stress management applications have the potential to reduce chronic stress, depression, cardiovascular diseases, and other severe mental and physical health problems. Basing on the clinical effectiveness of these applications and the relevance of user satisfaction for the success of information systems, we take a user perspective and investigate the role of application features for the satisfaction of potential users with such applications. We identify that there are 44 application features and reveal that these can be classified into different categories, depending on how they bring about satisfaction or dissatisfaction with the application. We show how each application feature influences potential user satisfaction and highlight that the influence can be linear, but also asymmetrical and non-existent. Based on the results, we derive recommendations for the development of online stress management applications and contribute that the complex relationship between application features and user satisfaction requires a feature- and context-specific investigation.
1	Medical errors are the third leading cause of death in the United States, claiming more lives than car accidents, HIV, and breast cancer combined, and cost approximately $20 billion every year. About half of these errors are electronic health record (EHR)-related and can thus be prevented. Patient portals as an online platform connecting patients and healthcare providers can play a pivotal role in mobilizing patients as their own best advocates to reduce EHR errors. Thus, we aim to investigate the effectiveness of patient portals and the optimal ways to incentivize patient engagement in error discovery and reporting. We start by distributing two surveys, one to physicians and healthcare administrators, and another to patients in order to understand the best possible incentives for maximizing patient engagement. These insights are then used to adjust our theoretical model derived from prior literature, before being tested in a field experiment via an econometric approach.
1	Observational data, such as electronic medical records in healthcare, have provided new opportunities for clinical discoveries. However, analyses using observational data are challenging due to the absence of a control group and selection bias between treatment groups. We propose a personalized treatment using counterfactual regression based on deep learning, which reduces concerns related to selection bias in a technical way. Using the data on the use of biologics in patients with rheumatoid arthritis, we estimate the individual treatment effect to determine which drugs to prescribe. In addition, we compare the individual treatment effect for each variable to show the relationship between clinical variables and the drug effectiveness. This study contributes to establishing evidence-based guidelines that recommend the optimal drug for each patient, thereby improving treatment responses clinically.
1	Drawing upon computer-mediated theory and hyperpersonal model, this study investigates the interplay of relationships underlying the adoption of telehealth, patient-perceived quality, and specialty risk categories driven by COVID-19. We identify the mechanism of the positive influence of telehealth adoption that different strategies of telehealth adoption (i.e., doctors who adopt telehealth and only accept video visits, doctors who adopt telehealth and accept both video visits and in-person visits, and doctors who do not adopt telehealth) is the driving force for the improvement of patient's perception quality. Further, our empirical results show that the connection between the degree of doctors’ telehealth adoption and patients' perceived healthcare quality is operating through the pandemic phases. Finally, our results suggest that the adoption of telehealth is more beneficial for a doctor with high-risk specialties than in low-risk specialties during the emergence phase of the pandemic outbreak.
1	Using the massive data sets, some industries such as health-insurers have started to roll out smart services that have the potential to provide clear benefits to both themselves and their customers. Health-insurers are providing a service called Pay-As-You-Live (PAYL) to encourage healthy behaviors by rewarding customers’ progress. When using a PAYL service, customers are expected to allow the insurer to collect current data about their lifestyle through existing and new data sources, such as wearables. The question, however, is – since PAYL services are inherently risky to their customers, will they adopt such a service? To answer this question, drawing on balance theory, this research-in-progress provides a conceptual framework to explore the underlying potential sources of contention consumers regarding their adoption of PAYL services. This study plans to apply the framework to the analysis of interviews with potential PAYL services participants in both the United States and New Zealand.
1	When transitioning from the hospital to the home context, elderly patients need motivation and help to engage in rehabilitation. Prior research supposes Virtual Coaches (VCs) to keep motivated and to serve as a companion. Yet, using VCs for home rehabilitation requires various technologies (e.g., smart sensors and machine learning) and stakeholders. In this paper, we report about a case study as part of the vCare project on using clinical pathways to set the procedural precept for integrating diverse technologies and stakeholders for home rehabilitation. We contribute to design artifacts and to design knowledge. For design knowledge, we systemize requirements that help design further modeling languages for modeling clinical pathways, derive further clinical pathways, or integrate further technologies or stakeholders in supervised care. Furthermore, our design artifacts may be used to investigate research ideas of other IS domains such as gamification or anthropomorphism.
1	Free-text feedback from patients is increasingly used for improving the quality of healthcare services and systems. A major reason for the growing interest in harnessing free-text feedback is the belief that it provides richer information about what patients want and care about. The use of computational approaches such as structural topic modelling for analysing large unstructured textual data such as free-text feedback from patients has also been gain traction lately. However, its use for generating insights is constrained by the apparent lack of statistical rigour and explanatory capability required for credible evidence in decision making. From the theoretical perspective, theory-building from unstructured textual data is also currently problematic in IS and health service research. This study presents an approach to address this challenge by integrating text analytics, predictive and quantitative models as part of a computational grounded theory approach to determine factors that significantly determine overall patient experience.
1	IS research is increasingly interested in well-being-oriented impacts of information systems (IS). Compassion is an underlying prosocial behavior that arguably empowers such well-being-oriented IS impacts. The overall purpose of this panel is to ask: "Should IS research pursue research on compassion as a serious stream of inquiry?" To address this core purpose, we raise two questions regarding the controversial roles of IS in a) raising suffering and b) facilitating versus hindering the expression of compassion. Our panel of junior, senior, and mid-career IS researchers take on positions for and against the core motion of including Compassion research in IS research as they argue each controversial question. We offer considerations/recommendations on issues salient to conducting and publishing IS-Compassion research. Audience participation is baked into the structure of the panel as an integral component. Our panel will appeal to a global IS audience interested in using IS research to impact positive change.
1	The Covid-19 pandemic has shocked the world and caused disruptions in social and business life. The pandemic has highlighted the need to create a better understanding of resilience, which refers to the capacity to tolerate existential threats, while maintaining core attributes and continuing to learn, adapt and develop responses to change. In particular, we need an understanding of how digital resilience enables individuals, organizations and societies to respond to threats such as the pandemic. This panel aims to create a conversation with the broader Information Systems community to: (1) Create a deeper understanding of the concept of digital resilience in the face of existential threats, as opposed to digital resilience in the face of organizational change; (2) Share key learnings of research on digital resilience with the IS community; and (3) Discuss key issues and research questions that are important to examine to improve our understanding of digital resilience.
1	This is a College of Senior Scholars' Panel covering the "Future of Work" that will take place during ICIS 20201 in Austin, Texas.
1	Increasingly scholars are called to engage in high-impact research projects, which are research efforts that contribute academic knowledge and provide benefits beyond academia, such as positive outcomes for organizations or society. Within the information systems discipline, journals are supporting high-impact research through special issues or recognition of high-impact research. The Association for Information Systems offers an annual AIS Impact Award for scholars with exceptional, high-impact research contributions. However, how should we define high-impact research? How do we measure, evaluate, and reward high-impact research? How do we ensure that organizations and societies benefit more than they are harmed through the research effort? These issues, among others, will be discussed during this panel. Audience participants will engage with the panelists through a range of methods, including electronic polling, audience questions, and a fish-bowl discussion to identify how we can make an impact with information systems research.
1	The Covid-19 global pandemic has accelerated the adoption of digital technologies in societies. These developments are evident across institutions; even faith-based institutions that have previously been hesitant have experienced pressure to adopt digital technologies. In this realm, Meta launched two initiatives in 2020 where members of different religious communities engage with Meta to dialogue about applying digital technologies to their missions’ purpose and express and celebrate their faith. The impact of digital technologies utilization in this context has led, among others, to the boundary spanning behavior of religious communities. We use this case of Meta for Faith and leverage institutional theory to identify the affordances of digital technologies in faith based institutions. Thus, we contribute by explaining how technology is transforming faith-based institutions.
1	The Open Source Software Development (OSSD) community has been influenced dramatically because the COVID-19 pandemic makes work from home (WFH) the new way of working. While existing studies demonstrate the ef ects of WFH on the productivity of developers, few studies shed light on its impact on contribution to OSSD. The current study investigates the impact of disruptive events like COVID-19 and the changing working conditions on sustained contribution towards open source software development as a community service. The preliminary results based on an analysis of the GitHub ecosystem illustrate that the overall OSSD contributions have witnessed an increase post WFH. Our study has important theoretical and practical implications for scholars and open-source platform owners and users.
1	The police turn to social media to disseminate information, enforce laws, or involve people in crime investigation. This paper’s goal is to explore the police’s use of Instagram based on a comparative study of Instagram accounts of official police in selected socially deprived and non-deprived neighborhoods from a social justice perspective. Using digital trace data and interpretive findings, we seek to develop new insights and theories in the light of social justice. Our work aims to contribute to literature on digital technologies and social justice and thus to building a more inclusive, sustainable society.
1	The increasing complexity of managerial decision-making for digital innovation activities accelerates cognitive biases like escalation of commitment (EoC). Decision aids (e.g., AI agents) can assist managers in avoiding EoC scenarios. However, how AI-based decision aids affect EoC in this context remains a critical yet understudied topic. To address this gap, we develop a theoretical model and propose a randomized controlled post-test vignette experiment with a fictive decision-making simulation to study the de-escalating effect of an AI-based decision aid in the digital innovation context. Our model accounts for moderating (AI familiarity, personality traits) and mediating (decision aid reliance) factors. By entangling the de-escalating effect of AI agent decision aid in decision-making scenarios about digital innovation projects we contribute to the digital innovation, AI agent, and the EoC literature. The future implementation of the proposed research design lays the foundation for designing AI agent decision support systems that de-bias managerial decision-making.
1	Blockchain opens opportunities in a data-driven society. A prerequisite is building ‘trust in codes’ to support businesses across financial transactions, data exchange, identification, and decision-making. However, the media continuously introduces the friction of blockchain with several century-old regulations. Therefore, the PDW suggests three features of the blockchain architecture incompatible with the regulatory framework: decentralization, industrial multiplicity, and cross-borderness. Furthermore, the PDW discusses re-balancing between transparency and privacy, innovation and customer, and international and jurisdiction; and defining the legally responsible persons and the government’s intervening in the algorithm. Finally, the PDW will expand information systems theories to regulations from socio-economic-technological views.
1	Deep Tech is receiving increasing attention from investors and policy as a means to solve the most pressing societal challenges, leverage new business opportunities, and increase global economic growth. To respond to the increasing need to fundamentally understand deep tech and its implications for business and innovation policy, this PDW sets out to establish a shared understanding among information systems researchers of how we can build strong research within IS and with other management sub-disciplines. The PDW will start with a dynamic panel discussion among IS scholars with interests in deep tech from different perspectives in the disciplines (i.e. three broad themes), followed by semi-structured roundtable discussions with participants to shape emerging research opportunities for future deep tech research. The results of the workshop will be integrated into a joint statement that spells out the conceptual background of the topic and sheds light on future research directions.
1	Participants of this Professional Development Workshop will get an introduction to the theoretical foundations, history, and key methodological principles and practices of dialectical inquiry, and an analysis of the state-of-the art as well as exemplary cases of dialectics in IS research. The two 90-minute sessions feature presentations, group work, and plenary discussions. No prior knowledge of dialectics is required and ICIS participants at all levels are invited to attend. While early career researchers (PhD students and junior faculty) are expected to extend their methodological repertoire and knowledge of the IS literature, IS scholars with a more advanced understanding of dialectics are expected to benefit from stimulating discussions with a vibrant and inclusive group of junior and senior IS scholars passionate about dialectics. Learning Objectives •	Describe the theoretical and methodological foundations of dialectics. •	Explain different approaches to dialectical inquiry and related concepts and frameworks. •	Select relevant cases for dialectical inquiry and frame them appropriately. •	Analyze contemporary IS phenomena using dialectical concepts and frameworks. •	Reflect on the potentials and limitations of dialectical inquiry in IS research.
1	Design science research (DSR) aims to generate knowledge about innovative solutions to real-world problems. A comparably new stream of research, DSR has matured methodically, and is increasingly catching the interest of researchers, specifically for its potential to contribute to problem solving in society and the economy. Since research methodology curricula develop slowly, however, DSR is still underrepresented in most curricula and courses on research design and methods, and we lack guidance on what and how to teach in a DSR course in a way that enables junior academics to conduct DSR according to high standards. We report on teaching DSR methodology both on PhD and Master levels and for both managerially and technically oriented student populations. Our interactive on-site and distance formats have been refined over 14 years. The PDW presents an effective syllabus, teaching material and experience from conducting over 25 courses with students from over 20 countries across all three geographic AIS regions.
1	This is a video recording of the Keynote Address and AIS Awards Ceremony during ICIS 2021 on Monday December 13, 2021 in Austin, Texas. ICIS 2021 is a hybrid conference.
1	This is the video recording of Keynote II by Sukumar Rathnam and Best Paper Awards Ceremony during the ICIS 2021 Conference in Austin, Texas. ICIS 2021 was a hybrid conference.
1	App platforms have been plagued with concerns for users’ data privacy, with prior research pointing to two key user side trade-offs: privacy vs. utility and privacy vs. price. Motivated by literature on platform network effects and data effects, we shift our focus to the developers’ side, where developers’ choices play a key role in the overall privacy of an app. We focus on two main factors: choice of monetization strategies and use of third-party services (software development kits, abbrev. SDKs). Using data on 121,808 apps on Google Play, Android platform’s app market, we analyze the relationship between apps’ monetization strategies and developer’s requests for privacy sensitive information. Our results a) confirm prior research showing a negative relationship with request for privacy related information when apps are paid and b) show that compared to ad-based monetization, developers request for more privacy sensitive data when they monetize their apps with ‘in-app’ strategies (allowing future transactions to take place within the app).. Additionally, , we also investigate patterns underlying developer’s choice of third-party services i.e., typical versus atypical choices in the deployment of third-party services. Using unsupervised text clustering on 518 SDKs as seen in our data, we find that greater deviation from typified SDK choices is strongly and positively associated with increased requests for privacy sensitive data. Taken together, our results suggest that the mechanisms driving a developer’s requests for privacy sensitive requests point to a third functionality vs. privacy trade-off on the app developers’ side in app economies.
1	Consumers of crowdsourced data expect experienced contributors to report more useful data than inexperienced contributors. Guided by selective attention theory, we propose to examine the veracity of this expectation in two types of data crowdsourcing platforms – an online review platform and a citizen science platform. We contend that as the experience of data contributors increase, it leads to selective attention, causing contributors to report similar data over time. We, therefore, predict that data diversity – the number and type of attributes present in contributed data – and the usefulness of contributed data will decrease as contributors gain experience in a crowdsourcing task. We propose an experiment to test these predictions and find from our first study that increasing experience from participation in crowdsourcing tasks may be detrimental to collecting diverse and useful data from crowds.
1	A growing amount of research is dedicated to digital markets for knowledge work services and contracting decisions as one of their most critical bottlenecks. Large-scale studies with average samples exceeding 144,000 observations have considered over 70 variables explaining contracting decisions. Our review aims to facilitate the convergence towards commonly agreed categories of variables and overarching models of contracting decisions in this context. To mitigate the risk of propagating deflated p-values associated with large sample studies, we propose a robust version of vote-counting techniques. The principal findings of our research suggest that only a few variables related to the client-worker relationship, the bid and individual worker characteristics have a practically significant effect on worker selection, and that auction success is primarily affected by project value and client experience. These findings lay the groundwork for future research that will lead to better explanations of contracting decisions.
1	User-generated contents such as reviews and ratings are very important for online platforms. To better leverage such content, platforms enable users to vote on the helpfulness of reviews. The importance of helpful votes (received by a review) to the platforms, consumers, and reviewers, are substantive. Factors found to impact the helpfulness of a review include, among others, the novelty of the content in the review and the review’s credibility characteristics (i.e., source credibility and rating credibility). To better understand how consumers’ perceptions of review helpfulness are affected by these factors, we investigate the moderating impact of credibility on the influence of review novelty on helpfulness. We find that source credibility and review novelty are substitutes in terms of their contribution to review helpfulness. On the other hand, rating credibility positively moderates the effect of a review’s novelty on its helpfulness and complements review novelty.
1	Recently, some e-commerce platforms have started to provide advertising attribution tools for sellers, claiming to help them measure the effectiveness of multichannel advertising and optimize their budget allocation. It is unclear, however, what impact the attribution tools will have on key stakeholders involved in attribution, due to potential credibility concerns (e.g., the platform may underestimate the effectiveness of other publishers' ads). Through the lens of a game-theoretic model, we analyze whether the platform, sellers, and other publishers can benefit from the adoption of attribution tools. Our results show that adopting the attribution tools can sometimes be detrimental to sellers. Another interesting finding is that sellers’ adoption of attribution tools can benefit regular publishers even when the platform understates the effectiveness of their ads. Moreover, we find that introducing the attribution tools can sometimes hurt the platform. Our findings provide important implications for the key stakeholders involved in the attribution.
1	Many e-commerce platforms develop product ecosystems to capture and retain consumers. A product ecosystem consists of focal products, complementary products and supporting services, to deliver an entire experience to the consumers. Using the example of Amazon Devices, the authors explore how ecosystem bunding promotion affects consumers’ product evaluation. Results show that, compared to consumers who purchased the products themselves, those who got the same products for free through ecosystem bundling promotion tend to write longer and more positive product reviews, but give lower product ratings. Besides, the negative effect of ecosystem bunding promotion on product ratings is stronger for low-involvement products than for high- involvement products. In the future, we plan to explore how consumers’ evaluation of a bundled product may affect their perceptions of other products in the ecosystem. This research will enhance our understanding of the product ecosystem and how the elements within the same ecosystem are connected.
1	Viewers’ engagement with content on community-based question answering (CQA) platforms is critical for both platforms and content creators. In this study, we examine the effect of emotional intensity of an answer on the number of votes, comments and rewards that the answer receives and how this effect is moderated by answer length. There are two possible countervailing mechanisms by which emotional intensity affects viewer engagement, and we theorize that which mechanism dominates depends on whether the answer is long enough to provide sufficient information. Analyses of data from Zhihu, a CQA platform in China, support our theorizing. The results suggest that the effect of emotional intensity on viewer engagement is negative for short answers but positive for long answers. This study contributes to the literature on online knowledge sharing and emotional expression in user-generated content. It also has practical implications for platforms and content creators.
1	There is growing evidence showing that sellers manipulate product reviews either by embellishing themselves or by snipping at their competitors. We develop an analytical model to examine the effect of product prices and platforms’ commission on sellers’ review-manipulating strategies and profits under the “Verified Purchase” policy (e.g., Amazon.com). We find that under this policy, a higher platform’s commission rate may discourage both two types of review manipulation. As for the price effect, sellers’ snipping increases with their prices but decreases with competitors’ prices while their embellishment mainly depends on the commission rate. For the platform, we identify a non-monotone relationship between profit and the commission rate, which is driven by sellers’ review manipulation. Our results suggest that platforms should carefully set their commission rate to discourage sellers from manipulating reviews and meanwhile maintain a certain level of profit under the “Verified Purchase” policy.
1	An optimal information design is studied for efficient fleet redeployment in ride-sharing market. Inefficiency in form of mismatch between demand and supply could arise where drivers lack of information about market demand. Yet, providing complete public information, such as Uber surge map or Lyft prime time zone, to entire fleet might incur inefficiency as well (Braess 1968). A market designer, who observes both the demand and supply distribution, could send private message to persuade driver and thus induce coordinated relocation actions from the fleet. An ex-post consistency condition requiring the signalled matching likelihood must be equal to the realized distribution, was imposed for credible persuasion. Contrary to the current market practice of monetary incentive for redeployment, we provide a novel information design approach, achieving efficient outcome with strictly lower cost.
1	In this study, we examine the business impact of social media participation on individual seller’s sales activities, exploiting a natural experiment wherein a large social marketplace for second-hand goods (Xianyu) unexpectedly removed its social media features (Fishponds). We leverage a novel dataset capturing both social media participation and sales activities for individual sellers, comprising of more than 180,000 transactions during a multi-week period around the event. To estimate the causal effect of the event, we employ a difference-in-differences design. We also assess the robustness of our results to an alternative estimation approach, namely generalized synthetic control. Our results consistently demonstrate that sellers who initially participated in social media channel experienced a significant decline in their sales after the channel shutdown. Exploring heterogeneity in the effects around a variety of seller and product listing features, we find evidence consistent with the notion that the sellers who benefit most from social media participation are those who face the greatest difficulty establishing trust with buyers. Our results provide robust evidence that social media participation thus serves a crucial role in fostering trust in social marketplaces. Our work offers nontrivial managerial implications, both for marketplace operators and individual sellers.
1	Today's digital era facilitates the rise of crowdfunding markets by allowing investors to better gather and process information about crowdfunding projects. Yet the abundance of information creates greater distractions for investors and causes a hike in their cost of attention. Unlike traditional funders of entrepreneurs, crowdfunding investors aggregate information individually, and so understanding how distractions that divert investor attention influence investor behavior and crowdfunding performance is very important for entrepreneurs as well as crowdfunding platforms. To that end, we develop a model wherein investors with limited attention aggregate personalized information about (reward-based) crowdfunding projects and conduct comparative analyses on how rises in investors’ marginal attention cost (associated with greater distractions) affects investor attention, investment decisions and so crowdfunding performance. We then exploit a novel measure of distraction---news pressure---to test the effects of distraction on investor engagement and crowdfunding performance empirically, and the results support our model predictions.
1	A vast majority of charity projects fail to attract donor attention due to an oversupply of projects on crowdfunding platforms. They suffer from a lack of pecuniary incentives, making it particularly challenging to garner donor support. We empirically examine a novel method to gain donor attention by addressing the middle-period malaise effect in charity crowdfunding projects. We suggest that using the concept of top-page promotion helps charitable projects increase their amount of fundraising. We find outreach during less attended and lower performing periods—middle periods— helps gather 24 times more daily donations, outweighing initial period promotion which yields only 11 times more donations. As charity projects struggle to capture donor interest, using our new measure, composite status ratio-to spotlight projects during the lower performing middle periods could offer a crucial new way to overcome middle-period malaise and achieve charitable crowdfunding success across a broader range of social and environmental settings.
1	Firms are increasingly adopting crowdsourcing contests to acquire innovative solutions to challenging problems. As problems become increasingly complex, no individual may have the full range of requisite knowledge to develop an effective solution. There is a paucity of theory on the process that combines contestants’ diverse expertise via teaming. In this paper, we systematically explore: a) with whom to team up; b) when and how contestants should form teams; and c) the outcome of strategic teaming to develop a comprehensive theory from a (re)combination perspective. Using simulation experiments and empirical validation, we find that collaboration among contestants with different expertise increases team performance albeit conditionally depending on the extent of knowledge overlap between contestants and timing of team formation. More interestingly, there is a misalignment between contestant-level and platform-level outcomes. These findings provide new insights on contestant performance and crowdsourcing quality and have implications for the design of crowdsourcing platforms.
1	Algorithmic Management (AM) is becoming ubiquitous in new work arrangements, where intelligent algorithms automate control activities previously performed by human managers. Existing research points to opacity as a driving force behind resistance behavior to AM. Analyzing an online forum for ride-hailing drivers shows that drivers engage in individual and collective sensemaking to create stories and alternative truths that help them cope with opacity by giving them a sense of control and agency. Not only did we find different types of sensemaking ‘theories’, but we also to shed light on the manifold interaction effects between opacity, sensemaking and resistance behavior. Interestingly, opacity acts as catalyst for resistance behavior only when financial pain is involved. The void which opacity leaves is then filled with malevolent sensemaking, even intensifying resistance, while trust can mitigate negative consequences of opacity. Our nuanced understanding behind opacity will provide a much-needed foundation for further research in AM.
1	Platform operators invite complementors for their participation in value creation (e.g., platform sides such as application developers). While platform operators impose control on who gets to participate on each side of the market and how, the pursuit of growth due to competitive pressures leads to open-access policies. A consequence of providing open-access to a platform is that rivals can also act as complementors. In this study, we examine how platform operators acting as complementors can engage in a form of learning not available to outsiders and use this learning to achieve performance gains over the rivals. We find that the positive effects of complementor learning on performance deteriorate as market dynamism increases, unless platform users face high switching costs in a market. Furthermore, time to start learning plays a critical role, where learning can start “too early” due to negative implications of network effects in a nascent market.
1	Racial bias is an important issue and has been observed on sharing economy (SE) platforms. However, few studies examine how racial bias changes during large-scale social movement. We examine this issue with five million Airbnb host records in the Unites States during the Black Lives Matter (BLM), one of the largest social movements since the Civil Rights era. We find that racial bias evolves during the BLM movement, affecting host performance differentially. Critically, host performance varies by supply-side contextual heterogeneity, such as racial composition in neighborhoods. The results highlight the dynamic process of racial bias in the sharing economy, which continues to be reshaped by social movement such as the BLM as well as local environment. The paper contributes to an understanding of racial bias as a dynamic process, especially during race-related social movement that spills over to online platforms and affects user behavior.
1	Online synchronous platforms, such as live streaming, spend tremendous efforts engaging users in a real-time setting, which has gained considerable popularity very recently. While the existing literature finds that the group size of peers positively affects user engagement on asynchronous platforms, the effect of group size remains unexplored in the context of synchronous streaming. In this work, we leverage the unique raid functionality, an exogenous increase in live streaming viewers, and empirically examine how group size affects users’ real-time commenting engagement. Collecting and analyzing chat history in 13,382 playbacks on Twitch, our result suggests that existing viewers (users who engage in the live streaming channel before the raid) tend to engage less after the raid. The findings in this paper indicate a negative effect of group size on viewer engagement in the synchronous communication setting, which theoretically extends the prior literature in user engagement and crowd effects.
1	While digital labor platforms market entrepreneurship to workers in the sharing economy, their algorithmic management (AM) also has negative consequences. Earlier studies suggest that workers’ perceived lack of information about the AM practices negatively affects their control over their work and their feelings towards the organizations. Yet, besides first evidence that an increased information transparency leads to higher worker cooperation, there is a dearth of research on the effects of providing high levels of information about the applied AM criteria and the assigned jobs on workers’ intention to stay with the organizations. Drawing on principal-agent theory, organizational algorithmic transparency and work autonomy, I perform an experimental ridehailing study addressing this gap. Providing high levels of the two information types separately leads to higher staying intentions, whereas combining high levels of both information types shows no significant effect. The study contributes to the research stream about digital labor platforms using AM.
1	Crowdsourced idea evaluation is often seen as a viable, less expensive alternative to traditional idea selection mechanisms such as expert evaluation. However, previous research found several limitations on the reliability of crowdsourced decision making, especially when simple majority-based aggregation mechanisms are used. In response, researchers suggest new, advanced aggregation mechanisms that rely on additional information (e.g. the confidence of a judge) to overcome these limitations. Therefore, this study investigates whether advanced aggregation methods based on the judge’s confidence can be used to increase the reliability of crowdsourced idea evaluation. To answer this question, we post an idea evaluation task to a crowdworking platform, aggregate the judgements with different mechanisms and compare the predictions from each mechanism with results from expert evaluation. Our findings suggest that confidence-based mechanisms reduce the risk of discarding high quality ideas, at the cost of a slight increase in the misclassification rate on low quality ideas.
1	The gig-economy literature is rife with conflicting accounts of autonomy and empowerment versus exploitation and marginalization. To understand such contradictions, it is necessary to measure perceptions of algorithmic autonomy-support (PAAS); yet no validated instruments exist. To address this gap, we develop a theoretically-based measure for PAAS using Mackenzie et al.’s (2011) well-cited scale development process. To execute our scale development process, interviews were conducted with Uber drivers to support item generation; this was followed by content-validation with subject matter experts to develop and validate our instrument. Lastly, statistical validation was conducted using data collected from a total sample of 435 Uber drivers. The results of our survey confirm that: (i) PAAS is a second-order formative measure with four first-order reflective constructs; (ii) our 13-item scale demonstrates adequate psychometric properties; and (iii) PAAS is positively, and significantly, related to perceived organizational support and job satisfaction. Research contributions and applications are discussed.
1	Misinformation often has dire real-world consequences, so user-generated content (UGC) platforms have adopted misinformation mitigation strategies aimed at protecting their users. We study Reddit’s quarantine policy, a prominence reduction strategy that reduces the visibility of misinformation on the platform. We empirically assess the effectiveness, as well as spillover effects, of quarantine. We conceptualize misinformation spreading within problematic communities as being motivated by impure altruistic preferences. We further conceptualize user reactions to quarantine using attachment theory. We find for low-attachment behavior, quarantine diminishes misinformation contribution in the quarantined forum and pushes it to a topically related but ideologically neutral forums. For high-attachment behavior, quarantine does not affect misinformation contribution in the quarantined forum but increases the dispersion of misinformation on the platform. Our research sheds light on the efficacy, as well as potentially unintended consequences, of using prominence reduction strategies to fight misinformation.
1	The spread of COVID-19 has led to new challenges on organizations of every size. This also affects collaboration, which since then has had to be more digital than ever. While traditional collaboration tools, such as video- and audioconferences have reached their limits in terms of interactive and flexible collaboration, the development of multi-user virtual reality (VR) technology is introducing new possibilities. We investigate which conditions have an impact on the intention to collaborate in VR environments. To this end, we conducted a multi-user VR experiment and then interviewed participants individually and in focus groups on their collaboration behaviors. We were able to identify technological-, task-, and user-related conditions, which could be distinguished in necessary and sufficient conditions. Our research has helped to create evaluation opportunities to determine what conditions should be met to foster collaboration in VR.
1	Action patterns occur wherever people perform work. Research has provided important insights about how action patterns form, change and dissolve over time. However, it has neglected the question of why actors choose to enact certain action patterns. In this research-in-progress, we shed light on the socio-cognitive factors underlying the formation of action patterns. We focus on GitHub and argue that so-called ‘rockstars’ exert social influence on other developers’ action patterns over time. As a preliminary step, we showcase the plausibility of this argument by examining an actual project where we see indications that the influence of a rockstar is reflected in the types of actions that are predominantly used. We will continue this research by including (1) more projects and rockstars, and (2) the analysis of action networks to test if action networks converge with those of rockstars over time.
1	One culprit in the sharp increase in political polarization is social media use. We conduct two studies to explore this relationship. In Study 1, we explicate the mechanisms that link social media use to polarization—interpersonal political tension and political identity salience. We test this model with data from 2,820 Americans on two of the most popular platforms for political engagement: Facebook and Twitter. In Study 2, we drill into affordance use and ideological content on Facebook and Twitter to develop a theory of social media affordances and political polarization. Our theory explains the routes by which different ideological content are linked to a certain group of social media affordances and how this cultivates tension that activates political identities and thus polarization. We test our theory via a survey of 492 Facebook and Twitter users. We discuss the contributions and implications of our two studies for theory and practice.
1	Social media has become a vital platform for voicing product-related experiences that may not only reveal product defects but also impose pressure on firms to act more promptly than before. This study scrutinizes the rarely-studied relationship between these voices and the speed of product recalls in the context of the pharmaceutical industry. Using drug enforcement reports and social media data, we investigate whether social media can accelerate the product recall process in the context of drug recalls. Results based on discrete-time survival analyses suggest that more adverse drug reaction (ADR) discussions on social media lead to a shorter time to recall. We propose the information effect and the publicity effect to better understand the underlying mechanism. Estimation results from two mechanism tests support the existence of these conceptualized channels. This study offers new insights for firms and policymakers concerning the power of social media and its influence on product recalls.
1	Online platforms promise to alleviate social stratification by encouraging and facilitating resource exchange in the digital economy. While the distribution and flow of online attention are deemed critical to online businesses, there has been limited discussion on stratification and resource exchange. This paper takes the first step to empirically examine stratification and the causal attention flow on social media platforms. Leveraging detailed observations of visitor traffic on a live-streaming platform, we adopt the data-driven method integrating machine learning and econometrics analysis to identify the causal structure of resource flow among broadcaster strata. Our findings reveal that although there is stratification on social platforms, resources are not stagnated. Attention flows between all strata have an overall positive impact on the platform, alleviating the worries of strata consolidation caused by resource stagnation.
1	The COVID-19 pandemic accelerated the implementation and adoption of new features in web-conferencing systems (WCSs), such as custom backgrounds (CBs) that mask the real physical background with a custom, i.e., user chosen, background. In this work, we explore what types of backgrounds are selected and why they are used by analyzing text and images from Twitter. We find that different types of CBs allow users to satisfy psychological needs in virtual collaboration and identify emerging practices regarding the selection of backgrounds, in general, and with respect to gender differences. Our analysis reveals that CBs showing real objects get commonly replaced or augmented by artificial, non-photorealistic content such as cartoon style memes, scenes of computer games or company logos. By leveraging novel image analysis techniques, we also contribute methodologically to social media analytics.
1	In innovative practices today, teams alternately collaborate face-to-face and online via technological interfaces. Starting from a theoretical lens that characterizes technology as an actor, we use data from an ethnographic study of an innovation contest to show how alternating online and offline activities affect trust dynamics. In doing so, we distinguish activities that both build and breach trust. Our most useful finding is that online face-to-interface practices influence the dynamics of trust in offline face-to-face interactions in two directions: Based on their affordances, digital technologies can foster the building of trust as well as the breach of trust in innovation teams that work together offline and online.
1	Understanding how online communities sustain their activities is a pressing need in a time where new forms of organizing and knowledge work develop in and through technological platforms. While research on online communities is rich, it is surprisingly disjointed and unable to provide a comprehensive explanation of how an online community achieves sustainability. In this review, we first highlight sustainability as a capacity that is achieved by an online community through several mechanisms. Next, drawing upon Complex Adaptive Systems theory, we organize past literature into four themes, and relate them to define six mechanisms that are needed for the development of sustainability. We therefore add order to the diverse literature and clarify how they fit in the big picture with regards to online communities’ sustainability. This also helps in better situating future work. This review also presents an agenda for future research aiming to have a greater relevance on theory and practice.
1	Social media (SM) has evolved into a common technology to establish and maintain relationships with friends, family, and colleagues. Yet, recently SM users have used the platforms less for their personal relationship management. Thus, the opportunities for companies to interact with their customers were reduced. This paper conceptualizes how companies need to approach a commercial friendship with SM users so that the user provides access for the firm representative to their personal sphere online. The focus is on understanding the heuristics users would apply based on certain decision factors. We build on the relationship marketing literature that suggests the willingness of SM members to engage in relationships with business representatives is tied to social, psychological, economic, and customization benefits. Our short paper presents the research model and describes the proposed research method to aid in understanding the factors motivating SM users to accept commercial representatives as a real friend.
1	Given current trends in digitalization and the increased need to understand ways to improve remote work, it's imperative for IS to understand the emergence of non-compliant behaviors of workers to ESN implementation. This research-in-progress proposed a model into how privacy concerns might trigger workarounds while implementing Enterprise Social Networks (ESN), expanding our understanding of the types of information-sharing workarounds that might be pursued and the steps that might explain such decision process. The model focuses on privacy triggers that might react to this visualization affordances of these technologies. The proposed model emphasizes the important role of privacy management and the alternative responses to managerial trends that favor openness and transparency for organizing. It contributes to the literature on how privacy and transparency jointly affect human behavior and, more specifically, technological use.
1	Previous theoretical research and empirical studies recognize a positive correlation between intra-team ties and team performance. This paper goes beyond correlational analysis and attempts to estimate the causal effect of intra-team ties on team performance. Our dataset with over 4 million game teams allows us to control confounding factors by comparing within pairs of adjacent networks. We find that in 5-member teams, one intra-team tie increases win rate by 0.26%. We further study how the heterogeneous effects of intra-team ties can depend on network structure measures. We find that when teams have low centralization, no cliques or no structural holes, one intra-team tie can increase win rate by 1.01%, 1.31%, or 0.44% respectively. On the other hand, when teams have high centralization, cliques, or structural holes, the effects are not significant. We conclude by discussing theoretical contribution and managerial implications as well as our plan for future work.
1	We contribute to the ongoing discussion on the design of legal regulation of harmful online content by analyzing the effect of the first regulation, the Network Enforcement Act (NetzDG), recently adopted in Germany on content generated on Twitter. This law aims at combating hate speech on large social networks in Germany by facilitating the reporting of hate speech for users and obliging social networks to timely react to this reporting. In a difference-in-differences framework, we measure the causal effect of the law on the intensity of hateful and attacking language used in tweets comparing tweets by German and Austrian users before and after the introduction of the law in Germany. Our results show a significant and robust decrease in average hate intensity. The effects are driven by the decrease of hate intensity among users with many followers and users who employed a strongly toxic language prior to the law while tweeting rarely.
1	Information System research has shown an increasing interest in the study of social media as tools for organizing and coordinating social movements. Studies find that online communication networks are instrumental in social media-enabled social movements. Yet, little is known about the temporal evolution of such networks. This research-in-progress fills this research gap by exploring how online communication networks unfold in online social movements campaigns. Specifically, it proposes a conceptual framework to study the evolution of the campaigns’ networks by looking at network structure and content. This study features an analysis of the Movember health campaign on Twitter and uses various methods to analyze Twitter trace data over time. Preliminary findings from the temporal analyses reveal that online social movements campaigns appear as hybrid forms of collective and connective action. Our future work will be developing a process model explaining networks’ evolution in online collective action
1	In early 2021, the finance world was taken by storm by the dramatic price surge of the GameStop Corp. stock. This rise is being, at least in part, attributed to a group of Redditors belonging to the now-famous r/wallstreetbets (WSB) subreddit group. In this work, we set out to address if user activity on the WSB subreddit is associated with the trading volume of the GME stock. Leveraging a unique dataset containing more than 4.9 million WSB posts and comments, we assert that user activity is associated with the trading volume of the GameStop stock. We further show that posts have a significantly higher predictive power than comments and are especially helpful for predicting unusually high trading volume. Lastly, as recent events have shown, we believe that these findings have implications for retail and institutional investors, trading platforms, and policymakers, as these can have disruptive potential.
1	News posts are popular among social media users. Since the reading of news is critical for both social media platforms and news providers, it is common practice for news providers to use attention-grabbing tactics, such as hyperbole, in an effort to pique user’s interest. However, there is scant empirical evidence to support that these tactics are effective. Our paper explores how and why the use of hyperbolic statements in news headlines influences users’ interest and intention to read the news. Drawing on humor and psychological contract violation literatures, we developed a theoretical model and proposed competing hypotheses. We conducted two experiments to examine the impact of hyperbole and test the competing mechanisms. Our findings challenge the prevailing notion that the use of attention-grabbing tactics, such as hyperbole, in news headlines are effective.
1	Driven by artificial intelligence (AI), in-home voice assistants (VA) emerge as an integral part of an individual's everyday life. As social structures and processes expand into a digital society, information system (IS) research started to examine this intertwinement and identified information technology (IT) identity as a concept to grasp IT use behavior. However, research on IT identity in connection with VAs is rare. To close this gap, we developed and empirically tested a conceptual model (n = 322 VA users). Our results show that symbolic value and hedonic motivation strengthen IT identity, while privacy concerns relate negatively. Moreover, IT identity enhances deep use and social presence and mediates the relationship between symbolic value/hedonic motivation and deep use/social presence. Our study provides critical theoretical and practical implications regarding use behavior and social presence of in-home VAs. Moreover, we contribute to a deeper understanding of IT identity.
1	The advancement of information and communication technologies (ICTs) has profoundly changed the way individuals address societal issues. Increasingly, scholars examine how individuals can use information and communication technologies, such as social media, to effect change. Individuals from marginalized groups are using social media to voice concerns related to the need for social inclusion among those who are marginalized. Social media enables individuals to make their voices heard to battle inequal opportunities and insufficient resources within these groups. We develop our research model using the theoretical lens of organizational and employee voice. We describe our intended survey-based data collection effort from individuals within a marginalized group to identify factors affecting one's decision to voice concerns on social media about the experiences of marginalized individuals. The findings of this study will inform additional opportunities for research regarding factors that affect one's decision to voice concerns on social media to effect social change.
1	We study the economic and social impact of two campaign design features -- visual cues (gaze direction) and textual cues (story narrativity and story length) along with their interactions -- on medical crowdfunding outcomes. We synthesize objective self-awareness theory from social psychology and transportation theory from communication into an information processing framework. We compile a unique dataset with 59k medical crowdfunding campaigns from the Gofundme platform in the U.S. during 2017-2019. Leveraging computer vision and text mining techniques, we extract interpretable gaze cues from images and story narrations from texts. We find that the presence of direct gaze and story length increase medical crowdfunding outcomes, and that the presence of direct gaze can substitute story narrativity when the story narrativity is low. Our findings speak directly to the design of a better medical crowdfunding campaign and shed light on the nuanced interactions among visual and textual unstructured data.
1	To counteract climate change, organizations are providing online carbon footprint calculators (CFCs) for consumers, to facilitate their sustainable consumption. Even though consumers are increasingly using these CFCs, their effectiveness remains an open question. Based on interviews of CFC developers, drawing on encoding-decoding framework, and the concepts of representations of users and usage, we uncover the organizations’ enablers and barriers to the design of effective CFCs. We show how developers perceive their enablers and barriers propagate, rippling through the design process, affecting the CFC artifact’s symbolic meaning and perceived effectiveness: the rippling of enablers reinforces CFC’s symbolic meaning for developers as solid, facilitating the consumers’ reading it as effective. The rippling of barriers, reinforces CFC’s symbolic meaning for developers as precarious, provoking consumers’ reading it as indicative, otiose, or exacerbating. We discuss the implications of the enablers and barriers, especially regarding the technical infrastructure, to the organizations’ design of effective CFCs.
1	The field of digital social innovation (DSI) seeks to address societal challenges, including fighting poverty and inequality, strengthening justice, human rights, and gender equality, and addressing environmental issues affecting the planet and climate by leveraging digital technologies. In this article, we present a multilevel design framework for DSI. It originates from a reflection resulting from the learnings of an action design research project that aimed to develop a digital donation system for homeless neighbors. The framework draws upon the research stemming from DSI, value sensitive design, and service science spheres. It allows value-sensitive interaction in DSI projects to be captured, structured, and reflected from their overarching purposes to detailed design decisions and individual actions. We demonstrate the application of the presented framework by analyzing episodes of value election and inscription of the digital donation project.
1	The rapid growth of the ‘Buy Nothing’ group phenomena provides an alternative to the global Sharing Economy platforms run by companies such as Uber and Airbnb. Created and run by a loosely-affiliated network of volunteers, using Facebook groups and other simple consumer technologies, Buy Nothing groups create online spaces for community members to locally share their unneeded possessions, and some services, without commercial transactions or fees. This short paper reports on preliminary research into the Buy Nothing phenomena, using key informants from urban locations in the Western US. The goal of the research is to create conceptually-driven, design science knowledge about the successful design and operation of non-commercial Sharing Economy platforms. By better understanding how to create more decentralized, democratic, and non-commercial Sharing Economy alternatives, this research hopes to contribute to the wider discussions of sustainable consumption and social inclusion through technology.
1	For issues receiving heightened media attention (e.g., natural disasters or social movements), charitable crowdfunding platforms offer an easy way of generating donations. However, since media attention is typically short-lived, little is known about the long-term impact of temporary attention spikes on donation behavior for different types of crowdfunding campaigns. To address this gap, we examine how the Black Lives Matter movement and the associated media attention after the death of George Floyd have influenced fundraising behavior for campaigns supporting the black community. By applying a differences-in-differences approach on a dataset from GoFundMe, we find that campaigns with a smaller funding goal only see an increase in donations for about one month, compared with over four months for campaigns with a larger funding goal. If charitable crowdfunding platforms aim to help smaller campaigns, they need to signpost donors to such campaigns well beyond the temporal spike in the associated media attention.
1	Information technology (IT) enables access to up-to-date information and provides a basis for collective self-organizing and problem solving which are critical for addressing crises. However, less is known how specific and vulnerable groups of populations, such as older people, use IT for addressing and solving crisis. This paper investigate how older people adopt IT to become active in solving a political crisis in Belarus in late 2020, where the country has experienced peaceful mass protests, widespread violation of human rights, and repressions. Our findings highlight key processes underlying IT adoption and engagement in crisis solving by older person and illustrate that IT adoption and engagement in crisis management are interrelated.
1	Based on its popularity and wide dissemination eSports (i.e., the competitive play of video games) can be considered a tangible manifestation of the digital culture. Despite several positive outcomes and innovative opportunities, this playful evolution came with some questionable aspects as well affecting contemporary societies. One such instance is the gender gap in professional eSports, which describes the circumstance that the professional eSports scene is strongly male dominated, and females are substantially underrepresented. Despite some infrequent attempts, empirical research to better understand the gender gap in eSports is still scarce. Building on expectancy theory, we propose an empirical approach to comprehend achievement motivation as an intra-individual process consisting of different components (i.e., expectancy, instrumentality, first- and second-order valence) that in parts should differ based on gender. For this, we plan to use a survey and co-variance-based-statistics to compare the motivational processes, while controlling for demographic variables.
1	The mental health crisis is growing in every country and has been labeled the 21st century silent epidemic. Affordability, accessibility, and lack of trained personnel are among the most pressing issues, acting as obstacles for arriving at a comprehensive solution for this critical global problem, which has been further exacerbated by the COVID-19 pandemic. Telehealth has the potential to address all the above-mentioned issues. We will conduct an online experiment with 500 subjects who will view a video vignette of a conversation between a patient and a psychiatrist and will be told that the patient has either ADHD or schizophrenia and that the therapy session is either face-to-face (i.e., in-person) or remote (i.e., virtual). Information disclosure is the first step in seeking mental health treatment. Our results will inform scholars and practitioners about the features of systems that encourage people to share/disclose risky and stigmatized information.
1	Online extremism and radicalization on social media are major concerns of governments, social media companies, and civic organizations worldwide. Online extremist threats are further amplified through conspiracy theories surrounding the current pandemic, increased social isolation, and more time spent online. Evidenced by the recent extremist attack of the US Capitol, the consequences of failure to intervene are dire. Current extremist interventions online, however, are not comprehensive and timely enough to contain the threat of extremist messages. Research calls for new methods that address, both, the cognitive and technical components to counter extremism. Accordingly, we explore the potential of social bots as AI-based automated agents on social media to online counter radicalization processes online. We apply dual-process theory to social bot intervention messages in an empirical study.
1	Over the past decade, as private organizations have increased the amount of data that is being processed, governmental institutions and other non-profit organizations (who shall be referred to as public organizations) have followed suit. Both types of organizations carry responsibilities regarding personal data processing, yet as clients of public organizations are not always able to avoid services provided, additional questions arise regarding the transparency of processing. Currently, this transparency is lacking, causing privacy concerns of clients to rise due to what is termed informational uncertainty, or the lack of information regarding the content, the process, and the purpose of data processing. By taking an information systems (IS) perspective, this research shows five principles to be effective in enhancing the privacy of the general public for public organizations: transparency, accountability, purpose specification, security, and ownership. In addition, results show the influence of the absence of time and experience on these principles.
1	eLoyalty provides a major competitive advantage for online businesses, however it is difficult to define. To understand the underlying mechanisms of eloyalty, a neuroimaging study was conducted, identifying possible “neural” activations that point to antecedents of the users’ eloyalty. Our results show that antecedents to eloyalty might be associated with an activation of the bilateral dorsolateral prefrontal cortex (PFC), and the right orbitofrontal/ventromedial cortex. Furthermore, a non-hypothesized deactivation of the left superior dorsomedial PFC was identified. This activation pattern can be related to increased pleasantness and reward attribution of the target website, as well as higher associated familiarity and user preference. Based on this it is discussed that antecedents to eloyalty may be associated with neural lock-in effects. These lock-in effects may be elicited through continuous, positive, and pleasant user experiences which ultimately lead to an emotional attachment that is “neurally encoded” into the user’s brain.
1	A growing body of research underscores the importance of empowering online users through web customization. Yet, little research on web customization has hitherto shed light on the practice of advertisement (ad) customization and particularly ad quantity customization (AQC). To uncover the potential of AQC as a tool to foster user engagement, we conducted a large-scale randomized field experiment comprising 17,241 visits to a news website. Our findings reveal that users spend more time on the website and explore more pages when users can (vs. cannot) customize the quantity of ads. Most interestingly, users engage more with the website when given the possibility of AQC than when experiencing a website that is ad-free by default. Additionally, users respond particularly positively to AQC when using mobile (vs. stationary) devices. With our research, we contribute to literature on web customization and provide insightful guidance whether and how website providers can harness ad customization.
1	Individuals spend increasing amounts of time on social networking sites (SNSs), despite them potentially harming their well-being. This is at odds with theories postulating that people strive to engage in hedonically beneficial activities. However, research shows that people frequently mispredict the affective outcomes of future actions, and then base their behavior on such faulty predictions. In this research, we test the hypothesis that people mispredict the hedonic effects of using SNSs and base their usage time on their faulty predictions. In two studies, we demonstrate that people mispredict both the positive and negative affective outcomes of Instagram use. Participants’ forecasted positive affect of Instagram use was too optimistic, while also being related to higher amounts of time spent on the platform. This implies that users overinvest their time in Instagram use. Results further show that negative emotions during Instagram use were overestimated. These predictions were not associated with usage time.
1	The cold-start problem is salient in the current online two-sided markets. Theoretically, machine-generated content can serve as the information signal, thus helps to address the cold-start issue. To empirically examine whether machine-generated content helps mitigate the cold-start problem, we study the impact of machine-generated content on dataset adoption in a leading online public dataset community (Kaggle). We found that machine-generated content helps to solve the cold-start problem in the online public dataset community in two ways. First, we show that machine-generated content shortens the time needed for datasets to get their first adoption and that machine-generated content increases the dataset adoption in the initial stage and increases the dataset diffusion rate. Second, our results show that the positive effect of machine-generated content is stronger for dataset sharers who lack reputation signals. Our research opens a discussion on the role of machine-generated content in mitigating the cold-start problem in online public dataset platforms and also other two-sided markets.
1	Recognition of activeness inferiority can motivate users to take on physical activities more frequently. Yet, at times, users can be daunted by other users’ greater activeness and become reluctant to take part in further relative evaluation. Drawing on the self-evaluation maintenance model, we integrate relative evaluation research, past works on social network, and the motivation literature into a research model. Specifically, this study examines the effects of peer activeness and network overlap on users’ need satisfaction and need frustration, and how these needs influence subsequent social fitness app usage. Results of our longitudinal field experiment show that peer activeness and network overlap jointly influence users’ perceived autonomy satisfaction and perceived competence frustration. Furthermore, whereas perceived autonomy satisfaction stimulates promotional compensation, it inhibits preventive compensation, and whereas perceived competence frustration dissuades promotional compensation, it advances preventive compensation. The theoretical and practical implications of the findings are discussed.
1	Techno-stressors lead to detrimental consequences for employees, e.g. job burnout. To mitigate such consequences, it is essential to understand how employees cope with techno-stressors. We draw on the broader technostress literature and the concept of dyadic coping and theorize that employees also cope with techno-stressors together with another close person at work, e.g. a close colleague. We use two sequential studies. Study 1 uses interviews to identify for which techno-stressors dyadic technostress coping is most relevant. In the ongoing Study 2, we will conduct a survey to evaluate whether dyadic technostress coping reduces the effect of the techno-stressors identified in Study 1 on job burnout. Our findings demonstrate that dyadic technostress coping is not equally relevant for all techno-stressors. With our results, the study contributes to technostress coping literature by developing a theory-based understanding of coping behaviors going beyond individual coping and broadens the view on actors of coping.
1	Advancements in social media have complicated issues of unauthorized disclosure of organizational information by adding new avenues for disclosure. In this short paper, we examine how emotions influence employee’s unauthorized disclosure of organizational information via social media. We find that emotions increase the permeability of the boundary between an individual’s professional and personal lives, and consequently make them more likely to disclose organizational information without authority on social media. Our initial findings suggest that positive emotions (i.e., satisfaction and excitement) arouse the intention to reveal more about success, accomplishments, and opportunities that could be classified as confidential or unauthorized disclosures. Negative emotions (i.e., anger and anxiety) can trigger an individual to use social media to vent or seek emotional support and potentially lead to unauthorized disclosures. Our future work will test a theoretical model that explains the emotional dimensions of individuals disclosing organizational information via social media without authority.
1	The increasing application of Conversational Agents (CAs) is rapidly changing the way customers access services. In practice, however, CAs are frequently flawed, such as they often misinterpreting users’ requests. Even through future advancements in technology, it remains unlikely that a CA will be flawless and error-free. Thus, understanding how to design CAs so that users are more willing to overlook errors constitutes an important area of research. In this regard, our study positions the human-like design of CAs as a potential way to ensure service satisfaction when errors occur. We examined the effect of human-like designed flawed CAs by conducting a two-condition experiment with 421 participants, analyzing its effect on users’ emotional state and service satisfaction. Our results show that the human-like design of flawed CAs leads to more positive emotions, which improve users’ service satisfaction. Therefore, designers should adopt a human-like design to prevent dissatisfaction with flawed CAs.
1	As one of the leading online review platforms, Yelp leverages a gamification design, Yelp Elite Squad (YES), to engage Yelp users. To become a YES member, users need to demonstrate a high level of community involvement, and therefore, they should significantly contribute to the Yelp community in the pre-YES period. To date, however, it is not clear how YES members behave after they receive this status. In this paper, we leverage a large volume of Yelp data to examine this question empirically. Using a dynamic panel analysis, we find that a YES member will produce more content in the subsequent period. We further find that if a user receives the YES status for two consecutive years, the most current YES status has a positive effect on users' future contributions, but this effect is not as strong as the effect of a first-time status recipient. This research contributes to the online user engagement and gamification literature. We also offer practical implications for online community design.
1	As artificial intelligence (AI) can increasingly be used to support decision-making in various areas, enhancing the understanding of human-AI collaboration is more important than ever. We study delegation between humans and AI as one form of collaboration. Specifically, we investigate whether there exist distinct patterns of human delegation behavior towards AI. In a laboratory experiment, subjects performed an image classification task with 100 images to be classified. For the last 50 images, the treatment group had the option to delegate images to an AI. By performing a cluster analysis on this treatment, we find four types of delegation behavior towards AI that differ in their overall performance, delegation rate, and their accuracy of self-assessment. Our results motivate further research on delegation of humans to AI and act as a starting point to research on human collaboration with AI on an individual level.
1	User engagement has been widely discussed as a critical factor for the success of digital platforms. Among various mechanisms for increasing user engagement, of particular interest are gamification techniques. In this study we focus on competition and use a behavioral economics lens to investigate how competitive structures presented to users in the form of leaderboards affect user engagement. We construct a novel leaderboard design, which we refer to as “local leaderboards”, to create competitive structures unique to each user by showing the competitors around them and compare this design against the traditional “global leaderboards”, which typically show only the top ranked users. Our randomized field experiment suggests that the localized leaderboards may be more effective than the traditional leaderboards in influencing the level of user engagement on digital platforms. Our data analysis using generalized linear regression models with fixed effects reinforces our findings and provides additional insights.
1	Existing empirical studies examined how review and reviewer characteristics impact review helpfulness evaluation, assuming that helpfulness votes are casted based on voters’ perception of the review’s helpfulness and the helpfulness ratio aggregated at the review level can accurately represent review helpfulness. However, this assumption has not been explicitly tested, partially due to the lack of vote-level dataset. In this study, we utilize a unique vote-level dataset and investigate how context-driven biases (i.e., confirmation bias and bandwagon effect) and voter-level heuristics (i.e., cognitive inertia and declining perfectionistic strivings) impact the (un)helpful votes. Our result shows that voters’ helpfulness evaluation is vulnerable to these heuristics and biases. It suggests that past review-level research may over-estimate the impact of review-level and reviewer-level characteristics on review helpfulness evaluation, and the assumption that the review level helpfulness ratio is able to reflect consumers’ perception of a review’s helpfulness may not hold.
1	In-store mobile shopping apps can reduce queues in supermarkets and improve the consumer's shopping experience, yet their usage rates remain low. Despite the advances in mobile commerce literature, only scant research discusses the design inhibitors of in-store mobile shopping apps. This paper uses the Theory of Technology Usage Inhibitors to investigate inhibitors in designing in-store mobile shopping apps. We adopt a topic modelling approach to analyse 1800 Apple store reviews to identify the most significant inhibitors in a UK supermarket in-store mobile shopping app. Our findings identify redundancy, malfunctioning, overcharging, fiddliness, poor support, difficulty registering and objecting benefits as the critical design inhibitors. Our study expands the theory of technology usage inhibitors (Cenfetelli 2004) in the context of in-store shopping apps by identifying and conceptualising design-based inhibitors that discourage use. This paper contributes important design insights and recommendations to support app designers and retailers in effectively designing in-store shopping apps.
1	Despite participating in a growing number of tournaments, but can e-sports players continuously learn and improve? This question is deeply connected with experiential learning theory since players could acquire experience through repetitive participation but with increasingly smaller marginal returns. Observational learning is touted as a complementary learning mechanism by drawing on external sources of knowledge (or others) to accelerate learning progress. This study hence advances a dynamic and integrated research model that scrutinizes the moderating influence of peer- and rival-based observational learning on the relationship between experiential learning and team performance. Analyzing data gathered on 8,410 teams and 29,078 players in e-Sports games, we reveal that peer-based observational learning attenuates the negative effect of experiential learning on players’ incremental absorptive capacity, whereas rival-based observational learning reinforces the negative relationship between experiential learning and players’ incremental absorptive capacity. Players’ incremental absorptive capacity, in turn, enhances team performance.
1	Business process modeling plays a fundamental role in organizations that are restructuring their processes to meet the challenges of increasing digitalization and globalization. However, the geographic distribution of process stakeholders, the abstract non-contextual modeling languages, and the resulting low motivation to participate make process modeling difficult. In this paper, we present a design science research approach that resolves these problems using virtual reality. Based on empirical evidence, we first developed design principles to increase employee engagement. Subsequently, a virtual reality application was generated, that enables the placing of process models in realistic and immersive working environments. We developed the application continuously in four evaluation cycles and finally tested it in terms of usefulness in three field studies. The results of this study contribute to more context awareness in business process management and provide design knowledge for future industrial virtual reality applications.
1	Digital signages are the most diffused in-store technologies with their effects on perceived store atmospherics and behavioral outcomes relying heavily on their visual design and context. To further inform and understand the effects of visual design, this research investigates the effect of digital signage designs from the lens of Fuzzy-Trace Theory which differentiates between a verbatim and gist-based processing of (visual) information. The designs were embedded within a store environment and without this context to validate the design's effect in context. The results of our study using functional near-infrared spectroscopy show activated brain areas in the medial prefrontal cortex (PFC) accompanied by a lateral PFC deactivation, which indicates cognitive relief and increased emotional processing for gist-based designs. In store context, the cognitive relief is no longer found, yet the emotional attribution was still found. These results provide several theoretical and practical implications for the visual design of digital signages.
1	Understanding what makes a review helpful is important for consumers and online retailers. However, empirical research has shown mixed findings regarding the effect of review length on review helpfulness. Additionally, review helpfulness has not been studied adequately in terms of review-related features (syntactic complexity, cohesion, review quantity). Drawing on the elaboration likelihood model, this study developed a research model that incorporates four review-related features and product type. Specifically, we posit that four review-related features (i.e., syntactic complexity, cohesion, review length, and review quantity) impact review helpfulness differentially. These relationships vary depending on product type. To validate the research model, we collected a dataset from Amazon.com to conduct data analysis. This research will not only contribute to a better understanding of review-related features that affect review helpfulness but also provides practical insights to online platforms and consumers regarding how to select and write a helpful review.
1	In this paper, we study how the decoy effect, a well-established context effect, impacts the effectiveness of personalization systems. By conducting a controlled experiment and using a real-world movie recommendation system, we find different behavioral effects in personalized and non-personalized settings. Including a decoy item in a set of recommended items negatively impacts the effectiveness of the personalized recommendations, while it does not hurt the efficacy of a non-personalized one.
1	People often exhibit present-biased preferences where they tend to give more weight to gratification or costs that are closer to the present when completing a challenging task. One plausible approach to tackle this problem is to divide the whole task (program) into several small chunks, termed microtasking in our paper. Yet, it is far from clear when the microtasking strategy is helpful and how to design the microtasks under different settings. Our paper aims to fill these gaps by investigating when the firm should adopt the microtasking strategy and how to decide the optimal dividing and pricing of the program. We find that when the initial growth rate of the diminishing-return program is very low, the firm should make the starting microtask relatively easy to complete. Interestingly, our results show that microtasking cannot always benefit the firm when the initial growth rate of the diminishing-return program is not very low.
1	Workarounds are a common phenomenon in the context of information systems use and while they can be necessary for the completion of work, they can also have negative consequences for individuals and organizations. Therefore, understanding why workarounds occur and how the need for them can be reduced is important for the effective design, implementation, and use of information systems. Affordances offer a useful theoretical perspective for the study of information systems and technology workarounds. This research demonstrates through a study of nurse’s work practices on hospital patient care units that insufficient affordance potency and in particular, the concept of the affordance cliff, offers a useful explanation for workarounds and an effective mechanism for understanding how systems can be designed and adapted to improve post-implementation use.
1	Envy constitutes a serious issue on Social Networking Sites (SNSs), as this painful emotion can severely diminish individuals' well-being. With prior research mainly focusing on the affective consequences of envy in the SNS context, its behavioral consequences remain puzzling. While negative interactions among SNS users are an alarming issue, it remains unclear to which extent the harmful emotion of malicious envy contributes to these toxic dynamics. This study constitutes a first step in understanding malicious envy’s causal impact on negative interactions within the SNS sphere. Within an online experiment, we experimentally induce malicious envy and measure its immediate impact on users’ negative behavior towards other users. Our findings show that malicious envy seems to be an essential factor fueling negativity among SNS users and further illustrate that this effect is especially pronounced when users are provided an objective factor to mask their envy and justify their norm-violating negative behavior.
1	Driven by digitalization and accelerated by the COVID-19 pandemic, the ‘war for talents’ is increasingly shifting towards the digital world, transforming traditional personnel selection systems into information systems. While these digital personnel selection systems (DPSSs) make use of video interviews or gamified assessments to realize benefits in terms of cost-efficiency and sustainability, the use of digital technologies has a significant impact on user behaviors and selection outcomes. Against this backdrop, we present a common, interdisciplinary conceptualization of DPSSs in terms of a framework for technology-driven factors, which we inferred from a systematic literature review. Our interview-based evaluation showed that such an overview is urgently needed in research and practice. Our findings represent a theory for analyzing and add to the descriptive knowledge by providing an information systems lens on personnel selection. With this understanding of technology-driven factors, we provide a profound basis for developing user-centric DPSSs.
1	The digitalization of one’s personal data is of growing interest in the information systems discipline. Parents digitalizing the private health data of the most vulnerable in society—infants--in the form of infant monitoring is of particular interest. This study examines emotions and affordances within the context of remote infant monitoring. Through an affordance lens, this research conducts an exploratory field study to investigate the role of emotions in the use of an infant monitoring system (IMS). A qualitative analysis of 2,741 online reviews identifies several emotional and behavioral affordances available through the use of IMS features. Existing in tension with the affordances, constraints limit the affordances of the technology. Several positive and negative outcomes result from both the affordances and constraints. This research furthers the digitalization of self literature through the development of the Affordance-Constraint-Outcome model that reveals the tension between affordances, constrains, and positive and negative outcomes.
1	Organizations have become increasingly aware of the role of information integration in emergency response. Despite their efforts to transform, emergency organizations still face barriers against information integration. We misunderstand how actors practically deal with these barriers. This research addresses this gap through a longitudinal case study of an emergency organizational field in France. In 7 years, we collected data mainly through 70 hours of interviews and 166 hours of observation. We followed a grounded theory methodology. We also relied on practice as a lens to induce a comprehensive model of soldiering, a set of trans-local activities and broader outputs that both support and undermine information integration. Our findings enrich knowledge on information integration. They can help emergency organizations manage the dark side of information integration by supporting early detection of soldiering features.
1	Online reviews help potential customers make a more informed decision by reducing the inherent information asymmetry between sellers and buyers. One of the problems reported by the literature is, however, that reviews tend to be written by customers with more extreme experiences and expectations, biasing reviews. To address this issue, review platforms use elicitation measures, e.g. emails, to encourage greater participation and reduce this review bias. Our research examines whether the timing of email invitations influences the response rate. By analyzing a unique data set (Trusted Shops), we find that invitations sent during daytime lead to a significantly higher response rate. More specifically, we identify invitation patterns for individual time slots that are associated with a higher or a lower response rate. Our fine-grained analysis of invitation timing contributes a novel aspect to the online review literature and helps online review platform operators improve their email invitation success rates.
1	Based on asymmetric effect, this study attempts to classify distributive, procedural, and interactional justices in an online service failure recovery into different types and explore their interaction styles. The sufficiency of justice is first evaluated with the level of disconfirmation. Justices were then classified into different types based on whether the variable is significantly associated with satisfaction under negative and positive disconfirmation conditions. The interaction effect is determined by regressing the perceived justice with satisfaction under different levels of disconfirmation of another justice. The results show that distributive justice is a satisfier, procedural justice is a dissatisfier, and interactional justice is a delighter. While interaction justice is complement by distributive and procedural justice, the latter two are substitutes.
1	Online interactive texts, danmuku, have become an essential tool for people to maintain participation in online activities. This research investigates why people are willing to participate in sending danmuku from the perspective of the herding effect. We used the dataset from two popular live broadcasting platforms, Douyu and BiliBili, and use a policy shock on each platform for identification to examine the herding effect. We find heterogeneous herding effect on danmuku volume, scale, and frequency: while the herding effect can positively affect people’s volume and scale of sending danmuku, the effect is not significant on frequency.
1	In this study, we examine the effect of mobile app unbundling on new app adoption and post-adoption engagement from the perspective of path dependency theory that views unbundling results as a function of users’ pre-unbundling experiences. To identify the impact of unbundling on individual usage behavior, we exploit a natural experiment in which one app unbundles a feature and launches it as a new app while another app keeps the bundled app. Our results provide empirical evidences of a causal relationship between mobile app unbundling and users’ adoption decisions and post-adoption engagements. First, we find robust evidence that unbundling on average generates user churn while enhancing adopters’ post-adoption engagement volume. Second, we find that usage regularity and the recency effect facilitate the unbundling results. Our findings elucidate adoption and post-adoption engagement mechanisms in a mobile app unbundling context, enabling targeting schemes that mobile app providers can utilize.
1	Grounding our work in the literature on emotion dynamics and affect theory of social exchange, we theorize an emotion-centric model for visual social media posts comprising both static and dynamic aspects of emotions. Specifically, we hypothesize and empirically evaluate the relative salience of ‘peak visual emotion’ displayed in a social media post versus the ‘emotional variability’ and ‘emotional inertia’ of posts from a profile owner in determining the longevity of user engagement. Through this study, we make three key contributions. First, we emphasize the importance of considering dynamic emotional features in addition to the static emotional attributes embedded in the context of visual social media posts. Second, we reiterate the economic rationale for examining the ‘longevity of user engagement’ with the visual social media posts as a measure of social media success. Finally, we complement prior studies offering novel emotion dynamics constructs and invaluable insights for social media practitioners.
1	The information systems (IS) discipline has long been critically questioning its identity to determine its central research avenues, its distinction from other disciplines, and the future directions for the field. Although this question is central to all stakeholders of the IS field, so far the debates have been conducted primarily in research papers, editorial commentaries, and opinion pieces published by influential IS scholars. Our study explores how the broader IS community engages in the discourse about IS identity by examining podcasts as an increasingly popular means of communicating IS viewpoints. We apply a podcast ethnography to study the IS podcast universe, consisting of 51 shows with 660 episodes. Our preliminary findings offer insights about the stakeholders, podcast topics, and intellectual core of the audio tracks that shed light on the role of podcasts in constructing and reflecting on IS identity.
1	Although dialectical inquiry has been sporadically and selectively applied in the Information Systems (IS) discipline, and premier IS journals increasingly welcome dialectical inquiries, we lack methodological guidance on its application and evaluation, hindering its adoption as an important and valid IS research method. In response, we present a critical analysis of general dialectics literature and 63 extant IS dialectical inquiry publications in 18 journals spanning three decades, revealing that there is a growing and sizeable community of IS researchers using dialectical inquiry explicitly or implicitly to examine how sociotechnical phenomena change. Based on this analysis, we synthesize six principles for dialectical inquiry that are firmly rooted in dialectical philosophy, evidenced in IS publications, clearly distinct from each other, and together comprehensive. As such, our contribution can help IS researchers, reviewers, and editors to extend and solidify their methodological repertoire.
1	The proliferation of connected devices into every corner of the planet gives rise to a world of mesh computing. In this conceptual paper, we analyze the implications of this emerging computing environment for information systems (IS) research. We first discuss how mesh computing can be related to but differs from other views of computing that emerged along the history of IS research. We then advance a provocative perspective for studying the uniqueness of mesh computing—digital mesh. We conclude with a discussion of the opportunities and challenges for future IS research. The discussion deals with the need to expand IS use research from effective use to generative use and IS design research from substantial architecture to processual architecture. These shifts, however, come with new research challenges for which we propose process structure analytics as a valuable methodological solution.
1	New constructs are routinely introduced and validated along with their measures to capture emerging IS phenomena. Although statistical validation procedures abound in the literature, there remains confusion on how to best engage with the first few steps of the construct development process. Consequently, authors may have difficulties navigating these crucial steps, and reviewers may be unsure what standards they should enforce. This short paper seeks to clarify the standards that are espoused and enacted by the IS community as part of its construct development activities. We selected 96 construct development focused papers published in IS journals between 2000 and 2020, and we systematically coded how their authors engaged with the conceptualization, item generation, and content validation steps. Our preliminary findings indicate that despite some apparent homogeneity, construct development papers employ widely divergent practices, some of which may not be adequate to address the most pressing methodological challenges of our time.
1	We present material-affective assemblage thinking as an alternate theoretical lens to sociomateriality in the study of scalar phenomena that scale multidimensionally and multitemporally. We argue that given their unbounded characteristics, such phenomena are challenging to study using sociomateriality that anchors ontological realism to intra-action contingent on observed practices, operating within bounded spaces. Inspired by Deleuze and Guattari, we propose an ontology of immanence where there are no inherent boundaries between bodies, only co-existence, and extension of assemblages through affective flows. Conceiving three conceptual devices – assemblages as virtual worlds, affects as flows, and time-space as rythmscapes – we exercise our theoretical lens on the case of the George Floyd big bang to illustrate how material-affective assemblage thinking can be used to research phenomena that flow as an ongoing process of becoming, agglomerating larger territories in co-functioning that transcend material, non-material, time, and space boundaries.
1	Field experiments are an integral part of the social sciences as they hold the promise of generalizable scientific findings. Yet, notwithstanding new opportunities brought upon by digital technologies, they are conducted seldomly, due to associated costs of alignment between industry and researchers. Against this background, we propose a new method for digital natural field experiments that offers an improved organizational and technical process for industry-academia alignment by limiting the requirement for the industry partners to change their systems for the experiment implementation. The method is demonstrated in a field setting, exploring the influence of carbon offsetting options on the purchasing behavior of consumers.
1	This paper uses the pre-renaissance philosopher Nicolas Cusanus (1401-1464) and his concepts of ratio (calculating rationality) and intellectus (a relation to not-knowing) to assist in understanding the differences between human and artificial intelligence. The intention is to contribute to the ongoing discussions and debate pertaining to AI implementation and use, arguing that philosophy can be of ample use when it comes to understanding different types of intelligence in the digital world. The presented conceptual framework outlines the human and the artificial intelligence in terms of their characteristics in relation to Cusa’s ratio and intellectus. This helps to apprehend the different forms of intelligence and, more specifically, their strengths and how they operate.
1	The information systems field has a long-standing interest in how individual actions co-evolve with social structures. Yet, studying the exact process of co-evolution turned out to be elusive. We propose a novel way to study this co-evolution using digital trace data. By analyzing the sequence of individual actions through digital trace data and the process of emergent social structuring expressed in collective action patterns, we can measure the recursive influence of individual actions and the process of emergent social structuring over time. We illustrate our approach using data from GitHub. We analyze the social structuring expressed through collective action patterns of a project and compare them with the idiosyncratic action patterns of individual developers. Our research has implications for studies that examine the connection between social structures and individual actions. Our approach particularly allows us to investigate the role of power and social influence in structuration processes, which has been typically neglected in existing research.
1	The growing relevance of algorithmic systems, including artificial intelligence, for processes of value creation raise theoretical and practical interest in the conceptualization of actorhood and the balancing of human and technological agencies within socio-technical ensembles. Prominent theories of the IS discipline still reflect a human-centric conceptualization of agency, which we deem challenged by advances in machine learning technology. We therefore motivate a revised theorizing of the concept of agency with a socio-technical lens. For that, we apply an inductive top-down theorizing approach. In this short paper, we present the first inductive step by describing tensions, oppositions and contradictions in the discourse on agency in IS literature of the last 30 years in the AIS Senior Scholars’ Basket of journals. The preliminary findings uncover a conceptual and ontological incoherence surrounding the concept of agency in IS scholarship, and a gap between reviewed publications and the agency claims of algorithmic systems.
1	Theory is an essential part of design research and helps us to explain what we see or guide what we design. In the paper, we shed light on how kernel theories are used in developing design principles in Design Science Research (DSR). We do this by reporting on a systematic literature review, from which we have extracted a set of six mechanisms to operationalize kernel theory. Each mechanism consists of an activity (e.g., “transform to” or “derive from”) and an application point (e.g., meta-requirements or design principles) representing wherein the chain of concepts the kernel theory was used. The paper reflects on what we have learned about the use of kernel theories and translates this into recommendations and issues for further research. We provide researchers with guidance to use kernel theories more efficiently and give a big picture of the possibilities of kernel theory operationalization.
1	Machine learning models are widely used in many business contexts, but there is a growing concern that strategic individuals may manipulate their features to obtain desirable outcomes from the machine learning models. This paper offers a theoretical analysis of the impact of feature manipulation on the performance of the machine learning models and the payoffs of firms in an online lending context. Contrary to the common belief, our interesting finding is that manipulation may not be harmful to a firm under some circumstances. Instead, it could increase the classification model's performance and raise a firm's payoff and the social welfare when high-quality individuals manipulate more. Overall, our findings suggest that manipulation can bring strategic value to machine learning models instead of just being a harmful activity. Our findings provide useful insights for feature engineering and lay a foundation for future research about optimal strategies to cope with manipulation activities.
1	Algorithmic decision-making systems (ADMSs), consisting of the two distinct but related concepts of artificial intelligence (AI) and big data analytics (BDA), represent the most current computing advances for decision-making. ADMSs are associated with significant opportunities and challenges in a wide range of high-impact application areas. However, the conceptual confusion around ADMSs limits information systems (IS) research in comprehensively studying them and their impacts within a clearly defined cumulative tradition. This literature review develops an inclusive conceptualization of ADMS through the ideas of AI and BDA to mitigate such shortcomings. The conceptualization of ADMS is inductively generated following a grounded theory approach used to analyze the content of 54 IS articles. The resulting conceptualization includes eleven key aspects representing the intricate socio-technical nature of current computing processes for decision-making. Lastly, a green IS research agenda is proposed to illustrate the applicability of the ADMS conceptualization to IS scholarship.
1	In the era of the experience economy, “customized tours” and “self-guided tours” have become mainstream. This paper proposes an end-to-end framework for solving the tourist trip design problems (TTDP) using deep reinforcement learning (DRL) and data analysis. The proposed approach considers heterogeneous tourist preferences, customized requirements, and stochastic traffic times in real applications. With various heuristics methods, our approach is scalable without retraining for every new problem instance, which can automatically adapt the solution when the problem constraint changes slightly. We aim to provide websites or users with software tools that make it easier to solve TTDP, promoting the development of smart tourism and customized tourism.
1	Artificial Intelligence (AI) is often presented as a new phenomenon that is primarily driven by advances in contemporary machine learning technologies. Despite the steep rise, conceptualizations of contemporary AI technologies tend to be vague in many studies. This is problematic not only for positioning and focusing such research, but also for theorizing on the pervasive AI phenomenon. This paper presents a systematic literature review to understand and synthesize distinctive characteristics of contemporary AI technologies. In the course of our ongoing research, the preliminary findings encompass the changing role of data, feature extraction, adaptivity, transparency, and biases. With our future research, we seek to provide guidance on the conceptualizations of AI in IS research and to facilitate a more nuanced and focused theorization of AI in future IS studies.
1	Machine learning (ML)-based software’s deployment has raised serious concerns about its pervasive and harmful consequences for users, business, and society inflicted through bias. While approaches to address bias are increasingly recognized and developed, our understanding of debiasing remains nascent. Research has yet to provide a comprehensive coverage of this vast growing field, much of which is not embedded in theoretical understanding. Conceptualizing and structuring the nature, effect, and implementation of debiasing instruments could provide necessary guidance for practitioners investing in debiasing efforts. We develop a taxonomy that classifies debiasing instrument characteristics into seven key dimensions. We evaluate and refine our taxonomy through nine experts and apply our taxonomy to three actual debiasing instruments, drawing lessons for the design and choice of appropriate instruments. Bridging the gaps between our conceptual understanding of debiasing for ML-based software and its organizational implementation, we discuss contributions and future research.
1	We propose an adversarial deep learning model for credit risk modeling. We make use of sophisticated machine learning model’s ability to triangulate (i.e., infer the sensitive group affiliation by using only permissible features), which is often deemed “troublesome” in fair machine learning research, in a positive way to increase both borrower welfare and lender profits while improving fairness.  We train and test our model on a dataset from a real-world microloan company. Our model significantly outperforms regular deep neural networks without adversaries and the most popular credit risk model XGBoost, in terms of both improving borrowers’ welfare and lenders’ profits. Our empirical findings also suggest that the traditional AUC metric cannot reflect a model's performance on the borrowers’ welfare and lenders’ profits. Our framework is ready to be customized for other microloan firms, and can be easily adapted to many other decision-making scenarios.
1	Patients are increasingly turning to physician rating websites to help them make important healthcare decisions, such as selecting primary care doctors, specialists, and supplementary medical care providers. Previous research has identified a variety of topics and themes that emerge on these review platforms. However, there is little or no work that has been done to create an automated classifier that automatically categorizes these reviews into distinct topics after they have been explored in this context. Building such an automated classifier could assist IS developers and other stakeholders in automatically classifying patient reviews and understanding patient needs. Furthermore, using design science research we strategize how such machine learning systems can be built using design guidelines in turn having the potential to be generalized to other specific contextual problem spaces. Our work focuses on laying the foundation to design guidelines that need to be followed while building automated systems in specific contexts.
1	The past few years have seen an unprecedented explosion of interest in fair machine learning algorithms. Such algorithms are increasingly being deployed to improve fairness in high-stakes decisions in organizations, such as hiring and risk assessments. Yet, despite early optimism, recent empirical studies suggest that the use of fair algorithms is highly unpredictable and may not necessarily enhance fairness. In this paper, we develop a conceptual model that seeks to unpack the dynamic sensemaking and sensegiving processes associated with the use of fair algorithms in organizations. By adopting a performative-sensemaking lens, we aim to systematically shed light on how the use of fair algorithms can produce new normative realities in organizations, i.e. new ways to perform fairness. The paper contributes to the growing literature on algorithmic fairness and practice-based studies of IS phenomena.
1	This study questions how AI developers consider the potential consequences of their work. It proposes an imagined futures perspective to understand how AI developers imagine the futures associated with AI. It examines qualitatively the case of some AI developers and their work and find that they consider the future consequences of the AI they participate in developing as tangential – i.e., loosely connected to what they do - or integral – i.e., closely associated with what they do - to their work. These imaginations of the future are in tension, prompting some AI developers to work at connecting them as they adjust how they view the future and their work. This study reveals how AI development relies upon distinctive imaginations of the future, illuminates how practitioners engage speculatively with the future, and explains the importance for IT development of developers’ answers to what their work may do in the future.
1	Artificial Intelligence (AI) is becoming increasingly essential for enhancing many conventional business processes and generating market opportunities. And yet, for AI to truly gain mainstream acceptance, there is a need to develop a vast array of different applications to cater to the myriad needs of the market. This, however, cannot be achieved by any AI firm in isolation. Instead, there is a need for the collectivization of a synergistic ecosystem of entities. How such an AI ecosystem is developed, however, has not been the subject of research to date. To address this gap, we conducted a case study of iFlytek, one of the most successful AI firms in China and the world. Based on our ongoing study, we developed a theoretical framework that illustrates the stages of AI ecosystem development, which can provide guidelines for other technology firms and policymakers on the orchestration and governance of market-driven AI applications.
1	Research accepts that ML-based AI tools’ accuracy is a defining characteristic for AI implementation. Yet, the understanding of accuracy in relation to the “ground truth” remains under-researched, especially the understanding of universally recognized practices for the “ground truth” in specific knowledge domains. This short paper investigates how knowledge workers’ expertise can be used effectively to redefine the “ground truth” and produce training datasets conducive to more accurate ML predictions. It approaches the question empirically with a case study of ARUP, a global engineering and consultancy firm that uses various AI tools for its advisory services. The paper highlights how executives often overlook data preparation and the role of knowledge workers during this phase, thus questioning the meaning of “ground truth”. It provides valuable insights on how a total and constructive collaboration of stakeholders is essential for organizing existing data, contributing to existing literature on ML implementation and data in general.
1	Alongside growing external pressure for implementation of Artificial Intelligence (AI) technologies, multi-stakeholder demands for responsible conduct have led to an increasing number of organizational AI principles. As previous research on AI principles has mainly focused on their content, restrictions, and external functions, little is known about their relevance for organization-internal stakeholders. Concurrently, while organizational identity was shown to play a central role in technology implementation’s success or failure, with respect to AI implementation, the concept has remained unexplored. Building on 25 expert interviews as part of an ongoing research that involves a qualitative, cross-industry multiple-case study with 13 organizations, we reveal AI principles’ capacities for managing organizational identity towards AI implementation by: (1) redefining organizational identity, and (2) aligning organizational identity’s facets. Our findings accentuate the relevance of organizational identity for AI implementation, and indicate AI principles’ role as a tool to manage this transformative change in an identity-conforming way.
1	Machine Learning (ML) algorithms, as approach to Artificial Intelligence (AI), show  unprecedented analytical capabilities and tremendous potential for pattern detection in  large data sets. Despite researchers showing great interest in these methodologies, ML  remains largely underutilized, because the algorithms are a black-box, preventing the  interpretation of learned models. Recent research on explainable artificial intelligence  (XAI) sheds light on these models by allowing researchers to identify the main  determinants of a prediction through post-hoc analyses. Thereby, XAI affords the  opportunity to critically reflect on identified patterns, offering the opportunity to enhance  decision making and theorizing based on these patterns. Based on two large and publicly  available data sets, we show that different variables within the same data set can  generate models with similar predictive accuracy. In exploring this issue, we develop  guidelines and recommendations for the effective use of XAI in research and particularly  for theorizing from identified patterns.
1	Service failure is inevitable and service providers have a stake in minimizing the adverse consequences of service failure. As companies increasingly deploy Artificial Intelligence (AI) agents to augment or substitute conventional human customer service agents, there are growing scholarly attempts to elucidate the role of AI agents in shaping consumers’ reactions to service recovery. Synthesizing extant literature on service failure and recovery with restorative justice, this study contextualizes restorative justice to service recovery and examine the interplay of recovery components with agent type (AI vs. human) on restorative justice. We then conducted a scenario-based online experiment to validate our hypothesized relationships. Analytical findings point to the positive effects of empathy and remorse on affective restorative justice, but these relationships are attenuated when they are conveyed by AI agents. Insights from this study hence extends our understanding of AI deployment in customer service and yields practical guidelines for AI agent developers.
1	Artificial Intelligence (AI) represents today's most advanced technologies that aim to imitate human intelligence. Whether AI can successfully be integrated into society depends on whether it can gain users’ trust. We conduct a comprehensive review of recent research on human trust in AI and uncover the significant role of AI’s transparency, reliability, performance, and anthropomorphism in developing trust. We also review how trust is diversely built and calibrated, and how human and environmental factors affect human trust in AI. Based on the review, the most promising future research directions are proposed.
1	Forecasting support systems (FSSs) support demand planners in important forecasting decisions by offering statistical forecasts. However, planners often rely on their judgment more than on system-based advice which can be detrimental to forecast accuracy. This is caused by a lack of understanding and subsequent lack of trust in the FSS and its advice. To address this problem, we explore the potential of extending the traditional static assistance (e.g., manuals, tooltips) with conversational assistance provided by a conversational assistant that answers planners’ questions. Drawing on the theory of effective use, we aim to conduct a framed field experiment to investigate whether conversational (vs. static) assistance better supports planners in learning the FSS, increases their trust, and ultimately helps them make more accurate forecasting decisions. With our findings, we aim to contribute to research on FSS design and the body of knowledge on the theory of effective use.
1	An increasing number of organizations are investing in Artificial intelligence (AI), but not all AI implementation leads to improved performance. To contribute to organizational business value, two components of AI resources, AI assets and AI capabilities, should be complementary in the business value creation process. In this study, based on IT business value literature and through the lens of dynamic capabilities, the role of AI resources in organizational value creation is explored. It is proposed that AI resources would enable organizations to develop process-oriented dynamic capabilities (PDCs), contributing to business value. This study will examine how organizations build AI capabilities and the roles of AI resources in creating business values through case studies. This research will offer a framework that guides and assists practitioners in utilising AI resources and building AI capabilities. A deeper understanding of the subject through this study also enriches the growing body of literature on AI.
1	Decentralized Autonomous Organizations (DAOs) are trustless organizations that automate transactions, operations, and decisions without a trusted third party (Wang et al. 2019). So far, this research area is missing a taxonomy that investigates the different dimensions and characteristics of DAOs and the many different forms they can take. This paper addresses this research gap by creating a data-driven taxonomy analyzing 72 DAOs. In doing so, we identify the three main categories treasury, community, and governance, seven sub-categories, 20 dimensions, and 53 characteristics. In addition, we provide dimensions with inadmissible characteristics DAOs cannot take, as well as dimensions used to assess DAOs. The results of our agglomerative clustering are five distinct DAO types: On-chain product and service DAOs, off-chain product and service DAOs with community focus or with investor focus, investment-focused DAOs, and networking-focused community DAOs.
1	This short paper presents a comprehensive systematic and bibliometric analysis of blockchain technology. It extends beyond the information systems field, to include management, finance, economics, and others, to assess the true reach and impact of the technology. We present a summary of the who, when, where, and what of blockchain research. Informed by these findings we continue to explore how researchers are contributing and whether we (as researchers) are indeed advancing and contributing to theory and/or practice. We present some of the early findings and look forward to presenting the completed analysis at the conference.
1	Decentralized Finance (DeFi) emerged rapidly in recent years and provided open and transparent financial services to the public. Due to its popularity, it is not uncommon to see cybersecurity incidents in the DeFi landscape, yet the impact of such incidents is under-studied. In this paper, we examine two incidents in DeFi protocol that are mainly caused by misgovernance and mistake in the smart contract. By using the synthetic control method, we found that the incident in Alchemix did not have a significant effect on the total value locked (TVL) in the protocol, whereas the incident in Compound caused a 6.13% decrease in TVL. One factor that contributed to the difference in the result could be the incident response in social media platforms, and further study should investigate the possible moderating or mediating effects of public opinions and sentiment.
1	In this paper, we investigate the impact of the absence of trading bots on human traders’ investment returns. Using comprehensive data set obtained from a large cryptocurrency exchange platform, we find that trading bots play a market-making role, and they boost human traders’ investment returns. We use the natural experiment setting that transforms a heterogenous market co-created with trading bots and human traders into a human-only financial market for empirical design. This paper extends the traditional investment decision under uncertainty by considering human attitudes toward algorithms while providing significant contributions to policymakers and regulators by providing empirical evidence on trading bots.
1	Quality management (QM) and efficient information sharing among value chain partners have been important IS research topics for decades. Today, IS researchers and practitioners hope to overcome various information inefficiencies in complex supply chains using blockchain approaches. Additionally, future traceability regulations increase companies’ interest in innovative blockchain-based enterprise solutions. We identified several factors that could hinder BC adoption, due to a lack of standards. This paper sheds profound light on organizational and technical aspects of blockchain enterprise applications to support future collaboration initiatives. Furthermore, it develops a terminology that researchers and practitioners can reuse. A case study describes several quality-related objects and events that characterize multiple dimensions and traceability types. Based on these findings, we provide a set of design principles to assist future design features. Finally, this paper provides a holistic orientation and implications for researchers and practitioners moving forwards.
1	Social media sentiment is proven to be an important feature in financial forecasting. While the effect of sentiment is complex and time-varying for traditional financial assets, its role in cryptocurrency markets is unclear. This research explores the predictive power of public sentiment on Bitcoin trading volume. We develop a novel sentiment analysis pipeline for processing Bitcoin-related tweets and achieve state-of-the-art accuracy on a benchmark dataset. Our pipeline also leverages information gain theory to incorporate the impact of textual and non-textual features. We use such features to discern a non-linear relationship between public sentiment and Bitcoin trading volume and discover the optimal predictive horizon for Bitcoin. This research provides a useful module and a foundation for future studies and understanding of Bitcoin market dynamics, and its interaction with social media buzzing.
1	The financial sector is undergoing a massive transformation, with new technology-driven players challenging established mechanisms and transforming the sector into a fast-moving market. With the gradual transition from a scale economy to a platform-driven network economy, enterprise networks are gaining strategic importance. Despite the growing interest in fintech’s, research has so far lacked a conceptualization of value creation in fintech ecosystems. Therefore, this research paper aims to analyze key players, value creation activities, and value streams based on the analysis of the business models of payment services, personal financial management, robo-advisory, peer-to-peer lending, trading, and cryptocurrency. We present a holistic value network for the fintech ecosystem based on structured literature review and analysis of 171 fintech companies. We were able to show that fintech platform orchestrates multiple market sides and that customers take four distinct roles at the center of the ecosystem when using fintech services.
1	Through the lens of the technology-organization-environment framework, this study aims to identify the relevant influencing factors and future opportunities for blockchain technology (BCT) adoption in the automotive industry. By applying an exploratory qualitative empirical study with semi-structured interviews with blockchain experts from the German automotive industry, a revised TOE framework is proposed in this context, confirming previous findings while also incorporating the newly discovered contextual factors of education & skills and sustainability. The analysis of a subsequent quantitative study reveals that while all factors affect BCT adoption, not all context factors have an equally strong impact. The most emphasized emerging BCT opportunities are autonomous driving, decentralized network, digital identity management, and traceability of the supply chain. The findings of this study provide guidance to organizations, politicians, consultants, and managers for defining strategies that aid in the successful adoption and value creation of BCT applications.
1	Initial Coin Offerings (ICOs), a novel form of entrepreneurial finance, are characterized by extreme information asymmetry. To reduce this asymmetry and attract potential investors, entrepreneurs send signals about unobservable characteristics of their ventures to investors. This paper examines the signaling effects of entrepreneurs’ fundraising goals in ICOs. Using a hand-collected dataset of more than 400 ICOs, we show that the relationship between the size of fundraising goals and actual funding obtained follows curvilinear shapes. We also find that these curvilinear shapes are contingent on the presence of simultaneously sent, costless signals. Our findings enhance understanding how fundraising goals affect fundraising success and provide evidence that in ICOs costless signals are interpreted by investors in conjunction with each other.
1	The emergence of novel applications using distributed ledger technologies (DLTs) has gathered pace since the introduction of Bitcoin and the subsequent release of the Ethereum platform for decentralized applications (dApps). Such decentrally governed DLT systems are accelerating the displacement of intermediaries in regulated contexts such as the financial system and challenging the efficacy of governance regimes that have conventionally levered governance controls on identifiable, accountable decision-makers. The governance challenges of DLT systems are exacerbated by the arrival of digital autonomous organizations (DAOs) that use on-ledger decision-making mechanisms to further displace or eliminate human decision-makers. When DAOs are augmented with artificial intelligence (AI), their potent combination of computational power and access to large on-platform data sets and resources, signals a significant disruption to conventional institutional, regulatory, and legal governance regimes. This paper discusses the governance challenges of AI-enabled DAOs and presents a research agenda to address these challenges.
1	Non-fungible Tokens (NFTs) have received increased attention since 2021. NFTs can be susceptible to fraudulent activities such as washtrading or trading of counterfeit digital assets. Such behaviors threaten the trust in this new trading space and for this reason, NFT skeptics are suspicious of the true values of highly priced digital assets. In this paper, we propose a two-step methodological approach to identify washtraded assets, and the suspicious communities of washtraders. Our approach uses bipartite graph characteristics to provide an efficient algorithm that does not require computationally intensive methods. We also identify the challenges in this stream of research and propose suggestions to address those challenges. Our method demonstrates practical applicability on real life networks of NFT transactions and opens doors for several future directions for investigating and exploring the communities of suspicious washtrading actors.
1	The stability of proof-of-work consensus underlying decentralized networks such as Bitcoin relies on an incentive compatible mining design. While theoretically well studied, the empirical composition of the mining process remains largely opaque due to the un-known distribution of miners and technology. This paper proposes a model that leverages innovation-driven convergence in the Bitcoin ecosystem to reconstruct market conditions and study miners’ behavior. Numerical simulations using 10,000 bootstrap samples sup-port the implications of the model. The results quantify considerable variation in miners’ profits and costs and proof consistent with the proposed theory. The estimates suggest that the cost of a capacity majority, and thus the ability to successfully attack the network, can be astonishingly low (e.g., $2.13 million in May 2020) when adverse events coincide. This pronounces the relevance of cyclical patterns when assessing the immutability of proof-of-work consensus.
1	Prior research has demonstrated that blockchain announcements are associated with significant stock market reactions on the day of the announcement. However, it is unclear what factors may influence the positive market reaction at the firm level. Moreover, it is unclear whether national policies will affect positive market reactions. Using an event study methodology, we examine investors’ reactions to blockchain announcements issued by Chinese listed companies, taking organizational factors and national policies into account. Results indicate that the stock market reacts positively to blockchain announcements in the IT sector on the day of the announcement. However, there are no significant differences between manufacturing companies and other companies regarding abnormal stock returns. In addition, a CIO (or CTO) and a high percentage of executives with a background in R&D will enhance the positive stock market reaction. Furthermore, we demonstrate that national policies play a significant role in influencing positive stock market reactions.
1	Numerous studies have raised concerns over the limited scalability of blockchain technologies and, in particular, Bitcoin. Layer 2 technologies have emerged as an advanced array of complementary innovations designed to solve this problem. Despite the growing optimism around layer 2 technologies, however, there is little evidence to show how they impact blockchain’s long-term success. This paper argues that the use and expansion of layer 2 technologies have a positive impact on the adoption and security levels of the underlying blockchain systems. Building on the Bitcoin and Lightning Network case, we use a time-series model based on 1,494 daily observations to demonstrate that the growing activity on the Lightning Network precipitates increased use and better security for Bitcoin. These results highlight the importance of layer 2 technologies for blockchain systems and suggest several further research avenues in this nascent domain of inquiry.
1	Investments in fashionable IT do not make organizations more successful than investments in less fashionable alternatives. Many organizations nevertheless associate with fashionable IT to signal compliance with norms of progress and rationality. These decisions can be risky as they require the ability to navigate hype narratives and fit the new technology into the adopting organization. In this paper, we explore a so far understudied fit perspective: cultural fit between the values attributed to the fashionable IT and those of the recipient organizational context. Through an interpretivist case study of two blockchain projects, we find that cultural sensemaking and dissonance reduction can be important determinants for successful adoption of fashionable IT. Moreover, we identify two recursive paths for how organizations can reduce cultural dissonance. They can adapt their implementation and the narratives surrounding the fashionable IT or they can transform their local or overarching organizational culture.
1	Decentralized autonomous organizations (DAO) received many discussions and attempts recently with the rapid development of blockchain. Token incentive is one of its most important features and owns multiple attributes of equity, property, and currency. To explore its unknown effect, we utilize a quasi-experiment setting in the NFT marketplaces. We find that the token incentives with DAO implementation in Rarible can significantly motivate users’ participation compared with SuperRare at the platform level. At the seller level, by the comparison of cross-platform users and only-OpenSea users, we find it significantly changes users’ trading behavior which reflects in the increment in transactions number and average prices. However, through the equilibrium analysis based on the supply and demand model, the growth rate of the average prices is far beyond the magnitude it should be at the equilibrium state. Therefore, we argue that buyers’ purchase decision is driven by the high expectations of token value.
1	Social trading platforms have, over the last decade or so, been gaining a strong foothold in individual investment markets. Users on these platforms can observe (“view”) traders’ detailed transactions over time. They can also ‘‘follow’’ anyone of those traders, just like with other social media platforms, investing their money in accordance with the strategies of their trader of choice. We study whether and how the disposition effect bias of individual traders is affected by two social features of the platform, “Views” and “Followers.” We find a differentiated impact on this bias from those two social features, which is conditional on the level of market turbulence. We attribute this to how traders assess the signal originating from Views and Followers in relation to how committal it is.
1	Cybersecurity is a critical success factor for more resilient companies, organizations, and societies against cyberattacks. Artificial intelligence (AI)-driven cybersecurity solutions have the ability to detect and respond to cyber threats and attacks and other malicious activities. For this purpose, the most important resource is security-relevant data from networks, cloud systems, clients, e-mails, and previous cyberattacks. AI, the key technology, can automatically detect, for example, anomalies and malicious behavior. Consequently, the market for AI-driven cybersecurity solutions is growing significantly. We develop a taxonomy of AI-driven cybersecurity business models by classifying 229 real-world services. Building on that, we derive four specific archetypes using a cluster analysis toward a comprehensive academic knowledge base of business model elements. To reduce complexity and simplify the results of the taxonomy and archetypes, we propose DETRAICS, a decision tree for AI-driven cybersecurity services. Practitioners, decision-makers, and researchers benefit from DETRAICS to select the most suitable AI-driven service.
1	With increased digitalization and the evolving digital economy, consumers, regulating agencies, and business partners alike demand more transparency for organizational privacy practices, generating increased pressure on organizations to establish privacy programs and initiatives. The Chief Privacy Officer (CPO) role is central to the development of these privacy initiatives and is becoming more strategic. However, the role of the CPO appears to vary significantly across organizations. This study aims to investigate how an organization's privacy initiatives implementation influences the CPO role and understand how an organization needs to transform to support the emerging CPO roles in the digital economy. We present our initial findings and elaborate on a transformation model that shows the stages an organization follow to support the CPO role strategically.
1	Increasingly, large organisations are turning to cybersecurity leaders such as chief information security officers (CISOs) to protect their information resources against attack. The role of the cybersecurity leader is distinct from other cybersecurity professionals in its need for strategy and collaboration, and distinct from other business leaders in its need to maintain situational awareness against active adversaries. Because the role is so new, however, organisations and educators continue to conceptualise it as a senior technological role rather than a strategic, business-oriented role. This representation leaves open a gap between what is viewed as ‘business’ and what is viewed as ‘IT’ – a gap that can leave organisations vulnerable to attack. In this systematic review, we examine the literature on cybersecurity leaders to develop a picture of the competencies required. Following analysis, we propose a preliminary matrix of competencies required for cybersecurity leaders. We conclude with an agenda for further research.
1	This paper examines how individual security inputs i.e., security compliance intention and perceived security knowledge, are processed to produce workgroup information security effectiveness in the workgroup. Based on the input-process-output framework, we investigate the multi-level relationships between focal variables. For the analysis, multi-level structural equation model will be used. In particular, the study potentially contributes to the understanding of the security management by showing how individual compliance intention can be mediated by security knowledge coordination and how this mediation works conditionally based on empowering security leadership and perceived security knowledge. Further possible contributions are discussed in the paper.
1	We provide a systematic overview of the interdisciplinary discourse on ethical AI by combining bibliometric and text mining approaches on a corpus of 23,870 ethical AI publications from journals and conference proceedings. In our research in progress, we offer three contributions of interest to IS scholars: First, in our term analyses, we empirically delineate ethical AI and related terms such as responsible or trustworthy AI. Second, we unearth the intellectual structure of the field and identify five thematic clusters, some of which are directly relevant to IS scholars. Third, we identify that IS research on ethical AI should more intensely consider fairness and transparency as well as the link to explainability. Additionally, we suggest that IS scholars contribute towards policymakers’ ethical AI guidelines by contributing their strong expertise in practical applications.
1	In today’s interconnected world, Internet users are increasingly concerned about losing control over the data they share with peers, which indicates a need for higher levels of control and notification mechanisms. We address this need by building on design science methodology and developing a socio-technical artifact, i.e., a peer-privacy-friendly online messaging service. We draw on Malhotra et al.’s (2004) Internet Users’ Information Privacy Concerns framework and refine and evaluate our artifact via focus groups, interviews, and a survey among users of online messaging services. Our artifact provides senders with the ability to control how their personal information is processed by peers and allows receivers to be made aware of the sender’s privacy expectations. We contribute to the growing literature on peer privacy concerns by developing and evaluating design requirements, principles, and an instantiation that can mitigate peer privacy concerns that go beyond concerns about organizational data practices.
1	Connecting mindfulness with organizational information security (InfoSec) is an increasingly attractive research topic. This paper conceptualizes InfoSec mindfulness as a dynamic InfoSec-specific trait evident when handling organizational information assets. We investigate the motivational factors that contribute to InfoSec mindfulness and the effects of InfoSec mindfulness on employees’ proactive extra-role information security behaviors (ISBs), which refers to self-initiated and future-oriented behaviors that go beyond an organization’s information security policies (ISPs) and are independent on rewards or punishments. This paper provides significant theoretical contributions to InfoSec behavioral literature by conceptualizing InfoSec mindfulness and deepening the understanding of proactive extra-role ISBs. We also summarize our research methodology to develop the scale of InfoSec mindfulness and test its validity for our future study.
1	Malicious web content includes phishing emails, social media posts, and websites that imitate legitimate sites. Phishing attacks are rising, and human-centered phishing risk mitigation is often an afterthought eclipsed by technical system-centric efforts like firewalls. Training tools can be deployed for combating phishing but often lack sufficient labeled training content. Using signal detection theory, this paper assesses the feasibility of using citizen science and crowdsourcing volunteers to label images for use in cybersecurity training tools. Crowd volunteer performance was compared to gold standard content and prior studies of Fortune 500 company employees. Findings show no significant statistical differences between crowd volunteers and corporate employees' performance on gold standard content in identifying phishing. Based on these findings, citizen scientists can be valuable for generating annotated images for cybersecurity training tools.
1	Transparency is viewed as an essential prerequisite for consumers to make informed privacy decisions in digital markets. However, it remains an open research question whether and when individuals actually prefer transparency about privacy risks when given a chance to avoid it. We investigate this question with a randomized controlled online experiment based on an Ellsberg-type design, where subjects repeatedly choose between risk and ambiguity while facing the threat of an actual disclosure of their personal data. We find empirical support for ambiguity attitudes as a novel behavioral mechanism underlying people's transparency choices in privacy contexts. In particular, we find that most individuals avoid ambiguity and prefer transparency for low likelihood privacy losses. However, this pattern reverses for high likelihood losses and when subjects perceive data disclosure as a gain. Most notably, a significant share of people seek ambiguity and thus prefer to avoid transparency when facing high likelihood privacy risks.
1	The role of the chief information security officer (CISO) has emerged as critically important to organizations in managing cybersecurity risks. Unfortunately, many CISOs are limited by perceptions of boards and executive teams that the CISO is not a strategic partner. This study investigates CISOs’ struggles for legitimacy in their ascendancy into the executive suite and in directly reporting to the board of directors. In a grounded theory interview study, we use legitimacy theory as a lens to develop a model of a virtuous cycle of legitimacy, wherein a CISO’s legitimacy gains at the board level feed into successful bids for legitimacy within the executive suite, extending legitimacy theory to include legitimacy assessments within related hierarchal groups (i.e., the board and executive team). Given the growing importance of CISOs, we inform research and practice on how they can become full-fledged members of the executive team and legitimate partners of the board.
1	Social media users tend to disclose a large amount of private information despite their high privacy concerns, which is termed as the privacy paradox in existing literature. Recent studies have found that privacy paradox can be explained by a privacy risk compensation process: users engage in privacy protection behaviors to cope with privacy threats, which in turn increase their privacy disclosure. However, it remains unclear under what condition the privacy risk compensation process can take place. In this study, we integrate the psychological ownership theory with risk compensation theory, and find that psychological ownership plays a moderation role in strengthening the privacy risk compensation. An online survey was conducted with 300 Facebook users, and our hypotheses were greatly supported. Our findings encourage social media platforms to provide more functional design elements to support users’ privacy protection behaviors and satisfy their motivational needs of psychological ownership of privacy.
1	Data breaches lead to inherent uncertainty among customers due to the compromise of information and its potential consequences for customers, e.g., identity theft or credit card misuse. Previous research has focused on outcome-based strategies to address these negative impacts. However, informed by reactance theory, we argue that customers feel a loss of control due to the induced uncertainty and that companies need to tackle these impacts. We test our hypotheses in two empirical studies. The results of Study 1 suggest that data breaches indeed lead to an increased perception of uncertainty among customers. Study 2 examines to what extent the establishment of control can mitigate the negative uncertainty effects. We highlight that by providing customers with control, companies can reduce the degree of uncertainty and increase satisfaction with the response. By conceptualizing choice as a catalyst for perceived control, we offer practitioners a novel strategy for responding to data breaches.
1	Recently, news exposure about privacy practices has brought substantial negative effects on companies’ reputation and trust, which, in essence, reflects the escalating tension between data access and privacy protection that companies are currently facing. Accordingly, we design an active privacy transparency measure and implement it on our self-developed app. Through a two-task experiment, we simultaneously explore the profound and immediate effects of privacy transparency on firms and the underlying mechanisms. Results from our analyses show that active privacy transparency significantly mitigates users perceived psychological contract violations, which in turn helps companies prevent negative word-of-mouth and loss of trust. Moreover, it also ensures companies’ immediate access to user data, and the moderating role of privacy literacy provides an explanation for this insignificant effect and previous inconsistent findings. More interestingly, we find that active privacy transparency might better elicit users’ actual privacy preferences and help companies identify their targeted users.
1	Despite its ambitious goals of protecting personal data and generally being well-received, the General Data Protection Regulation (GDPR) can be exploited for identity theft by weaponizing subject access requests (SARs). To understand this threat and investigate the impact of victims’ privacy awareness and public exposure on its effectiveness, we selected three victims – highly privacy aware person, average user, and semipublic figure – and tasked six realistic attackers with stealing their personal data. Based on 718 submitted SARs, we provide novel insights from a realistic case study of a law being weaponized and advance the understanding of GDPR-based identity theft by demonstrating its practical viability. Further, we derive patterns from common flaws observed in SAR handling processes, and explore threat mitigation options for individuals, organizations, and lawmakers. Generalizing our findings, we uncover approaches for cybersecurity researchers to probe further laws for flaws.
1	Organizational cybersecurity is threatened by increasingly sophisticated cyberattacks. Early detection of such threats is paramount to ensure organizations’ welfare. Particularly for advanced cyberattacks, such as spear phishing, human perception can complement or even outperform technical detection procedures. However, employees’ usage of reporting tools is scarce. Whereas prior cybersecurity literature has limited its scope to utilitarian motives, we specifically take hedonic motives in the form of warm glow into account to provide a more nuanced understanding of cyber incident reporting behavior. Drawing on a vignette experiment, we test how the design features of report reasoning and risk indication impact users’ reporting tool acceptance. The results of our mediation analysis offer important contributions to information systems literature by uncovering the dominant and under-investigated role of hedonic motives in employees’ cyber incident reporting activities. From a practice perspective, our findings provide critical insights for the design of cyber incident reporting tools.
1	Artificial intelligence, especially based on machine learning, is rapidly transforming business operations and entire industries. However, as many complex machine learning models are considered to be black boxes, both adoption and further reliance on artificial intelligence depends on the ability to understand how these automated models work – a problem known as explainable AI. We propose a novel approach to explainability which leverages conceptual models. Conceptual models are commonly used to capture and integrate domain rules and information requirements for the development of databases and other information technology components. We propose a novel method to embed machine learning models into conceptual models. Specifically, we propose using a Model Embedding Method (MEM), which is based on conceptual models, for increasing the explainability of machine learning models, and illustrate through an application to publicly available mortgage data. This machine learning application predicts whether a mortgage is approved. We show how the explainability of machine learning can be improved by embedding machine learning models into domain knowledge from a conceptual model that represents a mental model of the real world, instead of algorithms. Our results suggest that such domain knowledge can help address some of the challenges of the explainability problem in AI.
1	Big Data and Data Analytics (BDA) has the potential to enhance the government sector by providing a better understanding of current challenges, external environment, and citizens' needs to assist with effective design and implementation of policies and services. Although BDA can bring enormous benefits, organisations are finding it challenging to realise the true potential of data. The focus of the paper is on studying the effect of organisational maturity in effective application of BDA in the public sector. Drawing from theories of policymaking and information systems, the study treats BDA as a complex phenomenon from the social-technical perspective. The contribution of this study is to provide an initial understanding of how BDA can be applied more effectively to enhance decision-making across a range of public sector areas.
1	Extracting typical career paths from large-­scale and unstructured talent profiles has recently attracted increasing research attention. However, various challenges arise in effectively analyzing self-­reported career records. Inspired by recent advances in neural networks and embedding models, we develop a novel career path clustering approach with two major components. First, we formulate an embedded Markov framework to learn job embeddings from longitudinal career records and further use them to compute dynamic embeddings of career paths. Second, to cope with heterogeneous career path clusters, we estimate a mixture of Markov models to optimize cluster-­wise job embeddings with a prior embedded space shared by multiple clusters. We conduct extensive experiments with our framework to investigate its algorithmic performance and extract meaningful patterns of career paths in the information technology (IT) industry. The results show that our approach can naturally discover distinct career path clusters and reveal valuable insights.
1	Crises become the norm for organizations, as recent years have shown. Especially the automotive industry is still facing disruptive changes such as e-mobility, connected cars or autonomous driving. Disrupted supply chains, related production downtimes and associated financial losses are consequences. Procurement departments are the interface between internal and external stakeholders in supply chains, and therefore, the central authority for managing crises. In such situations, effective decision-making is essential. Positive effects of data analytics on decision-making were part of numerous research endeavors, as well as related data analytics competencies. We conducted semi-structured interviews with experienced experts about relevant data analytics competencies in procurement departments. We present an overview specifically for procurement departments and derive implications of these competencies on decision-making. As a result, we apply our findings to existing research from a theoretical perspective and support procurement leaders and their departments in facing current and future challenges from a practical perspective.
1	Sports officials around the world are facing societal challenges due to the unfair nature of fraudulent practices performed by unscrupulous athletes. Recently, sample swapping has been raised as a potential practice where some athletes exchange their doped sample with a clean one to evade a positive test. The current detection method for such cases includes laboratory testing like DNA analysis. However, these methods are costly and time-consuming, which goes beyond the budgetary limits of anti-doping organisations. Therefore, there is a need to explore alternative methods to improve decision-making. We presented a data analytical methodology that supports anti-doping decision-makers on the task of athlete disambiguation. Our proposed model helps identify the swapped sample, which outperforms the current state-of-the-art method and different baseline models. The evaluation on real-world sample swapping cases shows promising results that help advance the research on the application of data analytics in the context of anti-doping analysis.
1	This paper contributes to the issue of big data analysis and data quality with the specific field of time synchronization. As a highly relevant use case, big data analysis of work stress and strain factors for driving professions is outlined. Drivers experience work stress and strain due to trends like traffic congestion, time pressure or worsening work conditions. Although a large professional group with 2.5 million (US) and 3.5 million (EU) truck drivers, scientific analysis of work stress and strain factors is scarce. Driver shortage is growing into a large-scale economic and societal challenge, especially for small businesses. Empirical investigations require big data approaches with sources like physiological and truck, traffic, weather, planning or accident data. For such challenges, accurate data is required, especially regarding time synchronization. Awareness among researchers and practitioners is key and first solution approaches are provided, connecting to many further Machine Learning and big data applications.
1	Recommender systems are widely used for assisting consumers finding interested products, and providing suitable explanations for recommendation is particularly important for enhancing consumers’ trust and satisfaction with the system. Tags can be used to annotate different types of items, yet their potential for providing interpretability is not well studied previously. Therefore, it is worthy to study how to leverage tags to enhance recommendation systems in terms of both interpretability and accuracy. This paper proposes a novel model that seamlessly fuse topic model and recommendation model, where the topic model can analyze tags to infer understandable topics, and the recommendation model can conduct accurate and interpretable recommendations based on these topics. We develop variational auto-encoding method to take advantage of neural networks to infer model parameters. Experiments on real-world datasets illustrate that the proposed method can not only achieve great recommendation performance, but also provide interpretability for the recommendation results.
1	Extensive efforts have been made by both academics and practitioners to understand interfirm competitive relationship. However, it has never been an easy task to fully characterize firms and assess their competitive relationship owing to the challenge of information heterogeneity. In this regard, we propose a novel IT artifact for firm profiling and interfirm competition assessment guided by Information System Design Theory (ISDT). We start by constructing a Heterogeneous Occupation Network (HON) using employees' occupation details and education attainments. Then we adopt a Methpath2Vec-­based heterogeneous network embedding model to learn firms' latent profiles (embeddings). Using firm embeddings as input, we train multiple classifiers to assess the competitive relationship among the firms. We demonstrate the utility of our IT artifact with extensive experimental study and in-­depth discussions. Our study also reveals that employees’ occupation and education information significantly contribute to the identification of the focal firm's potential competitors.
1	The rapid development of FinTech drives the growing popularity of digital payment transactions. This phenomenon, especially given the increasing number of offline and online transactions being recorded in a real-time manner, offers great opportunities for financial service platforms to track consumers’ consumption tendencies and dynamically monitor and evaluate their creditworthiness. In our recent research, we first theorized the value of category-level consumption tendency based on the self-regulatory theory and employed econometric methods to empirically test the relationship between category-level consumption tendency and credit behavior. Then, we proposed a Deep Hierarchical Partial Attention-based Model (DHPAM) to predict credit default risk with full employment of product category features. We provided strong experimental evidence to show that the proposed DHPAM outperforms the state-of-the-art machine learning models. This paper, based on theories, empirical analyses, and a prediction model, offers comprehensive and practical guidance on the optimal utilization of consumption information in credit risk management.
1	The proliferation of video data in retail marketing brings opportunities for researchers to study customer behavior using rich video information. Our study demonstrates how to understand customer behavior of multiple dimensions using video analytics on a scalable basis. We obtained a unique video footage data collected from in-store cameras, resulting in approximately 20,000 customers involved and over 6,000 payments recorded. We extracted features on the demographics, appearance, emotion, and contextual dimensions of customer behavior from the video with state-of-the-art computer vision techniques and proposed a novel framework using machine learning and deep learning models to predict consumer purchase decision. Results showed that our framework makes accurate predictions which indicate the importance of incorporating emotional response into prediction. Our findings reveal multi-dimensional drivers of purchase decision and provide an implementable video analytics tool for marketers. It shows possibility of involving personalized recommendations that would potentially integrate our framework into omnichannel landscape.
1	Organizations have long been trying to assess job applicants' personality using self-reported psychometric tests, such as the Big Five personality test. However, these tests are not robust against incentives to pretend having certain desirable traits, for example, the disposition for being a good team player. We test whether machine learning classifiers trained on written self-descriptions, such as cover letters, predict people's true cooperativeness better than psychometric tests. Based on data from a controlled online experiment with 400 participants, we find that - when people have incentives to fake their personality - linguistic classifiers based on self-descriptions significantly outperform psychometric classifiers based on the Big Five. Moreover, we find that a fine-tuned, pre-trained natural language model can detect incentives to fake in people's self-descriptions. While further research is needed to achieve tamper-proof models, our findings illustrate the potential of automated personality tests based on job applicants' cover letters.
1	The design of targeting policies is fundamental to address a variety of practical problems across a broad spectrum of domains from e-commerce to politics and medicine. Recently, researchers and practitioners have begun to predict individual treatment effects to optimize targeting policies. Although different research streams, that is, uplift modeling and heterogeneous treatment effect propose numerous methods to predict individual treatment effects, current approaches suffer from various practical challenges, such as weak model performance and a lack of reliability. In this study, we propose a new, tree- based, algorithm that combines recent advances from both research streams and demonstrate how its use can improve predicting the individual treatment effect. We benchmark our method empirically against state-of-the-art strategies and show that the proposed algorithm achieves excellent results. We demonstrate that our approach performs particularly well when targeting few customers, which is of paramount interest when designing targeting policies in a marketing context.
1	Stereotypical gender representation in textbooks influences the personal and professional development of children. For example, if women do not pursue a STEM career because of gender stereotypes, this is not only an individual problem but also negative for society in general. It is hence crucial that textbooks do not convey gender stereotypes but are gender-balanced. Currently, textbook analysis is predominantly conducted manually, if at all. However, this is time-consuming and consequently cost-intensive. Therefore, as part of a design science research project, we developed a gender language analyzer. Our initial prototype is already capable of automatically analyzing textbooks and recommending suggestions regarding gender-balancing. We will further improve our prototype in the next design science research cycle (e.g., by integrating self-learning techniques). With this tool, publishers will be able to automatically analyze textbooks to reduce gender bias. Moreover, we provide the scientific community with design knowledge regarding automated identification of gender bias.
1	Argument mining (AM) represents the unique use of natural language processing (NLP) techniques to extract arguments from unstructured data automatically. Despite expanding on commonly used NLP techniques, such as sentiment analysis, AM has hardly been applied in information systems (IS) research yet. Consequentially, knowledge about the potentials for the usage of AM on IS use cases appears to be still limited. First, we introduce AM and its current usage in fields beyond IS. To address this research gap, we conducted a systematic literature review on IS literature to identify IS use cases that can potentially be extended with AM. We develop eleven text-based IS research topics that provide structure and context to the use cases and their AM potentials. Finally, we formulate a novel research agenda to guide both researchers and practitioners to design, compare and evaluate the use of AM for text-based applications and research streams in IS.
1	Fast fashion has emerged as a prevalent retail strategy shaping fashion popularity. However, due to the lack of historical records and the dynamics of fashion trends, existing demand prediction methods do not apply to new-season fast fashion sales forecasting. We draw on the Social Contagion Theory to conceptualize a sales prediction framework for fast fashion new releases. We posit that fashion popularity contagion comes from Source Contagion and Media Contagion, which refer to the inherent infectiousness of fashion posts and the popularity diffusion in social networks, respectively. We consider fashion posts as the contagion source that visually attracts social media users with images of fashion products. Graph Convolutional Network is developed to model the dynamic fashion contagion process in the topology structure of social networks. This theory-based deep learning method can incorporate the latest social media activities to offset the deficiency of historical fashion data in new seasons.
1	Brick-and-mortar (B&M) firms are increasingly required to provide a seamless omnichannel experience for customers across channels. However, when integrating online and offline channels to provide seamless omni-experiences, B&M firms often face challenges in effectively orchestrating their scarce assets between these competing channels. Therefore, we ask, "How to manage channel conflicts to achieve a consistent omnichannel experience." We adopted a resource orchestration perspective as a theoretical sense-making lens to address our question, based on a case study of successful omnichannel integration at a leading B&M firm in Asia. We found that B&M firms can achieve omnichannel consistency by structuring centralized leadership resource and centralized IT resource; bundling these resources to create sustainable competitive collaboration capability; and leveraging this capability to achieve omnichannel consistency. Our study contributes to omnichannel integration literature and provides practical guidelines to B&M managers for a successful omnichannel implementation.
1	Understanding the effects of credit on consumption is crucial for guiding users’ consumption behavior, designing financial marketing strategies, and identifying credit's value in stimulating the economy. Whereas several studies have endeavored on this issue, most simply utilize observations of a single credit channel and/or focus on an overall effect without considering the potentially heterogeneous short-term and long-term consumption changes. This study, leveraging a quasi-experimental design with high-resolution transaction data, examines how people respond to credit in both short- and long-term periods. Results show that credit users’ consumption amount significantly expand by 51.74% after getting access to credit in the short term. However, they ultimately cut their consumption by 4.02% to cope with financial constraints in the long term. We also reveal and quantify the spillover effects of credit on consumption with savings channels. We draw on regulatory focus theory to rationalize the changes on consumers’ consumption behavior after credit activation.
1	Search engines are an essential part of our lives. However, we do not fully understand what affects users' search "inputs." One of the most notable features affecting search inputs is autocomplete, an intelligent agent suggesting queries while typing. Understanding the impact of autocomplete helps eCommerce companies retain customers; examining its impact is difficult since all search engines have adopted it, and experiments are risky for firms. We overcome the challenges by leveraging a novel natural experiment of an eCommerce company. Our preliminary results suggest that the deactivation of autocomplete for the incorrect keyword led to a substantial drop in website visits in the PC channel compared to the mobile channel. In addition, website visits substantially shifted from the incorrect keyword to the correct keyword in the mobile channel but not in the PC environment. This short paper is expected to shed new light on our understanding of autocomplete's impact.
1	Live-streaming has emerged as a popular direct selling channel to foster synchronous interaction between streamers and consumers, with the avatar streamer largely underexplored. Using the data from a fashion retailer, we adopt the Generalized Synthetic Control (GSC) method to examine the effect of gamified and human live-streaming on product sales and return rate. We find that (1) the gamified live-streaming reduces product sales and the return rate simultaneously; (2) human live-streaming boosts product sales but increases the return rate, and (3) the dual-type live-streaming can increase product sales and decrease return rates. Furthermore, we proposed that the reason for the differentiated effects between gamified and human live-streaming could be driven by the impulse-buying behavior of viewers only in human live-streaming. Our findings contribute to the growing literature on the business value of AI technology and gamification in live-streaming and shed light on practical decisions made by online retailers.
1	Although conversational agents (CA) are increasingly used for providing purchase recommendations, important design questions remain. Across two experiments we examine with a novel fluency mechanism how recommendation modality (speech vs. text) shapes recommendation evaluation (persuasiveness and risk), the intention to follow the recommendation, and how modality interacts with the style of recommendation explanation (verbal vs. numerical). Findings provide robust evidence that text-based CAs outperform speech-based CAs in terms of processing fluency and consumer responses. They show that numerical explanations increase processing fluency and purchase intention of both recommendation modalities. The results underline the importance of processing fluency for the decision to follow a recommendation and highlight that processing fluency can be actively shaped through design decisions in terms of implementing the right modality and aligning it with the optimal explanation style. For practice, we offer actionable implications on how to make effective sales agents out of CAs.
1	With the increasing use of online matching platforms, predicting matching probability between users is crucial for efficient market design. Although previous studies have constructed various visual features to predict matching probability, facial features, which are important in online matching, have not been widely used. We find that deep learning-enabled facial features can significantly enhance the prediction accuracy of a user’s partner preferences from the individual rating prediction analysis in an online dating market. We also build prediction models for each gender and use prior theories to explain different contributing factors of the models. Furthermore, we propose a novel method to visually interpret facial features using the generative adversarial network (GAN). Our work contributes the literature by providing a framework to develop and interpret facial features to investigate underlying mechanisms in online matching markets. Moreover, matching platforms can predict matching probability more accurately for better market design and recommender systems.
1	Consumers differ regarding need for touch, and for some consumers haptic exploration plays a more important role in shopping. While online shopping lacks in haptic exploration, we focus on the possibilities of a 360-virtual store in compensating this deficiency. Using latent class analysis and a sample of 300 responses, we first identify four subgroups based on the need for touch scale, with two groups at the high and low ends of the need for touch continuum. Additionally, we identify a group that has a neutral need for touch and a group that is negative regarding autotelic need for touch, and positive regarding instrumental need for touch. To illustrate between-group differences, we examine how consumers with different levels of need for touch differ in the relationship between hedonic / utilitarian shopping experience and purchase intention stimulated by a 360-virtual store visit.
1	Retargeting ads (RA) aim to convert customers who previously browsed the websites or abandoned shopping-carts. We exploit several randomized field experiments to test how the effects of RA vary depending on ad-copy content and purchase-funnel stages. Results suggest that compared to hold-out without retargeting, RA in lower funnel based on shopping-cart abandonment history significantly enhances purchase responses. The effects are driven by ad content that highlights product return information rather than product reminder or shipping information. Due to lack of touch/feel/trial of online orders, such ad-copy can nudge customers to try the products by reducing shopping risks and, thus, increase purchase rates. Net revenue for RAs with product return information is 49.7% larger than conventional RAs with product information. Also, lower funnel retargeting is 2.25 times as effective as upper funnel retargeting in lifting purchase rates. These findings indicate how to design RAs to recover abandoned carts and boost sales.
1	This study analyzes and predicts consumer viewing response to e-commerce short-videos (ESVs). We first construct a large-scale ESV dataset that contains 23,001 ESVs across 40 product categories. The dataset consists of the consumer response label in terms of average viewing durations and human-annotated ESV content attributes. Using the constructed dataset and mixed-effects model, we find that product description, product demonstration, pleasure, and aesthetics are four key determinants of ESV viewing duration. Furthermore, we design a content-based multimodal-multitask framework to predict consumer viewing response to ESVs. We propose the information distillation module to extract the shared, special, and conflicted information from ESV multimodal features. Additionally, we employ a hierarchical multitask classification module to capture feature-level and label-level dependencies. We conduct extensive experiments to evaluate the prediction performance of our proposed framework. Taken together, our paper provides theoretical and methodological contributions to the IS and relevant literature.
1	Live streaming is adopted by many digital and e-commerce platforms. As a popular way for people to obtain information products and entertainment, live streaming generates tremendous business revenue through pay-as-you-wish gifting (PAYW). As one of the most important channels to earn profits by live streaming hosts, PAYW faces the trouble of free riders who watch live content without payment. To encourage free-rider audiences to pay, some platforms in China have introduced a lottery mechanism, which allows people to get high-value prizes by paying a threshold price. We conduct a quasi-experiment with lottery block on a popular live streaming platform to estimate its causal effect on PAYW gifting. The study identifies heterogeneous effects based on user opportunism and impulsive tendencies. The proposed typology of lottery provides a framework for improving PAYW gifting in live streaming e-commerce.
1	Live streaming e-commerce (LSE) has emerged as a popular third-party service for improving product sales. It persuades consumers through streamers’ storytelling or narratives, which encompass descriptions and depictions of their own product experiences. However, the sales impact of a story or narrative in LSEs has been overlooked in the literature. Extending the narrative transportation theory to the LSE context, we posit that the dual landscapes of narrative—the landscapes of action and the landscape of consciousness—can improve product sales through their influence on consumers’ imagination of story plotline and empathy for streamers’ product experiences. We also propose that the efficacy of the dual landscapes is contingent on streamers’ interaction response to consumer query. By collecting LSE data from the Taobao Live platform, we manually and algorithmically measured these variables and proposed to empirically examine their effects.
1	The burgeoning popularity of social media has shifted how social media users share and seek information through online platforms. Social media users are often motivated to show the “perfect side” of themselves on the platform, resulting in sharing manipulated appearances and positive aspects of their lives in order to garner more “likes” when comparing their popularity to others. Thus, social media users may often face inauthentic information, which may affect their behaviors on the platform. In this study, we utilize a change in Instagram policy—where they hide the number of likes from the platform— which started in September 2019 in East Asia. Specifically, we examine influencers’ post-generating behavior and post characteristics (e.g., whether it is focused on product vs influencers themselves and the degree of image manipulation). The results show that the number of endorsement postings increases, and influencers are more likely to generate influencer-focused postings after the intervention. In addition, we find that such effects are accentuated when influencers have a the larger follower base. Lastly, our findings suggest that the economic benefit (e.g., total weekly sales) that influencers gain increases after the intervention; however, such an effect is attenuated with influencers having a larger number of followers.
1	Compared with traditional e-commerce, livestreaming e-commerce is characterized by direct and intimate communication between streamers and consumers that stimulates instant social interactions. This study focuses on streamers’ three types of information exchange (i.e., product information, social conversation, and social solicitation) and examines their roles in driving both short-term and long-term livestreaming performance (i.e., sales and customer base growth). We find that the informational role of product information (nonpromotional and promotional) is beneficial not only to sales performance, but also to the growth of the customer base. We also find that social conversation has a relationship-building effect that positively impacts both sales and customer base growth, whereas social solicitation has both a relationship-building and a relationship-straining effect that positively affects customer base growth but can hurt sales. Furthermore, our results show that streamers’ social interactions with consumers can stimulate consumer engagement in different ways, leading to different effects on livestreaming performance.
1	Digital infrastructures provide a space where possibilities for innovation continuously emerge. They are not stable entities but are evolving. Their boundaries are subject to constant negotiation among multiple organizational actors as well as changing connections of digital technologies, operations, and users. In this paper, we explore the evolution of an Industrial Internet of Things (IIoT) infrastructure in a leading manufacturing company. We find that the IIoT infrastructure provided actionable spaces upon which organizational actors discovered opportunities for improving process performance which, in turn, led to investment decisions. We explain this process through the lens of digital options theory and highlight how IIoT infrastructure provides the material foundation for the identification of digital options, how the realization of digital options leads to the emergence of more digital options, and how these “cascading” digital options are implicated in the evolution of IIoT infrastructure. We discuss theoretical and practical implications.
1	The emergence and growth of sharing economy platforms have engendered significant research interests recently. These platforms have witnessed increased entry of professional service providers, who have large amounts of excess assets and standardized business practices. Meanwhile, sharing economy platforms have witnessed an astounding growth, much of which is not attributed to professional service providers. This paper examines two seemingly contradictory phenomena – increased concentration among professional service providers and rapid growth of non-professionals on sharing economy platforms. Using the resource partitioning theory from the organizational literature, we explain how these two phenomena are inherently related. We further emphasize the role of income inequality that affects the resource partitioning process. The empirical analysis uses 1.4 million zip-code level Airbnb data, with Airbnb Plus policy as a natural experiment. Findings reveal that professional service provider concentration facilitates non-professional growth but reduces their performance, and the effects are significantly moderated by income inequality.
1	The discourses essential to effective information and communication technologies for development (ICT4D) are subject to constraints. Using a critical discourse lens, we ask (1) how geography constrains digital innovation discourses on a digital platform and (2) how these constraints can be alleviated. Informed by the ICT4D and digital innovation literatures, we analyzed 3.3 million tweets from 1,476 accounts from 54 countries. Using text mining, we surfaced distinct digital innovation discourses and subjected ten of them to in-depth analysis. The network structures of these discourses show geographic constraints on authoring, citing, and tie formation, answering our first research question. Further analysis revealed that complex and novel discourses enjoy more diversity in authoring and citing, and more freedom in tie formation, answering our second question. We conclude that digital innovation discourses are geographically constrained; however, entrepreneurial actors advance complex and novel discourses, creating opportunities for emancipation of marginalized countries.
1	Despite the rhetoric that “data is the new oil” organizations continue to face challenges in data monetization, and we don’t have a reliable way to measure how easily data assets can be reused and recombined in value creation and appropriation efforts. Data asset liquidity is a critical, yet underexamined, prerequisite for data monetization initiatives. We contribute to the theorizing process by advancing a definition, conceptualization, and measurement of data liquidity as an asset level construct. Based on interviews with 95 Chief Data and Analytics Officers from 67 distinct large global organizations, we identify three determinants of data liquidity: inherent asset characteristics, structural asset characteristics, and asset environment characteristics. We theorize the existence of equifinal configurations that yield liquid data assets, configurations that should prove helpful to academics and practitioners seeking to understand data liquidity and its impact on firms’ data monetization efforts as well as society at large.
1	Many digital startups fail in their pursuit of niche business value for three reasons: underdeveloped digital affordances, inadequate digital capabilities, and perhaps most importantly, a misalignment between digital affordances and digital capabilities. Digital affordances depict the potential involvement of digital technologies by groups in value creation while digital capabilities represent the ability to leverage and make changes to digital resources to fulfil specific objectives (e.g., affordance actualization). Based on insights derived from a longitudinal in-depth case study of six AI startups, we propose a co-evolution framework that illustrates several iterative loops between digital capabilities and digital affordances. Our analysis also reveals key properties of digital affordances and digital capabilities. Specifically, we find that digital startups with mutually reinforcing digital affordances and digital capabilities are most likely to succeed. We also develop a typology of digital startups using a 2 by 2 affordance-capability matrix.
1	Current research offers limited knowledge on digital transformation of micro-enterprises, and even less so, micro-enterprises suffering systemic resource constraints. Addressing this gap, we examine how micro-enterprises use digital technologies to change and improve their businesses in the context of multiple resource constraints. Based on a large-scale qualitative study of micro-enterprises in Ghana and data from government and technology firms, we examine the question: how do micro-enterprises undergo a process of digital transformation by engaging in bricolage with digital technology? The preliminary findings show that digital transformation of micro-enterprises in resource-constrained environments emerges across three paths: (1) resource mobilization through the constitution of digital resources over time, (2) resource combination through digital / non-digital configurations, and (3) resource deployment through a specific way of using these resources. Based on the findings, we develop an initial process model of digital bricolage that advances understanding of digital transformation of micro-enterprises.
1	To drive digital transformation, firms are increasingly adding IT-experienced executives to their top management team (TMT). Yet, whether IT-experienced executives can aid firms to achieve digital transformation remains unresolved theoretically and empirically. Drawing on human capital and group literature, we propose that there are limits to the benefits received from adding IT-experienced executives to a TMT, resulting in a curvilinear relationship between the share of IT-experienced executives in the TMT and a firm’s digital orientation. We also propose that this relationship is moderated by CEO entrepreneurial orientation and power concentration in the TMT. We test and find support for most of our hypotheses using a secondary panel data set comprising 1,855 firm-year observations from 256 firms listed in the S&P 500 between 2005 and 2017.
1	While a wealth of research has suggested the increasing importance of software-driven innovation, prior studies have mostly focused on the economic outcomes or consequences of software-driven innovation. This study conceptualizes CEO political orientation drawing upon upper echelon theory and examines how CEOs’ values and beliefs are associated with the software-driven innovation of their firms. Specifically, we suggest that liberal CEOs are more likely to be supportive of software-driven innovation because they prefer to endorse uncertain and unstable strategic decisions, compared to conservative CEOs. Further, we find that the volume of CEO compensation strengthens CEOs’ tendency for software-driven innovation. Our findings underscore the need to consider CEO political orientation in IS research for examining its influence on IT-related strategic decision makings.
1	To face the challenge of digital transformation as well as to implement digital innovation many incumbent companies have set up digital innovation units (DIUs). Despite a steadily growing body of knowledge, there is a rather static picture of DIUs in the literature to date, and we have little knowledge of how these units evolve over time to continuously contribute to digital transformation and innovation. To lay the foundation for an understanding of DIUs as dynamically evolving entities, we conduct a multiple-case study with DIUs of five manufacturing companies and identify DIU evolution as a process driven by an interplay of life-cycle and dialectic motor of change. In the course of this, we also outline specific triggers, sequences, and the nature of change. We generalize our findings with a conceptual process model of DIU evolution and three propositions on their current and future development to inform the existing and forthcoming literature.
1	High-growth ambitions are typically vital in platform markets. Yet, it is increasingly clear that the time window to occupy a novel platform market before it becomes saturated is surprisingly short. To this end, a differentiation strategy based on distinctive positioning across markets is increasingly prominent for new entrants to be competitive in early saturated markets. In the literature, two types of such tactics figure: (i) platform bundling, which aims to replicate the functionality of incumbents as part of a multimarket bundle, and (ii) platform piggybacking, which aims to tap into the functionality of incumbents through boundary resources use. In this paper, we employ a fixed-effect time series modelling approach using data from Apple’s App Store to develop and test the influence of these two tactics on platform competition in terms of user base and user engagement in early saturated app markets. We contribute to a distinctiveness logic of platform competitiveness by leveraging the dualism of digital platforms as both markets and technological architectures.
1	Companies worldwide are adopting real-time feedback applications as part of their digital transformation strategies. However, the emphasis on real-time feedback stems from many organizations perceiving that the human factor is the missing link to achieving successful outcomes, specifically regarding listening to associates about operational insights – both challenges and solutions. Furthermore, employees’ feedback towards entities such as operational insights and working processes can have different effects and mechanisms with feedback generated for a person. This research aims to understand whether and how feedback towards an entity (nonperson) versus a person affects the generated feedback ratings and comment quality. Leveraging the field data from two large, global companies, our research found that feedback toward an entity has lower ratings and is shorter, more negative, less subjective, and more specific than feedback toward a person. Additionally, we found managerial position moderates the main impacts. This research has both theoretical and practical implications.
1	Data ecosystems are an emerging theme in IS research. They represent the complex dynamics of inter-organizational value co-creation based on data sharing. Interestingly, empirical research on the value that the various actors can extract from participating in a data ecosystem is still sparse. We address this issue by analyzing 64 Gaia-X use cases, each representing a data ecosystem. From them, we derive roles relevant to data ecosystems and describe them according to typical ontological business model elements (value proposition, value creation, value delivery, and value capture). To visualize the value co-creation in data ecosystems, we use the modeling language e3-value. We illustrate this approach by modeling the specific Agri-Gaia use case. Our work contributes to understanding value co-creation in data ecosystems more in-depth as we extract roles, demonstrate how to model actors and their value co-creation, and discuss the implications of a service ecosystems perspective.
1	As digital technologies move toward the core of an organization’s offerings, the identity of many contemporary organizations is now born in association with the digital technology that characterizes them. Entrepreneurs largely rely on setting up high expectations to attract initial resources to materialize the idea for their digital innovation. However, such a tactic may be problematic when their eventual digital artifact contradicts their core organizational identity, leading to their legitimacy loss. In this ongoing study, we explore a novel phenomenon of hyperbolic organizational identity. Drawing on longitudinal archival sources, we conduct a comparative case study of IBM Watson and DeepMind, whose identities both became hyperbolic, yet experienced different outcomes in their healthcare innovations. From our findings to date, a preliminary dialectical process model is presented that depicts the interplay between organizational and technical identities of the digital artifact in leading to the formation and change in hyperbolic organizational identity.
1	The role of the top management team (TMT) and IT executives in organizing and leading digital innovation are increasingly recognized. However, past studies often focus on the role of IT expertise of TMT members. Relative power and interactions of the elite members remain under-researched. In this study, we draw on the literature on power distribution in the TMT to investigate the impact of power dispersion between IT executives and the TMT on digital innovation. We theorize that power dispersion has a non-linear, inverted U-shaped relation to digital innovation. Further, we introduce the moderating role of two relevant expertise of the IT executives, technological expertise and firm-specific expertise. Using panel data from US firms belonging to the list of top innovators, we find support for our theory. Our study offers potential contributions to the digital innovation literature as well as to research in IS leadership.
1	Digital transformation is often characterized as a liminal process as organizations move from established practices to new ways of organizing afforded by digital technology. Two contrasting views exist, however, on the liminality of digital transformation. One view sees liminality as a discrete transient process, while the other sees it as an on-going continuous transition. Building on a case study around a digital innovation initiative of an incumbent automotive car manufacturer, we offer a third view. We find that digital innovation triggers a phase of punctuated, multi-layered liminality that has a material, structural and temporal layer. We explain how material, temporal and structural tensions unfold at the level of practice, triggering new forms of liminal practices. We further develop three mechanisms (boundary testing, temporal bridging, and structural recoupling) that underpin punctuated multi-layered liminality. We contribute by unpacking the relationship between digital innovation and digital transformation.
1	Inherent properties of digital technologies offer promising possibilities such as rapid scalability and exponential growth. However, we observe that firms pursuing digital transformation (DT) initiatives face difficulties in realizing these benefits, as they face competing organizational demands (tensions) in the DT context. By considering digital technologies’ unique properties and adopting a paradox theory lens, we conducted a qualitative study with 28 interviewees across three companies from which we derive six drivers of tensions and three novel paradoxical tensions within the DT context. We show how these drivers and tensions lead to firms pursuing short-term successes at the cost of strategic benefits that DT offers (what we call “local” instead of “global” optima). We provide scholars and practitioners with a fundamental understanding of how digital technologies define challenges in the DT process so that firms can proactively structure DT initiatives to reach global optima.
1	Prior research has expanded our understanding of the platform business and its success factors, but scant attention has been paid to the launch of digital platforms by “pipeline” firms. Our study examines the effect of a firm’s status on the strategic decision to launch a digital platform and its consequences. By analyzing panel data of Fortune China 500 companies, we found that high-status incumbents are more likely to add a digital platform than their low-status counterparts, indicating that status can be seen as a promoter of launching digital platforms. However, once a digital platform is added, high-status firms are slower in improving performance than their low-status counterparts. Thus, status may serve as an inhibitor of a firm’s dedication to the new platform business. This research contributes to our understanding of the social contingency of digital transformation and the important constraints that must be overcome for incumbent firms to successfully transit.
1	Over the last years, Augmented Reality (AR) technology has been increasingly used in various settings. Yet, AR is still often considered as experimental, which is partly due to the unclear picture of the benefits of using AR. This study systematically reviews research on using AR in learning settings. By examining 93 relevant articles, we identified 21 benefits related to AR learning gains and outcomes. To obtain a comprehensive and coherent overview of the benefits, we classified them based on Fink’s taxonomy of significant learning. Our analysis shows that the positive effects of using AR on learners’ motivation and joy have been well-studied, whereas the effects on independent learning, concentration, spontaneous learning, critical thinking, and practical skills have not yet been examined in detail. Our study provides directions for future studies on using AR in learning settings and can also help to improve learning designs.
1	The business event industry was hit particularly hard by the COVID-19 pandemic. Within a few weeks, all trade fairs, congresses and events were cancelled in the spring of 2020 and partly replaced by video-conferencing formats as fastest possible alternative in order to reach the goals of the respective industries at least digitally. After more than a year of pandemic, many marketing and business travel budgets were forced to either be cut, frozen, or shifted into online initiatives. The crisis winners of shifted budgets were, for example, the advertising business segments of social media business networks such as LinkedIn. Trade fairs were forced to leverage digital technologies and undergo a significant transformation of their business model in order to survive. This teaching case addresses various aspects of modern live communication in the business event industry and the challenge of combining these elements with digital technologies and services to create added value.
1	Responsible AI (RAI) is an emerging topic in the Information Systems (IS) literature. RAI entails ensuring ethical, transparent, and accountable use of AI technologies in line with societal values, expectations, and norms. The challenge for research on IS education at university level is to accompany the growing research on RAI with approaches to educate students about this emerging theme. Research on IS education on responsible AI remains scarce to date, however. We ask: How can we design a course to educate students about responsible AI? We build on earlier research and an experiential learning-based approach to propose a course design promoting students’ multidisciplinary, problem-based learning about RAI applied to the case of public welfare services. Our study is based on participatory observations of student groups and group interviews after a project, acting as an arena where the students could reflect on the learning process and evolving awareness of RAI.
1	Emotional regulation in learning has been recognised as a critical factor for collaborative learning success. However, the “unobservable” processes of emotion and motivation at the core of learning regulation have challenged the methodological progress to examine and support learners’ regulation. Artificial intelligence (AI) and learning analytics have recently brought novel opportunities for investigating the learning processes. This multidisciplinary study proposes a novel fine-grained approach to provide empirical evidence on the application of these advanced technologies in assessing emotional regulation in synchronous computer-support collaborative learning (CSCL). The study involved eighteen university students (N=18) working collaboratively in groups of three. The process mining analysis was adopted to explore the patterns of emotional regulation in synchronous CSCL, while AI facial expression recognition was used for examining learners’ associated emotions and emotional synchrony in regulatory activities. Our findings establish a foundation for further design of human-centred AI-enhanced support for collaborative learning regulation.
1	This study discusses how students' temporal, adaptive processes, and learning regulation can be understood using multi-channel data. We analyzed the behaviors of 189 students to identify a range of self-regulated learning (SRL) profiles that lead to different achievement in a large-scale undergraduate course seeing how students' SRL unfold during the course, which helps understand the complicated cyclical nature of SRL. We identified three SRL profiles by administrating and analyzing the Motivated Strategies for Learning Questionnaire (MSLQ) three times. We looked at how students adopt different SRL profiles as the course progressed through process mining and clustering techniques to clarify the cyclical nature of SRL. We demonstrated how process mining was used to identify process patterns in self-regulated learning events as captured. Analyzing sequential patterns indicated differences in students' process models. It showed the added value of taking the order of learning activities into account, contributing to theory and practice.
1	In the last two years, there has been a massive use of videoconferencing tools for distance learning all over the world. However, a feeling of fatigue has been found among students. Researchers have proposed multiple problems in the online interaction with human faces that may contribute to videoconference fatigue (VCF). To contribute to this upcoming new research domain, this study investigates whether VCF can be reduced if we change the unnatural interaction with multiple enlarged faces on videoconferencing tools. We compare Zoom’s “speaker view” with “gallery view”, and based on theoretical insights from the information processing and brain research domains, we argue that Zoom “gallery view” leads to higher fatigue and stress levels than “speaker view”. Moreover, we investigate whether the face manipulation (“gallery view” vs. “speaker view”) affects learning outcome and learning satisfaction, as well as the role of fatigue and stress as mediators in this relationship.
1	The Covid-19 pandemic is seen by many in the world as a “booster dose” for more rapid digitalization in schools and universities. Research in the past mainly reported inconclusive results about students’ preferences for digital textbooks, although there are significant advances in information technology development and the availability of digital textbooks. We think that preferences have many contextual underlying causes, and they cannot be simply detached and measured only through the behavioral choice (print vs. digital textbook) neglecting the broader context. Hence, we propose conducting a thorough systematic literature review (SLR) on studies that investigated the preferences of three key stakeholders’ students, teachers, and parents to better understand the potential contextual circumstances that trigger certain preferences. This short paper forms the first step towards creating a deeper understanding of preferences for digital textbooks as a phenomenon and research area among IS researchers.
1	E-Learning, as a prevalent instructional approach in the midst of the COVID-19 pandemic, is often criticized for reducing motivation and increasing mental fatigue among learners. Despite the attractiveness of various gamification designs to resolve these issues, there still exists a lack of comprehensive and integrated understanding of the pedagogic effectiveness of gamification rewards. Motivated thus, this study assesses and compares four different types of gamification rewards: unexpected-hedonic rewards, expected-hedonic rewards, unexpected-utilitarian rewards, and expected-utilitarian rewards. Drawing from self-determination theory and opportunity cost model of subjective effort and task performance, this study evaluates the effect of gamification reward type on learning motivation and mental fatigue. The effect of gamification reward type will be examined in a longitudinal field experiment in an introductory undergraduate computer science course.
1	Design Kitchen is a typical, small business about to secure a major deal with a prospective customer. The crux of this deal: Design Kitchen’s ability to work as a reliable subcontractor. Business continuity (BC) teaching cases usually describe a disruption that requires reaction. This teaching case elucidates the importance of BC for making business. It provides a rich description of Design Kitchen receiving an audit, and posits the task of creating a BC plan based on this audit’s findings. Completing this case, students will learn how to analyze and identify BC risks; how to craft a BC plan; and about the complications stirring when top management is not engaged in BC. While fictional, the case description presents a composite narrative based on empirical studies of several companies’ BC risks. Besides teaching BC, lecturers can use the case text for courses of information security management or business process modeling.
1	Africa is well known as the continent with the largest youth population. Digitisation of the continent is an essential part of making sure that its youths can actively participate in the digital economy. Efforts to digitise various sectors of the economy have encountered several challenges including that of skills shortages. The Information Systems (IS) discipline, as one of the computing disciplines, has a role to play in addressing the skills shortage. However, there’s limited research that provides macro analysis of the state of IS education research in the continent. To address this gap, we conduct a systematic literature analysis of publications focusing on IS education. After applying inclusion and exclusion criteria, 25 publications were analysed. The findings highlight some interesting discussions on issues such as context, model curricula, skills, stakeholders, geographical focus, theoretical lens, pedagogy, role of IS and education level.
1	Formal education like higher education oftentimes emphasized on strict non-digital setting. This approach can lead to issues during stressful times (e.g., Covid crisis) or when learners’ needs in general are not considered. Moreover, these times highlighted how important self-regulated learning is and how much this capability is lacking in our educational system. To address these issues, we follow an Action Design Research approach and develop a gamified conversational agent (CA) that considers the learners’ needs. We present our CA and conduct a first small-scale evaluation following a mixed-method approach. First results show that students universally liked a CA for self-regulated digital learning and many enjoyed the gamified experience which helped students to be motivated to learn. As next steps we will develop the next iteration of our CA and conduct a long-term field test at a university.
1	Artificial Intelligence (AI) recently has received substantial attention. This paper provides a case that allows investigation of an AI application that is designed to read contracts and provide a structured summary of the contract. The system uses an ontology to drive the user through reading the contract. The system also uses a "human-in-the-loop" to help the system choose which snippets of information correspond to parts of the ontology. This paper asks the reader to analyze some possible extensions to the approach, as the head of an innovation lab analyzes how the system works and looks for extensions. Finally, the paper investigates the impact of AI on key organizational issues such as responsible and trusted AI, the impact of AI on organizations and work and system acceptance issues.
1	Organic food waste recycling is one of the final frontiers for a sustainable food lifecycle. Digital twins may be helpful to close the loop in more advanced food supply chains, but there is a lack of guidelines on how to adopt this emerging technology in community composting. This paper presents a digital twin-driven design to address this need in a UNESCO-protected region with geological relevance (Geopark). The design science research project offers the foundations for creating intelligent composting networks supported by digital technologies. Six initial design principles are suggested for developing digital twins at a regional level. This study contributes to the deployment of layered digital twins in sustainable regional development. Moreover, our proposal assists the integration of local food producers in regional food supply chains and increases the impact of sustainability brands like GEOfood.
1	Environmental, social, and corporate governance (ESG) reporting has become an important instrument for the sustainable transition of the next generation of business-startup. Nonetheless, poor ESG data quality impedes effective reporting, especially in domains such as Fintech where top-down ESG metrics may overlook pertinent material issues. This action research study applies a design probe in the form of the notion of an ESG data commons to explore possible strategies to improve ESG data quality in Fintech startup. By reporting on the initial results of an ongoing study of a Danish Fintech startup cluster, we develop a practice-based approach that highlights the changing processes, teleoaffective structures, and sociomaterial dynamics of ESG data commons. We contribute to information systems (IS) research in two areas. First, we contribute to the call for a data-driven approach to ESG reporting.  Second, the study extends the IS design literature by applying data commons as a design probe.
1	Social inclusion of people with disability is one of the priorities of the United Nations Sustainable Development Goal 10 – Reduced Inequalities. However, people with disability still face exclusion in society, such as inaccessible facilities and negative social attitudes. In this research, we explore virtual reality (VR) as a tool to raise the general public’s awareness of disability. We conduct an action design research to develop an immersive VR video that promotes disability inclusion, in collaboration with disability practitioners, VR practitioners, disability researchers, and individuals using wheelchairs. In this Short Paper, we present three initial design principles that are critical to addressing three key challenges in disability awareness raising. Our VR artefact, once completed, will serve as a tool for disability awareness raising. The prescriptive knowledge developed could inspire Information Systems researchers and practitioners to explore a similar class of artefacts that promote social inclusion through cultivating awareness and behavioural changes.
1	The digital business environment offers opportunities to develop new business models and support organizational growth. It also brings fast-paced changes, global competition, and turbulence. To benefit from these opportunities, business leaders and managers must adapt and develop the relevant capabilities in this context. However, the literature falls short in describing the competencies required by professionals that lead the digital transformation journey. To address this gap, we conducted a panel with digital transformation experts. We also performed a systematic literature review to identify the competencies needed to lead optimally in the digital business environment. Our results present four categories of competencies based on experts and literature: Technical, Managerial, Social, and Motivational. Our findings bring insights for future research on long-life learning for business leaders and managers. They also inform practice about the relevance of digital literacy in leading an organization’s digital transformation successfully and sustainably.
1	One of the most anticipated questions in the digital age is how the generation who grew up with digital technologies will behave in the workplace. We investigate the role of early-age digital experience on performance drawing on IT identity theory. Specifically, we hypothesized that early-age digital experience indirectly relates to job performance and work innovation sequentially via IT identity and digital creativity. Additionally, perceived managerial support amplifies IT identity’s influences on digital creativity as well as the indirect effects of early-age digital experience on work results. Data collected via a multiple-source and multiple-wave survey from 281 employees in a large Internet company support the research model. This research enriches the understanding of what drives individuals’ digital creativity and demonstrates that employees with early-age digital experience are critical resources for organizational competitive advantage in a digital economy. Practical implications for employees’ early-age digital use and workplace management are discussed.
1	Digital transformation continues to profoundly impact employees in organizations in many industries. While past research has extensively investigated the impact of digital transformation on organizations, we still lack a comprehensive understanding of the factors that are relevant to examine how employees perceive the effects of digital transformation. One theory that explicitly has been built to account for a digital transformation in the past is the theory of the smart machine. To provide insights about relevant factors, we apply the theory of the smart machine and develop and evaluate an instrument to measure several of its key concepts for the first time. We build on established research guidelines to provide a 36-item survey instrument to measure the concepts Introduction of IT, Effects of automation, and Effects of informating. Finally, we provide implications for practice and further research.
1	Social media’s value proposition heavily relies on recommender systems suggesting products to buy, events to attend, or people to connect with. These systems currently prioritize user engagement and social media providers’ profit generation over individual users’ well-being. However, making these systems more “empathetic” would benefit social media providers and content creators as users would use social media more often, longer, and increasingly recommend it to other users. By way of a design science research approach, including twelve interviews with system designers, social media experts, psychologists, and users, we develop user-centric design knowledge on making recommender systems in social media more “empathetic.” This design knowledge comprises a conceptual framework, four meta-requirements, and six design principles. It contributes to the research streams “digital responsibility” and “IS for resilience” and provides practical guidance in developing socially responsible recommender systems as next-generation social media services.
1	There is increasing tension between the service quality improvements consumers receive when they allow their data to be analyzed by a firm, and the costs they incur in terms of privacy sacrifices. This is motivating firms to explore new models to attract and retain a new generation of privacy-active customers (i.e., customers who act in order to protect their privacy). This paper sets the foundation to solve the mechanism design problem for a firm that offers a continuous set of menus allowing its customers to chose their optimal combination of privacy and price. The solution of the problem shows that the relationship between prices and the amount of data shared is non-monotonic. This is a surprising result that may contribute to a better understanding of the privacy paradox, as well as to help scholars and practitioners to push the known boundaries of privacy-based versioning.
1	Advances in artificial intelligence accelerate the digitization of pricing in various industry domains, from home appliances to hotels. At the same time, economic theories and legal cases claim that the advances benefit some players through synchronous algorithmic pricing, so-called Algorithmic Tacit Collusion (ATC). However, the previous studies rely on artificial intelligence’s broad collusion mechanisms in various pricing scenarios. The paper unravels the mixture of ATC problems by specifying the algorithmic pricing context in platform providers’ oligopoly and adjusting the underlying assumptions to the context. Our simulation of Iterative Prisoner’s Dilemma (IPD) games with various heterogeneous pricing algorithms show that ATC emerges in rare conditions (i.e., algorithm and information symmetries). Our findings suggest that understanding the technology and business architectures should precede deriving the theories of ATC and implementing the legal cases and policies of digitization for the next generation’s pricing.
1	A significant acceleration of research-online and purchase offline behavior makes online channels crucial for brick-and-mortars. Realizing this trend, many retail stores have been using a novel strategy, called virtual appointments, whereby consumers can access rich information online before visiting the stores. This study investigates the multifaceted business value of virtual appointments by using a rich dataset related to the car dealers’ business across the U.S. We empirically show that virtual appointments increase dealers’ sales. However, the effect depends on the competition level. The presence of within-brand competitors decreases the benefits of virtual appointment services, while the existence of between-brand competitors increases the benefit of the service.
1	Massive Open Online Courses (MOOCs) are a scalable technology for upskilling but have primarily been successful with highly resourced and well-educated populations. In spite of their low barriers, people experiencing homelessness have not generally benefited from MOOCs. Work-Learn is a new model which utilizes action design research in order to provide individuals experiencing homelessness with the skills and scaffolding that will enable them to benefit from the digital economy. This Work in Progress reports the results of interviews and focus groups focusing on minimum hiring requirements, conducted with eight individuals who are in senior leadership, hiring managers, or training for web development or enterprise computing functions. Preliminary results suggest that after a baseline aptitude has been established, that person-position fit is the critical aspect to consider in instances of homelessness. The mainframe community may represent a unique opportunity due in part to its need for new talent.
1	Organizational use of digital technologies to monitor employee performance is increasing in response to competitive pressures and the emergence of hybrid working scenarios. These technologies provide employers with insights that can be used to increase organizational effectiveness, but have asymmetric power implications that accentuate employee privacy concerns. This study applies a psychological contract theory lens to explore the impact of these concerns in relation to mandatory location tracking, examining the psychosocial outcomes that affect the individual and their trust in the organization. Data collected from 709 Irish police officers are tested using covariance-based structural equation modelling. The findings confirm that information privacy concerns regarding digital location tracking impact the employee and shape their trust relationship with the organization through three pathways, the first being a direct effect, the second arising from the associated loss of autonomy and the third being through employee strain resulting from those privacy concerns.
1	There is an increased interest in data infrastructures that accompany digitalization in the public sector where such infrastructures support serving the next generation of citizens better. Literature on information infrastructures provides a robust foundation, but so far theorization of what differentiates data infrastructures has been limited. We conducted a case study of a data infrastructure to share gender identity data in U.S. higher education. By tracing how a university navigated around the cultural, structural, content and material layers of the data infrastructure to share student gender identity data with the state and federal government, we uncover how data were flowing through the infrastructure. Because of differences in the layers of the data infrastructure between institutions, the flow was subject to blockages, bends and bottlenecks. Our findings demonstrate that the nature of data brings challenges in developing data infrastructures across four levels with implications for theory and practice.
1	IS research often seeks to deliver practical impact, in addition to the traditional requirement for theoretical contribution. While an admirable goal, it is nevertheless a challenging prospect, as key questions remain around how to best facilitate a relationship between IS academic and practitioner communities. To explore this question, our paper investigates boundary spanning by ‘practitioner doctorates’ - PhD students with professional experience who seek to span the fields of academia and practice during their research. Drawing on in-depth interviews with practitioner doctorates, our findings point towards several factors for practical impact such as researcher legitimacy, expectation management, and adapting to changes in industry requirements. In doing so, we contribute towards an understanding of engaged scholarship in IS and take steps towards addressing the dearth of research on doctoral studies in the IS field to date.
1	We propose a theoretical approach informed by a power-in-practice perspective that allows us to examine the emergence of leadership in online communities. We theorize leadership emergence as a process of co-influencing that is constituted by forces of ‘pushing’ and ‘pulling’ different enactments of power that are formative of communal interactions. More specifically we identify three pathways for emergent leadership based on different modes of community influence. These insights are based on a detailed exploration of interactions in one particular online community #WeAreNotWaiting, offering distinct contributions to the literature on leadership emergence, particularly in online communities without formal roles and hierarchies.
1	Online communities thrive on the basis of interactions between like-minded individuals, and usually involve some form of feedback or evaluations by peers. In these contexts, there is systematic evidence of gender-based biases in evaluations. How can such biases be attenuated? We study the efficacy of one approach—anonymization of gender information on the community. We use data from a large-scale digital discussion platform, Political Science Rumors, to examine the presence of gender bias. When users on the community post a discussion message, they are randomly assigned a pseudonym in the form of a given (or first name), such as “Daniel” or “Haylee,” and each post subsequently garners positive and negative votes from readers. We analyze the up votes, down votes, and net votes garnered by 1.4 million posts where names are randomly assigned to posters. We find that posts from randomly assigned “female” names receive 2.5% lower evaluation scores, all else equal. Further, when “female” users post emotive content with a negative tone, the posts receive disproportionately more negative evaluations.
1	One of the key tasks for strategic management accountants is to estimate the size of the market in which their firm operates. For such an estimation to be correct, strategic management accountants need to have access to private information from the firm’s competitors. Such access is impossible since no competitor is willing to share internal documents, resulting in a problem of imperfect information. This problem hinders strategic management accountants’ efforts to perform their main tasks, forcing them to just approximate the size of their firms’ markets. In this paper we show how, by applying text analysis techniques to publicly available documents from their firm’s competitors, strategic management accountants can significantly increase the accuracy of their forecasts. This increased accuracy implies that the use of techniques from the Information Systems (IS) field can help mitigating the thus far unsolvable problem of imperfect information from which the strategic management accounting field has traditionally suffered.
1	Social media overuse is becoming prevalent across the globe, hurting users’ mental health and productivity. To reduce social media usage and improve productivity, many users turn to social media blockers that rely on users to specify a social media reduction goal. However, as there is no empirical evidence and guidance on how users should choose the goal optimally, the user-chosen goals may not produce the intended benefits. In this study, we introduce two new dimensions of social media reduction goals — goal difficulty and goal immediacy. We found that the relationship between goal difficulty and productivity is of an inverted-U shape. In addition, the effect of goal difficulty further depends on the prior social media consumption level. We also found that changing goal immediacy from radical to incremental significantly improves the performance of relatively difficult goals, especially for users with higher prior social media consumption levels. Practical implications are discussed.
1	Regulators have emphasized on mandating compatibility between competing platform ecosystems. In this paper, we study the welfare implications of compatibility by building a stylized model that reflects the competitive dynamics of the current mobile ecosystems market. We consider a device funded and an ad-funded platform that compete for attracting developers and consumers. If compatibility is mandated on the developer side in a way that eliminates the cost of developers to multi-home, then mandated compatibility reduces the welfare of both consumers and developers because it introduces strategic complementarities that limit platform competition for developers. As a result, developers are charged higher prices which through network externalities imply that also consumers are worse off. If compatibility is mandated on the consumer side, by allowing consumers to multi-home, then under strong network effects it can be a Pareto improvement and result in a win-win outcome for all market participants.
1	Artificial intelligence (AI) is becoming increasingly complex, making it difficult for users to understand how the AI has derived its prediction. Using explainable AI (XAI)-methods, researchers aim to explain AI decisions to users. So far, XAI-based explanations pursue a technology-focused approach—neglecting the influence of users’ cognitive abilities and differences in information processing on the understanding of explanations. Hence, this study takes a human-centered perspective and incorporates insights from cognitive psychology. In particular, we draw on the psychological construct of cognitive styles that describe humans’ characteristic modes of processing information. Applying a between-subject experiment design, we investigate how users’ rational and intuitive cognitive styles affect their objective and subjective understanding of different types of explanations provided by an AI. Initial results indicate substantial differences in users’ understanding depending on their cognitive style. We expect to contribute to a more nuanced view of the interrelation of human factors and XAI design.
1	Organizations are undergoing digital transformation in an increasingly more technological world, pushing traditional businesses, more specifically family businesses, to adopt advanced technologies to remain competitive. Digital transformation is one of the key challenges faced by many family businesses today, however, there is little research around this topic. To address this gap in literature, this study asks: How do the paradoxical tensions of a family business influence its digital transformation? We report on an ongoing historical case study at one of the oldest family businesses in the building and construction industry. In our preliminary analysis, we identify three paradoxical tensions that influence the digital transformation initiatives in a family business. Our next step is to further investigate the approaches that the family businesses have taken to revolve these tensions. We contribute to research and practice by understanding the tensions to digital transformation in family businesses.
1	Digital technologies democratise the development of digital innovation. The resulting employee-driven digital innovation has become a major driver for digital transfor-mations and especially important during crisis times, such as the COVID 19 pandemic. To better understand cognitive factors influencing employee-driven digital process innovation (EDPI), we investigate the role of individual mental models for EDPI during times of a crisis compared to ‘normal’ times. Drawing from longitudinal data before and during the COVID 19 crisis, we find mental models having a significant influence on EDPI behaviour during ‘normal’ times. This relationship, however, loses robustness during the crisis, when employees with more accurate mental models show significant less EDPI behaviour before slowly recovering. We relate these findings to the mental models’ explanatory power and derive recommendations for management. Our study contributes explanatory knowledge on employee-driven digital innovation and related cognitive antecedents.
1	Sensors, actuators, and controllers are digital objects fundamental to automation-intensive industries such as transportation, manufacturing, and energy. As technologies that enable and arbitrate the transition from physical to digital worlds, they are increasingly pervasive in all facets of industry and logistics, consumer technologies, or even medicine. Hybrid digital objects with physical and digital components are composed of bitstrings that are inscribed onto a material bearer. Translational action refers to how bitstrings are accessed in the material bearer or how they are moved from one layer of the bearer to another. We perform an inductive study of 170 sensing, computational, and imaging technologies originating from leading scientific research institutions to better understand the nature of translational action. Across four physical and digital configurations, we identify seven forms of translational action. The findings offer insight into cybernetic control theory central to automated systems to understand the nature of their logic, processes, and interdependence.
1	In this paper we investigate the relationship between algorithmic search tools and the innovation process. Today, search algorithms are used for all tasks, yet we know little about their impact on the well-studied innovation process. We suggest a theoretical framework based on centripetal and centrifugal forces that conceptualizes the relationship between the algorithmic design logics of search tools and the innovation process. We use it to illustrate the current challenges with the use of informational search tools based on design principles of popularity and personalization, for innovation. We propose the need to develop and use exploratory search models and tools for innovation.
1	The advent of Artificial Intelligence (AI) challenges our understanding of firms’ knowledge search behavior. While scholars agree on the transformational potential of AI, recent conceptual research offers opposing perspectives on how AI shapes corporate search activities. Through the lens of managerial attention, we examine how AI as a strategic choice affects corporate exploration and exploitation. An analysis of S&P 1500 firms from 2013 to 2021 reveals that AI directs managerial attention toward novel opportunities and the realization of existing ones, thus simultaneously increasing exploration and exploitation. We further find that under high technological turbulence, firms increase their AI activities for explorative purposes. Overall, we contribute to the literature on search behavior, attention-based view, and AI by highlighting the role of a strategic AI orientation for corporate search. Furthermore, we refine the measurement of AI orientation by developing a dictionary based on unsupervised topic modeling eligible for computer-aided textual analysis.
1	In this paper, we propose that contextual “language style matching” between CEO and CIO - a form of similarity in verbal style based on the unconscious use of function words - can provide insight into the quality of collaboration between CEO and CIO. Following upper echelon and managerial cognition research, we argue that high levels of language style matching between the CEO and CIO when discussing the role of technology for the business reflects a shared understanding of the role of technology. As CEO-CIO shared understanding aligns technology innovation with overall business strategy, the economic value of the firm’s technological innovations increases. Counterintuitively, we expect the relationship to weaken when CEOs are overly optimistic, as CEOs are less likely to question technological innovation from a business standpoint. Thus, the shared understanding of the CEO and CIO is misguided. Using panel data, we find empirical support for these predictions.
1	While IT capabilities are an important concept in IS research, past studies often focus on the internal impacts of firms’ IT capabilities. Less is known about how firms’ IT capabilities drive their alliance formation with IT partners. This question is particularly pertinent to firms in non-IT industries as these firms often lack these important capabilities to succeed in the digital era. In this study, we combine the alliance literature and the organization-stakeholder fit theory to hypothesize a U-shaped relationship between the IT capabilities of non-IT firms and their alliance formation with IT partners. We further theorize a complex moderating role of environmental dynamism in this relationship. A panel data set of 8808 non-IT firms in 2012-2020 provides partial support to our theory. This study potentially contributes to the business value of IT literature as well as the alliance literature.
1	Digital innovation is both a necessary and a challenging endeavor for most firms. To achieve progress in this regard, firms across contexts increasingly set up digital innovation units (DIUs). Despite its popularity in practice, the prospects of this initiative are to date unclear. Building upon dynamic capabilities theory, we hypothesize the performance implications of DIUs and employ panel data regressions to a longitudinal and cross-industry data-set to investigate our predictions. We find that DIUs increase performance and that this effect is strengthened by the presence of digital ventures in the industry as well as the degree to which the industry relies on tangible assets. Our additional analyses provide a nuanced perspective on the implications of DIU establishments. On the base of these findings, we derive important implications for IS research about digital innovation and transformation as well as DIUs and provide recommendations for managerial practice.
1	Technological developments are challenging incumbent organizations’ business models, increasing the need for the Top Management Teams (TMT) to initiate a Digital Transformation (DT). This requires a Digital Business Strategy (DBS), which is executed using Managerial Actions (MAs). However, DT success is low, and MAs do not address the complexity and digital leadership skills required to execute the DBS and embed DT into the organization (DT Normalization). To explore these MAs in the context of DBS execution and DT Normalization, we conducted seven in-depth case studies of Dutch incumbent firms in the process of implementing a DBS across a range of industries. Our findings identified eight granular Digital Managerial Dimensions (DMDs), and we have related them to the previously identified MAs. We also related the DMDs to DT Normalization, providing pathways from DBS execution to DT Normalization. Our research contributes to the TMT’s role in guiding the organization through DBS implementation.
1	Digital transformation (DT) is a prevalent phenomenon across multiple industries with substantial impacts at the organizational, industry, and societal levels. Although DT have been explored in various contexts, most studies have taken for granted that the focal organizations could afford and own the human, technological, and monetary resources required for successful DT. However, not all organizations would want to bear the costs of owning these resources, and could seek to access them, but not own them, in a dynamic and transient arrangement. Using the case study of a mega church in New Zealand, we find that successful DT of such organizations is underpinned by attaining resource fluidity, which consists of 3 phases: the (1) Acquisition, (2) Activation, and (3) Application phases. This paper elaborates on each of these phases and presents a framework that could guide organizations to leverage resources they have access to, but not own to enact DT.
1	The advent of the Internet of Things (IoT) forces incumbent firms to reshape their organizational structures toward platform ecosystems. However, prior research lacks concrete insights about how incumbent firms can foster value co-creation to become ecosystem orchestrators. In particular, it only sheds little light on the complex challenges incumbents face in designing and governing IoT platform ecosystems. In response, we present a single case study describing how the departments of Robert Bosch GmbH, a leading IoT company, overcame these challenges in three dimensions—IoT ecosystem, IoT platform, and value co-creation. We tie in our research with the existing body of literature, identify four prevailing tensions in ecosystem establishment, and provide actionable design and governance recommendations to resolve them.
1	Digital transformation is an important and omnipresent topic for corporations that want to stay relevant and survive in today’s business environment. The success of digital transformation depends on a multitude of factors, one of which is digital transformation governance. This study investigates the moderating effect of CIO membership in the top management team on the relationship between enabling factors of digital transformation and digital transformation success. Furthermore, different digital transformation governance configurations are investigated regarding their effect on the success of digital transformation. We make use of a large scale, multi-national survey among manufacturing firms to answer these two research questions. With our study, we demonstrate the importance of including the CIO in the top management team to take advantage of existing resources and capabilities. Additionally, we show that firms with a CIO being responsible for digital transformation perform significantly better in their digital transformation endeavors.
1	Business Intelligence and Analytics (BI&A) systems form the key information processing artifact that enables firms to process, store, and use the data generated by the Internet of Things (IoT) in the supply chain context. We empirically investigate how firms create value from IoT through a ‘capability creation’ path model for Supply Chain Analytics Capability. Partial least square analysis of primary survey data collected from 127 firms in India provides two key findings: 1) a modular system architecture and decentralized governance across supply chain partners are important precursors to build a robust Supply Chain Analytics Capability which can utilize IoT based data 2) Supply Chain Analytics Capability influences Firm Performance in two ways - directly, through Supply Chain Integration, and interactively with Supply Chain Integration. Overall, this study establishes the antecedents and consequences of Supply Chain Analytics Capability, which is an important precursor to value creation through IoT.
1	IS integration problems are often an important determinant of negative value creation in Mergers and Acquisitions.  To date, these problems are commonly attributed to mis-aligned Business and IS Integration Strategies, flawed preparation or execution or negative synergies, but the role of IS itself is underemphasized. Based on a case study and expert interviews, we propose a theory addressing this issue. Our explanation focuses on the concept of IS Antagonism, referring to the destructive interaction between previously independent information systems, which occurs when these are operationally combined. This concept offers novel explanations beyond strategic misalignment and considers the nature of the information systems themselves in the integration phase. IS antagonism is omni present in M&A, which has the practical implication that we need to account for its value destructing characteristics in pre-merger synergy predictions and by securing necessary IS resources to mitigate during execution.
1	Recent research has shown managers’ tendency to cut discretionary investments to meet short-term earnings targets. However, how these aspirational levels of performance and associated conflicts of interest between managers and shareholders influence information technology (IT) investments has hardly been examined. Drawing on behavioral agency theory, we analyze how earnings pressure – the pressure managers feel to meet or beat analysts’ consensus earnings forecast – influences IT investments. We find that earnings pressure is associated with a reduction in firms’ IT investment commitment, based on the frequency of sentences within 10-K filings emphasizing IT investment. This finding points towards a hitherto unconsidered influence of capital markets on IT investments in literature on IT investment determinants. Further, we plan to analyze if this reduction entails negative or positive stock market performance consequences. Understanding the performance consequences will allow us to lay the foundation towards effectively addressing this issue within corporate governance.
1	Organizations need to develop digital competences to utilize digital technologies to succeed in digital transformation. Yet, current efforts on organizational strategies to develop digital competences, i.e., digital M&A or appointing a CDO, have been conducted in isolation. However, following digital ecodynamics, material, organizational and environmental factors are in fact interwoven. We aim to cater to this confluence by taking a configurational perspective on digital competence development and its effect on digital transformation. We integrate prior findings of digital competence on (1) the firm level and the role of knowledge resources, (2) the leadership level and competence of the firm’s upper echelon, and (3) the role of contextual complexities in the form of the firm’s environment and structure. Subsequently, we employ fsQCA on a unique dataset. Thereby, we disentangle the multifaceted complexity of digital transformation and provide more fine-grained conceptual insights into the phenomena.
1	In the public sector, projects are one of the main mechanisms of implementing strategies and that is evident in healthcare. Extant studies of health information systems (IS) implementations find that IS alignment enables organisations to meet their strategic objectives; conversely, misalignment can lead to unintended, and often adverse, results such as the abandonment of health IS projects. Studies attribute misalignments to strategic drift, which we find more likely in pluralistic settings given multiple parties, with potentially competing goals and interests, attempt to implement a shared strategy. This study contributes to extant literature by exploring IS alignment as a dynamic process in the context of a 20-year health IS implementation involving multiple organisations from government, public, and private sector. The question we aim to address for governments, developers, and implementers - how do we collectively move beyond the short-lived success of projects to achieve the envisioned strategic benefits of health IS?
1	While current research on digital transformation strategy (DTS) promotes the usefulness of planning rationality, the role of serendipity and its impact on the DTS formation has been neglected so far. Given the environmental volatility and disruptions executives face today, we contend that reconfiguring through planning alone is impossible. Instead, as organizations encounter unexpected occurrences, they need to improvise with the means immediately at hand. In our study, we explore the intersection of DTS formation and improvisation. Drawing on Mintzberg and Waters’ (1985) strategy formation concept, we present a typology of three DTS formation approaches and analyze their viability for satisfying the demand for organizational improvisation in the DTS formation context. Our study contributes to the nascent discussion on strategic improvisation by shedding light on a crucial but latent layer of strategy-making that provides untapped insights for better understanding the mechanisms affecting DTS formation.
1	Digital innovations are not only associated with opportunities for value creation but also lead to threats, for example, additional cybersecurity risks. Dealing with the conflicting requirements of innovations and cybersecurity can lead to a trade-off for organizations that companies setting up innovation units outside their core organization need to address. We conducted a cross-industry interview study to investigate the impact of organizational design of innovation units on the consideration of cybersecurity. Our results, embedded in Galbraith’s star model, reveal five types of innovation units and three patterns of organizational design that impact this consideration. The effect of these patterns ranges from an ill- or over-consideration to a cybersecurity-innovation equilibrium. Thereby, we extend the existing literature on the trade-off of innovation and cybersecurity by organizational design considerations regarding strategy, structure, and processes. This theoretical contribution has implications for the organizational design of innovation units in practice.
1	The transportation sector generates the largest share of greenhouse gas emissions, which concerns governments and communities worldwide. Electric vehicles (EVs) are believed to be the future. Various incentives have been provided to further broaden their acceptance and accelerate their adoption, including toll-exemption programs for EVs. At the same time, an individual’s driving behaviors are largely shaped by navigation applications that provide real-time traffic conditions. In this paper, we aim to understand how information provision affects the optimal structure of the EV-exempt toll. By analyzing a Bayesian routing game, we illustrate the optimality of a non-monotonic tolling strategy as a function of the EV adoption rate. For policymakers, our finding reveals the importance of understanding how the IT-enabled information provision has altered individual drivers' behavior. In addition, the results uncover the general impact of IT, which expands the action space of individuals and the effective regimes of policies.
1	With the rapid development of online media, in which personalized recommendations are provided, users are gaining increasingly narrow access to information, trapping them in so-called “information cocoons.” At the same time, the increase in homogenized content has brought boredom and fatigue, which are not conducive to the long-term interests of a platform. Grounded in the entertainment consumption context, as represented by the Tik Tok short video platform, this study focuses on the information cocoon reinforcement and browsing fatigue phenomena caused by the lack of proper diversification. Then, to mitigate these issues, this paper proposes relevant diversified content and diversification timing countermeasures to optimize the “what” and “when” technical designs. We explore the role of perceived serendipity as a key path toward user diversity acceptance and browsing duration, thus alleviating the phenomenon of information cocoons and browsing fatigue and facilitating the common development of platforms and users.
1	Smart home technologies (SHTs) perform tasks in the most intimate areas of life and therefore require blind user trust from the start. To build this trust, vendors often rely on creating human-like interactions with devices, such as by incorporating humor. Although humor in SHTs is becoming more advanced, e.g., through advanced joke selection algorithms, its actual impact is largely unexplored. In this work, we address this gap and study the impact of affiliative humor as a human-like characteristic on perceived social presence and initial trust in SHTs. To this end, we conducted a vignette-based experiment with potential users (N=63). Our results contribute by uncovering the mechanisms underlying humor as a trust-building characteristic in SHTs. Moreover, in this way, we also provide important insights for the design and communication of SHTs, which can be valuable for vendors to foster perceived human-likeness and thus initial user trust in smart technologies.
1	As metaverses are growing users need self-avatars to complete more and more tasks, such as virtual clothing-try-on or virtual work-meetings. To design these self-avatars the application of self-congruity theory suggests that avatars' appearance should ei-her be congruent with users’ actual view of themselves or ideal view of themselves. Past research has focused on comparing outcomes of using idealised avatars versus realistic avatars. Yet, it remains unclear what constitutes the ideal versus the actual self in avatar design. This short paper represents ongoing research to develop a theoretical avatar design framework to evoke either users' ideal or actual self. Building on evolutionary and computational approaches to facial beauty, we identify three groups of design factors that stimulate the ideal self: skin-homogeneity, face-symmetry, and balanced face-ratios. This work advances our understanding of the antecedents of the ideal versus actual self and makes self-congruity theory applicable to avatar design.
1	Voice commerce allows customers to carry out sales dialogues with voice assistants (VAs) through natural spoken language. However, its adoption remains limited. To help determine how to overcome existing barriers to adoption, we conducted a series of three empirical pre-studies and a laboratory experiment (N = 323) investigating the role of VAs’ humanness in interactions with customers; research has reached no consensus on this matter. Our results reveal that humanizing VAs increases customers’ perceptions of social presence and parasocial interaction, thereby enhancing perceived relationship quality and ultimately leading to increased intentions to shop using the VA. Although, we also find a negative direct effect of humanization on parasocial interaction, it is offset by the larger positive indirect effect via social presence. This may provide one explanation for the inconsistencies in the literature. For practitioners, our findings highlight the importance of careful design in humanizing VAs to increase voice commerce adoption.
1	Algorithmic forecasts outperform human forecasts by 10% on average. State-of-the-art machine learning (ML) algorithms have further expanded this discrepancy. Because a variety of other activities rely on them, sales forecasting is critical to a company's profitability. However, individuals are hesitant to use ML forecasts. To overcome this algorithm aversion, explainable artificial intelligence (XAI) can be a solution by making ML systems more comprehensible by providing explanations. However, current XAI techniques are incomprehensible for laymen, as they impose too much cognitive load. We contribute to this research gap by investigating the effectiveness in terms of forecast accuracy of two example-based explanation approaches. We conduct an online experiment based on a two-by-two between-subjects design with factual and counterfactual examples as experimental factors. A control group has access to ML predictions, but not to explanations. We report results of this study: While factual explanations significantly improved participants’ decision quality, counterfactual explanations did not.
1	Service robots that interact with customers have penetrated various industries. With a basis in social identity theory, this study examines how customers respond to frontline service robots (FSRs) by investigating norm-compliant versus norm-violating behaviors compared with similar behaviors by human frontline employees (FLEs). In experimental studies, a black sheep effect occurs, such that customers downgrade norm-violating FLE behaviors more than similar behaviors by FSRs. They also upgrade norm-compliant behaviors by human FLEs more than those of FSRs. In service failures, this effect manifests as greater anger and frustration toward the FLE. We establish the underlying mechanism driving the black sheep effect: customers assign FSRs to an outgroup but categorize FLEs to their social ingroup, across different service encounters and independent of interaction frequency.
1	To avoid the detrimental consequences of global warming, digital nudges were recognized as effective means to steer individual behavior toward sustainability. We investigated the applications, contexts, and outcomes of green digital nudges by conducting a systematic literature review of 64 nudge interventions. We found six distinct types of nudges—priming, goal-setting, default, feedback, social reference, and framing—and 18 sustainable target behaviors (e.g., energy conservation). To explain how behavior changes through green nudges, we clustered the identified target behaviors into three behavior change outcomes: (i) altering an existing behavior, (ii) reinforcing an existing behavior, and (iii) forming a new behavior. Based on our findings, we propose guidance for researchers, practitioners, and policymakers who seek to design choice architectures that facilitate pro-environmental behavior.
1	Empirical evidence has supported the idea that eSports players' emotions could be reflected in their mouse usage. Still, findings from IS literature on the exact relationships between users' mouse usage patterns and their emotional states have been mixed. Possible causes include adjustment effects and offsetting effects. To address these problems, this study proposes a self-developed game named Hearth, which supports non-intrusive and concurrent tracking of players' emotions and mouse usage. The game design supports the examination of the two possible effects. Results show that negative emotion was positively associated with the total mouse movement distance in a game turn, average task-level distance, and average task-level speed. Moreover, the open-source game proposed in this study facilitates further data collection from natural experiments due to its triadic design that addresses reality, meaning, and play.
1	Technostress research asserts that the use of information systems (IS) can be challenging or hindering. Previous literature has mostly focused on the challenge or hindrance subprocesses. However, research suggests that these subprocesses may interact with each other. Positive user responses can be derived from events that were originally perceived as hindering. The present research-in-progress paper focuses on this interaction. We investigate whether successful coping – the elimination of a stressful IS use situation – leads to positive user responses in the hindering subprocess. Therefore, we develop an online experiment, which emulates different IS use situations. A hindrance techno-stressor situation (HTS), a control situation without a techno-stressor (non-HTS), and one in which users can successfully cope with the hindrance techno-stressor (SC). The experiment allows us to analyze the interactions between the subprocesses. We expect to contribute to the literature on technostress and IS coping by focusing on the interaction between the two subprocesses
1	Chatbots are increasingly equipped to provide choices for customers to click and choose from when communicating with the chatbots. This research investigates when and why implementing choices enhances or impairs customers’ service experience. Based on the concept of fluency, we posit that the implementation of choices is beneficial only after a conversational breakdown occurs because the value of choice provision for facilitating fluency may not be recognizable or realized in the absence of service breakdowns. We further propose that the implementation of choices is counterproductive when the choice set is perceived as incomprehensive because it decreases the perception of fluency. We conducted several experiments to test these hypotheses. By illuminating when and why choice implementation may help or harm customers during a chatbot-initiated service interaction, we augment the current understanding of a chatbot’s role in customers’ service experience and provide insights for the deployment of choice-equipped chatbots in customer service.
1	In this paper, we examine how advice from an AI algorithm should be provided to decision-makers that work in a crowd setting. With a theoretical model and numerical experiments we show that the harmful effect of incorrect advice relative to the beneficial effect of correct advice increases with increasing crowd size. Thus, for larger crowds, more advice should be withheld so that it does not negatively affect the crowd accuracy. We propose a mechanism for AI advice personalization that takes the crowd size into account. In an experimental study where subjects classified images, we demonstrate that the crowd size-dependent advice personalization reduces the detrimental effects of incorrect advice and leads to an increase in crowd accuracy.
1	Conversational agents (CAs) increasingly permeate our lives and offer us assistance for a myriad of tasks. Despite promising measurable benefits, CA use remains below expectations. To complement prior technology-focused research, this study takes a user-centric perspective and explores an individual’s characteristics and dispositions as a factor influencing CA use. In particular, we investigate how individuals’ self-efficacy, i.e., their belief in their own skills and abilities, affects their decision to seek assistance from a CA. We present the research model and study design for a laboratory experiment. In the experiment, participants complete two tasks embedded in realistic scenarios including websites with integrated CAs – that they might use for assistance. Initial results confirm the influence of individuals’ self-efficacy beliefs on their decision to use CAs. By taking a human-centric perspective and observing actual behavior, we expect to contribute to CA research by exploring a factor likely to drive CA use.
1	Real-world cognitive load monitoring promises to be an essential foundation for positive adaptive information systems that foster knowledge workers’ productivity and well- being. Towards this goal, our research combines established EEG load monitoring principles with low-cost, 3D-printed headphones. This provides a means for continuous, unobtrusive, and real-time cognitive load detection. Results from two experiments document strong relationships of cognitive load reports with EEG frequency bands (Theta and Alpha), both around the ears and on top of the head. While limitations with Theta band power sensitivity are observed, Alpha band modulations are robust across sessions, task repetitions, and three tasks. Furthermore, short setup durations are found, and only minimal influences of hairstyle or glasses on setup times and load relationships. With a discussion of the remaining challenges for more naturalistic studies, this article documents the system’s future potential for load sensing during knowledge work.
1	There is an emerging trend in digital interface design to include the dark mode (i.e., font in white against a dark background) in addition to the traditional default light mode (i.e., font in black against a white background). While this innovation was motivated by usability considerations, it is unknown whether and how different screen display modes can influence user behaviours. Drawing on the findings from environmental psychology, we propose that screen display mode can influence users’ moral decision making. Specifically, we focus on users’ decisions to conduct financial crimes and predict that users are more likely to conduct financial crimes when using dark (vs. light) mode. We propose perceived anonymity as the underlying mechanism and theorize the moderating effect of screen size. Two laboratory experiments were designed to test on two financial crimes, namely, insurance fraud and insider trading. The potential theoretical and practical contributions are discussed.
1	This study investigates how visual stimuli influence cancer-related charitable online giving. Particularly, the study investigates how different types of crowdfunding campaign pictures affect donors’ decision to contribute to specific campaigns. We gathered crowdfunding campaigns from GoFundMe and divided them according to the main picture used in each campaign, i.e., cancer-related pictures vs. non-cancer-related pictures and pictures of individuals vs. pictures of groups. We then conducted an online experiment and a laboratory experiment using physiological measures. The results from the experiments show that cancer-related pictures receive more money and more immediate attention and arousal than non-cancer-related pictures. Furthermore, group pictures receive more money and more total attention than individual pictures. The physiological measures from the laboratory experiment provide valuable knowledge about the underlying emotional mechanisms involved in the donation process.
1	Human Computer Interaction scholars have predominantly adopted a “user-centered” approach to study changes in product functions and aesthetics. Relatively few, however, have explored how designers radically innovate by proposing new product meaning with digital technology. Product meaning refers to an impression, conveyed collectively by a defining set of functional and/or aesthetic qualities of a product, that cause it to be perceived as a particular kind of product. We draw on design and innovation theories and use comparative case studies of electric vehicle to articulate three mechanisms by which designers propose “what would be desirable” for users: designers use digital technology (1) as a part of the product, in the process of delivering product, and in new contexts to propose innovative meaning (2) to collect user feedback, and (3) to selectively take user feedback to re-propose innovative meaning. The paper extends previous user-centered IS literature with a designer-centered approach.
1	With the promise of autonomy, flexibility, and “being your own boss” the gig economy is growing to be one of the most important economic and social developments of our time. This growth is possible due to the platform’s reliance on algorithmic control, which comprises the use of algorithmic technologies to control and align workers' behavior. Conducting a multiple-case study on the use of algorithmic control in two app-work platforms (Uber & Mjam) and two crowdwork platforms (Upwork & Freelancer) on the basis of established control concepts, we develop a holistic understanding of algorithmic control and show how platforms realize this new form of control along three dimensions: control allocation, control formalization, and control adaptiveness. We contribute also by introducing the concepts of control artifacts and internalized control as a step forward in explaining algorithmic control phenomena.
1	Organizations’ introduction of algorithmic technologies fundamentally affects employees’ work processes, tasks, and responsibilities in organizations. Employees often find their professional identities threatened by the introduction of IT (a phenomenon labeled as IT Identity Threat). While prior studies have examined which mechanisms employees use to deal with such a perceived threat, it remains unclear how an IT Identity Threat affects employees’ work attitude in response to advanced IT such as algorithmic technologies. Employees’ work attitude is a recognized antecedent to workers’ well-being or performance. Based on a mixed-method study in the banking industry, our study reveals that an IT Identity Threat negatively affects employees’ work engagement. Further, our study uncovers how this effect comes about by showing that an IT Identity Threat decreases employees’ perceived autonomy and experienced responsibility for their work outcomes. Overall, both factors contribute to a negative relationship between an IT Identity Threat and employees’ work engagement.
1	On digital labor platforms, interactions between workers and clients are algorithmically managed. Previous research found that algorithmic management can disadvantage workers. In this paper, we empirically examine algorithmic unfairness from a sociotechnical perspective. Specifically, we conduct online focus groups with 23 workers who directly interact with algorithmic management practices on digital labor platforms. In using grounded theory methodology, we pursue to understand how algorithmic management promotes unfairness on digital labor platforms. Our emergent theory understands algorithmic unfairness as algorithmic management practices that give rise to systematic disadvantages for workers. Algorithmic management practices either automate decisions or automate the delegation of decisions. Workers experience systematic disadvantages in the form of devaluation, restriction, and exclusion. Our findings serve as a starting point for mitigating algorithmic unfairness in the future.
1	We review the literature on algorithmic management to help future researchers acquire a comprehensive "recap" of past research with detailed discussions on the main findings and develop a taxonomy as a tool of summarization that assists researchers in reflecting critically on their systems and identifying potential gaps. We determine five critical areas of algorithmic management: the mechanisms of algorithmic management, effects of algorithmic management, second party's response to algorithmic management, concerns around algorithmic management, design of algorithmic management, and policy implications. These topics are analyzed and discussed.
1	Asynchronous video interviews (AVIs) provide scalable, low-cost opportunities for matching interviewees and organizations. However, the implications of a shift from synchronous interviews aren’t fully understood, especially when design choices such as AI evaluations are employed. To better understand the impact of AVIs, we undertook an exploratory qualitative study in addition to an experiment. The first study involves 100 qualitative responses and exploratory quantitative tests on the relationships between coded values and demographic and trait variables of the respondents. Our second study tests the impact of AI feedback using a large online AVI service while accounting for various disadvantaged groups that could experience discrimination in their AVI interactions. We developed 5 propositions regarding the interaction of interviewee traits and AVI design. Additionally, we did not find support that AI feedback increases the performance of interviewees, though we identify several traits that lead to high AI scores and human-rater performance.
1	Acquiring and retaining skilled IS professionals is a crucial success factor for organizations. Skilled IS professionals become even more important considering that organizations are constantly trying to improve through new innovative information technology. We plan to uncover the effects of conflicting explicit and implicit attitudes towards innovative technologies on job satisfaction and turnover intention, using blockchain technology as an example. We hypothesize that organizational factors such as IT capabilities and innovation support might have different effects on explicit and implicit attitudes, leading to attitude discrepancy, which negatively affects job satisfaction and increases turnover intentions. We plan to use a single concept implicit association test and a survey to assess implicit and explicit attitudes. Through our research, we expect to provide a better understanding of antecedents of job satisfaction and turnover intention on an organizational level that allows to derive better management practices for handling innovative technologies.
1	Recently Artificial Intelligence (AI)-enabled conversational agents or chatbots (ICA hereafter) have been widely introduced in online customer service, and are expected to transform the frontline workforce. However, most studies from employees’ perspectives have been qualitative in nature. Moreover, extant empirical studies perceive ICA as a tool rather than considering ICA as an AI-enabled digital workforce. Besides, rare papers moved further to explore the rooted psychological drivers (such as identity) underlying the employees’ actions. To address these gaps, our paper integrates the identity theory and cooperation perspectives to examine the impact of ICA’s human-like capability on employees' job identity through the enhancement in work experience. Our study is expected to provide an innovative perspective viewing ICA as a human-like agent rather than a tool in behavior studies. This study also enriches the identity theory and cooperation-competition theory and promotes their applications in IS literature.
1	With the advent of artificial intelligence (AI), individuals are increasingly teaming up with AI-based systems to enhance their creative collaborative performance. When working with AI-based systems, several aspects of team dynamics need to be considered, which raises the question how humans’ approach and perceive their new teammates. In an experimental setting, we investigate the influence of social presence in a group ideation process with an AI-based teammate and examine its effects on the motivation to contribute. Our results show a multi-mediation model in which social presence indirectly influences whether human team members are motivated to contribute to a team with AI-based teammates, which is mediated by willingness to depend and team-oriented commitment.
1	Continuous developments in information technology and fast, constantly changing environments are challenging our notions of work within organizations. Researchers and practitioners often cite less-hierarchical organizations, which radically decentralize decision authority, as a possible solution to this issue. While some concepts such as Holacracy are captured in the literature under the terms ‘future of work’ or ‘reinventing organizations,’ so-called Decentralized Autonomous Organizations (DAOs) can provide researchers with additional possibilities to test and challenge assumptions about work and organizations. However, DAOs heavily rely on their members' active participation to collectively manage, improve, and govern the organization, which introduces a risk to the organization. Therefore, our work aims at uncovering the drivers for participation within DAOs. We plan to contribute to research by opening up a new facet of participatory drivers in less-hierarchical organizations. From a practitioner's perspective, our insights can be helpful in supporting their members’ active participation.
1	Prior empirical work provides contradictory findings on the consequences of information and communication technologies (ICT) use at work, indicating both positive and negative consequences for employees. Integrating insights from the deep work framework with flow literature and cognitive load theory, we build and test a dynamic theoretical model to explain these contradictory findings. We move the research perspectives of ICT use and deep work to a dynamic perspective and investigate day-to-day changes in ICT use and deep work on the within-person level. The results of our quantitative diary study (n=387) provide evidence that technical ICT use has a positive effect on deep work whereas social ICT use has an inverted U-shaped relationship with deep work. The effects of social and technical ICT use on deep work depend on workplace telepressure, work experience, and knowledge intensity of work.
1	How post-pandemic workplaces will evolve is one challenging decision organizations must consider. Prior studies have explored remote work and digital workplace transformations. However, literature offers only little insight into the status quo of employees' preferences for their future workplace and its consequences. This paper posits that employees' openness to digital change influences hybrid workplace preference. Performance and personal outcome expectations further have a mediating role in this relationship. Finally, hybrid workplace preferences can lead to office resistance and the intention to leave. This paper draws on social cognitive theory and sheds light on the interplay of employees' preferences and potential consequences for businesses. We empirically tested the proposed model with survey data from U.S. employees. Findings show that hybrid settings are critical to attracting talent open to digital change. The contribution to IS literature is manifold and contains implications on how to envision the future workplace successfully.
1	Recently, businesses are introducing low-code development platforms (LCDP) that enable employees with little to no development expertise to develop their own systems to improve their work. These so-called business unit developers (BUDs) possess necessary domain knowledge to understand how to use LCDPs to create useful (self-) services. Using job resource demand theory and the job crafting model, we conceptualize that BUDs use of LCDPs can be framed using the theoretical lens of job crafting. Job crafting stems from vocational psychology and provides well-researched positive consequences, such as wellbeing and meaningfulness. Thus, our research objective is to understand how BUDs can use LCDPs to job craft to gain access to positive job crafting consequences. We interviewed 17 experts across three organizations that employ an LDCP for chatbots. Our results suggest that job crafting is a suitable framework for understanding the effects of LCDP use.
1	This research seeks to unearth Information Technology (IT) use by disaster responders (DRs) deployed by their affiliated disaster response organizations (DROs) for natural disaster response missions. Our on-ground analysis sheds insights into how several types of IT use behavior are surfacing as the DRs concurrently serve the role of a member of the ephemeral disaster response organization and the affiliated DRO. Informed by the role expansion lens, role stacking and its consequential IT use behavior emerge to explain how behavior towards institutional IT tasks is shaped by the location and activities of the DRs. This research expands the understanding of IT use in situations where users are disentangled from a preexisting institutional boundary through mission deployments. Such an understanding is particularly important since providing IT applications to the employees is a substantial investment committed by an institution. However, users do not necessarily use the institutional IT applications in certain situations.
1	Videoconferencing fatigue or ‘Zoom’ fatigue has emerged as a distinct and pressing phenomenon in light of rapid videoconferencing adoption during and after the COVID- 19 pandemic. As part of an ongoing broader literature review project, we find that extant literature primarily conceptualises videoconferencing fatigue as an error that needs to be detected and corrected based on techniques derived from medical, psychological, technological and media theories. However, we observe that videoconferencing is also a work activity, and thus consider what additional insights on videoconferencing fatigue could be obtained by deconstructing videoconferencing according to the labour that is involved in videoconferencing. Based on thematic analysis of the extant literature, we thus develop a perspective on videoconferencing in relation to the performative and interpretive labour that videoconferencing entails. This new way of thinking about videoconferencing fatigue, as labour cost, enables us to offer implications for theory and practice, and comment on directions for future research.
1	During the global pandemic, information workers were abruptly forced to engage in virtual work. This paper reports on an experiment seeking to formalize the formalization of small team coordination at London Blockchain Lab through the use of blockchain-supported tokenization. The Web3 organizing vision promotes the technology as an enabler of new ways for individuals and organizations to engage in the transparent exchange of scarce digital rights. However, little attention has been paid to the use of blockchain technologies to coordinate distributed collaborative activities. This paper seeks to understand the viability of this vision amongst a community of expected early adopters through design experimentation resulting in interview data. The study points towards the significant gap between the Web3 vision and the problems of realizing this in practice. This highlights fundamental barriers to using blockchain for team collaboration while also pointing toward its potential. Even the most willing and able find it hard to turn code into law through tokenizing collaboration.
1	While augmentation is commonly presented as a desirable path in AI development and implementation, we have not yet found a shared definition for this concept. As the verb “to augment” needs to be followed by a target, we raise the question: What is augmented with AI? Building on a literature review of the augmentation narratives in five different disciplines – i.e., labor economics, computer science, philosophy, management, and information systems – we identify eleven distinct augmentation perspectives taken by scholars of those fields, including the underlying theoretical concepts that indicate what is intended to be augmented. This paper contributes to theory by “going beyond augmentation as collaboration” and helping us to move “towards collaboration for augmentation”.
1	This study focuses on hybrid working in the university sector. The diversity of the roles performed in universities provides a unique opportunity to study how the ‘where and when’ of knowledge work is evolving in the post-pandemic era. Our research aims to understand how hybrid working is 1) planned and 2) practiced in a university context. Through an analysis of ten university policies, we find that the ambition for hybrid working reveals several contradictions. Further, technology is mostly backgrounded in discussions of this new way of working. These preliminary findings challenge us consider how the initiatives predicted in a hybrid working policy take place in practice, at a large Dutch university. Based on interviews and systematic observations, we aim to enrich discussions of hybrid working with an open stance towards how and what role technology plays in the new era of work in the university context.
1	Deploying Artificial Intelligence (AI) proves to be challenging and resource-intensive in practice. To increase the economic value of AI deployments, organizations seek to deploy and reuse AI applications in multiple environments (e.g., different firm branches). This process involves generalizing an existing AI application to a new environment, which is typically not seamlessly possible. Despite its practical relevance, research lacks a thorough understanding of how organizations approach the deployment of AI applications to multiple environments. Therefore, we conduct an explorative multiple-case study with four computer vision projects as part of an ongoing research effort. Our preliminary findings suggest that new environments introduce variety, which is mirrored in the data produced in these environments and the required predictive capabilities. Organizations are found to cope with variety during AI deployment by 1) controlling variety in the environment, 2) capturing variety via data collection, and 3) adapting to variety by adjusting AI models.
1	Antimicrobial resistance is described as a global health emergency, particularly affecting low and middle-income countries. A key strategy to engage with this challenge is effective monitoring to improve knowledge and support evidence-based interventions. However, LMICs lack the capacity, resources, infrastructure, and culture to implement digital interventions. To engage with this challenge, empirical work is carried out within the context of a public hospital in India to study the problem of antibiotics use followed by the design, implementation, and use of an AMR monitoring system and associated challenges in its digitization. An ADR approach is used to guide the design of the system to facilitate responsible antibiotic prescriptions by physicians. Three broad design principles are proposed which can help guide future implementation efforts for other contexts. This paper makes an important contribution to IS research of immense societal value, in informing how the potential of the digital can be effectively materialized.
1	While cryptocurrencies are related to profit-driven actors, communitarian movements have decades of experience with social-driven currencies, such as community currencies. This research investigates the meshing of these two disparate worlds that results in the design of a solidarity cryptocurrency, a phenomenon that connects the blockchain infrastructure of cryptocurrency to scaling the social perspective of community currencies. However, making the connection between these two technologies brings a new question to IS design: how can different frames from multiple social actors be integrated into designing a solidarity cryptocurrency infrastructure? We drew upon the design ethnography methodology and actively participated in designing a solidarity cryptocurrency to answer that question. Based on concepts from infrastructuring, a multi-relational and socio-technical approach to infrastructure designing, we propose that designing a solidarity cryptocurrency lies on a dialogic tension between techno-centered and community-centered frames, representing the relational process that emerges when connecting two disparate technologies
1	Digital infrastructures are socio-technical arrangements of physical objects, digital technologies, users, and processes. They constantly evolve and provide the foundation for the emergence and implementation of a variety of applications. But how and why do digital infrastructures evolve? In this short paper, we report on an ongoing study of an Industrial Internet of Things (IIoT) system by drawing on digital trace data about the system’s development process and using code complexity analysis. We present interaction patterns between the development of a digital infrastructure’s core services on the one hand and the development of specific applications on the other. Based on these patterns, we point to several puzzles that we identify around digital infrastructure evolution.
1	Although the existing literature portrays information technology projects as complex, dynamic, socio-technical endeavors, research on cost and schedule performance mostly focuses on single factors in isolation. In this paper, we use a configurational approach to overcome this disconnect. We collected interview and objective data from 30 IT projects and conducted a qualitative comparative analysis to examine how configurations of project conditions relate to cost and schedule adherence. Our analysis shows four configurations associated with budget or schedule adherence: Prestigious feedback-based projects, lucky projects, disciplined projects, and team-based projects. We discuss the implications of these results for information technology project management research and practice.
1	While information systems development (ISD) projects play a pivotal role in maintaining a competitive advantage, ISD project distress evolves dramatically. Given the complex and dynamic nature of ISD projects, they are prone to Escalation of Commitment (EoC), the irrational tendency to persist with failing courses of action. While EoC has been studied to a great extent in management and psychology literature, research on its role in the context of ISD project distress is fragmented, making it challenging to develop de-escalation strategies. To address this gap, we conduct a literature review on EoC in the context of ISD project distress. The proposed nomological net including triggering factors, consequences, mediators, and moderators, as well as a set of developed de-escalation strategies can serve as an inspiration and foundation for future IS researchers. By presenting this review we hope to inform future IS research to acknowledge the role of EoC in ISD projects.
1	For years, there has been an emphasis on how to efficiently and effectively identify, evaluate, and implement innovative information systems in both design science research (DSR) and practice. Nonetheless, still today, these efforts continue to be hampered by the temporal gap between ideation and evaluation. Usually, innovative ideas are implemented at a late stage of maturity (e.g., prototypes) to test their viability in practice. This widespread approach results in waste of resources and time if the viability of an idea fails outside the lab environment. This paper discusses an ex-ante evaluation approach derived from “pretotyping” that allows innovative ideas to be tested in naturalistic settings even before they have been implemented. Thus, we call them “phantoms”. We show how this approach reduces temporal and relevance gaps, and we provide a preliminary assessment of its practicability by presenting and discussing three case studies conducted with real organizations and prospective users.
1	Software projects rely on what we call project archetypes, i.e., pre-existing mental images of how projects work. They guide distribution of responsibilities, planning, or expectations. However, with the technological progress, project archetypes may become outdated, ineffective, or counterproductive by impeding more adequate approaches. Understanding archetypes of software development projects is core to leverage their potential. The development of applications using machine learning and artificial intelligence provides a context in which existing archetypes might outdate and need to be questioned, adapted, or replaced. We analyzed 36 interviews from 21 projects between IBM Watson and client companies and identified four project archetypes members initially used to understand the projects. We then derive a new project archetype, cognitive computing project, from the interviews. It can inform future development projects based on AI-development platforms. Project leaders should proactively manage project archetypes while researchers should investigate what guides initial understandings of software projects.
1	Open source software projects rely on the continuous attraction of developers and therefore access to the pool of available developer resources. In modern software ecosystems, these projects are related through technical dependencies. In this study, we investigate the influence of these dependencies on a project’s ability to attract developers. We develop and test our hypothesis by observing the dependency networks and repository activities of 1832 projects in the JavaScript ecosystem. We find that dependencies to other projects have a positive effect on developer attraction while we did not find an effect of dependencies from other projects. Our study contributes theoretically and practically to the understanding of developer attraction and highlights the role of technical interdependencies in software ecosystems.
1	Agile IT projects need employees who not only follow agile structures but have a specific attitude called the agile mindset. While the relevance of the agile mindset is clear, findings on when it can be developed, are very limited. Stable personality traits, like the big five, influence attitude. Providing how these traits interact with the agile mindset gives orientation regarding in which cases an agile mindset is more trainable than in other cases. To investigate these relationships, we conducted an online survey with 327 students of a project management lecture. As a result of our SEM and QCA analysis, we found three combinations of personality traits that influence the agile mindset including different extents of conscientiousness, openness, agreeableness and neuroticism. We deepen and extend the theory around the agile mindset and enable practitioners to choose data-driven cases for development activities. Limitations and future research based on these results are given.
1	Companies must optimize their information technology (IT) project portfolio to achieve goals. However, IT projects often exceed resources and do not create their promised value, for example, because of missing structured processes and evaluation methods. Continuous IT portfolio management is thus of importance and a critical business activity to reach value-driven goals. Guided by Design Science Research with literature reviews and expert interviews, we develop, evaluate, and adjust an IT project portfolio management process model, a holistic IT project evaluation framework, and implement a decision support system prototype. Our results and findings synthesize and extend previous research and expert opinions and guide decision-makers to make more informed and objective IT project portfolio management decisions aligned with optimal value creation. Furthermore, we deduce new research opportunities for IT project portfolio management process models, decision tools, and evaluation frameworks.
1	With regard to the complexity of emerging technologies, technology acceptance research examines the role of transparency, trusting beliefs, and perceived risks in individuals’ technology adoption process. However, IS research lacks an established theoretical explanation of the interrelationship of those multi-dimensional constructs. Referring to organizational science, an integrative model considering transparency, trusting beliefs, and perceived risks is developed. To obtain generalizable results, we collected quantitative survey data for two different technologies: A banking app and a food-sharing app. The proposed research model was then tested using structural equation modeling and multigroup analysis. The results indicate that transparency should be understood as an antecedent of trusting beliefs. This relationship is technology-independent, whereas the relationship between trusting beliefs and individuals' intention to use depends on the technology in question. The study also suggests that perceived risks are context-specific and should not generally be considered as moderators.
1	This research focuses on the process of how flexible information technologies assimilate into work processes and become routinized in an organization’s activities. The success of this post-implementation phase is essential if organizations are to reap the benefits of their investments in these technologies. However, many organizations struggle to integrate them with their processes and consequently, do not fully realize the benefits and value of those technologies. We attempt to study IS assimilation using the grounded theory approach to respond to the call for researchers to take the sociomaterial nature of the IS phenomenon into account. To develop a better understanding of the sociomaterial findings of this research, we view these findings through the lens of the Imbrication concept in order to better explain how technology assimilates into organizational practices through gradual interactions of technological and social elements. Several implications for research and practice are discussed.
1	Individual adoption of technology in humanitarian settings is linked with expectancies of performance and effort, and users tend to easily default to “whatever works”. A deeper understanding of the process of field users’ adoption of a new technology is of utmost importance for humanitarian organizations. We used Affordance Potency framework for comparing competing affordances of a mobile based solution with existing paper-based processes, against 4 identified affordances. The mobile solution was deployed in seven inpatient wards of two humanitarian field hospitals during a period of four months. This analysis demonstrated how technology features are as important as it is to be flexible to respond to contextualised work practices. The affordance potency lens helped understand how competing systems can shape the data collection process. A key contribution of this study is the use of affordances from a “competing” lens perspective within the unique context of a humanitarian organization.
1	Managers and researchers lack an understanding of how specific new product development (NPD) software champions, i.e. digital innovation champions, function. This is particularly the case for digital innovation management systems (DIMS). Innovation, R&D, and IT managers and top management all might take distinct roles that influence the usage of such software. These distinct roles suggest the need for more detailed investigations of 1) which champions drive the usage of DIMS, 2) if they enhance distinct NPD applications, and 3) the links to NPD performance. This study addresses this need by developing and analyzing hypotheses for the role of digital innovation champions for DIMS usage. Therefore, the authors use unique survey data from managers together with objective patent and revenue data. The results reveal that executives as digital innovation champions encourage the usage and NPD performance of DIMS, whereas, IT and R&D managers can even hinder these outcomes.
1	Diffusion of new technology can be approached as a good marriage between business model innovation and technological innovation. With maturing and converging technological innovations ranging from Internet-of-Things, Artificial Intelligence, Blockchain to digital platforms comes a fundamental shift in how companies can do business and what can be offered. They enable greenfield services (i.e. services not pre-existing), including servitization of existing products to compete. Whilst businesses are experimenting with services for emerging domains like industry 4.0, research embedding service logic in business model design for delivering and diffusing greenfield services is nascent. Using a Design Science approach, this paper contributes a method (SL-BMD) for designing service logic embedded business models that forefronts how to incorporate the customer’s perspective. We instantiate and evaluate SL-BMD by charting the experimental journey of a Predictive Maintenance offering for manufacturing settings, and highlight implementation considerations for SL-BMD and the experimental case chosen.
1	The adoption of Instant Messaging (IM) applications in the workplace remains contentious due to difficulties in adequately quantifying organizational benefits and how it affects individual performance. Previous research on the impact of IM usage on employee performance has been limited to analyzing primary data (i.e., survey methods), making it difficult to extrapolate the findings to a constantly changing workplace. In contrast, we investigate the relationships between these individuals' IM usage at the workplace and their primary assessment metric in their organization, performance evaluation, using longitudinal data of employees' IM activities and their performance evaluation collected from a US Fortune 500 financial company. Using cutting-edge text-mining techniques, we identify the primary purposes of IM utilization in organizations and assess the impact of those attributes on employee performance. Our findings show that IM in the workplace can improve team communication, knowledge-sharing experience, and social networking among employees, but it can also be disruptive. However, the combined effect of team communication and knowledge sharing on employee performance can overshadow the negative impact of IM interruption on employee performance.
1	In a longitudinal study of differences in framing an introduction of Grammarly, we find a gain framing to be more effective. Grammarly is an online spell-checking tool that can, for example, improve the text quality of students. We used the unified theory of acceptance and use of technology UTAUT to study framing effects on adoption behavior. In a survey experiment, we presented students with a description of Grammarly that was either framed towards potential gains or potential losses use would avoid. Contrary to prior findings in the context of IS security, we find a gain framing to be more effective. We discovered, however, that the framing was only effective after three months, which offers initial insights into the effects of framing in information systems and raises new questions. We plan to study this effect further in a follow-up study with a larger number of participants.
1	Developing organizational resilience (OR) is now one of the core competencies for organizations’ survival. Yet, OR development, as a response to disruptions, is context specific. With previous studies highlighting the type of disruption addressed, we find that the technology-incurred disruptions have received less spotlight due to the prevailing ‘pro-ICT bias’. However, technology may also heavily disrupt organizations. Should an organization not be resilient towards it, its survival can be at risk. Among various methods and means of developing OR, digital resilience, which is to utilize information systems to develop resilience, is known to be critical. Therefore, we ask the following research question “How do organizations develop digital resilience addressing technology-driven disruptions?". Using the improvisational capabilities and the red queen hypothesis as our guide, we conduct an exploratory case study on the Korean online gaming industry. Preliminary analysis and results are shared and concluded with plans for future research development
1	The rapid development of blockchain technology enables the application of cryptocurrency payment in real business practice. However, no research has examined the net effect of adopting such technologies on firm performance. Leveraging a unique research context that Booking.com starts cooperating with Travala.com, this study collects consumer reviews of the same hotels listed on Expedia and Booking.com in all states of the US and employs difference-in-differences designs. This study has three main findings. First, the adoption of cryptocurrency payment via Travala.com induces a net decrement in online sales on Booking.com. Second, the sales decrement is mainly driven by upscale hotels. Third, the sales decrement is severer when the cryptocurrency price is lower. These results are robust across hotels in all the states of USA and can be generalized using a later adoption of cryptocurrency payment. We interpret the mechanism as users’ negative associations and provide evidence by an event study analysis.
1	Society's accelerating digital transformation during the COVID-19 pandemic highlighted clearly that the Internet lacks a secure, efficient, and privacy-oriented model for identity. Self-sovereign identity (SSI) aims to address core weaknesses of siloed and federated approaches to digital identity management from both users' and service providers' perspectives. SSI emerged as a niche concept in libertarian communities, and was initially strongly associated with blockchain technology. Later, when businesses and governments began to invest, it quickly evolved towards a mainstream concept. To investigate this evolution and its effects on SSI, we conduct design science research rooted in the theory of technological transition pathways. Our study identifies nine core design principles of SSI as deployed in relevant applications, and discusses associated competing political and socio-technical forces in this space. Our results shed light on SSI's key characteristics, its development pathway, and tensions in the transition between regimes of digital identity management.
1	The centrality of information systems (IS) customization to match companies’ needs with software systems available in the market has been researched extensively. The distinctive characteristics of Artificial Intelligence (AI) systems compared to other types of IS suggest that customization needs a new conceptualization in this context. We draw on evidence from expert interviews to conceptualize customization of AI systems as composed of four layers: data, models, algorithms, infrastructures. We identify a continuum of levels of customization, from no to complete customization. Since companies customize AI systems in response to business needs, we develop a theoretical model with six antecedents of AI systems’ customization choices. In so doing, we contribute to both AI management research, by introducing the IS customization perspective in the field, and IS customization literature, by introducing AI systems as a novel class of systems and enlarging the understanding of customization for a specific class of software systems.
1	Regulators of healthcare systems continue to investigate ways to improve continuity of care (COC) for patients given its inherent fragmented nature. Integrated healthcare information technology (HIT) system is touted as one of the ways to improve COC. Yet, studies show that there are still challenges in achieving effective COC even when supported by integrated HIT. These persistent challenges are likely due to deep-seated tensions among the various parts of the healthcare system that are involved in providing care. Drawing on HIT impact literature and paradox theory, we study the implementation of an integrated electronic medical record (EMR) system aimed at improving COC for the specialist referrals process in a hospital cluster. We found that while the system had positive impacts on some aspects of the COC, we uncover two types of paradoxical tensions occurring in this healthcare context that interfered with those positive impacts and contributed to ongoing COC challenges.
1	Treating chronic diseases often involves repeated assessments from the patient’s perspective to guide therapy decisions and promote quality of care. Therefore, patient- reported outcome measures (PROMs) have been established in the form of questionnaires. One promising approach for collecting PROMs are embodied conversational agents (ECAs), which have the potential to make the questionnaire completion more engaging, interactive and lower the response burden for the patient. Building on Satisficing Theory, this research-in-progress paper reports on the design and preliminary evaluation of an ECA for multiple sclerosis patients. The results indicate that such a system meets the needs of the patients and motivates a comparative study to contribute further evidence on the use and advantage of ECAs for this purpose. Based on a literature review, an evaluation approach including a research model is derived, and implications for future research are discussed.
1	Question answer (QA) assistants are vital tools to address users’ information needs in healthcare. Knowledge graphs (KGs) and language models (LMs) have shown promise in building QA systems, but face challenges in their integration, and performance. Motivated thus, we take the case of a specific disease, skin eczema, to design a QA system combining KG and LM approaches. We present design iterations for systematically developing the KG, then fine-tuning a LM, and finally carrying out joint reasoning over both. We observe that while KGs are effective for fact finding, fine-tuned LMs perform better at answering complex queries. Initial results suggest that combining KG and LM approaches can improve the performance of the system. Our study contributes by laying out the design steps and developing a QA system that addresses various gaps in the related literature. Our future plan is to refine these techniques towards building a full-fledged healthcare QA assistant.
1	Many elderly individuals are aging at home (AAH) as the planet is graying. In the U.S., the number will rise to more than 64 million people by 2040. The AAH are more susceptible to health-related issues due to the normal process of aging coupled with the incidence of multiple chronic conditions (MCC). Self-management of MCC can be cognitively and operationally challenging for the AAH. We are designing digital resources for AAH with MCC following the action design research methodology. In this paper, we describe outcomes from early rounds, including problem formulation with semi-structured interviews of AAH with MCC, exploring theoretical precursors and technology frames, and evaluating design genres to establish a design vision. The outcomes are described as a theory-ingrained, layered digital artifact, MyHealthNotes; along with the results of an initial applicability check and formative usability test. The paper concludes with a discussion of contributions so far and next steps.
1	As a solution for the pressing issue in medicine of “black-box” artificial intelligence (AI), models that are hard to understand, explainable AI (XAI) is gaining in popularity. XAI aims at making AI more understandable by explaining its working, e.g., through human understandable explanations. However, while prior research found that such explanations must be adapted for the given expert group being addressed, we find limited work on explanations and their effect on medical experts. To address this gap, we conducted an online experiment with such medical experts (e.g., doctors, nurses) (n=204), to investigate how explanations can be utilized to achieve a causal understanding and respective usage of AI. Our results demonstrate and contribute to literature by identifying transparency and usefulness as powerful mediators, which were not known before. Additionally, we contribute to practice by depicting how these can be used by managers to improve the adoption of AI systems in medicine.
1	Online healthcare communities (OHCs) facilitate two-way interaction. Examining users’ information disclosure-audience support response dynamics can reveal insights for fostering a supportive environment, community engagement, bond formation, knowledge sharing, and sustained participation in OHCs. We propose a structural vector autoregression (SVAR) model of user disclosure and response dynamics in OHCs. Based on the health disclosure decision-making model and daily time series data, we examine the two-way interaction of two dimensions of disclosure efficacy with audience support response acceptance. Findings of the impulse response functions reveal that user information density leads to positive support response acceptance, whereas support response acceptance reduces the information density of a user post over time. Further, higher information efficacy leads to more support response acceptance with long run improved information efficacy. Theoretically, findings extend the disclosure decision-making model in OHCs. Practically, the results provide insights for OHC management to facilitate two-way dynamic users’ interactions.
1	Following recommendations and complying with behavioral attitudes is one major key in overcoming global pandemics, such as COVID-19. As the World Health Organization (WHO) highlights, there is an increased need to follow hygiene standards to prevent infections and in reducing the risk of infections transmissions (World-Health-Organization, 2021). This urgent need offers new use cases of digital services, such as conversational agents that educate and inform individuals about relevant counter measurements. Specifically, due to the increased fatigue in the population in the context of COVID-19, (Franzen and Wöhner, 2021), CAs can play a vital role in supporting and attaining user’s behavior. We conducted an experiment (n=116) to analyze the effect of a human-like-design CA on the intention to comply. Our results show a significant impact of a human-like design on the perception of humanness, source credibility, and trust, which are all (directly or indirectly) drivers of the intention to comply.
1	We show that review platforms reduce healthcare interruptions for patients looking for a new physician. We employ a difference-in-differences strategy using physician retirements as a “disruptive shock” that forces patients to find a new physician. We combine insurance claims data with web-scraped physician reviews and highlight a substantial care-gap resulting from a physician’s retirement. We then show that online physician reviews reduce this gap and help patients find a new physician faster. Our results are robust to including a variety of controls and various instruments for the  availability of physician reviews, but are not found for patients of nonretiring physicians. By reducing interruptions in care, reviews can improve clinical outcomes and lower costs.
1	Social media has the potential to both catalyze and attenuate a crisis by influencing offline responses. In addition, it is effective at profiling individuals, thus helping with response detections. Drawing on the elaboration likelihood model, we build a social media communication profile consisting of message quality and framing. We argue that message quality and framing shed light on offline responses. We anchor our reasoning on psychological reactance theory which posits that individuals' resistance is driven by freedom threats in such a way that individuals resist when they perceive that their freedom is threatened. Our empirical results support that message quality and framing may reveal the extent to which a community perceives its freedom to be threatened, manifested as the adoption of irresponsible behaviors in a crisis. Our findings have important implications for growing literature on social media profiling and crisis management.
1	This research questions how the use of a hospital app affords patient engagement because it recognizes the lack of understanding of healthcare technology affordances that lead to patient engagement. Drawing affordance theory, this study first measures affordances by considering feature use patterns and the needs to use the features that engaged patients show in the use of a hospital app. Further, this study aims to show how affordances are actualized and how the affordances interact with each other to enhance patient engagement. This short paper explains the methods of computational analytics to achieve these goals. Computational analytics results reveal patients’ and clinicians’ feature use and their needs that are used to measure affordances from empirical data, event logs and conversation texts between patients and clinicians. This work contributes to affordance literature by deepening an understanding of mechanisms of affordance actualization and actualized affordances to desired outcomes.
1	Survival analysis is an important problem in healthcare because it models the relationship between an individual’s covariates and the onset time of an event of interest (e.g., death). It is important for survival models to be well-calibrated (i.e., for their predicted probabilities to be close to ground-truth probabilities) because badly calibrated systems can result in erroneous clinical decisions. Existing survival models are typically calibrated at the population level only, and thus run the risk of being poorly calibrated for one or more minority subpopulations. We propose a model called GRADUATE that achieves multicalibration by ensuring that all subpopulations are well-calibrated too. GRADUATE frames multicalibration as a constrained optimization problem, and optimizes both calibration and discrimination in-training to achieve a good balance between them. Empirical comparisons against state-of-the-art baselines on real-world clinical datasets demonstrate GRADUATE's efficacy. In a detailed analysis, we elucidate the shortcomings of the baselines vis-a-vis GRADUATE's strengths.
1	As demand for healthcare services continues to increase, hospitals are under constant economic pressure to better manage patient appointments. It is common practice in clinical routine to schedule appointments based on average service times, resulting in overtime and waiting times for clinicians and patients. To address this problem, we propose a data-driven decision support system for scheduling patient appointments that accounts for variable service times. We take advantage of the growing amount of patient- and treatment-specific data collected in hospitals. Using a simulation study, we evaluate the decision support system on the practical example of a Gastroenterology facility. Our results demonstrate improved appointment scheduling efficiency compared to the approach currently in use.
1	Chronic stress is a burden on mental and physical health. Despite the development and effectiveness of mobile stress management applications, their adoption and continued use remain low. Given that research revealed systematic differences in usage behavior among user types, we aim to investigate what drives these differences. We extend the affordance perspective and argue that accounting for psychological needs, actualized affordances, and actualization costs across different user types provides a deeper understanding of the factors driving the adoption and use of mobile stress management applications. The qualitative interview study of our mixed-methods study reveals eight affordances, eight actualization costs, and initial evidence for systematic differences among the user types. The quantitative questionnaire study will uncover the psychological needs, actualized affordances, and perceived actualization costs of the six user types. This work contributes a new theoretical perspective to overcome the gap in the adoption and usage of mobile stress management applications.
1	In the learning journey of young children, rewards are ubiquitous. Yet, psychologists and behavioral economists question the success of rewards and even claim that they displace intrinsic motivation, a phenomenon referred to as motivation crowding out. While information systems can help children learn everyday tasks, it is unclear if and when digital rewards produce motivation crowding out. Theoretically sound, empirical field studies on this topic are lacking and existing information system research on motivation crowding is limited to specific domains, not covering children’s behavior. Therefore, we aim to elicit how digital rewards influence an everyday health behavior that children learn in kindergarten – handwashing – and the underlying intrinsic motivation. We conduct a randomized controlled trial that is conceptualized in this paper. Our results will extend motivation crowding theory in the context of young children and inform the design of digital behavior change interventions.
1	Blockchain is touted as an ideal basis for many systems that facilitate management and exchange of health information. However, much is unknown about patient perceptions of blockchain and its use in the healthcare context. This paper reports on the initial findings of a hermeneutic literature review that explores factors affecting patient acceptance of blockchain for managing personal health information. Privacy emerges from this literature as a construct of interest with inconsistent effects on patient perceptions of blockchain. A conceptual framework grounded in Communication Privacy Management (CPM) theory is thus proposed to explain how patients develop privacy rules to govern their personal health information. This framework provides a preliminary understanding of privacy-related factors that may affect patient acceptance of blockchain. Opportunities to expand the findings of this ongoing literature review and further explore attributes of blockchain that affect patients’ privacy rules are discussed.
1	Poor usability of Electronic Health Records (EHR) solutions is directly associated with physician burnout. While the survey and observational methods have been utilized widely in the usability evaluation of EHRs, it does not seem to be helping with the continuous improvement of EHR design and user satisfaction. We address this gap by presenting a discrete event simulation-based model that can add objectivity to the extant EHR usability methods. Evaluating EHR usability from the perspective of operations and workflow can help vendors design and develop better systems. This short paper presents a proof-of-concept simulation model with assumed task-time distributions. Our main research question is how we can use simulation techniques to objectively evaluate EHR usability? The simulation model results in terms of resource (clinician) utilization metrics can serve as a proxy to evaluate the efficiency component of the EHR usability at the departmental level
1	During the era of COVID-19, many hospitals start to build their own online healthcare systems (hereafter referred to as online subsidiary healthcare systems). The presence of online subsidiary healthcare systems brings hospitals an additional traffic source; however, in the meanwhile, it cannibalizes hospitals’ traditional offline traffic. Using a game-theoretic model, we obtain the optimal design of such an online healthcare system and investigate its impact on the traditional offline healthcare system. Surprisingly, we find that instead of reducing the offline traffic, such an online subsidiary healthcare system actually incentivizes more patients to visit the hospital offline, failing to fulfill the purpose of adopting online healthcare during COVID-19. Moreover, we find that online healthcare systems do not necessarily improve the profitability of the hospital and the surplus of patients. Our findings provide important implications regarding the management and regulation of such online subsidiary healthcare systems.
1	While prior research has found evidence of the digital divide in health IT outcomes, we do not know the combination of conditions under which health IT is more or less effective at reducing readmissions. This study examines the average and heterogeneous treatment effect of Remote Patient Monitoring (RPM) adoption on chronic-disease related readmissions. We first apply a deductive, econometric-driven approach (difference-in-differences) and then an inductive, machine learning-driven approach (casual forest). We find that RPM reduces the excess readmission ratio of pneumonia, heart failure, and chronic obstructive pulmonary disease. Furthermore, we find heterogeneity in the impact of RPM on heart failure readmissions based on the levels of patient complexity and local competition. Our research contributes to the literature on health IT digital divide by highlighting the combination of conditions in which RPM effectively reduces chronic-disease related readmissions. This combined research approach also contributes methodological insights to IS literature.
1	Modern Artificial Intelligence (AI) models offer high predictive accuracy but often lack interpretability with respect to reasons for predictions. Explanations for predictions are usually necessary in making high-stakes clinical decisions. Hence, many Explainable AI (XAI) techniques have been designed to generate explanations for predictions from black-box models. However, there are no rigorous metrics to evaluate these explanations, especially with respect to their usefulness to clinicians. We develop a principled method to evaluate explanations by drawing on theories from social science and accounting for specific requirements of the clinical context. As a case study, we use our metric to evaluate explanations generated by two popular XAI algorithms in the task of predicting the onset of Alzheimer's disease using genetic data. Our preliminary findings are promising and illustrate the versatility and utility of our metric. Our work contributes to the practical and theoretical development of XAI techniques and Clinical Decision Support Systems.
1	The emerging telehealth platforms connect patients with physicians using telecommunication technologies and are transforming the traditional healthcare delivery process. Meanwhile, patient care journeys spreading across online and offline health service channels call for new research methodologies. Using a dataset from a telehealth platform, we develop a novel Poisson-factor-marked Hawkes process to model such a journey and quantify the mutual-modulating effects of various patient activities. Our estimation results demonstrate the disparate impacts of the patient’s health conditions and physician characteristics on choosing care channels. Taking advantage of the self-generation property of our model, we simulate policy and strategic interventions, which highlights the practical value of the proposed model and offers implications for better patient routing and service design for telehealth platforms.
1	Remote patient monitoring (RPM) refers to clinicians’ capabilities for maintaining and adjusting their patients’ plan of care by utilizing remotely gathered data such as vital signs to proactively make medical decisions that improve health outcomes and well-being. The focus of this healthcare capability has grown during the COVID-19 pandemic as it allows for patients to remain at home and thwart the spread of the highly contagious coronavirus and payee policies were quickly changed to adapt to the novel situation. We synthesize the literature and present a four-component digital infrastructure framework to think through the design and implementation of remote patient monitoring projects. We identify research questions that emerge from our description of each component. We believe this framework will be useful to research studying remote patient monitoring as it provides a holistic viewpoint of the technologies involved and how those core elements may interact.
1	Cities bring about economic dynamism through positive economic externalities; however, the concentration of people in dense locations has its costs — epidemics, social unrest, pollution, and congestion are some of the ills of cities. As cities evolve, they experience stress, and fault lines appear; the ability to pulse a city and provide early warning of these fault lines can prove advantageous for policymakers in managing and planning for cities. This paper outlines a research program that developed a city scanning tool to measure cities and detect aberrations as they surface. We aggregated data from various industry partners, governmental agencies, and public online sources to develop the measurement metric and applied social science theories to analyze and interpret the results. The results of this study contribute to information system (IS) research by showcasing the role IS research in city planning and for societal good.
1	Drawing on panel data, we undertake a fuzzy-set Qualitative Comparative Analysis (fsQCA) of the determinants of high levels of the e-Government Development Index (EGDI) in the period ranging from 2003 to 2020. Our analysis showcases multiple pathways to the outcome of interest, thus bypassing partially-conflicting findings regarding the role that human capital plays as a contributor to high EGDI. Furthermore, our analysis leverages the idea of equifinality by showing two models of change working in parallel, namely incremental models of change (typical of developed countries) and punctuated equilibrium models (typical of developing and middle-income countries). This, in turn, shifts the focus of configurational thinking from theorizing a limited number of stable patterns towards utilizing configurations as ways of understanding evolutionary trajectories of change and development. Implications for theory and practice are discussed by shedding a new light on e-Government maturity thanks to the use of fsQCA techniques in a deductive fashion.
1	In digital markets business decisions are increasingly taken by artificial intelligence (AI). Especially in e-commerce, a growing share of retailers uses AI-driven algorithmic pricing, whereas remaining vendors rely on manual price setting. However, policymakers have raised concerns about anti-competitive tacit collusion between humans and AI that could allow firms to soften competition. Therefore, we empirically investigate outcomes that arise when humans and AI repeatedly interact in digital market environments. Based on an economic laboratory experiment in near real-time, we compare the degree of tacit collusion among humans and reinforcement learning algorithms to market settings where only humans or only algorithms compete. Preliminary findings demonstrate that tacit collusion emerges between humans and AI, although at lower levels than in settings with only humans or only algorithms. Altogether, our study sheds light on competition in digital markets where AI plays an increasingly important role and thus bears timely policy and managerial implications.
1	While net neutrality guarantees equal access to the Internet and online content, it serves as a limiting factor in identifying and tracking criminal activities in cyberspace by ensuring that data packet is transmitted with equal priority, irrespective of its source and content. Exploiting a natural experiment in which net neutrality policies were officially repealed in 2018 in the United States, this study examines the impact of net neutrality on the occurrence of cybercrime. Our findings suggest that the repeal of net neutrality is negatively associated with the occurrence of malicious code and content in an attempt to compromise computer systems (e.g., malware and ransomware). In contrast, we do not find any significant relationship with cybercrime victimization, and cybercrime that may subsequently occur in compromised systems (e.g., data breaches and denial-of-service attacks). This study provides novel insights into the role of net neutrality and open Internet toward the preventive cybersecurity paradigm.
1	The ‘digital first’ paradigm and its ontological reversal proposition bring new risks and implications for governing and regulating digital technologies. This article reports the findings from a qualitative study of the justifications used in legislating a ‘digital first’ artifact: Australia’s COVIDSafe contact tracing app. We build on justification theory (‘orders of worth’ framework) and use deductive qualitative analysis for examining 74 parliamentary records of proceedings (Hansards) in 2020 and 2021. The findings are structured in 38 empirical themes and 15 conceptual categories, which pertain to five orders of worth used in justifying the actors’ positions. This research unpacks the complexities of the justifications invoked in the legislative debates and sheds light on the novel and important yet understudied practices of governing ‘digital first’ artifacts.
1	It has been acknowledged that the Information Systems (IS) discipline needs to pay attention to policymaking. However, the IS field has not yet sufficiently acknowledged complexities of policymaking and the resulting ambiguity. We present two worldviews that underlie how IS research has approached policymaking and, indirectly, policy ambiguity. In the dominant “representationalist” view, a policy is planned and implemented in a linear manner, and ambiguity is seen as problematic. The “enactivist” view sees a policy and its implementation as mutually constitutive: a policy does not exist without its implementation but it also guides the implementation. This can result in unresolvable paradoxes that manifest as ambiguities. Based on our review of the extant IS research we present existing perspectives to policy(making) and ambiguity. We call for IS researchers invested in policy/regulation-related research to be aware of and explicit about the views to policy(making) and ambiguity guiding their research.
1	Open Government Data (OGD) has become an important theme of digital transformation strategies as it promises data-driven innovation and greater transparency in government. Many governments have chosen to implement national data portals to grant access to large amounts of public sector datasets. However, the expected uptake by the economy and society has been slow, and criticism towards the basic idea of open government is growing. In this paper, we take an in-depth look at how the perceptions of the features of the portal lead to users’ satisfaction with the OGD portal. Taking a user- centred perspective, we apply a fuzzy-set Qualitative Comparative Analysis methodology to identify different configurations which lead users to be satisfied with the OGD portal. Based on our empirical analysis, we formulate concrete recommendations on how the results can be used to define tailor-based strategies targeting the features of the OGD portals.
1	The familiar 5-star ratings system is an important information source for consumers deciding where to eat or what products to buy. Ideally, a retail platform owner should safeguard ratings against various biases, yet platform owners sometimes let average ratings become inflated. We study a situation in which a platform faces competition from another platform that offers the same items and, consequently, consumers may see different ratings for items across the platforms. Using a series of experiments in an online food ordering setting, we show that consumers are more likely to buy the item from a platform where it is rated higher. Therefore, a platform that offers lower but perhaps more accurate ratings risks hurting itself by not letting its ratings become inflated. We explain this by a vertical spillover effect by which diverging average ratings across platforms influence platform choice and discuss implications to platform owners, regulators, and consumers.
1	Prior studies have evidenced that consumers can self-select themselves in submitting online reviews, thus introducing biases in the distribution of online review ratings. This kind of bias is termed self-selection bias. This research aims to explore the specific influences of self-selection bias on consumer satisfaction from a product type perspective. The adopted product classification system combines search and experience differentiation, as well as vertical and horizontal differentiation. An agent-based modeling approach is employed to systematically examine the combined effects of different types of self-selection bias and products. Based on experiment analysis, a novel theory is developed arguing that self-selection bias can have nuanced influences on consumer satisfaction with different kinds of products, by affecting the usefulness of online reviews in suggesting product quality information.
1	Online retail platforms have increasingly employed a new incentive strategy by recruiting review officers and offering monetary rewards for posting incentivized reviews. However, there may be unintended effects of such incentivized reviews on the review system. Using a weekly-product panel dataset, we bridge the causal links between the emergence of incentivized reviews and the reviewers’ subsequent posting behaviors. We show that the emergence of incentivized reviews exerts a positive impact on the reviewers’ overall subsequent contributions. However, we show detrimental effects of incentivized reviews on the subsequent regular (non-incentivized) reviews, reflected in reduced engagement, decreased quality, and usefulness. Our mechanism analyses by separating the title effect and the quality effect show that the positive effect of incentivized reviews can be explained by the observational learning of the high-quality content while the adverse effects may be attributed to the demotivation of regular reviewers in response to the incentivized officer-titles.
1	A user-generated review that is perceived as helpful is valuable for both customer and the retailer, and that is why online markets such as Amazon.com collect public opinion on reviews that are perceived more helpful. Review platforms allow customers to vote for reviews they deem helpful. While prior literature has examined what drives the helpfulness of reviews, many of these studies have looked at drivers of perceived helpfulness of reviews in isolation. Using the lens of dual process theory, this research examines how consumers evaluate the helpfulness of a review. We propose a framework and provide empirical evidence for the evaluation of the review helpfulness process. We find that extreme reviews have a higher effect on review helpfulness compared to moderate reviews, and this effect is mediated by the depth and sentiment of the review content.
1	Recommender systems are a common approach in retail e-commerce to support consumers in finding relevant products. Not surprisingly, user acceptance of personalized product recommendations tends to be higher, leading to higher click rates. Since contextual information also influences user search behavior, we analyze the importance of similarity between recommendations and the underlying context a currently inspected product provides. Using data from a midsize European retail company, we conduct a field experiment and investigate the role of similarities between focal product information and recommendations from a collaborative filtering algorithm. We find that contextual similarity, primarily visual similarity contributes much explanation to consumer click behavior, underlining the importance of contextual and content information in the recommender system's environment.
1	Topic models have great potential for helping researchers and practitioners understand the electronic word of mouth (eWoM). This potential is thwarted by their purely unsupervised nature, which often leads to topics that are not entirely explainable. We develop a novel method to iteratively generate seed words to guide the interactive topic models. We assess the validity and applicability of the proposed method by investigating the critical phenomenon of Contact Tracing Mobile Applications (CTMAs) post-adoption during a time of the COVID-19 pandemic. The results show that constructs developed through our interactive topic modeling can capture primary research variables related to the phenomenon. Compared to existing topic modeling methods, our approach shows superior performance in explaining users’ satisfaction with CTMAs.
1	Ample anecdotal evidence in the media notes that many businesses seek to ‘silence’ negative reviews, e.g., via legal threat. Despite attention toward this issue, we are aware of no systematic analyses addressing it. We address that gap here, leveraging review data from TripAdvisor.com. First, we estimate that ~1% of truthful reviews are deleted within six months of posting and that negative reviews are significantly more likely to be deleted, consistent with a mechanism of censorship. The effect is substantial; we estimate that a 1-star decrease in rating valence is associated with an approximate 25% (0.25pp) increase in the probability of deletion. Second, we examine how freedom of expression (FoE) in a country associates with characteristics of (uncensored) online reviews. We find that FoE associates with larger review volumes, lower review valence, and faster review posting. We discuss implications for online ratings platforms, consumers, and research opportunities.
1	Online entertainment platforms such as Youtube host a vast amount of user-generated content (UGC). The unique feature of two-sided UGC entertainment platforms is that creators’ content generation and users’ content usage can influence each other. However, traditional recommender systems often emphasize content usage but ignore content generation, leading to a misalignment between these two goals. To address the challenge, this paper proposes a prescriptive uplift framework to balance content generation and usage through personalized recommendations. Specifically, we first predict the heterogeneous treatment effects (HTEs) of recommended contents on creators’ content generation and users’ content usage, then consider these two predicted HTEs simultaneously in an optimization model to determine the recommended contents for each user. Using a large-scale real-world dataset, we demonstrate that the proposed recommendation method better balances content generation and usage and brings a 42% increase in participants’ activity compared to existing benchmark methods.
1	News recommender systems (NRSs) are an essential component of online news portals. To avoid the emergence of “filter bubbles” where users display an overly selective perception of the news situation, NRSs must not only display a diverse range of news, but also motivate users to engage with the diversified content. Many existing approaches attempt to achieve this by modifying the recommendation strategy or by applying selection control techniques such as digital nudging. Based on insights from self- determination theory, we present an alternative approach that relies on user control mechanisms to promote self-determined motivation for exploratory use and thus diverse news consumption behavior. We also outline a methodological design to empirically confirm the viability of our approach. As such, we not only contribute to the theoretical understanding of the role of user control in diverse news consumption behavior, but also provide guidance on validating the practical feasibility of our approach.
1	Online review mining has become an important way for businesses to understand consumer preferences and product characteristics. Many online review platforms have started to incorporate the extracted information as review tags to guide future reviews. In this study, we leverage a quasi-experiment from an online health service platform to investigate the value of incorporating the review tags (extracted from prior reviews) into the online review system in user review generation. Our preliminary results show that after the provision of review tags, more reviews are provided for doctors but the length of those reviews is shorter. Notably, we also find a decrease in sentiment and an increase in novel reviews. Our findings provide actionable managerial insights for platform managers to design online review systems.
1	This panel provides the Information Systems (IS) community with an update on and engages in a pivotal discussion regarding the impact of digital transformation and advanced information technologies—such as big data analytics and artificial intelligence—on the IS curricula of AACSB accredited business schools. The panel draws upon the work done by the IS task force of the AACSB MaCuDE project in collaboration with curriculum leaders of the IS community. The panel reflects on the role and identity of the IS discipline in the future based on the MaCuDE project findings, explores the role of IS as the natural boundary spanning leader in digital transformation, analyzes areas in which IS is most likely to continue to provide distinctive value, considers the resource implications of the emerging changes, and discusses the role of ethical implications of advanced information technologies in IS education.
1	This panel discusses the emerging socio-technical design challenges associated with autonomous driving systems (ADS) that have become increasingly common at least in specific circumstances both in on and off the road situations. Specific issues addressed include 1) How Safe Should Automated Vehicles Be? How should we organize and agree on appropriate norms to evaluate the safety and related measures associated with ADS? Who, or what is liable if the ADS is at fault with significant human and economic consequences?; 2) What Data Should Be Shared?; and 3) What New Human and Machine Behaviors Might Emerge? The panel will discuss emerging research issues and challenges associated with each topic and what current practice and research suggests how they will be addressed. The role of information system scholarship to contribute to this interdisciplinary field is also discussed
1	In recent years, a new refugee crisis has been sweeping the world due to the continuous violence in different places and countries. These developments have caused unexpected challenges on different levels, ranging from individuals (including migrants and refugees and hosting populations) to organizations, countries, and continents (including those fleeing violence and hosting countries of fleeing individuals). Despite the urgency and the potential risks associated with the current refugee situation, relatively little work has been carried out by IS researchers on how to find the intersection between this societal topic and the use of technology to alleviate this crisis. The outcomes of the panel have implications for both academia and practice. We would like to uncover the beneficial use of digital transformation solutions that could help and empower refugees and host communities using the bright side of existing technologies in integrating refugees into society.
1	Video games and its industry are leading practice in a variety of digital domains including autonomous design (procedural generation with AI) and real-time user/community engagement mechanisms. The gaming industry has been experimenting with various business and revenue models, pioneering many areas of data-driven design and innovation management, and blurring the lines between work and leisure. With the rising interest in building Metaverses and immersive experience design, many firms look at open-world videogames as the default model. Despite their cultural and digital importance, game environments are rarely the subject of IS research. They still carry stigmas of not being serious business or generalized enough for scholarly consideration. The PDW aims to formulate the effect of games, their artifacts, environments, and business models on the larger IS scholarship and draw a way forward for greater engagement of IS scholarship within the video game industry.
1	Shopping on amazon or booking holidays online are just two examples of AI-enabled services. Although of great practical importance, literature on AI-enabled services is scarce. Especially, no study exists that assesses the effect of the different AI levels of a service on its innovativeness. Moreover, empirical evidence on customer complaining behavior as a reaction to failed AI services is also missing. Using an online experiment (n=437), our paper strives to close this research gap. Our results show that customers will perceive a Feeling AI service (high degree of AI) as more innovative than a Mechanical AI service (low degree of AI). Moreover, customers using a failed Feeling AI service will complain more than customers using a failed Mechanical AI service. Finally, customers getting an AI Service Recovery will complain less than customers getting a Human Recovery. Our results highlight the importance of AI-level when creating and managing AI services.
1	Chatbots are increasingly used to provide customer service. However, despite technological advances, customer service chatbots frequently reach their limits in customer interactions. This is not immediately apparent to both chatbot operators (e.g., customer service managers) and chatbot developers because analyzing conversational data is difficult and labor-intensive. To address this problem, our ongoing design science research project aims to develop a conversation mining system for the automated analysis of customer-chatbot conversations. Based on the exploration of large dataset (N= 91,678 conversations) and six interviews with industry experts, we developed the backend of the system. Specifically, we identified and operationalized important criteria for evalu-ating conversations. Our next step will be the evaluation with industry experts. Ultimately, we aim to contribute to research and practice by providing design knowledge for conversation mining systems that leverage the treasure trove of data from customer-chatbot conversations to generate valuable insights for managers and developers.
1	How and why can smart technologies affect service innovation trajectories? We address this question via a multiple case study approach. First, we identify two dimensions for smart service innovation events: while performing and patterning characterize two distinct mechanisms of smart service innovation, flexible technologies and routines represent constituent elements of smart service systems. Building on these two dimensions, we delineate four ideal types of smart service innovation events and illustrate how these materialize in real-world contexts. Finally, we outline how our findings may serve as a starting point for future research and provide managerial guidelines for those aiming to shape innovation trajectories in smart service systems.
1	Applications of digital twin (DT) technology have gained momentum in IS research and cognate disciplines. Several studies have documented how DTs create value in contexts such as manufacturing or smart cities through virtual monitoring and decision-making. While these contexts benefit from DTs of products or production steps, this research is the first to investigate the potentials of human DTs to improve customer experience (CX) (i.e., customer twins). Drawing on a structured literature review, we derive new conceptualizations of DTs as (i) virtual mirrors that depict a physical entity and its interactions in virtual space, and (ii) virtual orchestrators which extend the virtual mirror by also simulating potential virtual interactions. These new conceptualizations, by applying them to human DTs, enable us to discuss DT’s implications to approach current CX potentials. The results of the discussion indicate that human DTs can support CX management to improve CX throughout the whole customer journey.
1	A new idea usually follows a stream of similar ideas yet simultaneously combines atypical elements from ideas outside this stream. A successful business idea usually balances well between familiarity and atypicality. To investigate the relationship between atypicality innovation and crowdfunding project performance, we collected data from one of the largest crowdfunding platforms in China. We build a similarity network of crowdfunding projects to measure the degree of atypicality innovation for these projects. Using a double machine learning model, we find that the atypical combination of mainstream and niche ideas has a significant positive effect on the individual project's funding, i.e., five times more successful than other projects. We also find the potential reasons that cause the poor performance of niche and mainstream projects. Donors are more conservative due to the high risk of niche projects and driven away by the monotonous repetition of mainstream projects.
1	IT-enabled food-delivery platforms increase restaurants' catchment area and reduce search costs for consumers. The former allows restaurants to pool their risks and decrease noise in demand, whereas the latter increases competition and noise in demand. Overall, it is unclear how these competing factors affect restaurants' ability to forecast their demand -a critical factor for profitability. In this paper, we empirically investigate the impact of IT-enabled food-delivery platforms on restaurants' demand forecast error. Using detailed transaction-level datasets, we find that a 10 percentage point increase in dependence on food-delivery platforms leads to a 2.83% increase in overall forecast error. We also find that the majority of increase in overall error is due to an increased error in forecasting intra-day demand "pattern", and a smaller portion is due to error in forecasting inter-day demand "amplitude". Based on our findings, we offer suggestions for restaurants on managing their relationship with IT-enabled food-delivery platforms.
1	The low-participation problem has long been a challenge facing many paid Q&A platforms. Recently, a new business model wherein users, in addition to raising a question and receiving a personalized answer, can pay a small amount of fees to view a non-personalized answer to a question asked by others, has drawn considerable public attention and is considered an effective means to tackling such a challenge. In this paper, we build a theoretical model to explore whether this new business model benefits the key stakeholders (Q&A platform, answerers, and users). We find that the platform is not always better off when the answer-viewing feature is introduced. Another interesting finding is that while answerers may engage in direct competition with the platform, they can sometimes be better off. Additionally, we find that although having a new way to participate in the platform, users may sometimes be worse off under the answer-viewing feature.
1	We focus on the equity problem of the IT-enabled public bike share systems and posit that shared bicycles become a heavily reliant mode of transportation in low-income neighborhoods during abrupt suspensions of subway systems caused by disasters. Although bike share is known to provide many social benefits, it has been often criticized for having low usage and lazy expansion among low-income neighborhoods. We leverage a natural experiment setting to estimate the effect of flood damage to subway lines on the demand for public bike share service. Through a quasi-experiment analysis, we find that the effect of flood significantly increases bike share usage in low-income neighborhoods while not profoundly affecting usage in high-income neighborhoods. From the results, we shed light on the previously undiscovered role of bike share in the disaster management context and its role in mitigating the transportation disturbance of low-income commuters.
1	We study how the introduction of exclusive third-party applications affects competing complementors’ innovation strategies in platform ecosystems. In this study, we exploit the exclusive entry of Super Mario Run into the complementary app market of Apple’s iOS App Store in Autumn 2016 as a quasi-experiment. We collect monthly time-series data throughout the observation period. We find that complementors show heterogeneous innovation behavior after the entry. First, we demonstrate that affected complementors with a similar market position as the entrant follow a competition strategy for update releases in the affected niche market. Complementors who do not hold a similar market position reduce the number of updates for existing products in the affected niche market following a differentiation strategy. Finally, independent of the complementor’s market position, affected complementors follow a differentiation strategy for new app releases, increasing the number of new app releases in other categories.
1	The rise of the content platforms has led to the new opportunity of advertising through the content creators, which, however, causes the strategy tradeoff for the platform owner. On the one hand, allowing creators to embed sponsored ads (CADS) may undermine the platform’s own ad sales (PADS); On the other hand, the platform might benefit from CADS through commissions. We develop a game-theoretical model to examine this tradeoff. We find that allowing CADS might be optimal for the platform, depending on the qualities of PADS and CADS. In addition, we show that the strategic relationship between PADS and CADS can be substitutable, complementary, or independent from each other, which is endogenously determined by the platform’s profit-maximizing decisions. We then conduct a case study based on the data collected from Bilibili to verify our analytical findings.
1	Disintermediation, where providers and customers transact bypassing an intermediary, has challenged the business model and dwindled profits of the multi-billion-dollar platform economy. Despite the platforms’ efforts to mitigate disintermediation, little is known regarding the extent of disintermediation or efficacy of the mitigation policies, largely due to unobservability of disintermediation. We tackle these challenges by designing a geo-analytic methodology to identify and quantify disintermediation by matching online Airbnb booking and offline granular mobile location data. We further leverage DiD with matching samples to causally examine the efficacy of four Airbnb policies; and finally propose a cost-and-benefit conceptual framework to interpret the findings and guide platform designs of mitigation policies. We find, for instance, a 5.4% of disintermediation in Austin, TX over Summer 2019; and Instant Bookable reduces disintermediation by 9%, with a stronger effect among the hosts without preference for long-term lease, with more repeated guests, and more hosting experience.
1	Idea refinement is a crucial step in the ideation process. Given that ideas are embedded within a network of other related ideas, how the network structure fluctuates throughout the idea refinement process may affect its end outcome. Through studying proposed ideas on TVTropes.org, this research seeks to understand how network embeddedness of an idea's citation network influences the idea quality perception, particularly through degree centrality, betweenness centrality and eigenvector centrality. We also study the boundary conditions of their impact on quality and analyze the moderating role of topic breadth and depth which capture the overall content of the idea. Our results suggests that the positive effect is stronger when the idea topic breadth is lesser, or the idea topic depth is greater. We further explore the mechanism behind these effects by analyzing their effect on change of positive votes and negative votes, showing the influence path might be different.
1	Metaverse is a post-reality universe, a permanent and persistent multi-user environment that combines physical reality and digital virtuality. These technologies realize seamless embodied user communication in real-time and dynamic interactions with digital objects, shape users’ perceptions of reality and can be widely utilized for various applications. However, it is still in an infant stage, and a minimal amount is known about why and whether users will adopt such fully immersive technology. The purpose of this article is to develop a theoretical model and validate it by a survey to examine what affordances and challenges affect metaverse adoption. Our study potentially contributes to the literature on IS adoption research, and to practitioners on what needs to pay special attention to when designing metaverse.
1	The rapid spread of misinformation on social media platforms has affected many facets of society, including presidential elections, public health, the global economy, and human well-being. Crowdsourced fact-checking is an effective method to mitigate the spread of misinformation on social media. A key factor that affects user behavior on crowdsourcing platforms is users' anonymity or identity disclosure. Within the crowdsourced-based fact-checking context, it is also unknown whether and how identity anonymity affects the users' fact-checking contribution performance. Leveraging a natural experiment policy happening on Twitter, we adopt regression discontinuity design to investigate two research questions: Whether and how the identity anonymity affects the crowdsourced fact-checking quantity and quality; how the characteristics of the crowdsourced users moderate the main impact. We find that the identity anonymization policy may not increase fact-checking users' contribution quantity, but the fact-checking quality does increase. Our research has both theoretical and practical implications.
1	Price-oriented functions have been prevalently used by sellers for attracting consumers on e-marketplace platforms. However, existing literature has mixed understandings about its influence on improving consumer satisfaction. Besides, few studies have considered how cause-related marketing moderates the impact of price-oriented function usage. Therefore, this paper firstly explores the curvilinear relationship between price-oriented function usage and consumer satisfaction by adopting the repertoire perspective, then further considers the moderating role of cause-related marketing. This study collected data on 29,506 products from one e-marketplace platform in China. By using fixed-effects regression models, it is found that price-oriented function usage (i.e., volume and heterogeneity) have inverted U-shaped relationships with consumer satisfaction. In addition, cause-related marketing weakens the impact of price-oriented function usage heterogeneity on consumer satisfaction. This study contributes to research about platform function usage and guides sellers in terms of using those functions to stimulate consumer satisfaction.
1	Commission is a common platform pricing strategy for charging a portion of transaction revenues. However, digital platform firms face long-standing disputes with app developers and even lawsuits regarding their commission rules. This study investigates how reducing platform commission affects mobile app performance and developers’ behaviors. We leverage a natural experiment based on a commission policy change implemented by Apple and conduct a difference-in-differences (DID) analysis. Surprisingly, we find a negative impact of commission reduction on app performance measured by daily active users and downloads. The impact of commission reduction on app performance is heterogeneous across apps with different ranks and across apps in the game and non-game app categories. Further analysis of the mechanism reveals that the apps eligible to enjoy the benefits of commission reduction are updated less frequently, indicating that developers devote less effort to improving and advancing these apps. Our findings provide important theoretical and managerial implications.
1	By offering products on self-owned platform marketplace, platform owner enters complementary market and poses competition that would trigger complementors to revise their product portfolio. Building on past works, we distinguish between three product-related decisions: new product launch, product variety and product differentiation. Using two-year data from Amazon.com, we empirically test how complementors re-strategize product-related decisions in response to platform owner’s entry. We find that complementors decrease new product launch, and revise their product portfolio by reducing product variety but increasing product differentiation (i.e., position away) from platform owner. Furthermore, we show that complementors agglomerate together, offering products with lower differentiation from one another. Our results inform about the implications of platform owner’s entry on complementors’ products. We call for potential mechanisms to incentivize complementors’ efforts in new products and direct complementors to specialize and agglomerate in products complementing platform owner’ product offerings.
1	Communication via social media is characterized by immediacy and anonymity, enabling free expression and sharing of opinions, but also the abuse of language in form of hate speech. Given the volume of online content, IS research offers approaches to efficiently detect hate speech. However, research and politics call for more independent, transparent, and social approaches to increase credibility and acceptance. In response, this two-part behavioral and neural study investigates flagging as a community-based solution to hate speech detection. By experimentally varying the displayed shares of flagging users and testing behavioral responses, results reveal opposing behavioral patterns as a function of the valuation of hate speech prevention. Moreover, by framing the display of the user community’s flagging behavior as a sort of social normative information and hate speech prevention as a public good, the theoretical model might help explain (seemingly) conflicting results in social norm and public goods research.
1	The importance of professional social media usage by business-to-business (B2B) sales employees has been well acknowledged, but the understanding of corporate influencers in B2B marketing a more thorough investigation. Literature has primarily focused on salespeople’s social media behavior, performance outcomes and effects on customer relations. However, little is known about salespeople’s motivation to act as corporate influencers, their actual activities on social media when they enact this role, as well as opportunities and challenges of salespeople who purposefully take on a corporate influencer role. We use a grounded theory approach to investigate this phenomenon. Based on in-depth interviews with experts from various B2B industries, we identify intrinsic and extrinsic motivational factors that drive LinkedIn usage as well as opportunities and challenges of corporate influencer activities. These findings extend previous corporate influencer research. Moreover, we offer actionable practical implications.
1	This study examines how an interesting technological phenomenon called the “Hashtag Dance Challenge” (HDC), made popular on the short video platform TikTok, may drive artists’ popularity on the digital music streaming platform, and, importantly, help women, artists, to achieve traction that is needed to succeed commercially. Using data from TikTok, Spotify, and music analytics companies, we analyze the impact of HDCs on artists’ popularity growth rate on Spotify. We find that artists with an HDC-related song achieve a significant daily increase in followership on Spotify, representing traction and appeal within the music industry, relative to similar artists who do not have an HDC. Importantly, the daily growth of Spotify followers increases by approximately 3% more for female artists than male artists, given an HDC-associated song. Our findings shed new light on the role of social media with respect to artist self-promotion, especially in making the music more inclusive and attractive to female music artists.
1	How can digital technology enable flexible interorganizational collaborations (IOCs)? This study investigates a challenge facing firms seeking to build highly flexible interfirm relationships to remain competitive in the digital age. It explores how flexible IOCs characterized by changing goals, organizations and organizational actors can leverage digital technology to rapidly generate interorganizational dynamic capabilities (IDCs) in the absence of pre-existing routines. Using multiple case studies of COVID-19 task forces in the US and Canada, we observe how digital generativity derives from a diverse and changing set of digital tools used together to respond to a rapidly changing environment. In doing so, this study extends digital generativity beyond digital platforms into more flexible applications of digital technology. This approach addresses a central problem in the IOC literature: how organizations competing in the digital age can shift their strategic focus from competition to collaboration (Gkeredakis & Constantinides 2019).
1	How is emotional content shared on microblogging platforms? Prior work has proposed that emotionally charged content is diffused more than emotionally neutral content because it can evoke physiological arousal in platform users. Drawing on recent research in IS, we argue that the real relationship between emotions and Information Diffusion is an inverse U-shaped relationship; moderately strong emotions lead to optimal diffusion. We further theorize that this relationship is moderated by discourse context and valence of emotions. We test these hypotheses by testing a Twitter dataset that includes tweets collected from multiple conversation contexts. Results show broad support for our hypotheses and extend prior work on emotional content in microblogging.
1	We study the association between people' participation in an ESN platform and job mobility events using employees' actions on an S&P500 company's enterprise social networking (ESN) platform. Using cutting-edge text-mining algorithms, we first determine the settings in which employees use these platforms and then assess the relationship between those qualities and job mobility. According to our topic-modeling analysis, employee participation on workplace social networking platforms has multiple dimensions; nonetheless, it is mainly employed for knowledge-sharing, social networking, employee engagement, and volunteer activities. We provide empirical evidence that employees' contributions to knowledge-sharing, social networking, and organizational engagement via ESN lead to a higher likelihood of job mobility; however, a higher number of complaints, perhaps surprisingly, is associated with a higher likelihood of job mobility. We contend that better levels of knowledge-sharing, social networking, and employee engagement with less complaints can be linked to self-promotion, resulting in a higher likelihood of promotion.
1	This paper examines the social media discourse of two real-world vigilantism incidents that had invited nation-wide debate: Murder of ‘Ahmaud Arbery’ (victim), a racially motivated hate crime and the fatal shooting of two men by ‘Kyle Rittenhouse’ (an aggressor). Both these incidents had invited a lot of debate in social media. However, little is known about the nature of discussions on vigilantism in social media. In this paper, first, through topic modeling, we examine the kind of discussions that were triggered by these incidents. We identify various dimensions of the on-line public conversations. Second, we study if there is polarization in the public discourses. We find that victim-oriented discourse on vigilantism displayed more polarization in a certain dimension and aggressor-oriented discourse on vigilantism displayed more polarization in another dimension. We also found that aggressor-oriented vigilantism discussions had higher negative emotion scores compared to victim-oriented discussion.
1	How should marketers engage with social media features in online communities to shape knowledge contributions from customers in their potential markets? This is an important question because customer contributions are important drivers of business value. We examine the effect of marketer generated content in online health communities on user-generated content, using longitudinal data from a leading online health community. We focus on the firm’s practice of knowledge investment, in which its marketers provide product information or share life experience by posting in the social interaction section of online health platforms. The results demonstrate that because of knowledge investment in healthcare markets, the use of platform’s social media feature by marketer influence both the quantity and linguistics features of customer-generated content.
1	This research investigates the effects of marketer-generated content (i.e., emotional and rational), sounds, and influencers on digital customer engagement (i.e., likes and shares) on TikTok. Data were collected from 10 Indonesian food and beverage brands. The results confirm that emotional content generates more likes and shares than rational (i.e., informational and transactional) content. Conversely, rational (i.e., informational and transactional) posts receive more likes and shares than emotional posts. Additionally, original sounds and influencers positively influence likes and shares. This study advances social media and content marketing literature in three ways. First, this research is among the first to discuss the emerging social media of TikTok. Second, this investigation is the first to examine the audio format as an element influencing engagement. Third, this study focuses on Indonesia, a developing country that is rarely investigated in content marketing studies. Practically, our research findings can guide brands in creating engaging TikTok videos.
1	Social media platforms often distribute content through different newsfeed channels, most commonly, social networks, algorithmic recommendations and trending content. Prior literature has investigated each channel’s impact on user-content engagement. However, little is known about the relationships between these channels. We investigate the impacts of limiting content display from the social network channel on the quantity and diversity of user-engaged content across channels. We leverage a natural experiment, where a social media platform hides friends’ liked content from the social network channel, to identify the impacts. Results show that hiding friends’ liked content reduces the quantity of users’ content engagement on the entire platform. Across channels, users increase their engagement with trending content but decrease their engagement with algorithmic recommendations. Further, restricting exposure to friends’ liked content reduces the diversity of users’ content engagement. Our results highlight the intercorrelation of user-content engagement across newsfeed channels and provide insights for newsfeed designs.
1	As collaborations between brands and influencers become increasingly popular, predicting the capacity of an influencer to generate engagement has garnered increasing attention from researchers. Traditionally, managers have been relying on follower-based statistics to identify individuals with potential to reach a vast number of users on social-media. However, this approach may often direct managers to accounts with millions of followers accompanied with high recruiting costs. In this paper, we argue that the network structure of influencers is a useful measure for capturing an influencer’s ability to generate engagement. Using Instagram data, we perform a deep-learning analysis on the social network of influencers and show that the network structure explains a large share of the variations in user engagement, even outperforming traditionally used variables such as the number of followers in the case of comments. This study contributes to the emergent literature on the importance of social ties in the digital environment
1	Creativity is the key element of organizational success. Yet, adequately incentivizing people to be creative remains a problem without uniform solution. This study investigates the effect of incentive systems that rely on supervisor discretion on creativity of virtual groups. Adopting Social Interdependence Theory, we experimentally assess the effect of forced distribution rating systems (FDRS) and unrestricted distribution rating systems (UDRS) on idea generation and idea selection of groups collaborating in a virtual setting. We show that the competitive FDRSs – in which not every group member can obtain a top ranking - enhance idea generation, idea selection and overall creativity of virtual groups. We contribute to the literatures on creativity, virtual collaboration and incentive systems.
1	Facial expressions have been seen as one of the most instinctive and efficient ways in the form of nonverbal communication. The CEO provides important facial information during the interview, which links to the firm's current market situation and future planning. In the literature, few studies have examined the relationship between CEO interview facial expressions and firm performance. This study explores how CEOs' facial emotions impact firm market value by analyzing the interview videos from the YouTube platform. We use the FER, a CNN algorithm-based method, to establish facial emotions and build multiple regression models to predict the firm's market value. Our findings show that CEO's negative emotion has a significant negative impact on market value. A positive emotional swing has a positive impact on the stock price. The high CEO’s emotional swing affects investors’ confidence in the firm performance and reflects on the stock price.
1	Online platforms have adopted various types of content moderation strategies to combat antisocial behaviors such as verbal aggression. This study focuses on two types of strategies: group prominence reduction and banning. This study aims to provide a holistic picture of all downstream effects of these strategies. Additionally, we assess the differential effects of content moderation on multihoming versus non-multihoming users. Preliminary findings indicate that prominence reduction strategies applied to a problematic group have the adverse effect of increasing verbal aggression in outside spaces. Banning strategies differentially impact multihoming versus non-multihoming users. These findings have important implications, as they show that group prominence reduction strategies produce negative spillover effects, and the behavior of multihoming users on multiple external platforms, and whether our results generalize across multiple contexts.
1	Given an emerging interest in the role of technology in social (in)justice and forced migration, we present preliminary findings from our three-phase study with Syrian refugees, whose aim has been to unpack the role played by social media in migrants’ journeys from their country of departure to their country of destination. Drawing on a range of complementary datasets, we discover that social media play different roles at different stages of refugees’ journeys (pre-departure, while on the move, and post-arrival), ultimately giving rise to a unique configuration which we term a “hybrid community”. These communities are hybrid because their members interact online in Stages 1 and 2, and then both online and face-to-face in Stage 3. With Phase 3 of our study, we hope to explore what these hybrid communities look like, by conducting reflective interviews with refugees who are settled and may be able to offer new insights that could complement our existing dataset.
1	Social media platforms generate huge profits from targeted advertising by collecting massive amounts of data from their users, usually referred to as data harvesting. However, practitioners from the social media industry suggest that data harvesting hurts users by promoting social media addiction and the spread of misinformation. Therefore, policymakers have recently been considering regulating social media platforms. This paper investigates how imposing the regulation on data harvesting impacts social media platforms and users by developing a game-theoretic model. Our main finding shows that while the objective of the regulation on data harvesting is to discourage platforms from collecting a massive amount of data from the users, imposing the regulation may sometimes increase the data harvesting levels and profits of social media platforms. We contribute to the Information Systems literature by broadening the knowledge of the impact of the government's regulation on social media platforms and users.
1	Despite the potential value of prosocial activities in enhancing user engagement in online communities, research on the relationship between prosocial activities and online community users’ behavior. In this research, we examine the impact of tangible donation on online community users’ engagement behaviors by using dataset from Reddit, a major online community platform. Our results indicate that, after donating, givers increase their engagement behavior by writing more posts and comments than non-givers. Furthermore, after receiving donation, receivers reduce their engagement behavior by writing fewer posts and comments than non-receivers. Our study serves as one of the first attempts to examine the role of peer-to-peer tangible donation in users’ engagement behavior in online community platform, which is a novel way to help people in needs and effective way to induce user participation.
1	The booming of online platforms has attracted academia’s increasing interest in cross-platform spillover of product consumption. This study investigates how physicians’ content creation in Tik Tok influences patients’ demands, comments and satisfaction towards the physicians on online health communities (OHCs). Using the difference-in-differences approach, we uncover asymmetric influences of cross-platform spillovers for high- and low-awareness physicians in Tik Tok. Specifically, low-awareness physicians do not enjoy the benefits (i.e., the increased volume of orders and comments on OHC) from content creation in Tik Tok, but their ratings turn to decline due to attention distraction caused by cross-platform activities. Conversely, for high-awareness physicians, we find a positive cross-platform spillover effect for orders and comments on OHC without decreasing their ratings. Despite the existence of attention distraction from cross-platform services for high-awareness physicians, the negative impact on feedbacks is offset by higher ratings from their cross-platform consumers.
1	Building on the componential theory of creativity, we studied how the crowdsourcing creativity support architectures and the task knowledge intensity levels affect the crowd’s creativity. Using an online experiment, we found that remixing can trigger the crowd to be more creative than external stimuli and using either architecture triggers the crowd to be more creative overall. Also, the crowd is more creative in solving low-knowledge-intensity tasks than in solving high-knowledge-intensity tasks. Interestingly, regardless of the knowledge intensity levels of tasks, crowdsourcing support architectures have a significant impact on the crowd’s creativity. Therefore, our paper contributes to the crowdsourcing literature on promoting crowd creativity and provides practical implications on solving societal challenges, especially large-scale problems.
1	The goal of this paper is to develop a complete overview of the current debate on artificial intelligence in organisation and managerial studies. To this end, we adopted the Computational Literature Review (CLR) method to conduct an impact and a topic modelling analysis of the relevant literature, using the Latent Dirichlet Allocation (LDA) technique. As a result, we identified 15 topics concerning the artificial intelligence debate in organisation studies, providing a detailed description of each of them and identifying which one is declining, stable or emerging. We also recognized two main branches of research regarding technical and societal aspects, where the latter is becoming increasingly important in recent years. Finally, focusing on the emerging topics, we proposed a set of guiding questions that might foster future research directions. This paper provides insights to scholars and managers interested in AI and could be used also as guide to perform CLR.
1	There are growing concerns for online tribalism and division, and many studies link the increasing polarization to the popularity of social media. One of the most widely proposed measure to alleviate online polarization is exposing users to opposing views, which shows conflicting results in the literature. Our study contradicts the conventional belief that people are more likely to accept the rationales of opposing views whenever deep cognitive efforts is engaged. This study addresses: (1) the causal pathway of counter-attitudinal information exposure onto opinion and affective polarization; and (2) how empathy moderates such causal effect and alleviates polarization in social media settings. We first propose a novel conceptual framework to explain the effect of motivated reasoning in user's opinion formation when receiving dissenting information and then conduct experiments to examine the effect of exposure to disconfirming information on polarization where motivation to empathize exhibits a crucial positive moderating effect.
1	Given the significant costs of suicidal behavior for society, suicide prevention is one of the most urgent issues for most countries. By considering suicidal ideation as a strong indicator of suicide, this paper examines how Internet use influences suicidal ideation and its underlying mechanisms in the context of older adults. Synthesizing the interpersonal theory of suicide with prior literature on Internet use, this study explains that Internet use can reduce suicidal ideation through enhanced social belongingness. Our results using data from 6,056 older adults show that Internet use is negatively associated with suicidal ideation in older adults. The present study further highlights the mediating role of social connectedness (i.e., perceived loneliness and social relationship satisfaction) as an underlying mechanism between Internet use and suicidal ideation. Contributions and practical implications for addressing elderly suicidal problems and future works are discussed.
1	Cyberbullying is a major issue that is regrettably on the rise. The growth and rapid proliferation of the Internet, social media, and smart mobile devices have widened the audiences, increased anonymity and interactions to further heightened the potential for cyberbullying. While there is a substantial body of literature on cyberbullying there exists two dominant gaps: a lack of studies on adult cyberbullying and novel empirical approaches to understanding cyberbullying. Using information obtained from 75 cyberbullying court cases, this study provides preliminary evidence to better understand cyberbullying amongst adults. Therein, we identify, how cyberbullying occurs in relation to four key entities: the 'offender,' the 'technology,' the 'victim' and the 'guardianship'. We also identify key themes and their relationships that emerged from the court cases that must be further investigated in order to better understand cyberbullying in future work.
1	One emerging area in Information Systems scholarship is understanding how social injustices are related to social media use. We conduct a theorizing review to offer a theory of social injustice on social media. We examine the current literature at the intersection of social media and social injustice by using a grounded theory method. Our review will result in a theoretical framework. We illustrate one example from our developing framework with propositions related to how organizations facilitate marginalization. We discuss the contributions and implications of our framework for theory and practice, along with future directions such as offering a research agenda.
1	Guiding individual decision-making in digital environments through persuasive system design (PSD) is a powerful tool. While some forms of PSD such as digital nudging are based on libertarian paternalism and mostly considered ethically acceptable, other forms have been criticized for violating user autonomy or disadvantaging users. Such “controversial PSD” has been labelled inconsistently in the literature, for example as dark patterns or (digital) sludging. Thus, Information Systems (IS) research currently lacks a common vocabulary and conceptual clarity which impedes realizing the potential of PSD in research and practice. To address this issue, we present first results of a systematic literature review on controversial PSD. By compiling an overview of prevalent concepts, this study identifies four focal points of the ethical debate on PSD (intentions, strategies, outcomes, process) and derives implications and a research agenda for IS research.
1	Questioning the IT governance structures of international NGOs through the critical lens of post-colonial theory allows us to postulate a new combined theory. We conducted a series of 20 interviews with both international and local NGOs to examine IT decision structures more closely. Using the six aspects of post-colonial theory Lin et al. (2015) established, we examine potential additional influences in more detail. Our proposed theory explains our findings of mainly centralized governance structures that do not correspond to the innovation and flexibility goals we observed in addition to security needs and standardization efforts. We find that both IT goals (as part of IT governance) and post-colonial structures shaping organizational structures influence IT decision structures. Our findings indicate that we do not only need to take biases in the design of information systems but also in their governance into account.
1	While academia attributes superior value potential to sustainable smart PSS (SSPSS), in practice, they are not widely implemented. To address this gap, we analyze how the notion of SSPSS value is constructed through sensemaking. Adopting a case study approach, we explore differences in organizational sensemaking. Moreover, we analyze how the three functional roles “digital innovation and technology”, “sustainability”, and “market” involved in innovating SSPSS make sense of the value proposition. We conclude that value is subjective and the value proposition of SSPSS is multi-faceted. Each facet is constructed through the interaction of organizational, functional roles’, and individual sensemaking. At the organizational level, commitment, identity, and expectations influence the creation of shared meaning. At the functional role level, actors differ in their sensemaking based on the cognitive frames applied. At the individual level, subjective beliefs impact sensemaking. Hence, sensemaking is a multi-level process that raises the question of alignment.
1	Crowdfunding platforms enable individuals or groups to appeal to the public to support a variety of ventures or campaigns. Whilst the majority of campaigns raise funds for private causes, some of the issues for which help is being sought have arisen as a direct consequence of world events and crises. Nevertheless, the research on charitable crowdfunding has largely ignored this connection. We use the COVID-19 pandemic, and related public health policies, to explore the impact of the global crisis on donation behavior on the donation-based crowdfunding platform GoFundMe. By using a quasi-experimental research design, we find that after the introduction of stay-at-home orders, campaigns in U.S. states with such measures experienced a significant decline in the number of donors and amounts donated, which is more pronounced for crisis-related than for non-crisis-related campaigns. Our findings contribute to the literature by providing novel insights on crowdfunding behaviors in times of societal crisis.
1	Online food delivery (OFD) provides consumers convenient access to a wide range of food options. As calorie-dense options are frequently reported by OFD providers as top popular selections, OFD platforms may facilitate individuals’ unhealthy food intakes and derive adverse effect on people’s health. To clarify this causal link, we use a large- scale individual-year data on all hospital outpatient visits from the 12 most populous counties in New York to measure individuals’ health. Since major OFD providers enter different cities at staggered time, we leverage difference-in-differences models to evaluate the health impact of OFD platforms. We find that the entry of OFD platforms significantly increases the hospital visits due to overweight and obesity, which is directly associated with unhealthy diets, but not those visits due to hyperlipidemia, hypertension, and diabetes, which are associated less directly. More identification strategies and robustness checks are discussed in our future study.
1	Due to the rapid development in technology and the increasing digitization of organizations and society as a whole, digital ethics is becoming an increasingly important topic for researchers and practitioners of information systems (IS). This literature review shows the state of the art of ethical views present in IS research, at first establishing the relevance of the topic and then showing recent developments. Using a holistic view on ethics, this article provides (1) an overview of the number of publications considering ethics in IS research and on the different ethical constructs and theories. Additionally, it provides an overview (2) on the different fields of application. The aforementioned concepts (3) are contrasted to identify research streams and derive research gaps. Additionally (4), we provide a categorization scheme to classify ethics research in IS into 4 different types and from there (5) derive research propositions for future projects.
1	Digitalization has impacted how new practices emerge. In this study, we examined the genesis of new practices during the implementation of an ICT platform-enabled ecosystem in communities of subsistence farmers who were opening to modern development. The platform ecosystem, led by the eKutir social enterprise (Odisha, India), leveraged micro-entrepreneurs to establish networks of actors to support farmers. We view the eKutir model as an interstitial space, i.e., a transition space (combining physical, digital, and human components) that brings actors from different institutional fields together. Five focus groups (n=83 farmers) and semi-structured interviews (n=18 individuals) were conducted. Transcripts were analyzed using inductive methods. The study reveals a six-step institutional process: (i) forming physical and digital connections, (ii) gaining legitimacy, (iii) establishing inter-field resource dependencies, (iv) distributing power, (v) standardizing practices, and (vi) sustaining changes in practices. This study contributes to research on ICT for development and interstitial spaces.
1	Water scarcity in India is a growing crisis that threatens the livelihood of many rural communities within the country. To begin addressing the problem of excessive groundwater use, a portal including an interactive dashboard was created to visualize and track groundwater wells in five different villages in the northwest region of India which is primarily a desert region. Design science research principles were followed to construct this design artifact to assist rural Indian communities and stakeholders with understanding local groundwater behavior. This dashboard can help prompt regulatory or policy actions that may help manage sustainable groundwater management and water quality. The design artifact has gone through preliminary evaluation and improvement.
1	An unmanned retail shelf is a new retail format made possible by the advance in mobile internet and digital payment technologies. With drastically reduced personnel costs and upfront equipment investment, unmanned retail shelves promise the potential of deeper penetration into more convenient locations for consumers. However, a lack of onsite staff can also mean that unmanned retail shelves are more vulnerable to incidences of theft as compared to traditional stores. We investigate the impact of formal and informal surveillance technology in the context of an unmanned retail environment. By investigating the impact of surveillance technology on consumer behavior outcomes such as theft rate, approach rate, and conversion rate, we hope to better understand if surveillance technologies live up to the promises of improving net economic outcomes by delivering better security.
1	As data breaches continue to rise, customers exhibit heterogeneous expectations regarding the company's response. Universal responses can show backfire effects since they fail to meet the expectations. Thus, the challenge arises that customer expectations must be known to mitigate the consequences while time is limited to publish the data breach announcement. By drawing on service failure, data breach, and justice research, we theorize that customer involvement provides a viable approach to this challenge. We argue that active customer involvement allows customers to formulate their expectations. Thus, enabling companies to leverage these expectations to provide tailored data breach responses. We test our hypotheses in a digital experiment (n=304). Our results provide a first indication that active customer involvement in a data breach drives positive group value and negative self-interest effects. We contribute to the data breach literature by revealing that customer involvement constitutes a suitable mechanism for identifying customer expectations.
1	Gullibility is a behavior set that includes insensitivity to cues signaling untrustworthiness, the propensity to accept false information, reject true information, or taking costly risks. It is a useful lens from which to view real-world adverse outcomes driven by the online behaviors of seemingly well-intentioned, or non-malicious, individuals. Though well established in pre-internet literature, gullibility has been largely sidestepped as a driver of adverse events in the digital era despite ample evidence for its existence. To better understand the drivers and contextual factors behind digital gullibility, we propose a comprehensive research agenda which aligns open research gaps with a set of research driven propositions. The agenda builds on existing models and discussions in related domains, structures open questions and provides guidance for IS researchers and practitioners in the face of ongoing digital gullibility.
1	Data-driven is widely mentioned, but the data is generated by user behavior. Our work aims to utilize a behavior-driven model design pattern to improve accuracy and provide explanations in review-based recommendations. Review-based recommendation introduces review text to overcome the sparseness and unexplainably of rating or scores-based model. Driven by users rating behavior and human cognitive abilities, we proposed a deep learning recommendation model jointing users and products reviews (DLRM-UPR) to learn user preferences and product characteristics adaptively. The DLRM-UPR consists of word, text, and context co-attention layers considering the interaction between each user-product-context pair. Extensive experiments on real datasets demonstrate that DLRM-UPR outperforms existing state-of-the-art models. In addition, the relevant information in the reviews and the suggestion for improving the user experience can be highlighted to explain the recommendation results.
1	Social media influencers endeavour to attract, retain, and engage their followers, which drives a prevalent issue of problematic engagement of followers. However, scant research has examined followers’ problematic engagement with influencers, and little is known about how followers may respond to their problematic engagement. In this study, we propose a research framework to fill this gap. Drawing from coping theory, we propose that three facets of problematic engagement (cognitive, affective, and behavioral) would lead to followers’ coping responses (emotion-focused and problem-focused) via the mediation of followers’ situational appraisal (perceived threat). Moreover, we suggest that two key factors in social media influencing, i.e., social identification with influencers and perceived opinion leadership, would moderate the effect of perceived threat on followers’ coping responses. Two waves of online survey will be conducted to test the hypotheses. Results will provide useful insights to social media users as well as influencers.
1	Artificial intelligence (AI) has the potential to dramatically change the way decisions are made and organizations are managed. As of today, AI is mostly applied as a collaboration partner for humans, amongst others through delegation of tasks. However, it remains to be explored how AI should be optimally designed to enable effective human-AI collaboration through delegation. We analyze influences on human delegation behavior towards AI by studying whether increasing users' knowledge of AI's error boundaries leads to improved delegation behavior and trust in AI. Specifically, we analyze the effect of showing AI's certainty score and outcome feedback alone and in combination using a 2x2 between-subject experiment with 560 subjects. We find that providing both pieces of information can have a positive effect on collaborative performance, delegation behavior, and users' trust in AI. Our findings contribute to the design of AI for collaborative settings and motivate research on factors promoting delegation to AI.
1	Many online platforms that thrive on user-generated content (UGC) face the under-provision issue. A new strategy that platforms deploy to tackle this problem is to offer extra content curation tools with lower entry barriers. We study one of such features in online knowledge-sharing platforms: An additional content curation tool that allows users to share free-format content apart from the standard question-and-answer (Q&A) knowledge content. By leveraging a natural experiment occurred in a large Chinese online knowledge-sharing platform, we identify the causal effects of users’ adoption of a new low-barrier content tool on their knowledge contribution in a difference-in-differences (DiD) framework. We find that the adoption of such a low-barrier tool complements knowledge contribution. Specifically, users increased their volumes of answers without compromising effort spent on each answer after adoption. We validate this finding by further addressing the selection bias through a two-step Heckman correction approach with instrumental variable and a look-ahead matching method. Our results bear important implications for platform design and intervention to motivate UGC contribution.
1	One of the most significant challenges of Massive Open Online Courses (MOOCs) is the low user engagement rate. Despite its importance, few studies have explored this issue. In this paper, we study user behavior in MOOCs through the lens of community design and investigate the role of distinct types of user interactions on engagement and performance. The results of our study demonstrate that both on-topic and off-topic discussions can enhance user engagement and learning outcomes. However, the impact of each discussion type varies based on specific user characteristics. The findings of our paper have significant implications for the providers and designers of MOOC platforms.
1	Live streaming platforms are promoting a novel format of entertainment called PK event where live streamers compete to solicit virtual gifts from viewers. Although the two live streamers in a PK event come across as rivals, they implicitly collaborate to emotionally arouse viewers and solicit virtual gifts. We advance a curvilinear moderated mediation model to disentangle the effects of streamers’ shared PK experience on revenue growth through enticing viewers’ emotional arousal, which is moderated by streamers’ within-team experience acquisition difference. Estimating a multilevel linear model on a sample of 118,323 PK records, we discovered that shared PK experience has an inverted-U-shape relationship with emotional arousal level, which is positively associated with revenue growth. We further attested to the moderating influence of experience acquisition difference in strengthening this curvilinear relationship. Our findings help platforms to improve team member recommendation systems and streamers to find the “right” teammates for optimizing PK performance.
1	The habitual use of information systems (IS) has gained increased attention over the last decade both in research and practice. Since habitual use signifies individuals’ repeated interaction with IS, it holds the potential for IS users and organizations to maximize their return on IS investment. Recent meta-analysis results have highlighted mixed results due to differences in context, mediators, and theoretical models. Recent research has begun exploring the duality of habit to explain both positive and negative impacts of habitual behavior through the dual process theory of habituation and sensitization with promising results. Therefore, to help address the mixed results across studies, we propose a unified theoretical framework investigating habit’s role in influencing users’ cognitions and resulting continued use decisions across multiple contexts and user types.
1	A large proportion of traffic congestion can be attributed to daily commute. While smart mobility systems (SMSs) intend to address the resulting challenges by actively changing users’ behavior, many SMSs suffer from users’ meaningful engagement. Research and practice have started examining engagement factors in order to increase meaningful engagement with SMSs. The question of how traffic participants can be continuously involved with SMSs to sustainably change their behavior has not been answered satisfactorily yet. In our paper, we identify relevant gamification elements suitable to improve meaningful engagement based on a literature study and market analysis. We used a design science approach to derive design requirements. Building on these, we assume comprehensive design principles and used them to derive initial design features. With these, we started a first feature configuration and a prototypical app implementation towards designing a sustainable SMS.
1	To motivate user engagement and generate desired engagement outcomes, some social trading platforms have introduced gamified systems with hierarchical badges and financial incentives. However, previous studies have not examined the effectiveness of such an application. Based on data collected from a popular social trading platform, eToro, we empirically examine the effectiveness of the gamified system and its mechanism. Results indicate that the gamified system under social trading is effective in inducing user-to-user and user-to-system interactions, thus leading to some desired engagement outcomes on social media. However, the gamified system does not contribute to the engagement outcomes on financial investment. Our paper contributes to the literature on both gamification and social trading, highlighting gamification’s important practical implications for platform managers, cautioning against the possible ineffectiveness of the dual-outcome gamified system and shedding light on the design of gamified systems.
1	The problem of worsening gender skew is particularly damaging in the context of matching platforms since it affects the welfare and user experience of women participants. In this paper, we look at how a platform-level intervention could reduce the congestion for women and thus improve their overall user experience on the platform. We specifically look at a form of "gender gating" intervention in a leading matrimonial platform within one of their sub-domains. The intervention restricts the profile visibility of women users based on age, education, income, and marital status related to social norms. Our analysis shows that the platform-level intervention had the desired effect – women in the treatment group received fewer unwanted requests for contacts, experienced more matches, and initiated more contacts themselves, representing a better user experience. Our work extends the platform literature by studying how platform owners can improve Women's welfare on matching platforms through market design.
1	Online communities, like Wikipedia and Stack Overflow, have made a vast repository of knowledge available as a public good. However, they suffer from under-contribution in terms of quantity and quality. To tackle this issue, online communities have increasingly been relying on gamification, the use of game elements in non-game settings, to incentivize their members. The consequences of introducing such features on members’ behavior have remained elusive—partly due to the lack of controlled experiments. Herein, we take advantage of a natural experiment in which a technical online community introduced gamified rewards, which are awarded contingent on performance thresholds—termed performance contingent symbolic awards. Employing a difference-in-differences design using a comparable online community as a control group, we find that the introduction of performance contingent symbolic awards has a negative impact on the contribution behavior overall and that experienced members reduce their contribution quantity while inexperienced members reduce their contribution quality.
1	Social nudges are well recognized for their effectiveness in promoting desired behaviors. However, online information overload makes social nudges less appealing. Reminding people about social nudges may boost their efficacy. We investigate the treatment and persistent effect of a reminding social nudge on user engagement with a new function and the spillover effect on user engagement with an existing function through a large-scale randomized field experiment. Our results indicate that compared with a social nudge, the reminding social nudge reduces user engagement with the new function over the treatment period. Interestingly, after removing the nudges, users who received the reminding social nudge are more engaged with the new function than users received the social nudge. The reminding social nudges designed for the new function also have a negative spillover impact on user engagement with the existing function. Theoretical and practical implications about using nudges to introduce new functions are discussed.
1	A great deal of time spent with information and communication technologies (ICT) makes knowledge workers especially susceptible to technostress. In this regard, the concept of digital detox has emerged to describe a strategic and periodic disengagement with ICT. Whereas extant research has put emphasis on untangling the characteristics and effects of digital detox, we do not know what motivators underly this practice other than the more general notion of technostress. Therefore, this study undertakes a qualitative approach with 10 phenomenological interviews to identify motivators of digital detox in knowledge work. The interview results culminate in a typology of individual digital detox motivators. We contribute to the theorization of digital detox by linking different types of motivators (prevention/coping/performance/physiological reboot) to digital detox and its effects on the experience of technostress. Implications for the design of digital detox policies that match with the motivations of knowledge workers can be derived.
1	Smart home technologies and apps are on a rise. This allows to implement digital nudging elements to foster energy-conservation behavior and, thus, contribute to mitigating climate change. Digital nudging via feedback can be effective in improving energy-conservation behavior, as substantial prior research has shown. However, the investigation of users’ preferences concerning feedback nudges is missing. This lack of knowledge is crucial, as user satisfaction influences their continuous app usage, a precondition for achieving positive effects. To close this gap, we perform a structured literature review, categorize the feedback nudge features from extant research, and conduct an online survey. Based on survey data and the Kano model, we analyze the effect of feedback nudge features on user satisfaction. Our study complements the traditional focus on the effectiveness of these nudges with a perspective on user satisfaction. The combination of both perspectives suggests which feedback nudge features should be considered for implementation.
1	We propose an empirical setting to discover sentiment contagion in social media.  We find that, after controlling for concurrent events, sentiment contagion exists in social media. We conduct additional analyses to explore how the source and valence of exposure contents and individual heterogeneity affect the degree of sentiment contagion. We find robust evidence of sentiment contagion not only in contents under the same thread but also under different threads of the same forum. Additional analysis provides evidence of negativity bias.  In terms of individual heterogeneity, we find that more experienced social media users are less sensitive to sentiments in social media. Last, we find that social media users are more likely to become inactive in the long run after being exposed to more negative contents. Managerial and practical implications are discussed.
1	Advanced technologies are introduced in warehouse operations, rendering the interplay between human worker behavior and information systems (IS) a critical issue. We investigate how IS supports manual order picking by studying how visual color-coding information on picking locations provided through personal digital assistants accelerates search and picking tasks. Considering real-world data on a storage system where 20 dissimilar items are stored together at one picking location, we apply a log-logistic accelerated failure time model with N=112,672 picks performed by N=190 workers and find that color-coding accelerates the picking process by up to 17.28%. To increase the internal validity of our field-based examination, we conduct one VR experiment (N=29 participants) providing evidence for an acceleration of 23.74%, and one online experiment (N=178 participants) indicating an acceleration of 24.29%. Based on an innovative method of triangulation, we demonstrate how IS can influence picker behavior and discuss how to better design IT artifacts.
1	Although social networking sites (SNS) users often share positive emotions in the content posted online, their satisfaction with SNS and intention to continue using it vary greatly across users. We argue that a key to addressing this puzzle is how content creators up-regulate their emotions on SNS. Building on emotion regulation theory and belongingness theory, we characterize digital emotion regulation in two ways (i.e., positive shift and emotional labor) and propose a dual-pathway model that involves two self-views. By constructing three complementary studies, we find that it is emotional labor, rather than positive shift, that drives a user’s sense of belonging through anticipated self-enhancement (i.e., communal self-view) and felt authenticity (i.e., authentic self-view) and explains the varying outcomes. Our findings reveal the benefits of deep acting and countervailing effects of surface acting. The present research provides important theoretical and practical implications.
1	Although considerable research effort has been devoted to understanding the adoption and use of commercially available intelligent assistants, the relationship between user expectations from assistants and users’ endogenous intrinsic motivation to perform an activity has not been explored. Doing so is important to meet user expectations, prevent adoption failures, and design for well-being. In this paper, we investigate whether a person's intrinsic motivation to perform an activity impacts (a) their expectations from an assistant, and, (b) the assistant feature set chosen to meet these expectations. Via a survey based study with N=296 participants, we provide empirical evidence showing that, after controlling for demographic factors, users' prior, endogenous intrinsic motivation influences their intrinsic expectations for competence, stimulation and influence, but not extrinsic and hedonic expectations. Users with low prior motivation prefer an assistant in a supervisor role. Implications for research and practice are discussed.
1	Endogeneity arises for numerous reasons in models of consumer choice. It leads to inconsistency with standard estimation methods that maintain independence between the model's error and the included variables. The authors describe a control function approach for handling endogeneity in choice models. Observed variables and economic theory are used to derive controls for the dependence between the endogenous variable and the demand error. The theory points to the relationships that contain information on the unobserved demand factor, such as the pricing equation and the advertising equation. The authors’ approach is an alternative to Berry, Levinsohn, and Pakes's (1995) product-market controls for unobserved quality. The authors apply both methods to examine households’ choices among television options, including basic and premium cable packages, in which unobserved attributes, such as quality of programming, are expected to be correlated with price. Without correcting for endogeneity, aggregate demand is estimated to be upward-sloping, suggesting that omitted attributes are positively correlated with demand. Both the control function method and the product-market controls method produce downward-sloping demand estimates that are similar.
1	The authors present a polytomous item randomized response model to measure socially sensitive consumer behavior. It complements established methods in marketing to correct for social desirability bias a posteriori and traditional randomized response models to prevent social desirability bias a priori. The model allows for individual-level inferences at the construct level while protecting the privacy of respondents at the item level. In addition, it is possible to incorporate covariates in to various parts of the model. The proposed method is especially useful to study social issues in marketing. In the empirical application, the authors use a two-group experimental survey design and find that with the new procedure, participants report their sensitive desires more truthfully, with significant differences between socioeconomic groups. In addition, the method performs better than methods based on social desirability scales. Finally, the authors discuss truthfulness in data collection and confidentiality in data utilization.
1	Predicting aggregate consumer spending is vitally important to marketing planning, yet traditional economic theory holds that predicting changes in aggregate consumer spending is not possible. Previous attempts to predict consumer spending growth using standard macroeconomic predictor variables have met with little success. The authors show that the lagged change in customer satisfaction, which contributes to future demand, has a significant impact on spending growth. However, this impact is moderated by increases in consumers’ debt service ratio, a key budget constraint that affects consumers’ ability to spend. Using an asymmetric growth model, more than 23% of the variation in the one-quarter-ahead spending growth is explained, which represents a notable improvement over prior specifications.
1	Suppliers in business-to-business settings are increasingly building a portfolio of multiple types of ties with individual customers. For example, in addition to supplying goods and services, a supplier may have a research-and-development alliance and a marketing alliance with a customer. This study investigates the effect of multiple types of ties with a customer on a supplier's performance with the customer. The findings from panel data on supplier–customer relationships suggest that an increase in the number of different types of ties with a customer results in an increase in supplier sales to the customer and a decrease in sales volatility to that customer. The effect of a change in relationship multiplexity (i.e., number of different types of ties) on the change in sales becomes weaker and its effect on the change in sales volatility becomes stronger as the competitive intensity in the customer's industry increases. The results also indicate that the effect of a change in the number of different types of ties on the change in sales volatility becomes stronger when the intangibles intensity in a customer's industry increases. The results are robust to alternative measures, alternative estimators, heteroskedasticity, and endogeneity, among other methodological concerns. These findings have clear implications for managing multiple types of ties with a customer and indicate that relationship multiplexity is a valuable nonfinancial metric.
1	How do vertical product line extensions influence a retailer’ sprice image? Conventional wisdom suggests that adding an upscale or downscale item to a product line has a directionally consistent impact on price image, such that upscale extensions increase price image and downscale extensions decrease price image. In contrast, this research argues that vertical extensions can have the opposite effect, such that upscale extensions can decrease rather than increase price image, and vice versa for downscale extensions. The authors further propose that the impact of vertical extensions on price image is a function of consumer goals and, in particular, whether consumers have the intent of browsing or buying. The authors test these predictions in a series of five empirical studies that offer converging evidence in support of the proposed theory.
1	The authors propose a new model to capture unobserved consideration from discrete choice data. This approach allows for unobserved dependence in consideration among brands, easily copes with many brands, and accommodates different effects of the marketing mix on consideration and choice as well as unobserved consumer heterogeneity in both processes. An important goal of this study is to establish the validity of the existing practice to infer consideration sets from observed choices in panel data. The authors show with experimental data that underlying consideration sets can be reliably retrieved from choice data alone and that consideration is positively affected by display and shelf space. Next, the model is applied to Information Resources Inc. panel data. The findings suggest that promotion effects are larger when they are included in the consideration stage of the two-stage model than in a single-stage model. The authors also find that consideration covaries across brands and that this covariation is mainly driven by unobserved consumer heterogeneity. Finally, the authors show the implications of the model for promotion planning relative to a more standard model of choice.
1	For Internet retailers, demand propagation varies not only through time but also over space. The authors develop a Bayesian spatiotemporal model to study two imitation effects in the evolution of demand at an Internet retailer. Building on previous literature, the authors allow imitation behavior to be reflected both in geographic proximity and in demographic similarity. As these imitation effects can be time varying, the authors specify their dynamics using a “polynomial smoother” embedded within the Bayesian framework. They apply the model to new buyers at Netgrocer. com and calibrate it on 45 months of data that span all 1459 zip codes in Pennsylvania. The authors find that the proximity effect is especially strong in the early phases of demand evolution, whereas the similarity effect becomes more important with time. Over time, new buyers are increasingly likely to emerge from new zip codes beyond the “core set” of zip codes that produce the early new buyers, and spatial concentration declines. The authors explore the managerial implications stemming from these findings through a hypothetical “seeding” experiment. They also discuss other implications for Internet retailing practice.
1	Conventional wisdom in marketing holds that (1) retailer forward buying is a consequence of manufacturer trade promotions and (2) stockpiling units helps the retailer but hurts the manufacturer. This article provides a deeper understanding of forward buying by analyzing it within the context of manufacturer trade promotions, competition, and demand uncertainty. The authors find that regardless of whether the manufacturer offers a trade promotion, allowing the retailer to forward buy and hold inventory for the future can, under certain conditions, be beneficial for both parties. Disallowing forward buying by the retailer may lead the manufacturer to lower merchandising requirements and change the depth of the promotion. In competitive environments, there are situations in which retailers engage in forward buying because of competitive pressures in a prisoner's-dilemma situation. Finally, when the authors consider the case of uncertain demand, they find further evidence of strategic forward buying. In particular, the authors find cases in which the retailer orders a quantity that is higher than what it expects to sell in even the most optimistic demand scenario.
1	The authors analyze primary demand effects of marketing efforts directed at the physician (detailing and professional journal advertising) versus marketing efforts directed at the patient (direct-to-consumer advertising). The analysis covers 86 categories, or approximately 85% of the U.S. pharmaceutical market, during the 2001–2005 period. Primary demand effects are rather small, in contrast with the estimated sales effects for individual brands. By using a new brand-level method to estimate primary demand effects with aggregate data, the authors show that the small effects are due to intense competitive interactions during the observation period but not necessarily to low primary demand responsiveness. In contrast with previous studies, the authors also find that detailing is more effective in driving primary demand than direct-to-consumer advertising. A category sales model cannot provide such insights. In addition, a category sales model likely produces biased predictions about period-by-period changes in primary demand. The suggested brand-level method does not suffer from these limitations.
1	This research uses a dual attitudes perspective to offer new insights into flattery and its consequences. The authors show that even when flattery by marketing agents is accompanied by an obvious ulterior motive that leads targets to discount the proffered compliments, the initial favorable reaction (the implicit attitude) continues to coexist with the discounted evaluation (the explicit attitude). Furthermore, the implicit attitude has more influential consequences than the explicit attitude, highlighting the possible subtle impact of flattery even when a person has consciously corrected for it. The authors also clarify the underlying process by showing how and why the discrepancy between the implicit and explicit attitudes induced by flattery may be reduced. Collectively, the findings from this investigation provide implications for both flattery research and the dual attitudes literature.
1	Educational placements increasingly appear to replace public service announcements as a means of communication, but their efficacy seems questionable because they contain mixed messages. Two experiments, conducted with 2850 adolescents, test versions of a real television program with an antismoking educational placement against a control program. Typical antismoking programs contain three indirectly competing messages: Smokers are attractive and prevalent but also worthy of disapproval. Experiment 1 tests several program versions with these three messages and reveals that the disapproval message dominates and elicits negative smoker thoughts and beliefs, despite the otherwise potent smoker attractiveness message. Experiment 2 indicates corresponding effects for intent but shows that adding a directly competitive smoker approval message nullifies this effect. Furthermore, Experiment 2 tests an educational epilogue designed to reinforce the disapproval message, but this message instead induces boomerang effects among smokers. That is, among smokers, an educational placement is counterattitudinal, so when the epilogue discloses the placement and evokes persuasion knowledge, smokers generate more positive smoker beliefs and intentions. These findings contribute to research regarding competing referent messages, disclosures, and persuasion knowledge.
1	In many product categories, consumer tastes are diverse, and firms use finely targeted advertising to inform consumers about their products. This article proposes a model of informative advertising that allows for diverse consumer tastes and multiple competing firms. Using this framework, the authors analyze how diversity in consumers’ tastes, informative advertising, and improvements in advertising technology may influence prices. First, informative advertising can lead to lower prices if consumer valuations are high. However, if consumer valuations are low, informative advertising can lead to higher prices. Second, when consumer valuations are high, price increases with greater diversity in tastes, though this result reverses if consumer valuations are low. Third, improvements in advertising technology lead to higher levels of advertising when consumer valuation is high, but the opposite effect can occur when consumer valuation is low. The authors relate these theoretical findings to previous empirical literature on advertising.
1	Respondents can vary strongly in the way they use rating scales. Specifically, respondents can exhibit a variety of response styles, which threatens the validity of the responses. The purpose of this article is to investigate how response style and content of the items affect rating scale responses. The authors develop a novel model that accounts for different types of response styles, content of items, and background characteristics of respondents. By imposing a bilinear parameter structure on a multinomial logit model, the authors graphically distinguish the effects on the response behavior of the characteristics of a respondent and the content of an item. The authors combine this approach with finite mixture modeling, yielding two segmentations of the respondents: one for response style and one for item content. They apply this latent-class bilinear multinomial logit model to the well-known List of Values in a cross-national context. The results show large differences in the opinions and the response styles of respondents and reveal previously unknown response styles. Some response styles appear to be valid communication styles, whereas other response styles often concur with inconsistent opinions of the items and seem to be response bias.
1	Are persistent marketing effects most likely to appear right after the introduction of a product? The authors give an affirmative answer to this question by developing a model that explicitly reports how persistent and transient marketing effects evolve over time. The proposed model provides managers with a valuable tool to evaluate their allocation of marketing expenditures over time. An application of the model to many pharmaceutical products, estimated through (exact initial) Kalman filtering, indicates that both persistent and transient effects occur predominantly immediately after a brand's introduction. Subsequently, the size of the effects declines. The authors theoretically and empirically compare their methodology with methodology based on unit root testing and demonstrate that the need for unit root tests creates difficulties in applying conventional persistence modeling. The authors recommend that marketing models should either accommodate persistent effects that change over time or be applied to mature brands or limited time windows only.
1	This research examines cross-cultural differences in brand dilution effects and the moderating role of motivation and extension typicality. Drawing from recent findings that indicate that culture affects the way people treat conflicting information, this research predicts that Easterners and Westerners react differently to failures by a brand extension. In contrast to previous findings that have suggested that failure in a typical extension leads to less brand dilution for Westerners when they are highly motivated (than when they are less motivated), this study argues that Easterners exhibit greater brand dilution when they are less motivated (than when they are highly motivated). The opposite pattern of results should emerge when the extension is atypical. Three studies provide support for these predictions and the underlying processes.
1	Socially desirable responding (SDR) has been of long-standing interest to the field of marketing. Unfortunately, the construct has not always been well understood by marketing researchers. The authors provide a review of the SDR literature organized around three key issues—the conceptualization and measurement of SDR; the nomological constellation of personality traits, values, sociodemographics, and cultural factors associated with SDR; and the vexing issue of substance versus style in SDR measures. The authors review the current “state of the literature,” identify unresolved issues, and provide new empirical evidence to assess the generalizability of existing knowledge, which is disproportionately based on U.S. student samples, to a global context. The new evidence is derived from a large international data set involving 12,424 respondents in 26 countries on four continents.
1	Social commerce is an emerging trend in which sellers are connected in online social networks and sellers are individuals instead of firms. This article examines the economic value implications of a social network between sellers in a large online social commerce marketplace. In this marketplace, each seller creates his or her own shop, and network ties between sellers are directed hyperlinks between their shops. Three questions are addressed: (1) Does allowing sellers to connect to each other create value (i.e., increase sales)? (2) What are the mechanisms through which this value is created? and (3) How is this value distributed across sellers in the network and how does the position of a seller in the network (e.g., its centrality) influence how much he or she benefits or suffers from the network? The authors find that (1) allowing sellers to connect generates considerable economic value, (2) the network's value lies primarily in making shops more accessible to customers browsing the marketplace (the network creates a “virtual shopping mall”), and (3) the sellers who benefit the most from the network are not necessarily those who are central to the network but rather those whose accessibility is most enhanced by the network.
1	When a multichannel retailer opens its first retail store in a state, the firm is obligated to collect sales taxes on all Internet and catalog orders shipped to that state. This article assesses how opening a store affects Internet and catalog demand. The authors analyze purchase behavior among customers who live far from the retail store but must now pay sales taxes on catalog and Internet purchases. A comparable group of customers in a neighboring state serves as a control. The results show that Internet sales decrease significantly, but catalog sales are unaffected. Further investigation indicates that the difference in these outcomes is partly attributable to the ease with which customers can search for lower prices at competing retailers. The authors extend the analysis to a panel of multichannel firms and show that retailers that earn a large proportion of their revenue from direct channels avoid opening a first store in high-tax states. They conclude that current U.S. sales taxes laws have significant effects on both customer and firm behavior.
1	This research studies the effect of consumers’ lay theories of self-control on their choices of products for young children. The authors find that people who hold the implicit assumption that self-control is a small resource that can be increased over time (“limited-malleable theorists”) are more likely to engage in behaviors that may benefit children's self-control. In contrast, people who believe either that self-control is a large resource (“unlimited theorists”) or that it cannot increase over time (“fixed theorists”) are less likely to engage in such behaviors. Field experiments conducted with parents demonstrate that limited-malleable theorists take their children less frequently to fast-food restaurants, give their children unhealthful snacks less often, and prefer educational to entertaining television programs for them. Similar patterns are observed when nonparent adults make gift choices for children or while babysitting. The authors obtain these effects with lay theories both measured and manipulated and after they control for demographic and psychological characteristics, including own self-control. These results contribute to the literature on self-control, parenting, and consumer socialization.
1	This research examines how death-related media contexts affect consumers’ preferences for domestic and foreign brands. Four lab and field studies demonstrate that death-related media contexts affect subsequent brand evaluations in distinct ways, which have not yet been documented in the extant literature on media context. First, death-related media contexts shift consumers’ preferences in favor of domestic brands by enhancing consumers’ patriotism. Second, the death-related media context effect appears only with a temporal delay between the media context and the brand evaluation. Even after a 24-hour delay, the effect still takes place as long as consumers have not been given an opportunity to attenuate death-induced anxiety. Finally, the authors show that foreign brands can counter the negative effects of a death-related media context by making a prodomestic advertising claim.
1	Five studies examine how the two distinct emotional states of shame and guilt influence the effectiveness of messages that highlight socially undesirable consequences of alcohol consumption. Appeals that frame others as observing versus suffering the negative consequences of binge drinking differentially activate shame and guilt. Given these emotional consequences of message framing, the authors examine the interaction between incidental shame or guilt and message framing on drinking intentions and behavior. Compatible appeals (i.e., appeals that elicit the same emotion as being incidentally experienced by the consumer) are less effective in influencing behavioral intentions and beverage consumption because of a process in which consumers discount the notion that they may cause the negative consequences outlined in the message. Such defensive processing of compatible messages is driven by a desire to reduce the existing negative emotion.
1	Consumers desire products that provide meaningful experiences. Therefore, a marketer's success often depends on familiarizing consumers with the unique experience a product offers. Marketers recognize the value in communicating about a product experience through analogy, but little research has investigated if and why these analogies are persuasive. By comparing a product to a familiar but disparate experience, an analogy has the power to focus consumers on the evaluative, emotional, and multisensory information associated with the product experience. This focus on subjective product experience enables the identification of base preference (i.e., a consumer's liking for the comparison experience) as an important moderator of analogical persuasiveness. In addition, the emotional knowledge transfer perspective applied in this research contributes to a better understanding of the role of emotional knowledge and experienced emotion in analogical thinking.
1	Sweepstakes and contests are some of the most frequently used promotional tools. Consumers participating in sweepstakes or contests have an opportunity to win prizes through a random draw. The authors examine how commonly used sweepstakes formats that vary in the number of winners and the allocation of the total reward money among the winners affect consumer valuations of the promotion. Given a fixed amount of reward money, the authors examine alternative reward formats based on the promotional objectives, consumer risk aversion, and degree of subadditivity. They test the analytical results using an experimental approach.
1	Four studies examine how consumers’ prior knowledge of a product category and the way they process product information affect evaluation. Consumers with extensive prior knowledge of a category evaluate the brand more favorably when the presentation of the product information prompts a sense of progress rather than facilitating a detailed assessment (Studies 1 and 2), as well as when the information presentation involves a high level of construal rather than a low level (Studies 3 and 4). Consumers with limited domain knowledge exhibit opposite outcomes. The subjective experience of processing fluency mediates these effects. The findings suggest that evaluations are more favorable when there is a fit between prior knowledge and message processing than when fit is absent.
1	Recent research challenges the idea that greater choice is always desirable, showing that larger assortments can increase choice deferral and switching. The current research demonstrates that even when consumers make a purchase, the same item may generate lower satisfaction when chosen from a larger rather than a smaller assortment. The authors explain this effect in terms of an expectation-disconfirmation mechanism. When assortments are small, consumers have low expectations about their ability to match their preferences. As assortment sizes increase, so do consumers’ expectations of the degree of preference match they can achieve. Subsequently, consumers may experience greater negative expectation disconfirmation or less positive expectation disconfirmation when a chosen item comes from a larger rather than a smaller set. Either less positive or more negative disconfirmation leads to lower choice satisfaction. The results from three studies support this expectation-based process and establish this mechanism in addition to alternative explanations, such as choice overload.
1	This research challenges the notion that increased search effort results in greater satisfaction with the choice by examining the impact of alignability on search quantity and search outcomes. Options that vary along comparable dimensions are alignable, whereas options that vary along unique dimensions are nonalignable. The results of three studies demonstrate greater search among nonalignable than among alignable options. Satisfaction initially increases but then declines with further search among nonalignable options. Although choice difficulty influences search and satisfaction, the primary mechanism driving the inverted U-shaped relationship of satisfaction with search among nonalignable options is feature learning. This research demonstrates a paradox: People continue searching more options precisely when their further search is detrimental to their satisfaction, falling down the slippery slope of search.
1	The majority of brand extensions reportedly fail, suggesting the need for methodologies that allow for better strategic prediction of categories into which a brand should extend or license. Prior literature suggests that brand extensions are likely to be more successful if a brand extends into another category into which its existing brand associations and imagery “fit” better and if the extending brand is “atypical” (if it possesses associations and imagery that are broad and abstract rather than tied too closely to the brand's original product category). The authors develop a methodology in this study to estimate brand and category personality structures, using a Bayesian factor model that separates the two by means of brand-level and category-level random effects. This methodology leads to measures of a brand's fit and atypicality. The authors illustrate and validate the model on two nationally representative data sets on brand personalities in three categories (jeans, magazines, and cars) and investigate the brand extension and licensing implications of the results obtained with the model.
1	Attitudes differ in terms of the functions they serve: Whereas attitudes toward some products may serve a utilitarian purpose of helping consumers maximize rewards, attitudes toward other products may symbolize or express consumers’ values. This article shows that branding alters the associations between products and attitude functions. Specifically, product categories that are generally associated with utilitarian attitudes are associated with less utilitarian, more symbolic attitudes when branded, whereas product categories that are generally associated with symbolic attitudes are associated with more utilitarian, less symbolic attitudes when branded. Branding also has important implications for persuasion and for the “function-matching” advantage: Although utilitarian appeals are most persuasive for “utilitarian” products (and symbolic appeals are most persuasive for “symbolic” products) at the category level, this article shows that this pattern does not emerge at the brand level, in part because attitude functions change with branding.
1	Using an assimilation and contrast framework, the authors assess the buffering and amplifying effects of relationship commitment on organizational buyers’ intentions to switch suppliers when a relationship is strained by the incumbent's own misbehavior. The results of three studies show that both calculative and affective commitment buffer incumbent suppliers against minor incidences of their own misbehavior but that affective commitment also reliably amplifies the adverse effects of an incumbent supplier's flagrant opportunism. Process tests indicate that buyer perceptions of supplier conformance to normative standards account for (completely mediate) the observed buffering and amplification effects in a manner consistent with the underlying assimilation and contrast framework.
1	The authors review the implicit association test (IAT), its use in marketing, and the methodology and validity issues that surround it. They focus on a validity problem that has not been investigated previously, namely, the impact of cognitive inertia on IAT effects. Cognitive inertia refers to the difficulty in switching from one categorization rule to another, which causes IAT effects to depend on the order of administration of the two IAT blocks. In Study 1, the authors observe an IAT effect when the compatible block precedes the incompatible block but not when it follows the incompatible block. In Studies 2 and 3, the IAT effect changes its sign when the order of the blocks reverses. Cognitive inertia distorts individual IAT scores and diminishes the correlations between IAT scores and predictor variables when the block order is counterbalanced between subjects. Study 4 shows that counterbalancing the block order repeatedly within subjects can eliminate cognitive inertia effects on the individual level. The authors conclude that researchers should either interpret IAT scores at the aggregate level or, if individual IAT scores are of interest, counterbalance the block order repeatedly within subjects.
1	Advertising recognition tests use advertisements as visual retrieval cues; they require consumers to report which advertisements they remember having seen earlier and whether they noticed the advertised brand and read most of the text at the time. Using a heterogeneous randomly stopped sum model, the authors establish the relationship between consumers' actual attention to print advertisements, as measured through eye tracking, and subsequent ad recognition measures. They find that ad recognition measures are systematically biased because consumers infer prior attention from the ad layout and their familiarity with the brands in the advertisements. Such biases undermine the validity of recognition tests for advertising practice and theory development. The authors quantify the positive and negative diagnostic value of ad recognition for prior attention and demonstrate how these diagnostic values can be used to develop bias-adjusted recognition (BAR) scores that more accurately reflect prior attention. Finally, the authors show that differences in the scores from ad recognition tests based on in-home versus lab exposure attenuate when the bias-adjustment procedure is applied.
1	The authors introduce the concept of consumer options and empirically validate it in the context of event ticket pricing. They demonstrate that consumer options can protect consumers from the downside related to uncertain outcomes and enhance seller profits by enabling superior market segmentation and increasing consumer willingness to pay. The authors examine the newly proposed ticket pricing mechanism in sports markets, in which there is uncertainty about the teams that will play in a final event (e.g., the National Collegiate Athletic Association Final Four basketball tournament). Fans who want to attend the game after knowing which teams will play are often disappointed because tickets typically sell out in advance. The authors propose that a fan can buy an option on a ticket before this uncertainty is resolved. Later, he or she can decide whether to exercise the option. The authors present a simple analytical model of consumer options in this setting. Then, they empirically demonstrate that profits under option pricing can exceed those from (1) advance selling and (2) pricing after uncertainty is resolved. The analysis and findings of this article lay a foundation for future work on consumer options in marketing.
1	Firms frequently compete across multiple segments. Such multimarket contact has been shown to deter aggressive competition, leading to “mutual forbearance.” Empirical support for this phenomenon derives mainly from studies on the direct effects of multimarket contact on a firm's decision variables. The analysis in this article extends the existing literature by empirically considering both the direct effects of multimarket contact (i.e., how it affects a firm's decision variables) and its strategic effects (i.e., how it affects a firm's reactions to its competitors' decision variables). The authors study the pricing and new product introduction decisions of firms in the personal computer industry. Consistent with prior research, the authors find that firms mutually forbear in price and new product introductions. More important, the authors find strong strategic effects that are asymmetric; namely, firms respond to competitive attacks by introducing new products but do not use price as a retaliatory weapon. Thus, firms isolate any competitive retaliation to only a single marketing variable. The results offer a deeper understanding of the influence of multimarket contact on firm behavior.
1	Nintendo lost its dominant position in the video game industry during the console war between its Nintendo 64 and Sony's PlayStation. However, Nintendo could have made several different strategic decisions to change the outcome. This article develops a structural model and investigates these alternative strategies through policy simulations. In particular, the author provides a framework to study firms' optimal pricing strategies under network effects, consumer heterogeneity, and oligopolistic competition. Consumer heterogeneity provides an incentive for a durable goods manufacturer to price skim, while network effects lead to an opposite motive for penetration pricing. The proposed framework incorporates these two competing motives under oligopolistic competition. The author estimates a demand system that allows for indirect network effects and consumer heterogeneity and then numerically solves for the Markov perfect equilibrium in firms' dynamic pricing game. Policy simulations indicate that Nintendo could have won the console war either with 10% more games or with a “head start” of one million units in installed base at the time of the PlayStation introduction.
1	Before a new product launch, marketers need to infer how demand will respond to various levels of marketing-mix variables to set an appropriate marketing plan. A critical challenge in estimating marketing-mix responsiveness from historical data is that the observed decisions were affected by private information possessed by managers about the heterogeneous effects of marketing-mix variables on sales. The authors refer to this as the “slope endogeneity” problem. Such endogeneity differs from the “intercept endogeneity” problem, which has been widely acknowledged in the literature. To correct for the slope endogeneity bias, the authors develop a conceptually simple control function approach that is amenable to multiple endogenous variables and marketing-mix carryover effects. The method is applied to forecasting advertising responsiveness in the U.S. DVD market. The results suggest that advertising responsiveness varies substantially across DVD titles and that estimated marketing-mix elasticities would be biased if the slope endogeneity problem were ignored. This analysis also yields findings of substantive interest to researchers and managers involved in entertainment marketing.
1	Although team-based selling is highly prominent in practice, research on the drivers of its effectiveness is sparse. Drawing from the literature on climate consensus, the authors propose that in addition to leadership and team factors, team consensus plays a critical role in boosting sales team effectiveness. Using survey and archival data from a sample of 185 pharmaceutical sales teams, the authors find that high team consensus regarding team-level leadership empowerment behaviors (LEBs) and team interpersonal climate quality enhances team potency given high LEBs but weakens team potency given low LEBs. In turn, team potency translates into sales team performance through both extra-role (team helping behavior) and in-role (team effort) behavior. The authors discuss the implications of these findings.
1	Quantitative models in marketing typically focus on the household as the unit of analysis while ignoring the individual family members' behavior and the behavioral interactions among household members. However, knowledge of such intrahousehold behavioral interaction enables marketers to target their communications more effectively. In this article, the authors propose a modeling framework to capture the intrahousehold behavioral interaction based on family members' actual consumption behavior over time. The authors develop a model to capture multiple agents' (more than two individuals') simultaneous choice decisions over more than two choice alternatives. This is extremely difficult with other previously developed modeling approaches. The authors apply the proposed model to a context of family members' television viewing and simultaneously model whether the television is on, which type of program is playing, and which family members are watching. The proposed model makes it possible to estimate the individual's intrinsic preference and the extrinsic preference from a joint consumption with other members. In turn, these estimates enable the authors to test several alternative group decision-making heuristics that may operate in those joint consumption occasions and to conduct managerially useful counterfactual simulations.
1	The authors test methods, based on cognitively simple decision rules, that predict which products consumers select for their consideration sets. Drawing on qualitative research, the authors propose disjunctions-of-conjunctions (DOC) decision rules that generalize well-studied decision models, such as disjunctive, conjunctive, lexicographic, and subset conjunctive rules. They propose two machine-learning methods to estimate cognitively simple DOC rules. They observe consumers' consideration sets for global positioning systems for both calibration and validation data. They compare the proposed methods with both machine-learning and hierarchical Bayes methods, each based on five extant compensatory and noncompensatory rules. For the validation data, the cognitively simple DOC-based methods predict better than the ten benchmark methods on an information theoretic measure and on hit rates. The results are robust with respect to format by which consideration is measured, sample, and presentation of profiles. The article closes with an illustration of how DOC-based rules can affect managerial decisions.
1	This research investigates how the valuation of delayed consumption of hedonic products, such as concerts and chocolate, varies with the passage of time between choice and consumption. The authors find that when consumers make their own choices, they exhibit increases in evaluations of delayed consumption, but only if the interval between choice and consumption is relatively short. The effect attenuates over longer periods, resulting in an inverted U-shaped relationship between evaluations and time. In contrast, when somebody else chooses the same option for the consumer, evaluations decrease with the passage of time. These effects depend on the extent of intrinsic motivation toward the object of consumption and occur only for consummatory consumption that is of inherent interest. Moreover, anticipatory increases in evaluations before consumption have ironic negative effects on postconsumption evaluations. The authors discuss implications and directions for further research.
1	The results of five experiments reveal that when sampling a series of experiential products (e.g., beverages, music), consumers prefer the product sampled second in a series of two desirable products but relatively prefer the product sampled first in a series of two undesirable products. The underlying process for both outcomes is a recency effect, such that there is better recall for the most recently sampled experiential product. The recency effect observed for experiential products reverses to a primacy effect when sampling nonexperiential products (e.g., scissors). The authors also demonstrate that the placement of an undesirable experiential product in conjunction with two desirable experiential products can exaggerate preference for the later-sampled desirable product (when the undesirable product is sampled first) or result in preference for the earlier-sampled desirable product (when the undesirable product is sampled between the two desirable products). However, the preference for the earlier-sampled desirable product holds only if there is no time delay between the sampling of the products or between the sampling and the choice evaluations.
1	It is proposed that a future time interval's perceived length will be affected by whether the interval ends with a gain or loss. Confirming this, several experiments indicate that consumers perceive intervals ending with losses as shorter than equivalent intervals ending with gains. The authors explore the mechanisms underlying these effects, and they identify several parallels between the current effects and loss aversion. The authors further show that these changes in time perception influence consumption decisions, and they consider the implications of the findings for theories of time perception and intertemporal choice.
1	This research explores how regulatory depletion affects consumers' responses to advertising. Initial forays into this area suggest that the depletion of self-regulatory resources is irrelevant when advertisement arguments are strong or consumers are highly motivated to process. In contrast to these conclusions, the authors contend that depletion has important but previously hidden effects in such contexts. That is, although attitudes are equivalent in valence and extremity, consumers are more certain of their attitudes when they form them under conditions of depletion than nondepletion. The authors propose that this effect occurs because feeling depleted induces the perception of having engaged in thorough information processing. As a consequence of greater attitude certainty, depleted consumers' attitudes exert greater influence on their purchase behavior. Three experiments, using different products and ad exposure times, confirm these hypotheses. Experiment 3 demonstrates the potential to vary consumers' naive beliefs about the relationship between depletion and thoroughness of processing, and this variation moderates the effect of depletion on attitude certainty. The authors discuss the theoretical contributions and implications for marketing.
1	Most literature on sales promotions focuses on responses to the promoted brand. Across two experimental studies and one field study, the authors examine how sales promotions may affect the size and composition of the overall shopping basket. The authors show that the framing of the savings message on sales promotions (e.g., “Save $x” versus “Get $x Off”), the expiration date restriction cue (immediate versus future expiration), and the familiarity of brands (well-known versus less familiar) are independent primes of regulatory focus. Furthermore, such cues, when compatible with one another or with a prior regulatory focus, lead to more unrelated purchases in the store. The authors discuss the findings in the context of theory on regulatory relevance and mind-sets, and they posit managerial implications for the design of sales promotions and store positioning.
1	This research examines the psychosocial cost associated with positive word of mouth (WOM), which can decrease the uniqueness of possessions and thus harm high-uniqueness consumers (pilot study). As a result, high- (versus low-) uniqueness consumers are less willing to generate positive WOM for publicly consumed products that they own. However, high uniqueness does not decrease willingness to generate WOM for privately consumed products (Study 1). Study 2 demonstrates that for publicly consumed products, WOM that includes positive recommendations is more persuasive than WOM that only contains product details. Consequently, the effect of uniqueness is more pronounced for WOM recommendations than for WOM that only provides details (Study 3). Study 4 confirms that high- (versus low-) uniqueness consumers are less willing to recommend a public product to others but are equally willing to discuss product details. Study 5 analyzes real-world WOM content and finds evidence in support of these results.
1	This research investigates the effects of across-consumer price comparisons on perceived price fairness as a function of culture. Collectivist (Chinese) consumers are more sensitive to in-group versus out-group differences than individualist (U.S.) consumers. The collectivist perspective orients consumers toward the in-group and heightens concerns about “face” (i.e., status earned in a social network) that arise from in-group comparisons. Process evidence for the causal role of cultural differences derives from manipulated self-construal and measurement of the emotional role of shame evoked by face concerns. Finally, in a robustness test, an alternative operationalization of the in-group/out-group distinction extends the findings to the context of firm relationships.
1	The authors examine incumbent retailers' reactions to a Wal-Mart entry and the impact of these reactions on the retailers' sales. They compile a unique data set that consists of incumbent supermarkets, drugstores, and mass merchandisers in the vicinity of seven Wal-Mart entries, as well as control stores not exposed to the entries. The data set includes weekly store movement data for 46 product categories before and after each entry and allows the authors to measure reactions and sales outcomes using a before-and-after-with-control-group analysis. They find that, overall, incumbents suffer significant sales losses as a result of a Wal-Mart entry, but there is substantial variation across retail formats, stores, and categories both in incumbent reactions and in their sales outcomes. Moreover, they find that a retailer's sales outcomes are significantly affected by its reactions, and the relationship between reactions and sales outcomes varies across retail formats. These findings provide valuable insights into how retailers in different formats can adjust their marketing mix to mitigate the impact of a Wal-Mart entry.
1	This article reviews the theory and empirical evidence of myopic management as it pertains to marketing practice. It documents empirically the stock market's inability to properly value marketing and innovation activity in the face of the potential for myopic management. The author assesses the total financial consequences of myopic management (the practice of cutting marketing and research-and-development spending to inflate earnings) and finds that myopia has a long-term net negative impact on firm value. Myopic management is contrasted with accounting accruals-based earnings inflation, and the author shows that the real activities (i.e., myopic management), and not the accounting numbers manipulation, have the greater negative impact on future financial performance. These results are consistent across alternative abnormal return measures and alternative benchmarks. The author argues that shareholders, managers, and marketing researchers can play a role in limiting myopic management practices.
1	It is widely recognized that business growth and shareholder value are engineered on the basis of investments aimed at acquiring and retaining customers. Along with this premise, however, the literature reveals a growing recognition that the manner in which important customer-based outcomes are constructed in the short term has vital implications for long-term firm performance. Adopting the view that customer satisfaction is a stochastic marketplace asset, the authors advance a mean-variance perspective that enables them to test two conjectures: (1) Objective service quality and advertising affect not only the level of customer satisfaction but also the heterogeneity in customer satisfaction, and (2) shareholder value is shaped by the interplay of satisfaction level and heterogeneity, through their impact on retention sales, acquisition sales, and servicing costs. The authors test these conjectures using secondary data from diverse sources that describe the dynamics in the U.S. airlines industry during a nine-year period (1997–2005). The results, derived from estimating structural models that account for the impact of several meaningful control variables, provide strong support for both conjectures. Importantly, the findings indicate that the return on satisfaction to shareholder value decreases by almost 70% in going from low to high satisfaction heterogeneity; at the same time, increasing levels of satisfaction heterogeneity serves to reduce the volatility in shareholder value.
1	Managers use numerical data as the basis for many decisions. This research investigates how data on prior advertising expenditures and sales outcomes are used in budget allocation decisions and attempts to answer three important questions about data-based inferences. First, do biases exist that are strong enough to lead to seriously suboptimal decisions? Second, do graphical data displays, real-world experience, or explicit training reduce any observed biases? Third, are the observed biases well explained by a relatively small set of natural heuristics that managers use when making data-based allocation decisions? The results suggest answers of yes, no, and yes, respectively. The authors identify three broad classes of heuristics: difference-based (which assess causation by comparing adjacent changes in expenditures to changes in sales), trend-based (which assess causation by comparing overall trends in expenditures and sales), and exemplar-based (which emulate the allocation pattern of the observations with the highest sales). All three heuristics create biases in some situations. Overall, exemplar-based heuristics were used most frequently and induced the greatest biasing of the three (sometimes allocating the most to an advertising medium that was uncorrelated with sales). Difference-based heuristics were used less frequently but generated the most extreme allocations. Trend-based heuristics were used the least.
1	The success of Internet social networking sites depends on the number and activity levels of their user members. Although users typically have numerous connections to other site members (i.e., “friends”), only a fraction of those so-called friends may actually influence a member's site usage. Because the influence of potentially hundreds of friends needs to be evaluated for each user, inferring precisely who is influential—and, therefore, of managerial interest for advertising targeting and retention efforts—is difficult. The authors develop an approach to determine which users have significant effects on the activities of others using the longitudinal records of members' log-in activity. They propose a nonstandard form of Bayesian shrinkage implemented in a Poisson regression. Instead of shrinking across panelists, strength is pooled across variables within the model for each user. The approach identifies the specific users who most influence others' activity and does so considerably better than simpler alternatives. For the social networking site data, the authors find that, on average, approximately one-fifth of a user's friends actually influence his or her activity level on the site.
1	The authors find that exposure to different types of categories or assortments in a task creates a mind-set that changes how consumers process information in subsequent tasks. That is, these mind-sets have a spillover effect that alters consumers' decision making in a variety of subsequent and unrelated tasks, from basic cognitive behaviors (e.g., grouping) and consumer decisions (e.g., new product adoptions) to more general decision-making strategies (e.g., susceptibility to heuristics). Consumers previously exposed to broad assortments or categorizations base their decisions on fewer pieces of information, typically those made salient by the environment. In contrast, consumers previously exposed to narrow assortments or categorizations employ multiple pieces of information, both salient and nonsalient, without exerting any extra effort. Consequently, prior exposure to broad versus narrow categorizations leads to greater susceptibility to some common context effects and to heuristic decision making.
1	Demonstrations of marketing effectiveness currently proceed along two parallel tracks: Quantitative researchers model the direct sales effects of the marketing mix, and advertising and branding experts trace customer mind-set metrics (e.g., awareness, affect). The authors merge the two tracks and analyze the added explanatory value of including customer mind-set metrics in a sales response model that already accounts for short- and long-term effects of advertising, price, distribution, and promotion. Vector autoregressive modeling of the metrics for more than 60 brands of four consumer goods shows that advertising awareness, brand consideration, and brand liking account for almost one-third of explained sales variance. Competitive and own mind-set metrics make a similar contribution. Wear-in times reveal that mind-set metrics can be used as advance warning signals that allow enough time for managerial action before market performance itself is affected. Specific marketing actions affect specific mind-set metrics, with the strongest overall impact for distribution. The findings suggest that modelers should include mind-set metrics in sales response models and branding experts should include competition in their tracking research.
1	Conjoint analysis has become a widely accepted tool for preference measurement in marketing research, though its applicability and performance strongly depend on the complexity of the product or service. Therefore, self-explicated approaches are still frequently used because of their simple design, which facilitates preference elicitation when large numbers of attributes need to be considered. However, the direct measurement of preferences, or rather utilities, has been criticized as being imprecise in many cases. Against this background, the authors present a compositional consumer preference measurement approach based on paired comparisons, otherwise known as PCPM. The trade-off character of paired comparisons ensures that the stated judgments are more intuitive than traditional self-explicated preference statements. In contrast to the latter, PCPM accounts for response errors and thus allows for the elicitation of more precise preferences. The authors benchmark PCPM against adaptive conjoint analysis and computer-assisted self-explication of multiattributed preferences to demonstrate its relative validity and predictive accuracy in two empirical studies using complex, high-involvement products. They find that PCPM yields better results than the benchmark approaches with respect to interview length, individual hit rates, and aggregate choice share predictions.
1	Traditionally, two approaches have been employed for structural equation modeling: covariance structure analysis and partial least squares. A third alternative, generalized structured component analysis, was introduced recently in the psychometric literature. The authors conduct a simulation study to evaluate the relative performance of these three approaches in terms of parameter recovery under different experimental conditions of sample size, data distribution, and model specification. In this study, model specification is the only meaningful condition in differentiating the performance of the three approaches in parameter recovery. Specifically, when the model is correctly specified, covariance structure analysis tends to recover parameters better than the other two approaches. Conversely, when the model is misspecified, generalized structured component analysis tends to recover parameters better. Finally, partial least squares exhibits inferior performance in parameter recovery compared with the other approaches. In particular, this tendency is salient when the model involves cross-loadings. Thus, generalized structured component analysis may be a good alternative to partial least squares for structural equation modeling and is recommended over covariance structure analysis unless correct model specification is ensured.
1	This article examines a silver lining of standing in line: Consumers infer that products are more valuable when others are behind them. Specifically, the value of a product increases as more people line up behind a person (Study 1) and when others are present (versus absent) behind a person in line (Study 2). Value increases further when directing consumers' attention to the presence of others behind them—that is, when they look backward versus forward (Study 3) and when the queue structure emphasizes the last person to join rather than the person being served (Study 4). This effect of people in line behind them is associated with increased expenditures by queuing consumers (Study 5).
1	Consumers often resolve trade-offs in a particular order. For example, when making flavor and size decisions, consumers might first decide which flavors to choose and then decide which sizes of those flavors to choose. This research examines the effect of decision order on purchase quantity decisions. The authors build on prior work on decision difficulty and conflict to show that consumers choose more overall, and more variety, when they consider a less replaceable attribute in an earlier, rather than a later, stage in the purchase decision. For example, consumers choose a greater quantity when flavor (or brand) decisions precede, rather than follow, size decisions. The authors find that the degree of attribute replaceability also moderates the effect of decision order on quantity chosen. Furthermore, marketers can influence the amount chosen by altering the organization of the shelf display. Finally, the authors find that when consumers explicitly consider the possibility of deferring their decisions, the effect of decision order declines.
1	How do consumers evaluate combinations of items representing conflicting goals? In this research, the authors examine how consumers form value judgments of combinations of options representing health and indulgence goals, focusing on how people estimate the calorie content of such options. The authors show that when evaluating combinations of healthy (virtue) and indulgent (vice) options, consumers tend to systematically underestimate the combined calorie content, such that they end up averaging rather than adding the calories contained in the vice and the virtue. The authors attribute this bias to the qualitative nature of people's information processing, which stems from their tendency to categorize food items according to a good/bad dichotomy into virtues and vices. The authors document this averaging bias in a series of four empirical studies that investigate the underlying mechanism and identify boundary conditions.
1	In a store-within-a-store arrangement, retailers essentially rent out retail space to manufacturers and give them complete autonomy over retail decisions, such as pricing and in-store service. This intriguing retailing format appears in an increasing number of large department stores worldwide. The authors use a theoretical model to investigate the economic incentives a retailer faces when deciding on this arrangement. The retailer's trade-off is between channel efficiency and interbrand competition, moderated by returns to in-store service and increased store traffic. The retailer cannot credibly commit to the retail prices and service levels that the manufacturers effect in an integrated channel, so it decides instead to allow them to set up stores within its store. Thus, the stores-within-a-store phenomenon emerges when a powerful retailer, ironically, gives manufacturers autonomy in its retail space. An extension of the model to the case of competing retailers shows that the store-within-a-store arrangement can moderate interstore competition.
1	Given the importance of new products, firms may be prone to overmanage sales personnel by using behavior-based control systems that dictate the performance of particular activities related to the introduction. Such controls may be especially tempting given the findings that favorable salesperson product perceptions actually yield less effort on the new product, and behavior-based controls can offset this tendency. However, using longitudinal data from a sample of 226 salespeople, along with external ratings from customers and archival measures of effort and sales performance, the authors demonstrate that such a strategy is shortsighted. Behavior-based controls constrain a salesperson's ability to appropriately allocate effort across his or her customer base, negatively affecting customer product perceptions and, ultimately, new product sales. In contrast, outcome-based control systems enable salespeople to work smarter, and their corresponding effort on behalf of the new product has a more positive effect on customer product perceptions and new product sales.
1	When managers are designing a contest to motivate effort by salespeople, service employees, franchisees, or product development teams, a key question is, What should the optimal proportion of winners and losers be? Prevailing marketing theory predicts that the proportion of winners in a contest should always be lower than the proportion of losers. Not only has this theory not been empirically tested, but it is also based on the assumption that contestants care solely about the value of the prizes they receive. This self-interested assumption has been increasingly challenged in marketing and economics. This article uses a behavioral economics model to formalize the idea that if contestants also care about their contest outcomes relative to other contestants, changing the proportion of winners in a contest can alter the reference points contestants use to make these social comparisons. Consequently, a contest with a higher proportion of winners than losers can yield greater effort than one with fewer winners than losers if the degree of social loss aversion in the contestants is sufficiently strong. Two incentive-aligned experiments show that this prediction can be valid in situations with public announcements of contest outcomes.
1	Three-tiered private-label (PL) portfolio strategies (low-quality tier: economy PLs, mid-quality tier: standard PLs, and top-quality tier: premium PLs) are gaining interest around the world. Drawing on the context-effects literature, the authors postulate how the introduction of economy and premium PLs may affect the choice of mainstream-quality and premium-quality national brands (NBs) and the choice of the retailer's existing PL offering. The authors use the natural experiment offered by Asda's and Sainsbury's introduction of economy and premium PL tiers in the corn flakes and canned soup categories in the United Kingdom to test their framework. Using brand choice models that accommodate context (compromise, similarity, and attraction) effects, the authors find that both economy and premium PLs cannibalize incumbent PLs. Economy PL introductions benefit mainstream-quality NBs because these NBs become a compromise or middle option in terms of quality in the retailer's assortment. The effects of premium PL introductions on premium-quality NBs are mixed: Their share improves in two of four cases but decreases in the other two cases.
1	Despite multiple calls for the integration of time into behavioral intent measurement, surprisingly little academic research has examined timed intent measures directly. In two empirical studies, the authors estimate individual-level cumulative adoption likelihood curves—curves calibrated on self-reported adoption likelihoods for cumulative time intervals across a fixed horizon—of 478 managerial decision makers, self-predicting whether and when they will adopt a relevant technology. A hierarchical Bayes formulation allows for a heterogeneous account of the individual-level adoption likelihood curves as a function of time and common antecedents of technology adoption. A third study generalizes these results among 354 consumer decision makers and, using behavioral data collected during a two-year longitudinal study involving a subsample of 143 consumer decision makers, provides empirical evidence for the accuracy of cumulative adoption likelihood curves for predicting whether and when a technology is adopted. Cumulative adoption likelihood curves outperform two single-intent measures as well as two widely validated intent models in predicting individual-level adoption for a fixed period of two years. The results hold great promise for further research on using and optimizing cumulative timed intent measures across a variety of application domains.
1	This article focuses on the measurement of the overall importance of brands for consumer decision making—that is, brand relevance in category, or BRiC—across multiple categories and countries. Although brand equity measures for specific brands have attracted a large body of literature, the questions of how important brands are within an entire product category and the extent to which BRiC differs across categories and countries have been neglected. The authors introduce the concept of BRiC (a category-level measure, not a brand-level measure). They develop a conceptual framework to measure BRiC and the drivers of BRiC, test the framework empirically with a sample of more than 5700 consumers, and show how the construct varies across 20 product categories and five countries (France, Japan, Spain, the United Kingdom, and the United States). The results suggest a high validity of the proposed BRiC measure and show substantial differences between categories and countries. A replication study two-and-a-half years later confirms the psychometric properties of the suggested scale and shows remarkable stability of the findings. The findings have important implications for the management of brand investments.
1	This article presents a meta-analysis of prior econometric estimates of personal selling elasticity—that is, the ratio of the percentage change in an objective, ratio-scaled measure of sales output (e.g., dollar or unit purchases) to the corresponding percentage change in an objective, ratio-scaled measure of personal selling input (e.g., dollar expenditures). The authors conduct a meta-analysis of 506 personal selling elasticity estimates drawn from analyses of 88 empirical data sets across 75 previous articles. They find a mean estimate of current-period personal selling elasticity of .34. They also find that elasticity estimates are higher for early life-cycle-stage offerings, higher from studies set in Europe than from those set in the United States, and smaller in more recent years. In addition, elasticity estimates are affected significantly by analysts’ use of relative rather than absolute sales output measures, by cross-sectional rather than panel data, by omission of promotions, by lagged effects, by marketing interaction effects, and by the neglect of endogeneity in model estimation. The method bias–corrected mean personal selling elasticity is approximately .31. The authors discuss the implications of their results for sales managers and researchers.
1	While much research has emphasized improving current new product concept techniques, little work has focused on trait-based approaches that specify which consumers are the “right” ones to use in the new product development process, particularly in the consumer goods industry. The authors propose that the right consumers to use possess what they call an “emergent nature,” defined as the unique capability to imagine or envision how concepts might be developed so that they will be successful in the mainstream marketplace. The authors draw on research on personality theory and information-processing styles to support their conceptualization and develop and validate a highly reliable scale to measure emergent nature (Study 1). In subsequent multipart studies, they show in both group (Studies 2a–2c) and individual (Studies 3a and 3b) settings across two distinct product categories that consumers high in emergent nature are able to develop product concepts that mainstream consumers find significantly more appealing and useful than concepts developed by typical, lead user, or even innovative consumers.
1	Few studies have considered the relative role of the integrated marketing mix (advertising, price promotion, product, and place) on the long-term performance of mature brands, instead emphasizing advertising and price promotion. Thus, little guidance is available to firms regarding the relative efficacy of their various marketing expenditures over the long run. To investigate this issue, the authors apply a multivariate dynamic linear transfer function model to five years of advertising and scanner data for 25 product categories and 70 brands in France. The findings indicate that the total (short-term plus long-term) sales elasticity is 1.37 for product and .74 for distribution. Conversely, the total elasticities for advertising and discounting are only .13 and .04, respectively. This result stands in marked contrast to the previous emphasis in the literature on price promotions and advertising. The authors further find that the long-term effects of discounting are one-third the magnitude of the short-term effects. The ratio is reversed from other aspects of the mix (in which long-term effects exceed four times the short-term effects), underscoring the strategic role of these tools in brand sales.
1	The authors quantify the impact of social interactions and peer effects in the context of physicians’ prescription choices. Using detailed individual-level prescription data, along with self-reported social network information, the authors document that physician prescription behavior is significantly influenced by the behavior of research-active specialists, or “opinion leaders,” in the physician's reference group. The authors leverage a natural experiment in the category: New guidelines released about the therapeutic nature of the focal drug generated conditions in which physicians were more likely to be influenced by the behavior of specialist physicians in their network. The authors (1) find important, statistically significant peer effects that are robust across model specifications; (2) document asymmetries in response to marketing activity across nominators and opinion leaders; (3) measure the incremental value to firms of directing targeted sales force activity to these opinion leaders; and (4) present estimates of the social multiplier of detailing in this category.
1	Analysts fitting a hierarchical Bayesian model must specify the distribution of heterogeneity. There are several distributions to choose from, including the multivariate normal, mixture of normals, Dirichlet processes priors, and so forth. Although significant progress has been made, estimating the models and obtaining measures for model selection remain ongoing areas of research for more flexible distributions of heterogeneity. As a result, the multivariate normal remains the default choice for many researchers and software packages. This article proposes model-checking statistics that signal the adequacy of the multivariate normal assumption for the distribution of heterogeneity; these methods do not require the analyst to fit alternative models. The authors use posterior predictive model checking to determine whether a discrepancy exists between the individual-level parameters and those implied by the assumed distribution of heterogeneity. In simulated and real data sets, the results show that these statistics are useful for identifying when the multivariate normal distribution is adequate, when there is a departure in the tails of the distribution, and when a multimodal distribution of heterogeneity may be more appropriate.
1	Four experiments examine why choices deplete executive resources. The authors show that the resolution of trade-offs is a driver of depletion effects arising from choice, and the larger the trade-offs, the greater is the depletion effect. The authors also find that choice difficulty not related to trade-offs does not influence the depleting effect of the choices. Finally, the authors find that though people can intuit some depletion effects, they do not intuit that choices or trade-offs within choices might be depleting and therefore fail to predict that larger trade-offs are more depleting.
1	Six studies show that subtle contextual cues that increase customers’ self-awareness can be used to influence their satisfaction with service providers (while holding the objective service delivery constant). Self-awareness cues tend to increase customers’ satisfaction when the outcome of a service interaction is unfavorable, but they tend to decrease customers’ satisfaction when the outcome of the interaction is favorable. This is because higher self-awareness increases customers’ tendency to attribute outcomes to themselves rather than to the provider. Self-awareness can even influence satisfaction with service interactions that occurred far in the past. The authors demonstrate these effects across a variety of lab and field settings with different simulated retail experiences and with different real-life service interactions, including college courses, meals taken at a university cafeteria, and items purchased at an actual clothing store. The results further show that attempts to shape customers’ satisfaction by means of self-awareness are more likely to be effective when there is substantial customer responsibility for the outcome; when customers’ responsibility is limited, such attempts may backfire.
1	The tendency to procrastinate applies not only to aversive tasks but also to positive experiences with immediate benefits. The authors propose that models of time discounting can explain this behavior, and they test these predictions with field data and experiments. A multicity study shows that people with unlimited time windows delay visiting desirable landmarks; however, procrastination is reduced when the window of opportunity is constrained. Similarly, people procrastinate in redeeming gift certificates and gift cards with long deadlines more than those with short deadlines, resulting in overall lower redemption rates. These results run counter to participants’ predictions and typical models of impulsive behavior.
1	The authors propose that power distance belief (PDB) (i.e., accepting and expecting power disparity) influences impulsive buying beyond other related cultural dimensions, such as individualism–collectivism. This research supports an associative account that links PDB and impulsive buying as a manifestation of self-control, such that those with high PDB display less impulsive buying. Furthermore, this effect manifests for vice products but not for virtue products. The authors also find that restraint from temptations can occur automatically for people who have repeated practice (i.e., chronically high PDBs). Taken together, these results imply that products should be differentially positioned as vice or virtue products in accordance with consumers’ PDBs.
1	This article proposes that merely considering outcomes associated with a positive approach emotion (e.g., happiness) can regulate negative emotions that evoke an approach orientation (e.g., sadness, anger). In contrast, outcomes associated with a positive avoidance emotion (e.g., calmness) best regulate negative emotions that evoke an avoidance orientation (e.g., anxiety, embarrassment). Although such orientation-matched (versus mismatched) positive outcomes might not address the problem that caused the negative emotion, they automatically signal a reduced need for affect regulation specific to the evoked orientation. Thus, orientation matching results in emotional benefit, increases preferences toward matched outcomes, and frees resources for subsequent tasks.
1	Prior consumer research has demonstrated the ability of promotion and prevention regulatory orientations to moderate a variety of consumer and marketing phenomena but also has used several different methods to measure chronic regulatory focus. This article assesses five chronic regulatory focus measures using the criteria of theoretical coverage, internal consistency, homogeneity, stability, and predictive ability. The results reveal a lack of convergence among the measures and variation in their performance along these criteria. The authors provide specific guidance for choosing a particular measure in regulatory focus research and suggest a composite measure.
1	Researchers have long been interested in understanding cognitive processing differences across consumer judgments and choices. Although it represents a focal outcome in much research, less attention has focused on the “choice-like” response of behavioral intentions. This research compares processing differences in the formulation of judgments of attractiveness and intentions. On the basis of the premise that different goals underlie these responses, the authors hypothesize that alternative reference points result from differential reference point diagnosticity. The authors test this prediction in the domain of price attractiveness and purchase intentions ratings. Study 1 provides evidence that endpoints of the range of alternative prices are more predictive of ratings of price attractiveness than of purchase intentions, while price rank and distribution mean are more predictive of purchase intentions ratings than of price attractiveness ratings. Study 2 replicates this effect using a different methodology. Finally, Study 3 provides a test of the external validity of these findings in a multicue setting.
1	The authors analyze a multimillion dollar, three-year field study sponsored by five firms to assess whether enabling skipping of advertisements using digital video recorders (DVRs) affects consumers’ shopping behavior for advertised and private label goods. A large sample of households received an offer for a free DVR and service, and close to 20% accepted. Each household's shopping history is observed for 48 consumer packaged goods categories during the 13 months before and the 26 months after the DVR offer. The authors fail to reject the null hypothesis of no DVR treatment effect on household spending on advertised branded or private label goods, either one or two years after the DVRs are shipped. The predicted DVR effect is tightly centered around 0, suggesting that the data have sufficient power to identify a true null effect. Using advertising exposure information for seven of the brands in the study, the authors offer suggestive evidence that ad skipping occurs for a relatively small fraction of the total television content viewed. The authors also discuss other potential explanations for the lack of a DVR effect.
1	The growing sales of private labels (PLs) pose significant challenges for national brands (NBs) around the world. A major question is whether consumers continue to be willing to pay a price premium for NBs over PLs. Using consumer survey data from 22,623 respondents from 23 countries in Asia, Europe, and the Americas across, on average, 63 consumer packaged goods categories per country, this article studies how marketing and manufacturing factors affect the price premium a consumer is willing to pay for an NB over a PL. These effects are mediated by consumer perceptions of the quality of NBs in relation to PLs. Although the results do not bode well for NBs in the sense that willingness to pay decreases as PLs mature, the authors offer several managerial recommendations to counter this trend. In countries in which PLs are more mature, the route to success is to go back to manufacturing basics. In PL development countries, there is a stronger role for marketing to enhance the willingness to pay for NBs.
1	Although store closures have become a common feature in grocery retailers’ strategic decisions, knowledge about their implications remains minimal. In this article, the authors study how closure of a particular store affects the chain sales of a multioutlet retailer operating multiple formats. The authors show that the chain sales losses from a closure vary widely across outlets (ranging from less than 30% to more than 80% of the closed outlet's revenue) and depend not only on the closed store's format and distance to competitors but also on the profile of its clientele and type of shopping trip. The authors offer a methodology for predicting the magnitude of these losses for specific store closures that outperforms models calibrated on observed “regular” shopping patterns. This approach offers guidance to retailers in deciding whether a particular store closure is beneficial to the chain or, if the objective is to prune an overly dense network, which of a set of local outlets is the best candidate for closure.
1	Although managers are interested in the financial value of customers and researchers have pointed out the importance of stock analysts who advise investors, no studies to date have explored the implications of customer satisfaction for analyst stock recommendations. Using a large-scale longitudinal data set, the authors find that positive changes in customer satisfaction not only improve analyst recommendations but also lower dispersion in those recommendations for the firm. These effects are stronger when product market competition is high and financial market uncertainty is large. In addition, analyst recommendations at least partially mediate the effects of changes in satisfaction on firm abnormal return, systematic risk, and idiosyncratic risk. Analyst recommendations represent a mechanism through which customer satisfaction affects firm value. Thus, if analysts pay attention to Main Street customer satisfaction, Wall Street investors should have good reason to listen and follow. Overall, this research reveals the impact of satisfaction on analyst-based outcomes and firm value metrics and calls attention to the construct of customer satisfaction as a key intangible asset for the investor community.
1	More than 200 studies suggest that metacognitive difficulty reduces the liking of an object. In contrast to those findings, the authors demonstrate that the effects of metacognitive experiences on evaluation are sensitive to the consumption domain. In the domain of everyday goods, metacognitive difficulty reduces the attractiveness of a product by making it appear unfamiliar. However, in the context of special-occasion products, for which consumers value exclusivity, metacognitive difficulty increases the attractiveness of a product by making it appear unique or uncommon. The authors reconcile their findings with prior research by positing that the effect of metacognitive experiences on evaluation depends on the naive theory people associate with product consumption. Four studies demonstrate the proposed effect and test for the role of lay theories in the interpretation of metacognitive experiences. The authors conclude with a discussion of theoretical and marketing implications.
1	Many consumer promotions involve uncertainty (e.g., purchase incentives offering the chance to receive one of several rewards). Despite retailers’ heavy reliance on such promotions, much academic research on uncertainty has demonstrated examples of consumers avoiding and/or disliking uncertainty, implying that promotions involving uncertainty may not be as effective for retailers as promotions offering certain rewards. In an effort to reconcile the prevalence of uncertain promotions with the existing research, this article explores the conditions under which uncertain promotions may be effective for retailers. The article concludes with a discussion of the theoretical and practical implications for these findings.
1	The authors examine the dynamic effects of category- and brand-level advertising for a new pharmaceutical in a market in which regulations require that the content of these two types of advertising be mutually exclusive. Specifically, category, or generic, messages should communicate information only about the disease without promoting any brand, whereas brand-level messages should be void of any therapeutic information. This brings up two questions of great managerial importance: Which type of message is generally more effective (category or brand level), and when is one type more effective than the other? The authors pursue these questions by analyzing the effects of advertising on new and refill prescriptions through the use of an augmented Kalman filter with continuous state and discrete observations. The findings suggest the presence of complex dynamics for both types of regulation-induced advertising messages. In general, brand advertising is more effective, especially after competitive entry. Extensive validation tests confirm the superiority of the modeling approach. The authors discuss implications for managers and regulators.
1	Marketers are increasingly offering bundles that combine cross-category, seemingly unrelated items. This article examines the conditions under which framing a discount as savings on certain items of cross-category bundles is more effective for increasing bundle purchase. Three experiments show that the purchase of a cross-category bundle is more likely when the discount is framed as savings on the relatively hedonic component rather than as savings on the utilitarian component or on the total bundle. The authors explain the findings based on the notion that a discount provides a justification that increases the likelihood of hedonic purchases but has little impact on utilitarian consumption. The article concludes with a discussion of managerial implications of the findings.
1	Producers of consumer packaged goods often offer several package sizes of the same product and charge a lower unit price for a larger size. In this article, the authors investigate the “quantity-discount effect,” or the phenomenon that consumers derive transaction utility from the unit price difference between a small and a large package size of the same product. The authors propose a modeling framework composed of a demand-side model and a supply-side model. The empirical results suggest that quantity-discount-induced gains or losses have a significant impact on consumer buying behavior. The authors also find a substantial amount of structural heterogeneity; that is, some consumers perceive quantity discounts as gains, whereas others perceive quantity discounts as losses. Conversely, the supply-side analysis suggests that manufacturers in the empirical application do not consider quantity-discount effects when setting prices. Through a series of policy experiments, the authors show that by accounting for quantity-discount-dependent consumer preferences, manufacturers can design more effective nonlinear pricing schemes and obtain greater profits.
1	This article examines the effects of sequential movie releases on the dilution and enhancement of celebrity brands. The authors use favorability ratings collected over a 12-year period (1993–2005) to capture movement in the brand equity of a panel of actors. They use a dynamic panel data model to investigate how changes of brand equity are associated with the sequence of movies featuring these actors, after controlling for the possible influence from the stars’ off-camera activities. The authors also examine the underlying factors that influence the magnitude and longevity of such effects. In contrast with findings from existing research in product branding, the authors find evidence that supports the general existence of dilution and enhancement effects on the equity of a celebrity brand through his or her movie appearances. They also find that star favorability erodes substantially over time. Finally, this research offers insights for actors regarding how to make movie selections strategically to maximize their brand equity.
1	Consumers’ impulsive choices have traditionally been attributed either to contextual factors, such as product attributes and store environment, or to individual personality traits. In this article, the authors find that type of food consumed can also influence impulsive choice. Specifically, food that enhances the levels of the neurotransmitter serotonin can reduce impulsive choice. To test the hypotheses on the influence of serotonin on postconsumption impulsive choice, the authors collected data on the eve of Thanksgiving. The occasion of Thanksgiving dinner provides a naturalistic setting in which people consume a tryptophan-rich meal (tryptophan is a precursor to serotonin). The authors replicate these findings and obtain converging evidence in a lab setting in which they give some participants a tryptophan-rich beverage and observe their postconsumption impulsive choices.
1	Prior research on attention shifts to advertisements has focused primarily on demonstrating how perceptual features can shift attention to advertisements. In this article, the authors demonstrate that certain semantic characteristics of nonfocal advertising elements may similarly attract attention when consumers are focused on a primary task elsewhere in the visual field. In three experiments, the authors investigate how orienting attention responses to highly emotional advertising elements influence ad and brand awareness in cluttered environments. Specifically, they demonstrate that preattentive processing of the semantic information in nonfocal ad headlines can elicit orienting attention responses that result in predictable increases in ad and brand awareness.
1	In three studies, the authors introduce and probe the role of emotional receptivity in consumer–marketer interactions. Emotional receptivity refers to a person's disposition toward experiencing a preferred level of emotional intensity. The results from the first two studies conducted in the laboratory and the field collectively demonstrate that consumers feel greater enjoyment and enhanced liking for the marketer when there is a close match between their emotional receptivity and the level of emotional intensity displayed by the marketer. In the third study, the authors show that consumers’ emotional receptivity can be temporarily increased or decreased using advertisements that depict more or less expressive social interactions, respectively. The findings are similar to those observed in the first two studies. In addition, a preliminary study establishes the discriminant validity of the proposed battery of emotional receptivity items from existing related constructs. The authors discuss the theoretical and managerial implications and offer suggestions for further research.
1	Recent discussions in academic literature and the business press often paint an unflattering picture of the contributions of chief marketing officers (CMOs) to the financial value of their firms. Some even suggest that CMOs, despite being the marketing leaders in firms, have little or no effect on firm performance. However, formal empirical research on the impact of CMOs on financial performance is scarce. This article presents conceptual arguments and empirical evidence about this controversial issue. The authors suggest that CMOs are far from irrelevant to the financial performance of firms. However, the impact of CMOs on financial performance is highly contingent on the managerial discretion available to them. Focusing on the role of customer power in limiting the managerial discretion available to CMOs, this study identifies individual and firm-specific conditions in which CMOs contribute more or less to firm value. Analyses of abnormal stock returns associated with the appointment of CMOs provide support for the hypothesized effects of customer power and managerial discretion.
1	This research develops a taxonomy of alphanumeric brand names (ABs) based on the alignment between the brand names and their links to products and attributes. Five empirical studies reveal that ABs have systematic effects on consumers’ product choices, moderated by consumers’ need for cognition, the availability of product attribute information, and the taxonomic category of the AB. In an identical choice set, the choice share of a product option whose brand name takes a higher versus lower numeric portion (e.g., X-200 versus X-100) increases, and it is preferred more even when it is objectively inferior to other choice alternatives. Consumers with low need for cognition use “the higher, the better” heuristic to select options labeled with ABs and choose brands with higher numeric portions. Consumers with high need for cognition process ABs more systematically and make inferences about attribute values based on brand name–attribute correlations. The effects of ABs on consumer preferences are prevalent for most technical products, even when consumers do not know the product category or meanings of attributes.
1	In 2007, Consumer Reports released, and two weeks later retracted, a flawed report on the safety of infant car seats. Analyzing data from 5471 online auctions for car seats ending before, during, and after the information was considered valid, this article shows that (1) consumers responded to the new information and, more surprisingly, (2) they promptly ceased to do so when it was retracted. Because of the random nature of the flawed ratings, this first finding demonstrates that expert advice has a causal effect on consumer demand. The second finding suggests that people's inability to willfully ignore information is not as extreme as the experimental evidence in the psychological literature suggests.
1	The authors propose a new method to visualize browsing behavior in so-called product search maps. Manufacturers can use these maps to understand how consumers search for competing products before choice, including how information acquisition and product search are organized along brands, product attributes, and price-related search strategies. The product search maps also inform manufacturers about the competitive structure in the industry and the contents of consumer consideration sets. The proposed method defines a product search network, consisting of the products and links that designate whether a product is searched conditional on searching other products. The authors model this network using a stochastic, hierarchical, and asymmetric multidimensional scaling framework and decompose the product locations as well as the product-level influences using product attributes. The advantages of the approach are twofold. First, the authors simultaneously visualize the positions of products and the direction of consumer search over products in a perceptual map of search proximity. Second, they explain the formation of the map using observed product attributes. The authors empirically apply their approach to consumer search of digital camcorders at Amazon.com and provide several managerial implications.
1	This study measures the degree of contagion or interpersonal influence in the diffusion of new consumer packaged goods (CPGs). The authors demonstrate that when an individual-level trial hazard model is properly specified to account for potential sources of biases, substantial contagion effects may be detected in the diffusion of many CPGs. Using longitudinal panel data on individual-level trial and repeat purchases of 67 newly introduced CPGs, they show that standard diffusion models fail to detect contagion. However, after extending the model to allow for spatial and temporal heterogeneity in contagion and controlling for various cross-sectional and temporal confounds, they find statistically significant contagion effects in 33 to 40 of the 67 sample products. The empirical evidence of contagion in the diffusion of many CPGs has important implications because most new product trial models for CPGs have assumed a priori that there is no contagion in the diffusion of these products. Moreover, the individual-level simultaneous analysis of the diffusion of 67 newly introduced CPGs provides useful insights into the unobservable network of influences among consumers. Such analysis allows a vendor to identify the most influential early adopters among its customers, who could help diffuse a new product more effectively in the market.
1	The authors propose a dynamic market equilibrium model to investigate how consumers form price expectations and how these expectations influence their sequential search behavior and prices in the market. They derive a perfect Bayesian Nash equilibrium and show that whereas higher current period prices induce search, consumers who observe higher historical prices form higher price expectations, become more pessimistic, and search less. Furthermore, the authors show that when prices increase as a result of an increase in marginal costs, consumers respond by searching more, which then leads to higher prices and instant price adjustment. In contrast, because consumer search activities decline when prices fall, sellers may respond by lowering their prices just enough to dampen consumers’ search tendencies when costs decrease. The authors test their theoretical model and its implications and find support for it in a series of simulated market experiments.
1	Everyday decisions present consumers with several trade-offs. In turn, these trade-offs can influence the decision outcome. The authors show that the level at which people construe a choice can affect trade-off making, such that a high construal of a choice decreases comparative trade-offs relative to a low construal. They use six studies to illustrate the idea in three important trade-off-relevant context effects. The results show that a high (versus a low) construal decreases the compromise and background-contrast effects and increases the attraction effect by reducing attribute-level trade-offs.
1	As service centers become crucial corporate assets for increasing customer relationships and profits, it is imperative to understand customer reactions to service allocations. Using customer call history from a DSL service, the authors empirically investigate how customers’ onshore and offshore experiences affect service duration and customer retention. They formulate service channel allocation decisions as solutions to a dynamic programming problem in which the firm learns about heterogeneous customer preferences, balances short-term service costs with long-term customer retention, and optimally matches customers with their preferred centers to maximize long-term profit. They demonstrate through simulations that learning enables a firm to make more customized allocations and that acting on long-term customer responses prompts the firm to make proactive decisions that prevent customers from leaving. As a result, the firm can improve customer retention and profit. The proposed framework also mirrors the recent trend of companies seeking solutions that transform customer information into customized and dynamic marketing decisions to improve long-term profit.
1	In Internet paid search advertising, marketers pay for search engines to serve text advertisements in response to keyword searches that are generic (e.g., “hotels”) or branded (e.g., “Hilton Hotels”). Although standalone metrics usually show that generic keywords have higher apparent costs to the advertiser than branded keywords, generic search may create a spillover effect on subsequent branded search. Building on the Nerlove–Arrow advertising framework, the authors propose a dynamic linear model to capture the potential spillover from generic to branded paid search. In the model, generic search advertisements serve to expose users to information about the brand's ability to meet their needs, raising awareness that the brand is relevant to the search. In turn, this can induce additional future search activity for keywords that include the brand name. Using a Bayesian estimation approach, the authors apply the model to data from a paid search campaign for a major lodging chain. The results show that generic search activity positively affects future branded search activity through awareness of relevance. However, branded search does not affect generic search, demonstrating that the spillover is asymmetric. The findings have implications for understanding search behavior on the Internet and the management of paid search advertising.
1	The authors examine the impact of gifts on deposit balances and customer satisfaction in a longitudinal field experiment conducted at a commercial bank. They find that gifts increased deposit balances, survey response rates, and customer satisfaction compared with the no-gift control. They manipulated several factors within the gift treatment: gift type, the accompanying message, and the sequence of gift value, which improved ($35 then $100 gift), worsened ($100 then $35 gift), or was a single gift. A highly detrimental effect of decreasing gift value occurred on deposit balances. This “deterioration aversion” persisted in a long-term follow-up analysis of deposit balances. A vignette experiment replicates deterioration aversion and extends the results, demonstrating increased effectiveness of improving gifts over constant gift sequences and indicating that the mechanism underlying deterioration aversion involves the violation of expectations.
1	The authors investigate the feasibility of unstructured direct elicitation (UDE) of decision rules consumers use to form consideration sets. They incorporate incentives into the tested formats that prompt respondents to state noncompensatory, compensatory, or mixed rules for agents who will select a product for the respondents. In a mobile phone study, two validation tasks prompt respondents to indicate which of 32 mobile phones they would consider from a fractional design of features and levels. The authors find that UDE predicts consideration sets better, across both profiles and respondents, than a structured direct-elicitation method. It predicts comparably to established incentive-aligned compensatory, noncompensatory, and mixed decompositional methods. In a more complex automotive study, noncompensatory decomposition is not feasible and additive-utility decomposition is strained, but UDE scales well. The authors align incentives for all methods using prize indemnity insurance to award a chance at $40,000 for an automobile plus cash. They conclude that UDE predicts consideration sets better than either an additive decomposition or an established structured direct-elicitation method (CASEMAP).
1	Product line design for consumer durables often relies on close coordination between marketing and engineering domains. Product lines that evolve as optimal from marketers’ perspective may not be optimal from an engineering viewpoint, and vice versa. Although extant research has proposed sophisticated techniques to handle problems that characterize each individual domain, the majority of these developments have not addressed the interdependent issues across marketing and engineering. The author presents a product line optimization method that enables managers to simultaneously consider factors deemed important from both marketing and engineering domains. One major advantage of this method is that it takes into account the strategic reactions from the incumbent manufacturers and the retailer in the design of the product line. The author demonstrates in a simulation study that this method is applicable to problems with a reasonably large scale. Using data collected in a power tool development project undertaken by a major U.S. manufacturer, the study illustrates that the proposed method leads to a more profitable product line than alternative approaches that consider requirements from these two domains separately.
1	The authors propose a Web-based adaptive self-explicated approach for multiattribute preference measurement (conjoint analysis) with a large number (ten or more) of attributes. The proposed approach overcomes some of the limitations of previous self-explicated approaches. The authors develop a computer-based self-explicated approach that breaks down the attribute importance question into a ranking of attributes followed by a sequence of constant-sum paired comparison questions. In the proposed approach, the questions are chosen adaptively for each respondent to maximize the information elicited from each paired comparison question. Unlike the traditional self-explicated approach, the proposed approach provides standard errors for attribute importance. In two studies involving digital cameras and laptop computers described on 12 and 14 attributes, respectively, the authors find that the ability to correctly predict validation choices of the proposed adaptive approach is substantially and significantly greater than that of adaptive conjoint analysis, the fast polyhedral method, and the traditional self-explicated approach. In addition, the adaptive self-explicated approach yields a significantly higher predictive validity than a nonadaptive fractional factorial constant-sum paired comparison design.
1	Trade-in transactions typically involve an exchange of an old, used version for a new or newer version of the product. When consumers trade in their used model for a new model, the firm faces the choice of paying the consumer a relatively low price for the used model and charging a commensurately low price for the new model or paying a relatively high price for the used model and charging a commensurately high price for the new model. The extant literature suggests that consumers always prefer to be overpaid in trade-in transactions because they disproportionately value the gain associated with the revenues from the sale of the used version of the product. The authors draw from the prospect theory value function to develop a simple analytical model that identifies a condition under which this preference for overpayment is reversed. Their model predicts that even when faced with economically equivalent price formats, consumers prefer to be overpaid when the ratio of the price of their used product to the price of the new product is low, but when that ratio is high, the preference for overpayment is reversed. They observe support for the predictions that emerge from the model in laboratory experiments.
1	This study compares the performance of four commonly used approaches to measure consumers’ willingness to pay with real purchase data (REAL): the open-ended (OE) question format; choice-based conjoint (CBC) analysis; Becker, DeGroot, and Marschak's (BDM) incentive-compatible mechanism; and incentive-aligned choice-based conjoint (ICBC) analysis. With this five-in-one approach, the authors test the relative strengths of the four measurement methods, using REAL as the benchmark, on the basis of statistical criteria and decision-relevant metrics. The results indicate that the BDM and ICBC approaches can pass statistical and decision-oriented tests. The authors find that respondents are more price sensitive in incentive-aligned settings than in non-incentive-aligned settings and the REAL setting. Furthermore, they find a large number of “none” choices under ICBC than under hypothetical conjoint analysis. This study uncovers an intriguing possibility: Even when the OE format and CBC analysis generate hypothetical bias, they may still lead to the right demand curves and right pricing decisions.
1	Researchers and practitioners alike frequently survey consumers to gain insights into their attitudes, preferences, and beliefs. The authors propose a potentially pervasive, but as of yet unidentified, source of bias in survey responding. Specifically, they propose that respondents’ answers to questions might sometimes reflect attitudes that respondents want to convey but that the researcher has not asked about, a phenomenon termed “response substitution.” The authors examine this proposition in a series of three experiments that demonstrate the phenomenon, provide support for the process account, and identify boundary conditions. They also discuss general theoretical, methodological, and practical implications as well as specific implications for research on attitudes and contingent valuation.
1	The authors find that consumers prefer a bonus pack to a price discount for virtue foods but they prefer a price discount to a bonus pack for vice foods. Prior research has shown that, all else being equal, consumers prefer bonus packs to price discounts. The authors propose that this preference does not hold for vice food, because consumers cannot generate good justifications for buying such food when a bonus pack is offered because this would mean consuming more of the vice. However, a price discount on a vice food can be justified because it acts as a guilt-mitigating mechanism. For virtue foods, the absence of both anticipated postconsumption guilt and the resultant need to justify leads consumers to prefer a bonus pack to a price discount. The authors demonstrate the proposed effect and test its underlying process across five experiments.
1	The authors examine whether the growth of the Internet has reduced the effectiveness of government regulation of advertising. They combine nonexperimental variation in local regulation of offline alcohol advertising with data from field tests that randomized exposure to online advertising for 275 different online advertising campaigns to 61,580 people. The results show that people are 8% less likely to say that they will purchase an alcoholic beverage in states that have alcohol advertising bans compared with states that do not. For consumers exposed to online advertising, this gap narrows to 3%. There are similar effects for four changes in local offline alcohol advertising restrictions when advertising effectiveness is observed both before and after the change. The effect of online advertising is disproportionately high for new products and for products with low awareness in places that have bans. This suggests that online advertising could reduce the effectiveness of attempts to regulate offline advertising channels because online advertising substitutes for (rather than complements) offline advertising.
1	Although much research has found that “birds of a feather flock together,” the authors suggest that opposites tend to attract when it comes to certain spending tendencies; that is, tightwads, who generally spend less than they would ideally like to spend, and spendthrifts, who generally spend more than they would ideally like to spend, tend to marry each other, consistent with the notion that people are attracted to mates who possess characteristics dissimilar to those they deplore in themselves. Despite this complementary attraction, tightwad–spendthrift differences within a marriage predict conflict over finances, which in turn predicts diminished marital well-being. These relationships persist when controlling for important financial outcomes (household-level savings and credit card debt). These findings underscore the importance of studying the relationships among money, consumption, and happiness at an interpersonal level.
1	Consumers’ purchase decisions can be influenced by others’ opinions, or word of mouth (WOM), and/or others’ actions, or observational learning (OL). Although information technologies are creating increasing opportunities for firms to facilitate and manage these two types of social interaction, to date, researchers have encountered difficulty in disentangling their competing effects and have provided limited insights into how these two social influences might differ from and interact with each other. Using a unique natural experimental setting resulting from information policy shifts at the online seller Amazon.com, the authors design three longitudinal, quasi-experimental field studies to examine three issues regarding the two types of social interaction: (1) their differential impact on product sales, (2) their lifetime effects, and (3) their interaction effects. An intriguing finding is that while negative WOM is more influential than positive WOM, positive OL information significantly increases sales, but negative OL information has no effect. This suggests that reporting consumer purchase statistics can help mass-market products without hurting niche products. The results also reveal that the sales impact of OL increases with WOM volume.
1	The authors empirically study consumer choice behavior in the wake of a product-harm crisis, which creates consumer uncertainty about product quality. They develop a model that explicitly incorporates the impact of such uncertainty on consumer behavior, assuming that consumers are uncertain about the mean product quality level and learn about product quality through the signals contained in use experience and the product-harm crisis and also that consumers are uncertain about the precision of the signals in conveying product quality and update their perception of this precision over time. They estimate this model using a scanner panel data set that includes consumer purchase history before, during, and after a product-harm crisis that affected Kraft Foods australia's peanut butter division in June 1996. The proposed model fits the data better than the standard consumer learning model, which assumes consumers are uncertain about product quality level but the precision of information in conveying product quality is known. This study also provides insights into consumers’ behavioral choice responses to a product-harm crisis. Finally, the authors conduct counterfactual experiments based on the estimation results and provide insights to managers on crisis management.
1	The authors examine the impact of successfully attaining a goal on future effort directed at attaining the same goal. Using data from a major frequent-flier program, they demonstrate empirically how success contributes to an increase in effort exhibited in consecutive attempts to reach a goal. They replicate the effects in a laboratory study that shows that the impact of success is significant only when the goal is challenging. They also show how progress enhances perceptions of self-efficacy and how successfully completing the task provides an added boost, supporting the notion that self-learning is the principle mechanism driving their results.
1	Decisions often involve trade-offs between a more normative option and a less normative but more tempting one. The authors propose that the intrapersonal conflict that is evoked by choices involving incompatible goals can be resolved through scope-insensitive justifications. The authors describe one such mechanism, the “mere token” effect, a new phenomenon in decision making. They demonstrate that adding a certain and immediate mere token amount to both options increases choices of the later-larger option in intertemporal choice and of the riskier-larger option in risky choice. The authors find this effect to be scope insensitive, such that the size of the token amount does not moderate the effect. They show that intrapersonal choice conflict underlies the mere token effect and that reducing the degree of conflict by increasing the psychological distance to the choice outcomes debiases the effect. Moreover, they show that the mere token effect is enhanced when (1) opposing goals in choice are made salient and (2) the choice options represent a starker contrast that generates greater conflict. The authors empirically rule out alternative explanations, including diminishing marginal utility, normative and descriptive utility-based models, liquidity constraints, and naive diversification. They discuss the direct implications of the mere token effect for the marketing of financial services and, more generally, for consumer preference toward bundles and multiattribute products.
1	Are you type A or type B? An optimist or a pessimist? Intuitive or analytical? Consumers are motivated to learn about the self, but they may not always accept what they learn. This article explores how the desire for self-discovery leads people to seek but not necessarily accept the feedback they receive and the implications this has for consumption behavior. Specifically, this article examines the case of consumers who value being unconstrained: people with independent self-construals and those who have high levels of reactance motivation. The authors argue that these people often view self-knowledge as a constraint on the self and subsequently reject it—even when the self-knowledge has neutral or positive implications for self-esteem. Results across five studies demonstrate that independents and high reactants feel constrained by self-knowledge, and this causes them to reject and make consumption choices inconsistent with it even as they actively seek to learn about themselves. In contrast, interdependents and low reactants do not feel constrained by self-knowledge, and consequently, they accept and incorporate it into their consumption decisions.
1	A great deal of research in consumer decision making and social cognition has explored consumers’ attempts to simplify choices by bolstering their tentative choice candidate and/or denigrating the other alternatives. The current research investigates a diametrically opposed process, whereby consumers complicate their decisions. The authors demonstrate that to complicate their choices, consumers increase choice conflict by overweighting small disadvantages of superior alternatives, converging overall evaluations of alternatives, reversing preference ordering, and even choosing less preferred alternatives. Furthermore, the results from five studies support a unifying theoretical framework: the effort–compatibility principle. Specifically, the authors argue that consumers strive for compatibility between the effort they anticipate and the effort that they actually exert. When a decision seems more difficult than initially expected, a simplifying process ensues. However, when the decision seems easier to resolve than anticipated (e.g., when consumers face an important yet easy choice), consumers artificially increase their effort.
1	Consumers often like fluently processed stimuli. The authors find that one source of fluency for numerical stimuli is the generation of a number through common addition (e.g., 1 + 1 through 10 + 10) and common multiplication (2 × 2 through 10 × 10) problems (study 1). Common addition and multiplication problems (arithmetic), or their operands, can be used to prime a number and increase its fluency (study 3). The benefits of arithmetic and operand primes are limited to single primes (i.e., more primes are not necessarily better) (Study 5). Number fluency is relevant to creating numeric brand names (Study 2), enhancing the liking of numeric brand names through advertising (Study 4), and executing price promotions (Study 6).
1	Demand uncertainty and supply rigidity often create inventory shortages. Inventory shortages can have adverse consequences on both short- and long-term customer behaviors. Using data from an online grocer, this study has the following three main objectives: First, the authors empirically investigate the true nature of shortage costs by examining how stockouts and fulfillment rates affect customer's purchase behavior in both the short and long run. Second, the authors study how consumers’ reactions to inventory shortages differ across customer segments. They use these observations to illustrate how the seller might benefit from prioritizing fulfillment policies according to customer traits. Third, the authors study how the impact of inventory shortages varies across product categories. Using these insights, the authors discuss how the seller should allocate limited resources to improve order fulfillment across these categories. Overall, the authors find that stockout rates have a dramatic but nonlinear impact on the seller's profitability. This suggests that the firm can achieve many of the benefits of improving fulfillment rates through small decreases in stockout rates. They also find significant differences in how different types of customers respond to stockouts and that prioritizing inventory according to transaction history measures and basket contents can lead to large increases in contribution while only requiring moderate reductions in stockout rates.
1	Exposing consumers to extreme prices can influence the price they are willing to pay for both related and unrelated products. Drawing on previous theories of anchoring and adjustment and selective accessibility of judgment-relevant knowledge, the authors provide an account of both asymmetries in the impact of price anchors across product categories and contingencies in the occurrence of these asymmetries. Four studies show the deliberate consideration of price anchors that can play a key role in whether the effect of the anchors will generalize across product categories. Specifically, an explicit comparison of a product to a price anchor increases the accessibility of features that represent a product available at this price. In turn, these thoughts influence the price that consumers are willing to pay for these products. In the absence of this deliberation, however, anchors influence both related and unrelated products, provided no other cognitive activity occurs in the interim.
1	In an increasingly globalized marketplace, it is common for marketing researchers to collect data from respondents who are not native speakers of the language in which the questions are formulated. Examples include online customer ratings and internal marketing initiatives in multinational corporations. This raises the issue of whether providing responses on rating scales in a person's native versus second language exerts a systematic influence on the responses obtained. This article documents the anchor contraction effect (ACE), the systematic tendency to report more intense emotions when answering questions using rating scales in a nonnative language than in the native language. Nine studies (1) establish ACE, test the underlying process, and rule out alternative explanations; (2) examine the generalizability of ACE across a range of situations, measures, and response scale formats; and (3) explore managerially relevant and easily implementable corrective techniques.
1	Consumers’ attraction to a product can often be based on the subjective reactions that they imagine they would have if they personally used it. Three experiments examine the effects of self-focused attention on the use of this criterion and the conditions in which it is applied. When features of the judgment (social or nonsocial) context are similar to those of the situation in which the products are normally used, self-focused attention increases participants’ disposition to imagine themselves using the products they evaluate, and in turn, these imaginings increase both their evaluations of these products and their likelihood of choosing these products as a gift for taking part in the experiment. The effects occur when features of the judgment context are manipulated both by incidental background music and by the presence of others in the situation at hand. However, when either self-focused attention is low or features of the judgment context are dissimilar to those in which the products are normally used, these effects are not apparent.
1	Four studies demonstrate how consumers resolve the aesthetic incongruity that arises between a newly acquired product and the existing consumption environment. The novel insight on which this research is based is that the aesthetic incongruity involving products high in design salience is more likely than aesthetic incongruity involving products low in design salience to be resolved by accommodating the product within the consumption environment, often through additional purchases. Furthermore, the relative presence of frustration versus regret is shown to mediate the relationship between design salience and the decision to buy more.
1	Using the self-regulatory strength model and prior research on self-esteem threats, the authors predict and show that delegating decisions to surrogates (e.g., financial advisors, physicians) depletes consumers’ limited self-regulatory resources more than making the same decisions independently, thus impairing their subsequent ability to exercise self-control. This is the case even though decision delegation actually requires less decision-making effort than independent decision making (Study 1). However, the resource-depleting effect of decision delegation vanishes when consumers have an opportunity to affirm their belief in free will (Study 2). Moreover, when people remember a past decision that they delegated, their self-control is impaired more than when they remember a decision made independently (Studies 3 and 4). The authors conclude with a discussion of the theoretical and practical implications of these findings.
1	Breast cancer communications that make women's gender identity salient can trigger defense mechanisms and thereby interfere with key objectives of breast cancer campaigns. In a series of experiments, the authors demonstrate that increased gender identity salience lowered women's perceived vulnerability to breast cancer (Experiments 1a, 3a, and 3b), reduced their donations to ovarian cancer research (Experiment 1b), made breast cancer advertisements more difficult to process (Experiment 2a), and decreased ad memory (Experiment 2b). These results are contrary to the predictions of several prominent theoretical perspectives and a convenience sample of practitioners. The reduction in perceived vulnerability to breast cancer following gender identity primes can be eliminated by self-affirmation (Experiment 3a) and fear voicing (Experiment 3b), corroborating the hypothesis that these effects are driven by unconscious defense mechanisms.
1	This article discusses the diffusion process in an online social network given the individual connections between members. The authors model the adoption decision of individuals as a binary choice affected by three factors: (1) the local network structure formed by already adopted neighbors, (2) the average characteristics of adopted neighbors (influencers), and (3) the characteristics of the potential adopters. Focusing on the first factor, the authors find two marked effects. First, an individual who is connected to many adopters has a greater adoption probability (degree effect). Second, the density of connections in a group of already adopted consumers has a strong positive effect on the adoption of individuals connected to this group (clustering effect). The article also records significant effects for influencer and adopter characteristics. For adopters, specifically, the authors find that position in the entire network and some demographic variables are good predictors of adoption. Similarly, in the case of already adopted individuals, average demographics and global network position can predict their influential power on their neighbors. An interesting counterintuitive finding is that the average influential power of individuals decreases with the total number of their contacts. These results have practical implications for viral marketing, a context in which a variety of technology platforms are increasingly considering leveraging their consumers' revealed connection patterns. The model performs particularly well in predicting the next set of adopters.
1	Research has shown that consumer online product ratings reflect both the customers' experience with the product and the influence of others' ratings. In this article, the authors measure the impact of social dynamics in the ratings environment on both subsequent rating behavior and product sales. First, they model the arrival of product ratings and separate the effects of social influences from the underlying (or baseline) ratings behavior. Second, the authors model product sales as a function of posted product ratings while decomposing ratings into a baseline rating, the contribution of social influence, and idiosyncratic error. This enables them to quantify the sales impact of observed social dynamics. The authors consider both the direct effects on sales and the indirect effects that result from the influence of dynamics on future ratings (and thus future sales). The results show that although ratings behavior is significantly influenced by previously posted ratings and can directly improve sales, the effects are relatively short lived once indirect effects are considered.
1	The authors conduct a meta-analysis of 751 short-term and 402 long-term direct-to-consumer brand advertising elasticities estimated in 56 studies published between 1960 and 2008. The study finds several new empirical generalizations about advertising elasticity. The most important are as follows: the average short-term advertising elasticity is .12, which is substantially lower than the prior meta-analytic mean of .22; there has been a decline in the advertising elasticity over time; and advertising elasticity is higher (1) for durable goods than nondurable goods, (2) in the early stage than the mature stage of the life cycle, (3) for yearly data than quarterly data, and (4) when advertising is measured in gross rating points than monetary terms. The mean long-term advertising elasticity is .24, which is much lower than the implied mean in the prior meta-analysis (.41). many of the results for short-term elasticity hold for long-term elasticity, with some notable exceptions. The authors discuss the implications of these findings.
1	Across three studies, this research elucidates when loss- versus gain-framed messages are most effective in influencing consumer recycling by examining the moderating role of whether a more concrete or abstract mind-set is activated. First, in a field study, the authors demonstrate that loss frames are more efficacious when paired with low-level, concrete mind-sets, whereas gain frames are more effective when paired with high-level, abstract mind-sets. This is an important, substantive finding that persisted over a significant time span. in addition, in two additional laboratory studies, they find further evidence for this matching hypothesis, in which a pairing of loss- (gain-) framed messages that activates more concrete (abstract) mind-sets leads to enhanced processing fluency, increased efficacy, and, as a result, more positive recycling intentions. The findings have implications for marketers, consumers, and society as a whole.
1	This article examines the interplay of social and temporal distance on consumers' responses to others' recommendations. Drawing on research on psychological distance and the “fit” literature, the authors hypothesize that others' recommendations are more persuasive when the construal levels associated with both social distance and temporal distance are congruent. Specifically, the authors first demonstrate a time-contingent effect of recommendation: Others' recommendations lead to a greater preference shift when people make decisions for distant-future consumption than for near-future consumption (Studies 1 and 2). Second, contrary to conventional wisdom, the authors find that close others do not always have a greater impact than distant others. instead, recommendations from close others are more influential in shifting near-future preferences than those from distant others, whereas recommendations from distant others are more influential than those from close others in shifting distant-future preferences (Study 3). The authors demonstrate that others' recommendations are perceived to be more relevant as the underlying mechanism when there is a match of construal levels between the social and temporal distance. Research and managerial implications are discussed.
1	Identifying winning new product concepts can be a challenging process that requires insight into private consumer preferences. To measure consumer preferences for new product concepts, the authors apply a “securities trading of concepts,” or STOC, approach, in which new product concepts are traded as financial securities. The authors apply this method because market prices are known to efficiently collect and aggregate private information regarding the economic value of goods, services, and firms, particularly when trading financial securities. This research compares the STOC approach against stated-choice, conjoint, constant-sum, and longitudinal revealed-preference data. The authors also place STOC in the context of previous research on prediction markets and experimental economics. Across multiple product categories, the authors test whether STOC (1) is more cost efficient than other methods, (2) passes validity tests, (3) measures expectations of others, and (4) reveals individual preferences, not just those of the crowd. The results show that traders exhibit a self-preference bias when trading. Ultimately, STOC offers two key advantages over traditional market research methods: cost efficiency and scalability. For new product development teams deciding how to invest resources, this scalability may be especially important in the Web 2.0 world.
1	The authors present a Bayesian simultaneous choice factor model that measures consumers' willingness to pay for brand-image associations. Previous research has found that general brand effects influence a brand's scores on specific image dimensions. To investigate the value of general versus specific brand image, the authors specify a higher-order factor model in which a set of correlated factor scores arise from a general brand factor and a set of orthogonal residual scores that measure the specific dimensions of brand image. The general brand factor is consistent with the concept of a halo effect, which theory ascribes to either an overall evaluative effect or errors in cognition. The authors apply the model to stated preference data on branded midsized sedans accompanied by data on consumer brand-image associations. The authors find that there is substantial value for the specific dimensions of brand image, but only after controlling for the general brand effect with the higher-order factor decomposition.
1	Six studies examine the influence of positive affect on self-control in intertemporal choice (consumers' willingness to wait for desired rewards) and the cognitive processes underlying this effect. Two studies measure participants' levels of thinking in two different ways, showing that positive affect can promote forward-looking, high-level thinking. Two studies using a delay-of-gratification paradigm demonstrate this forward-looking thinking and show it to be a mindful process. Participants in positive (vs. neutral) affect were more likely to choose a larger mail-in rebate over a smaller instant rebate when the reward differences were moderate (but not when they were small). Two studies demonstrate the impact of positive affect on intertemporal preference in another way, showing that participants in positive affect do not discount the value of delayed outcomes as much as people in neutral affect do (decreased present bias). Together, the results indicate that positive affect promotes cognitive flexibility and fosters a higher level of thinking and a more future-oriented time perspective, without obscuring practical considerations and other needed detail, including context and opportunity costs, when evaluating intertemporal options.
1	This article investigates the consequence of the choice process for mental resources and the desire to obtain the selected products. The authors draw a distinction between instrumental choice, which serves preexisting consumption goals, and experiential choice, which serves as its own end. Across four studies, they find that instrumental choice undermines mental resources and experiential choice increases these resources. As a result, although experiential choice is made with no consumption goal in mind, compared with instrumental choice, it increases the desire to obtain the selected product. The authors demonstrate these effects on choice among a variety of consumer products (e.g., vacation packages, novels, flower bouquets).
1	Previous research has shown that consumers frequently choose products with too many features that they later find difficult to use. In this research, the authors show that this seemingly suboptimal behavior may actually confer benefits when factoring in the social context of consumption. The results demonstrate that choosing products with more capabilities (i.e., feature-rich products) provides social utility beyond inferences of wealth, signaling consumers' technological skills and openness to new experiences and that consumers' beliefs about the social utility of feature-rich products are predictive of their choices of such products. Furthermore, the authors examine when impression management concerns increase consumers' likelihood of choosing feature-rich products. They find that public choices in which participants display their preferences to others encourage feature-seeking behavior but that the anticipation of having to use a product in front of others provides an incentive to avoid additional features.
1	This article studies how to endogenously assess the value of a “superior” advertising position in the price competition and examines the resulting location competition outcomes and price dispersion patterns. The authors consider a game-theoretic model in which firms compete for advertising positions and then compete in price for customers in a product market. Firms differ in their competence, and positions are differentiated in their prominence, which reflects consumers' online search behavior. They find that when endogenously evaluated within the product market competition, a prominent advertising position might not always be desirable for a firm with competitive advantage, even if it is cost-free. The profitability of a prominent advertising position depends on the trade-off between the extra demand from winning the position and the higher equilibrium prices when the weaker competitor wins it. Furthermore, the authors show that the bidding outcome might not align with the relative competitive strength, and an advantaged firm might not be able to win the prominent position even when it values that position. They derive two-dimensional equilibrium price dispersion with the realized prices at the same position varying and the expected prices differing across different positions. They find that the expected price in the prominent position might not always be higher, implying that an expensive location does not necessarily lead to expensive products.
1	Both customer and innovation assets are important to firm performance. Prior research has mostly examined these assets at the firm level and has not distinguished between the effects of asset depth relative to competitors and asset breadth across different segments. Using configuration theory and the resource-based view of the firm, the authors propose that how these assets interact to influence performance depends on both depth and breadth because these features reflect whether the assets are likely to create and/or appropriate value when deployed. Empirical results from two studies—one using secondary data and another using primary data from a survey of senior managers—indicate that performance is highest when firms employ configurations using deep customer and broad innovation assets or deep innovation and broad customer assets. In contrast, firm performance variability decreases in the presence of deep–deep and broad–broad asset configurations. The effect of configuration strategies on firm performance also is typically greater in dynamic than in stable environments.
1	This article examines governance decisions within an emerging and increasingly common form of channel: the partially integrated channel (PIC). The PIC is defined as a single vertical channel in which both market governance and hierarchical governance exist (i.e., the employees of one channel member work on a full-time basis at an exchange partner's facilities, performing functions that the exchange partner traditionally performs). Building on the transaction cost analysis and governance value analysis literature streams, the authors examine ongoing governance decisions within the PIC. Data were collected in the fashion apparel market of South Korea using multisource, reciprocally matched data in which a manufacturer's directly employed sales force is deployed at department store retailers as the sole frontline employees for their brand. The authors find that brand reputation, downstream market uncertainty, and sales force performance ambiguity influence manufacturer control over sales operations and manufacturer flexibility with retailers in unique ways. The authors discuss implications of this work for theory and practice.
1	Consumers increasingly inform one another about marketplace offerings in online review forums. The authors demonstrate that when given no information about a reviewer (i.e., when the reviewer's identity is ambiguous), consumers use an accessibility-based egocentric anchor to infer that ambiguous reviewers have similar tastes to their own, leading consumers to be (1) similarly persuaded by reviews written by ambiguous and similar reviewers and (2) more persuaded by reviews written by ambiguous reviewers than by reviews written by dissimilar reviewers. The authors demonstrate that this effect holds in a single-offering, single-reviewer context. The authors also show that when consumers are exposed to multiple offerings with multiple reviewers, there may be a slight “cost” to ambiguity as opposed to similarity but that ambiguity remains much more persuasive than dissimilarity. Finally, the authors demonstrate that the effects of egocentric anchoring on persuasion can be moderated, first, by making other-related thoughts accessible and, second, by providing external cues about potential reviewer heterogeneity. These findings have important implications for both the management and monitoring of consumer-to-consumer online communication.
1	Consumers choosing flat-rate contracts tend to have insufficient usage to warrant the cost, particularly for new products. We propose and estimate a Bayesian learning model of tariff and usage choice that explains this flat-rate bias without relying on behavioral misjudgments or tariff-specific preferences. For new products, consumers are uncertain about both their utility relative to the population mean and the mean itself. We show that this latter uncertainty inflates prior variances, which leads consumers to weight their private signals more heavily when updating beliefs. Posteriors are unbiased across products. For a given product, however, the unknown mean yields a “winner's curse”: Consumers with high posteriors tend to overestimate their utility. These consumers choose fixed-rate tariffs and lower their usage as they correct their beliefs. The flat-rate bias arises when switching costs deter them from changing tariffs. Using the estimated model, the authors find that tariff menus are ineffective screening devices for price discrimination by an online grocer. Predicted revenues increase by 20% when the per-use tariff is dropped, because more consumers choose and stay with the flat rate.
1	The authors show how to use microlevel survey data from a tracking study on brand awareness in conjunction with data on sales and advertising expenditures to improve the specification, estimation, and interpretation of aggregate discrete choice models of demand. In a departure from the commonly made full information assumption, they incorporate limited information in the form of choice sets to reflect that consumers may not be aware of all available brands at purchase time. They find that both the estimated brand constants and the price coefficient are biased downward when consumer heterogeneity in choice sets is ignored. These biased estimates can lead firms to make costly price-setting mistakes. In addition, the tracking data enable the authors to identify separately two processes by which advertising influences market shares. They find that advertising has a direct effect on brand awareness (inclusion in choice set) in addition to its effect on consumer preferences (increase in utility). This improved understanding of how advertising works enhances researchers’ ability to make policy recommendations.
1	Offline retailers face trading area and shelf space constraints, so they offer products tailored to the needs of the majority. Consumers whose preferences are dissimilar to the majority—”preference minorities”—are underserved offline and should be more likely to shop online. The authors use sales data from Diapers.com, the leading U.S. online retailer for baby diapers, to show why geographic variation in preference minority status of target customers explains geographic variation in online sales. They find that, holding the absolute number of the target customers constant, online category sales are more than 50% higher in locations where customers suffer from preference isolation. Because customers in the preference minority face higher offline shopping costs, they are also less price sensitive. Niche brands, compared with popular brands, show even greater offline-to-online sales substitution. This greater sensitivity to preference isolation means that these brands in the tail of the long tail distribution draw a greater proportion of their total sales from high–preference minority regions. The authors conclude with a discussion of implications for online retailing research and practice.
1	Firms are challenged to improve the effectiveness of cross-selling campaigns. The authors propose a customer-response model that recognizes the evolvement of customer demand for various products; the possible multifaceted roles of cross-selling solicitations for promotion, advertising, and education; and customer heterogeneous preference for communication channels. They formulate cross-selling campaigns as solutions to a stochastic dynamic programming problem in which the firm's goal is to maximize the long-term profit of its existing customers while taking into account the development of customer demand over time and the multistage role of cross-selling promotion. The model yields optimal cross-selling strategies for how to introduce the right product to the right customer at the right time using the right communication channel. Applying the model to panel data with cross-selling solicitations provided by a national bank, the authors demonstrate that households have different preferences and responsiveness to cross-selling solicitations. In addition to generating immediate sales, cross-selling solicitations also help households move faster along the financial continuum (educational role) and build up goodwill (advertising role). A decomposition analysis shows that the educational effect (83%) largely dominates the advertising effect (15%) and instantaneous promotional effect (2%). The cross-selling solicitations resulting from the proposed framework are more customized and dynamic and improve immediate response rate by 56%, long-term response rate by 149%, and long-term profit by 177%.
1	In this article, the authors present an information-processing model of self-regulation. The model predicts that consumers with an active self-regulatory goal will tend to focus on the cost (rather than the pleasure) of consumption, and as a result, they are better able to control their behavior. In contrast to prior research, the authors find that consumers with an active goal are most vulnerable to self-regulatory failure when the object of desire is farther away from them (in either time or space) because as the distance increases they focus less on the costs of consumption. Finally, results indicate that if product information is not externally available (i.e., it must be recalled from memory), people are more likely to focus on pleasure and fail at self-regulation. The results are robust across four experiments using a variety of stimuli, goal primes, and information-processing measures.
1	Firms are increasingly seeking to harness the potential of social networks for marketing purposes. Therefore, marketers are interested in understanding the antecedents and consequences of relationship formation within networks and in predicting interactivity among users. The authors develop an integrated statistical framework for simultaneously modeling the connectivity structure of multiple relationships of different types on a common set of actors. Their modeling approach incorporates several distinct facets to capture both the determinants of relationships and the structural characteristics of multiplex and sequential networks. They develop hierarchical Bayesian methods for estimation and illustrate their model with two applications: The first application uses a sequential network of communications among managers involved in new product development activities, and the second uses an online collaborative social network of musicians. The authors’ applications demonstrate the benefits of modeling multiple relations jointly for both substantive and predictive purposes. They also illustrate how information in one relationship can be leveraged to predict connectivity in another relation.
1	This research demonstrates that during self-customization, consumers use their experiences from prior feature decisions to form expectations about subsequent decisions. When the difficulty experienced during decisions later in the process deviates from that which occurs earlier in the process, consumer preference is affected by the discrepancy between actual and expected difficulty. Specifically, the results show that when the difficulty experienced during feature decisions deviates from expectations, consumers may spend more or less money on product features as a result of discrepant fluency than when they perform the same task and the level of difficulty is expected. The results demonstrate that discrepant fluency effects are not limited to sequential decisions but can influence a single feature decision, which was accomplished by altering consumers’ expectations before the decision. These discrepant fluency effects emerge even when the attributes of the alternatives and the composition of the focal decision settings remain the same.
1	Four studies investigate the interactive influence of the presence of an accompanying friend and a consumer's agency–communion orientation on the consumer's spending behaviors. In general, the authors find that shopping with a friend can be expensive for agency-oriented consumers (e.g., males) but not for communion-oriented consumers (e.g., females). That is, consumers who are agency oriented spend significantly more when they shop with a friend (vs. when they shop alone), whereas this effect is attenuated for consumers who are communion oriented. The results also show that this interactive effect is moderated by individual differences in self-monitoring such that friends are especially influential for consumers who are high in self-monitoring, but the effects occur in opposite directions for agency- and communion-oriented consumers (i.e., agentic consumers spend more with a friend, while communal consumers spend less when accompanied by a friend). Finally, the authors test the underlying process and document that the interaction of agency–communion orientation, the presence of a friend, and self-monitoring is reversed when the focal context is changed from “spending for the self” to “donating to a charity.” They conclude with a discussion of implications for research and practice.
1	Studies of consumer decision making often begin with the identification of a dimension on which options differ, followed by an analysis of the factors that influence preferences along that dimension. Building on a conceptual analysis of a diverse set of problems, the authors identify a class of related consumers choices (e.g., extreme vs. compromise, hedonic vs. utilitarian, risky vs. safe) that can all be classified according to their levels of self- versus other-expression (or [un]conventionality). As shown in four studies, these problem types respond similarly to manipulations that trigger or suppress self-expression. Specifically, priming self-expression systematically increases the share of the self-expressive options across choice problems. Conversely, expecting to be evaluated decreases the share of the self-expressive options across the various choice dilemmas. In addition, priming risk seeking increases only the choice of risky gambles but not of other self-expressive options. These findings highlight the importance of seeking underlying shared features across different consumer choice problems, instead of treating each type in isolation.
1	The behavioral literature provides ample evidence that consumer preferences are partly driven by the context provided by the set of alternatives. Three important context effects are the compromise, attraction, and similarity effects. Because these context effects affect choices in a systematic and predictable way, it should be possible to incorporate them in a choice model. However, the literature does not offer such a choice model. This study fills this gap by proposing a discrete-choice model that decomposes a product's utility into a context-free partworth utility and a context-dependent component capturing all three context effects. Model estimation results on choice-based conjoint data involving digital cameras provide convincing statistical evidence for context effects. The estimated context effects are consistent with the predictions from the behavioral literature, and accounting for context effects leads to better predictions both in and out of sample. To illustrate the benefit from incorporating context effects in a choice model, the authors discuss how firms could utilize the context sensitivity of consumers to design more profitable product lines.
1	Designing compensation plans with an appropriate level of incentives is a key decision faced by managers of direct sales forces. The authors use data on individual salesperson compensation contracts to show that firms design their pay plans to both discriminatingly select (i.e., attract and retain) salespeople and provide them with the right level of incentives. Consistent with standard agency arguments, the authors find that firms use higher-powered incentives as the importance of agent effort increases. At the same time, the authors find strong support for the selection role of these contracts. Specifically, agents with greater selling ability and lower risk aversion are associated with jobs offering higher-powered incentives. Finally, consistent with prior findings on incentive contracts, the authors find no support for the insurance implication of the typical agency model. The authors rule out alternative explanations for this anomalous result and find that the selection role of contracts best explains the result in their context.
1	Amid growing concerns about childhood obesity and the associated health risks, several countries are considering banning fast-food advertising targeting children. In this article, the authors study the effect of such a ban in the Canadian province of Quebec. Using household expenditure survey data from 1984 to 1992, authors examine whether expenditure on fast food is lower in those groups affected by the ban than in those that are not. The authors use a triple difference-indifference methodology by appropriately defining treatment and control groups and find that the ban's effectiveness is not a result of the decrease in fast food expenditures per week but rather of the decrease in purchase propensity by 13% per week. Overall, the authors estimate that the ban reduced fast-food consumption by Us$88 million per year. The study suggests that advertising bans can be effective provided media markets do not overlap.
1	This research documents an intriguing empirical phenomenon whereby states of relaxation increase the monetary valuation of products. The authors demonstrate this phenomenon in six experiments involving two methods of inducing relaxation, a large number of products of different types, and various methods of assessing monetary valuation. In all six experiments, participants who were put into a relaxed affective state reported higher monetary valuations than participants who were put into an equally pleasant but less relaxed state. This effect seems to be caused by differences in relaxed and nonrelaxed people's mental construals of the value of the products. Specifically, compared with less relaxed people, relaxed people seem to represent the value of products at a higher level of abstraction, which increases their perceptions of these products' value. The phenomenon appears to reflect an inflation of value by relaxed people rather than a deflation of value by less relaxed people.
1	In this research, the authors examine the role of process versus outcome simulation in product evaluation and demonstrate how manipulating the type of information-processing mode (cognitive vs. affective) leads to unique effects in process and outcome simulation. The article begins with the premise that when consumers do not have well-formed preferences for a product, they tend to focus on the usage process. The authors predict and find that outcome simulation is more effective than process simulation in increasing product evaluation under a cognitive mode, whereas process simulation is more effective than outcome simulation under an affective mode. Establishing boundary conditions, the authors further show the effect of two important moderators that alter consumers' focus on/away from the product's usage process. Specifically, they show a reversal of the effect for each type of mental simulation for hedonic products, for which product benefits are the more salient aspect (vs. the usage process). Furthermore, a distant-future (vs. near-future) evaluation frame shifts people's focus away from the usage process toward product benefits and reverses the effect of each type of simulation. The authors conclude with a discussion of theoretical and managerial implications.
1	To date, research on no-choice options has primarily examined the conditions that foster choice deferral, thus focusing on the frequency with which consumers select the no-choice option. In this article, the authors argue that even if the no-choice option is not selected, its mere presence in the choice set may alter consumers' choices. More specifically, they investigate how decision processes and preferences change when consumers have a no-choice option versus when they are forced to choose from a given choice set. They propose that the inclusion of a no-choice option in a choice set affects preferences by leading consumers to determine not only which alternative is best, but which, if any, are acceptable (i.e., meet the consumer's minimum needs). Accordingly, the authors demonstrate that the inclusion of a no-choice option in the choice set (1) leads to more alternative- (rather than attribute-) based information processing, (2) increases the importance of attributes that are more meaningful when alternatives are evaluated one by one (i.e., enriched attributes), and (3) increases the importance of attributes with levels that are closer to the consumer's minimum needs (thresholds). They demonstrate that such changes influence consumers' preference structures and ultimate choices. They conclude with a discussion of the theoretical, methodological, and managerial implications of these findings.
1	Across five studies, the authors investigate how social identification influences consumer preference for discount-based promotions (i.e., cents-off deals) versus donation-based promotions (in which purchase results in a donation to a charitable cause). In doing so, they demonstrate the interplay between self-construal and a specific social identity (i.e., that associated with the particular charity featured in a donation-based promotion) on consumers' preferences for these two types of promotions. The results show that, in general, consumers possessing interdependent self-construals prefer donations to a greater extent than those with independent self-construals. However, the findings further indicate that these effects of self-construal are attenuated if (1) the donation-based promotion does not involve a charity that is identity congruent or (2) a cause-congruent identity is more salient than self-construal at the time of decision making. The authors also identify boundary conditions of charity efficiency and product type for these self-construal effects. In addition to demonstrating how multiple identities interact to influence consumer promotion preferences, the authors discuss important managerial implications regarding the use of discount versus donation-based promotions.
1	Word of mouth (WOM) affects diffusion and sales, but why are certain products talked about more than others, both right after consumers first experience them and in the months that follow? This article examines psychological drivers of immediate and ongoing WOM. The authors analyze a unique data set of everyday conversations for more than 300 products and conduct both a large field experiment across various cities and a controlled laboratory experiment with real conversations. The results indicate that more interesting products get more immediate WOM but, contrary to intuition, do not receive more ongoing WOM over multiple months or overall. In contrast, products that are cued more by the environment or are more publicly visible receive more WOM both right away and over time. Additional analyses demonstrate which promotional giveaways in WOM marketing campaigns are associated with increased WOM. overall, the findings shed light on psychological drivers of WOM and provide insight into designing more effective WOM campaigns.
1	Market structure analysis is a basic pillar of marketing research. Classic challenges in marketing such as pricing, campaign management, brand positioning, and new product development are rooted in an analysis of product substitutes and complements inferred from market structure. In this article, the authors present a method to support the analysis and visualization of market structure by automatically eliciting product attributes and brand's relative positions from online customer reviews. First, the method uncovers attributes and attribute dimensions using the “voice of the consumer,” as reflected in customer reviews, rather than that of manufacturers. Second, the approach runs automatically. Third, the process supports rather than supplants managerial judgment by reinforcing or augmenting attributes and dimensions found through traditional surveys and focus groups. The authors test the approach on six years of customer reviews for digital cameras during a period of rapid market evolution. They analyze and visualize results in several ways, including comparisons with expert buying guides, a laboratory survey, and correspondence analysis of automatically discovered product attributes. The authors evaluate managerial insights drawn from the analysis with respect to proprietary market research reports from the same period analyzing digital imaging products.
1	Spending on political advertising has grown dramatically in recent years, and political campaigns have increasingly adopted the language and techniques of marketing. As such political marketing efforts proliferate, the factors that drive electoral success warrant greater attention and investigation. The authors employ a combination of laboratory studies and analysis of actual election results to reveal influences of candidate appearance and spending strategies in campaigns. They analyze how personality trait inferences based on candidate appearance interact with political party brand image, advertising spending, and negative advertising. The results indicate that appearance-based inferences about candidates influence election outcomes, but their impact is driven partially by trait associations at the party brand level. This interaction between appearance and party alters the effects of advertising spending, particularly the effects of negative advertising. The findings have implications for the marketing of political candidates in terms of their party's brand image.
1	When a new purchase involves trading in an old item, the prices of component transactions may be presented in different ways. The authors report three studies that examine how different price presentations influence evaluations and choice in an otherwise equivalent overall exchange. Although consumers play both buyer and seller roles in purchases involving trade-ins, the authors find that consumers' evaluation of the overall exchange typically places more weight on the price of the new purchase than the trade-in. However, they also show that the weights can be shifted systematically by (1) directly manipulating the importance of the trade-in and (2) indirectly priming a seller (vs. buyer) mind-set. In these instances, consumers place more weight on the trade-in price than the price of the new purchase.
1	The authors theorize that the inconsistencies in prior research regarding the relational behavior–performance relationship arise from the type of reciprocity the firm internalizes. The results of a longitudinal study of 284 buyer–supplier relationships indicate that relational behavior enhances financial performance when what is exchanged does not need to be directly comparable with what was received (i.e., heteromorphic equivalence reciprocity) or can be returned over a longer time horizon (i.e., long-term immediacy reciprocity). Moreover, relational behavior diminishes financial performance when what is exchanged must be directly comparable in form (i.e., homeomorphic equivalence reciprocity) or returned over a short time horizon (i.e., short-term immediacy reciprocity). The authors further conclude that a longitudinal interaction effect model provides a more accurate understanding of the relational behavior–performance relationship when compared with a cross-sectional interaction effect model, because it minimizes the unobserved heterogeneity effect, thus strengthening causal inference.
1	Few studies address the marketing budgeting problems of platform firms operating in two-sided markets with cross-market network effects, such that demand from one customer group in the platform influences the demand from the other customer group. Yet such firms (e.g., newspapers whose customers are both subscribers and advertisers) are prevalent in the marketplace and invest significantly in marketing. To enable such firms to make effective marketing decisions, the authors delineate the desired features of a platform firm's marketing response model, specify a new response model, and validate it using market data from a local newspaper. The results show that the firm faces reinforcing cross-market effects, its demand from both groups depends on marketing investments, and the model exhibits good forecasting capability. The authors use the estimated response model to determine optimal marketing investments over a finite planning horizon and find that the firm should significantly increase its newsroom and sales force investments. With this model-based recommendation, the firm's management increased its newsroom budget by 18%. Further normative analysis sheds light on how cross-market and carryover effects alter classical one-sided marketing budgeting rules.
1	This article examines the effect of the number of goals on consumers’ savings behavior. Drawing from research on implementation intention, the authors show that under certain conditions, presenting a single savings goal leads to greater savings intention and actual savings than presenting multiple savings goals. Multiple goals typically evoke trade-offs among competing goals and thus increase the likelihood that people will remain in a deliberative mind-set and defer actions. In contrast, the authors propose and demonstrate that a single goal evokes a stronger implementation intention, which in turn has a greater effect on behavior change. They also show that the advantage of a single goal over multiple goals on saving is attenuated when saving is easier to implement or when the multiple savings goals are integrated rather than competing among themselves. Theoretical and practical implications are discussed.
1	In four studies, the authors show that consumers’ savings can be increased or decreased merely by changing the way consumers think about their saving goals. Consumers can (1) either specify or not specify an exact amount to save (goal specificity) and (2) focus on either how to save or why to save (construal level). The results illustrate that specific goals help consumers save more when the saving goal is construed at a high level but that nonspecific goals help consumers save more when the saving goal is construed at a low level. The same pattern of results occurs with anticipated saving success and actual savings. Mediation analyses reveal that for high-level construers, specific (vs. nonspecific) goals lead to success because they are perceived as more important. However, specific (vs. nonspecific) goals are also perceived as more difficult, which is more discouraging for low-level construers.
1	This research investigates the process that underlies consumer decisions to abandon waits for service. The work centers on a hypothesis that stay-or-renege decisions reflect a process that blends two opposing psychic forces: escalating displeasure with waiting versus an escalating commitment to a wait that has been initiated. The consequence is a predicted tendency that abandonments are most likely near the midpoint of waits, which is suboptimal for many waiting time distributions. This study tests the hypothesis using data from three laboratory experiments in which participants play a time-management game that involves waiting for downloads from different computer servers, as well as field data about hang-ups in an emergency call center in India. The data lend support to the proposed competing hazards model and show that the trade-off between desires to abandon and persist is moderated by contextual factors, such as the initial number of alternative queues and the amount of distracting activity engaged in during a wait.
1	The U.S. motion picture industry has become increasingly reliant on posttheatrical channel profits. Two often-cited drivers of these profits are cross-channel substitution among posttheatrical channels and seasonality in consumer preferences for any movie. The authors use a differentiated products version of the multiplicative competitive interaction model to investigate these two phenomena. They estimate the model using data from 2000 and 2001 on two posttheatrical channels in the U.S. market: purchase and rental home viewing channels. Contrary to expectations based on business press commentary, after controlling for seasonality and movie attributes, the authors find low cross-channel price and availability elasticity for both channels. To measure the extent of cross-channel cannibalization, they simulate a 28-day window of sequential release with either purchase or rental channel going first. They find that windowing reduces the sum of revenues across both channels, because more consumers choose to not purchase or rent when faced with older movies in their favored channel rather than to switch to the alternative channel with newer movies.
1	A common belief states that more choice of creative inputs boosts consumer creativity because it expands consumers’ creative solution space. Two experimental studies, run in a knitting and a crafting context, challenge this intuition and suggest that restricting the choice of creative inputs actually enhances creativity for experienced consumers. The authors find that this outcome is due to consumers’ ability to enjoy the creative process more, which in turn positively affects their creative output, as judged by experts. In contrast, consumers perceive themselves as more creative (regardless of experience level) when they have a greater rather than a limited choice of inputs. The authors discuss how these findings open up new avenues for research on creativity and choice overload.
1	Marketers struggle with how best to position innovative products that are incongruent with consumer expectations. Compounding the issue, many incongruent products are the result of innovative changes in product form intended to increase hedonic appeal. Crossing various product categories with various positioning tactics in a single meta-analytic framework, the authors find that positioning plays an important role in how consumers evaluate incongruent form. The results demonstrate that when a product is positioned on functional dimensions, consumers show more preferential evaluations for moderately incongruent form than for congruent form. However, when a product is positioned on experiential dimensions, consumers show more preferential evaluations for congruent form than for moderately incongruent form. Importantly, an increase in perceived hedonic benefits mediates the former, whereas a decrease in perceived utilitarian benefits mediates the latter. The mediation effects are consistent with the view that consumers must first understand a product's functionality before engaging in hedonic consumption.
1	How does a rumor come to be believed as a fact as it spreads across a chain of consumers? This research proposes that because consumers’ certainty about their beliefs (e.g., attitudes, opinions) is less salient than the beliefs themselves, certainty information is more susceptible to being lost in communication. Consistent with this idea, the current studies reveal that though consumers transmit their core beliefs when they communicate with one another, they often fail to transmit their certainty or uncertainty about those beliefs. Thus, a belief originally associated with high uncertainty (certainty) tends to lose this uncertainty (certainty) across communications. The authors demonstrate that increasing the salience of consumers’ uncertainty/certainty when communicating or receiving information can improve uncertainty/certainty communication, and they investigate the consequences for rumor management and word-of-mouth communications.
1	One field study and five experiments show that seemingly irrelevant bodily actions influence consumer behavior. These studies demonstrate that arm flexion (in which the motor action is directed toward the self) versus arm extension (in which the motor action is directed away from the self) influences purchase behavior, product preferences, and economic decisions. More specifically, arm flexion increases the likelihood of purchasing vice products (Study 1a), leads to a preference for vices over virtues (Studies 1b and 2a), and leads to preference for smaller, sooner over larger, later monetary rewards (Studies 2b, 3, and 4). The authors argue that arm flexion induces present-biased preferences through activation of approach motivation. The effect of bodily actions on present-biased preferences is regulated by the behavioral approach system (Studies 3 and 4) and relies on the learned association between arm flexion and activation of this approach system (Study 4). The authors discuss implications for intertemporal decision making, embodied cognition, and marketing practice.
1	The authors explore the interplay between consumers’ progress levels toward attaining a goal and the perceived velocity in progressing toward the goal to determine consumers’ motivation for further goal pursuit. The authors propose that when progress toward attaining a goal is low, consumers are primarily concerned about the question “Can I get there?” Thus, a high (vs. low) perceived velocity in progressing suggests greater expectations of goal attainment, resulting in greater motivation for pursuing the goal. However, when consumers have achieved sufficient progress and are approaching the end point, their attainment of the goal is relatively secured, so they become more concerned about the question “When will I get there?” and focus more on whether they are effectively reducing the remaining discrepancy so that they can attain the goal quickly. In this case, a low (vs. high) perceived velocity in progressing elicits greater motivation because it suggests that continued effort is needed to ensure a speedy attainment. Empirical evidence from lab and field experiments supports this hypothesis.
1	The authors evaluate the impact of ad placement on revenues and profits generated from sponsored search. Their approach uses data generated through a field experiment for several keywords from an online retailer's ad campaign. Using a hierarchical Bayesian model, the authors measure the impact of ad placement on both click-through and conversion rates. They find that while click-through rate decreases with position, conversion rate increases with position and is even higher for more specific keywords. The net effect is that, contrary to the conventional wisdom in the industry, the topmost position is not necessarily the revenue-or profit-maximizing position. The authors’ results inform the advertising strategies of firms participating in sponsored search auctions and provide insight into consumer behavior in these environments. Specifically, they help correct a significant misunderstanding among advertisers regarding the value of the top position. Furthermore, they reveal potential inefficiencies in current auction mechanisms that search engines use. The authors’ results also reveal the information search strategies that consumers use in sponsored search and provide evidence of recency bias for immediate purchases.
1	This article develops a method for optimal allocation of resources based on an empirically validated model of how national and regional advertising generate sales over time. The authors derive the profit-maximizing total budget, its optimal split between national and regional spends, and its optimal allocation across multiple regions. They formulate a spatiotemporal model that accounts for spatial and serial dependence, spatial heterogeneity, neighborhood effects, and sales dynamics. Because of spatial and serial dependence, correlated multivariate Brownian motion drives the sales dynamics, resulting in a second-order differential equation for the Hamilton–Jacobi–Bellman (HJB) equation with multiple states (i.e., regional sales) and multiple controls (i.e., regional and national advertising expenditures). By solving the HJB equation analytically, the authors furnish closed-form expressions for the optimal total budget and its regional allocations. In addition, they develop a method to estimate the proposed model and apply it to market data from a leading German cosmetics company. Using the estimated parameters, they evaluate the optimal budget and allocations. Comparing them with actual company policy, the proposed approach enhances profit by 5.07%, and it not only identifies which regions under-or overspend but also reveals how much budget to shift from national to regional advertising (or vice versa).
1	The literature on self-control emphasizes that temptation is costly. The authors propose that temptation entails not only costs but also benefits for consumers. These arise from self-signaling effects of how consumers handle tempting choice options. Succumbing to temptation is a (costly) self-signal of weak willpower, whereas resisting temptation is a (beneficial) self-signal of strong willpower. Five experiments demonstrate that these self-signaling costs and benefits of temptation depend not only on the chosen item but also on the temptation from the nonchosen options. The authors discuss theoretical implications of their findings for research on impulsive choice and self-control and on self-signaling and managerial implications for pricing and assortment strategies.
1	In juxtaposition to the common belief that marketing cues highlighting product effectiveness will generate positive influences on consumer demand, the authors argue that signaling effectiveness is a double-edged sword. While effectiveness cues may increase initial purchase, they can curb postpurchase consumption and potentially decrease long-term product sales. Four studies demonstrate that salient cues in advertising or packaging (e.g., pictures, brand names) can increase perceived product efficacy and lead to a lower usage amount on a single occasion. The authors show that the impact of effectiveness cues on product usage is driven by inference making and is moderated by cue salience and people's need for cognition. Furthermore, the authors find that promoting effectiveness with certain cues does not increase product choice yet reduces product usage. These results stress the importance of seeking salient cues that work to stimulate both choice and usage.
1	The authors propose that a crowded product space motivates consumers to better discriminate between options of different quality. Specifically, this article reports evidence from three controlled experiments and one natural experiment that people are prepared to pay more for high-quality products and less for low-quality products when they are considered in the context of a dense, as opposed to a sparse, set of alternatives. To explain this effect, the authors argue that consumers uncertain about the importance of quality learn from observing market outcomes. Product proliferation reveals that other consumers care to discriminate among similar alternatives, and in turn, this inference raises the importance of quality in decision making.
1	The authors conduct an empirical investigation of a new retail loyalty program (LP), called an item-based loyalty program (IBLP), in which price discounts are replaced by reward point promotions that need to be accumulated and redeemed later. The main objective is to examine its impact on various aspects of consumer purchase behavior and a retailer's sales revenue. They find that after a retailer switched from a conventional LP to the IBLP, consumers became more responsive to reward point promotions than to price discounts of the same monetary value, were no longer responsive to competitors' reward point promotions, and exhibited stronger cumulative reward point effects. In addition, the new LP had a significantly different impact on “current” LP members and nonmembers (defined by their status right before the switch), resulting in decreased (increased) total spending by the former (latter) group, under the retailer's current promotion practice. Furthermore, it is critically important for retailers to offer sufficient promotions under the new LP to achieve its full potential; otherwise, they risk alienating their loyal customers. Finally, the IBLP reduced attrition among existing customers and attracted more new customers, which contributed to most of the retailer's sales revenue gain after adopting the IBLP.
1	For many services, customers subscribe to long-term contracts. Standard economic theory suggests that customers evaluate a contract on the basis of its overall net benefits. The authors suggest that rather than evaluating multiperiod service contracts at the contract level, customers use period-level bracketing. Customers evaluate the distinct per-period loss or gain they incur from choosing this contract. This has important consequences when benefits vary over the course of the contract—for example, due to “hassle costs.” If customers use period-level bracketing, they will value a lower price more in periods during which they have hassle than in other periods. The authors explore this using data from a field experiment for web hosting services. They find that a lower price in the initial period is more attractive to customers when they expect their hassle costs to be high at setup. In five lab experiments, the authors support and extend the field experiment's findings. They find evidence for period-level bracketing when customers have hassle costs, independently of whether hassle costs occur in the first, an intermediate, or the last period of a contract. They also rule out alternative explanations, such as hyperbolic discounting. The findings suggest that in setting prices, firms should consider the timing of hassle costs customers face.
1	Copycats imitate features of leading brands to free ride on their equity. The prevailing belief is that the more similar copycats are to the leader brand, the more positive their evaluation is, and thus the more they free ride. Three studies demonstrate when the reverse holds true: Moderate-similarity copycats are actually evaluated more positively than high-similarity copycats when evaluation takes place comparatively, such as when the leader brand is present rather than absent. The results demonstrate that blatant copycats can be less and subtle copycats can be more perilous than is commonly believed. This finding has implications for marketing theory and practice and trademark law.
1	In this study, the authors investigate the role of advertising in affecting the extent of bias in the media. When making advertising choices, advertisers evaluate both the size and the composition of the readership of the different outlets. The profile of the readers matters because advertisers want to target readers who are likely to be receptive to their advertising messages. The authors demonstrate that when advertising supplements subscription fees, it may serve as a polarizing or moderating force, contingent on the extent of heterogeneity among advertisers in appealing to readers having different political preferences. When heterogeneity is large, each advertiser chooses a single outlet for placing advertisements (single-homing), and greater polarization arises in comparison to when the media outlet relies on subscription fees only for revenues. In contrast, when heterogeneity is small, each advertiser chooses to place advertisements in multiple outlets (multihoming) and reduces polarization results.
1	Technological advances enable companies to offer information products such as books, music, and movies in electronic formats, in addition to the traditional physical formats. Although one format may appear more useful and be preferred, consumers may be enticed to consider the unique attributes of all formats if they deliver equally well on salient attributes. The authors investigate the impact of usage situations, relative attribute quality levels of the formats and their interactions on the perception of the formats as perfect or imperfect substitutes or complements, and the purchase likelihood of the bundle of formats. The study demonstrates that when formats have equivalent quality on a salient attribute, consumers perceive the formats as more complementary and are more likely to buy the bundle. This happens because consumers consider more usage situations for the formats and view the bundle as providing greater flexibility for future usages.
1	Brand schematicity refers to a generalized consumer predisposition to process information using brand schema. This research uses schema theory to build the theoretical groundwork for brand schematicity and reports seven studies conducted to measure, validate, and establish the nature of the construct. Studies 1 and 2 pertain to a scale developed to measure brand schematicity, Study 3 measures the construct using response times, and Study 4 situates the construct in a nomological network of associated constructs. Studies 5 and 6 test the predictive validity of the brand schematicity construct, and Study 7 uses memory clustering to provide evidence of the schematic nature of the construct that represents an inherent difference in the way consumers organize and utilize brand information. These investigations establish the validity of brand schematicity and its associated scale, as well as reveal that brand schematicity influences the role of brand concept consistency in brand extension evaluations, suggesting the possible moderating role of the construct in a wide variety of brand-related relationships.
1	Previous academic research on the expansion of dominant retailers such as Wal-Mart has examined implications for incumbent retailers, consumers, and the local community. Little is known, however, about Wal-Mart's influence on suppliers' performance. Manufacturers suggest that Wal-Mart uses its power to squeeze their profits. In this article, the authors study the validity of this claim. They investigate the underlying mechanisms that may cause changes in manufacturer profits following Wal-Mart market entry. The data contain information on supplier interactions with retail stores, including Wal-Mart, for a period of five years. They find that postentry supplier profits increased by 18% on average, whereas profits derived from incumbent retailers decreased only marginally. Their results show that wholesale prices are not the main driver of postentry supplier profit changes; market expansion is. They observe a significant increase in shipments to 50% of markets studied. Furthermore, their analyses demonstrate that supplier shipment and profit increases are highest for markets in which incumbents offer a wide variety of products and carry items that Wal-Mart does not sell.
1	This study shows how advertisers can leverage emotion and attention to engage consumers in watching Internet video advertisements. In a controlled experiment, the authors assessed joy and surprise through automated facial expression detection for a sample of advertisements. They assessed concentration of attention through eye tracking and viewer retention by recording zapping behavior. This allows tests of predictions about the interplay of these emotions and interperson attention differences at each point in time during exposure. Surprise and joy effectively concentrate attention and retain viewers. However, importantly, the level rather than the velocity of surprise affects attention concentration most, whereas the velocity rather than the level of joy affects viewer retention most. The effect of joy is asymmetric, with higher gains for increases than losses for decreases. Using these findings, the authors develop representative emotion trajectories to support ad design and testing.
1	Two sets of studies illustrate the comparative nature of disclosure behavior. The first set investigates how divulgence is affected by signals about others' readiness to divulge and shows a “herding” effect: Survey respondents are more willing to divulge sensitive information when told that previous respondents have made sensitive disclosures (Study 1a). The authors provide evidence of the process underlying this effect and rule out alternative explanations by showing that information on others' propensity to disclose affects respondents' discomfort associated with divulgence (Study 1b) but not their interpretation of the questions (Study 1c). The second set of studies investigates how divulgence is affected by the order in which inquiries of varying intrusiveness are made and suggests that divulgence is anchored by the initial questions in a survey. People are particularly likely to divulge when questions are presented in decreasing order of intrusiveness and less likely when questions are presented in increasing order (Study 2a). The authors show that the effect arises by affecting people's judgments of the intrusiveness of the inquiries (Study 2b). The effect is altered when, at the outset of the study, privacy concerns are primed (Study 2c) and when respondents are made to consider the relative intrusiveness of a different set of questions (Study 2d). This research helps illuminate how consumers' propensity to disclose is affected by continual streams of requests for personal information and by the equally unavoidable barrage of personal information about others.
1	The author proposes a strategic model of entry that allows for positive and negative spillovers among firms. The model is applied to a novel data set containing information about the store configurations of all U.S. regional shopping centers and is used to quantify the magnitude of interstore spillovers. The author addresses the estimation difficulties that arise due to the presence of multiple equilibria by formulating the entry game as a mathematical problem with equilibrium constraints (MPEC). Although this study constitutes the first attempt to use this direct optimization approach to address a specific empirical problem, the method can be used in a wide range of structural estimation problems. The empirical results support the agglomeration and clustering theories that predict that firms may have incentives to colocate despite potential business stealing effects. The author shows that the firms' negative and positive strategic effects help predict both how many firms can operate profitably in a given market and the firm-type configurations. The relative magnitude of such effects varies substantially across store types.
1	Why are certain pieces of online content (e.g., advertisements, videos, news articles) more viral than others? This article takes a psychological approach to understanding diffusion. Using a unique data set of all the New York Times articles published over a three-month period, the authors examine how emotion shapes virality. The results indicate that positive content is more viral than negative content, but the relationship between emotion and social transmission is more complex than valence alone. Virality is partially driven by physiological arousal. Content that evokes high-arousal positive (awe) or negative (anger or anxiety) emotions is more viral. Content that evokes low-arousal, or deactivating, emotions (e.g., sadness) is less viral. These results hold even when the authors control for how surprising, interesting, or practically useful content is (all of which are positively linked to virality), as well as external drivers of attention (e.g., how prominently content was featured). Experimental results further demonstrate the causal impact of specific emotion on transmission and illustrate that it is driven by the level of activation induced. Taken together, these findings shed light on why people share content and how to design more effective viral marketing campaigns.
1	It is well established that consumers' evaluations of brand extensions depend on the quality of the parent brand and the fit between that brand and the extension category. The authors propose that the relative importance of these two factors is influenced by two key features of a typical shopping environment: the presence of visual information and the availability of comparison brands. In particular, the authors demonstrate that adding pictures and enabling brand comparisons shift consumers' preference from extensions of better-fitting brands to extensions of higher-quality brands. The authors propose that this occurs because pictures and brand comparisons create a more concrete representation of the extension, which in turn increases the importance of parent brand quality relative to brand–extension fit. They provide support for this underlying mechanism and discuss the practical implications of their findings.
1	A common strategy for controlling spending is to impose a price restraint on oneself. For example, a consumer who is concerned with limiting expenses may decide before going shopping that he or she only wants to spend approximately $100 for a particular purchase. Although conventional wisdom predicts that self-imposed price restraints will decrease consumer spending, the authors show that salient price restraints can actually increase consumers' preferences for high-priced, high-quality items. The authors propose that making a price restraint salient has the effect of partitioning consumers' evaluations of price and quality, leading to larger differences in perceived quality between options and a greater focus on quality during the final decision. Thus, while budgets and other types of price restraints can limit spending by eliminating some high-priced options from consideration, this research suggests that they can also have the ironic effect of increasing consumers' spending relative to a situation in which consumers have not imposed a price restraint.
1	While some researchers view private labels (PLs) as a key tool for retailer differentiation, others imply that consumers do not distinguish between PLs of different chains. This debate raises the question: Do a retail chain's PL investments subsidize rival PLs? In addition, how does this affect their choice share vis-à-vis national brands? The authors investigate whether consumers generalize knowledge from product experience across PLs of different retail chains and whether such cross-brand learning depends on the PL brands' link with the chain name or on their quality differences. The proposed brand choice model captures cross-brand learning through quality perception spillovers (consumers adjust beliefs about PL quality on the basis of consumption experience) and familiarity spillovers (uncertainty about a PL diminishes with rival PL consumption). Household scanner panel data on dish soap and breakfast cereals yield clear evidence of cross-retailer learning among standard PLs, regardless of their name or quality differences. The results also reveal that familiarity spillovers dominate quality-level spillovers, implying that the presence of cross-learning benefits PLs and enhances their market position relative to national brands. The authors conclude with a discussion of managerial implications.
1	Highly innovative products may offer consumers greater benefits than incrementally new products, yet they have a higher failure rate. The current research addresses the challenge faced by new products that are extremely different from existing offerings by drawing on theory regarding the evaluation of schema incongruity. The authors posit that consumers' acceptance of extremely incongruent products will increase when firms use strategies that facilitate cognitive flexibility and thus the likelihood that consumers will be able to make sense of incongruent new products. The authors examine the influence of three manipulations of cognitive flexibility—positive affect, a future (vs. past) launch description, and a cognitive flexibility prime—on evaluations of new products. The results from four experiments show that these factors facilitate participants' ability to make sense of extremely incongruent new products and that incongruity resolution leads to more positive evaluations. The results also indicate that understanding the benefits provided by extremely new products, rather than affect arising from resolution, leads to higher evaluations of these products.
1	Grounded in agency theory, this study examines how franchisors' ex ante contracts and extracontractual incentives influence their ex post monitoring and enforcement efforts and how combinations of the ex post governance mechanisms drive franchisee behavior. Integrating three archival data sources and a survey of 206 franchisees across eight automotive brands, the authors find that franchisor reliance on contractual completeness appears to result in reduced ex post behavior monitoring and enforcement efforts, while contractual one-sidedness is associated with higher levels of behavior monitoring but reduced enforcement. Extracontractual incentives, when offered to the franchisee, are associated with increases in monitoring and enforcement. In isolation, franchisor monitoring and enforcement efforts are ineffective in eliciting desired franchisee behaviors. However, different combinations of franchisor monitoring and enforcement efforts affect franchisee compliance and opportunism, sometimes with counterproductive results. The study provides an initial baseline of understanding on how ex ante governance characteristics and combinations of ex post governance mechanisms function to facilitate or deter franchisee compliance and opportunism.
1	This article examines how a common form of decision assistance— recommendations that present products in order of their predicted attractiveness to a consumer—transforms decision processes during product search. Such recommendations induce a shift in consumers' decision orientation in search from being directed at whether additional alternatives should be inspected to identifying the best alternative among those already encountered, which is common when choosing from predetermined sets of alternatives. That is, recommendations cause consumers to search in “choice mode.” Evidence from three studies provides support for such a transformation of search decisions, which manifests itself in two respects. First, compared with unassisted search, recommendations lead consumers to assess a product they encounter in their search by comparing it less with the best one discovered up to that point and more with other previously inspected alternatives. Second, recommendations transform how variability in product attractiveness affects stopping decisions such that greater variability causes consumers to search less, which is contrary to what is commonly observed in search without recommendations.
1	The authors present a general consumer preference model for experience products that overcomes the limitations of consumer choice models, especially when it is not easy to consider some qualitative attributes of a product or when there are too many attributes relative to the available amount of preference data, by capturing the effects of unobserved product attributes with the residuals of reference consumers for the same product. They decompose the deterministic component of product utility into two parts: that accounted for by observed attributes and that due to nonobserved attributes. The authors estimate the unobserved component by relating it to the corresponding residuals of virtual experts representing homogeneous groups of people who experienced the product earlier and evaluated it. Their methodology involves identifying such virtual experts and determining the relative importance they should be given in the estimation of the target person's residuals. Using Bayesian estimation methods and Markov chain Monte Carlo simulation inference, the authors apply their approach to two types of consumer preference data: (1) online consumer ratings (stated preferences) data for Internet recommendation services and (2) offline consumer viewership (revealed preferences) data for movies. The results empirically show that this new approach outperforms several alternative collaborative filtering and attribute-based preference models with both in- and out-of-sample fits. The model is applicable to both Internet recommendation services and consumer choice studies.
1	The authors present a modeling approach to assess the purchase conversion performance of individual keywords in paid search advertising. The model facilitates estimation of daily keyword conversion and click-through rates in a sparse data environment while accounting for the endogenous position of the text advertisement served in response to a search. Position endogeneity in paid search data can arise from both omitted variables and measurement error. The authors propose a latent instrumental variable approach to address this problem. They estimate their model on keyword-level paid search data containing daily information on impressions, clicks, and reservations for a major lodging chain. They find that higher positions increase both the click-through and conversion rates. When advertisements are served in higher positions, approximately one-third of new conversions is due to increased click-through while approximately two-thirds are due to increased conversion rates. The authors show that the keyword list generated on the basis of their estimated conversion rates outperforms the status quo list as well as lists generated by observed conversion and click-through rates.
1	The diversification effect, a tendency toward greater variety as multiple choices are made simultaneously, in advance of consumption, is a robust and important phenomenon. Researchers have typically explained the diversification effect in terms of differences in the process by which people select items from among those available. This precludes the possibility that the locus of the effect lies in people's choice sets themselves—that is, how people decide which options to consider when making choices. The authors examine the effects of set formation using an experimental choice sequence task and conjoined stochastic model of set formation and conditional choice. The findings demonstrate that set formation plays a critical role in diversification: Previously chosen options are indeed discounted, but only for simultaneous choices and only in the set formation portion of the model. Furthermore, the expected number of choice set items is substantially greater in multiple- versus single- item choice. Specifically, when consumers choose simultaneously choice set sizes appear relatively larger overall, but the previously chosen item is less likely to be in a person's (latent) choice set. These findings cannot be attributed to alternative patterns of covariation, including latent error correlations; to temporal stochastic inflation; or to unobserved heterogeneity.
1	Consumers spontaneously construct attributions for negative events such as product-harm crises. Base-rate information influences these attributions. The research findings suggest that for brands with positive prior beliefs, a high (vs. low) base rate of product-harm crises leads to less blame if the crisis is said to be similar to others in the industry (referred to as the “discounting effect”). However, in the absence of similarity information, a low (vs. high) base rate of crises leads to less blame toward the brand (referred to as the “subtyping effect”). For brands with negative prior beliefs, the extent of blame attributed to the brand is unaffected by the base-rate and similarity information. Importantly, the same base-rate information may have a different effect on the attribution of a subsequent crisis depending on whether discounting or subtyping occurred in the attribution of the first crisis. Consumers who discount a first crisis also tend to discount a second crisis for the same brand, whereas consumers who subtype a first crisis are unlikely to subtype again.
1	The author examines conditions under which one firm's product line expansion can cause all firms to be more profitable in horizontally differentiated markets. Although a firm's profits might be expected to decrease when a competitor expands its product line because the firm loses some sales to the new product, this intuition is incomplete because a competitor's product line expansion can also soften price competition. The author first provides an example using the Hotelling model that demonstrates the possibility and mechanism of profit-increasing competitor entry. He then presents conditions under which a competitor's product line expansion increases profits under the mixed-logit model. The study demonstrates that firms benefit from a rival's entry most when a moderate number of customers are unserved before the new-product introduction and when the new product is positioned such that both the rivals' products appeal to similar sets of customers. With regard to extensions, this study demonstrates that the result continues to hold when firms choose product attributes endogenously and that a manufacturer's profits can increase from a rival's product line expansion even when the firms sell through a retailer.
1	When viewing advertisements, consumers must decide what to believe and what is meant to deceive. Accordingly, much behavioral research has explored strategies and outcomes of how consumers process persuasive messages that vary in perceived sincerity. New neuroimaging methods enable researchers to augment this knowledge by exploring the cognitive mechanisms underlying such processing. The current study collects neuroimaging data while participants are exposed to advertisements with differing levels of perceived message deceptiveness (believable, moderately deceptive, and highly deceptive). The functional magnetic resonance imaging data, combined with an additional behavioral study, offer evidence of two noteworthy results. First, confirming multistage frameworks of persuasion, the authors observe two distinct stages of brain activity: (1) precuneus activation at earlier stages and (2) superior temporal sulcus and temporal-parietal junction activation at later stages. Second, the authors observe disproportionately greater brain activity associated with claims that are moderately deceptive than those that are either believable or highly deceptive. These results provoke new thinking about what types of claims garner consumer attention and which consumers may be particularly vulnerable to deceptive advertising.
1	Three laboratory experiments explore how alternative brand name structures (i.e., family branded or subbranded) and varying degrees of category similarity (i.e., similar or dissimilar) influence extension evaluations and parent brand dilution. The results indicate that subbranded extensions (e.g., Quencher by Tropicana cola) evoke a slower, more thoughtful subtyping processing strategy than family branded extensions (e.g., Tropicana cola), which evoke a faster, category-based processing strategy. As a result, category similarity affects extension evaluations when the extension is family branded but not when it is subbranded. In addition, dilution effects are only evident when consumers have a negative experience with a similar family branded extension. Subbranding thus offers two key benefits to marketers: It both enhances extension evaluations and protects the parent brand from any unwanted negative feedback.
1	The current research explores the role of disgust in enhancing compliance with fear appeals. Despite its frequent use in advertising and prevalence in consumer settings, little is known about the specific role that disgust plays in persuasion. This article explores the unique characteristics of disgust and examines its distinctive effect on persuasion. The results across a series of four studies demonstrate that adding disgust to a fear appeal appreciably enhances message persuasion and compliance beyond that of appeals that elicit only fear. Importantly, the results trace the persuasive effects of disgust to its strong and immediate avoidance reaction.
1	Although consumers increasingly use online communities for various activities, little is known about how participation in them affects people's decision-making strategies. Through a series of field and laboratory studies, the authors demonstrate that participation in an online community increases people's risk-seeking tendencies in their financial decisions and behaviors. The results reveal that participation in an online community leads consumers to believe that they will receive help or support from other members should difficulties arise. Such a perception leads online community participants to make riskier financial decisions than nonparticipants. The authors also discover a boundary condition to the effect: Online community members are more risk seeking only when they have relatively strong ties with other members; when ties are weak, they exhibit similar risk preferences as nonmembers.
1	As private labels (PLs) continue to grow in power and market share, product innovation has become one of the strongest weapons in the national brand (NB) manufacturer's arsenal. In this article, the author assesses when and to what extent new products change NBs' market position. To address this question, more than 300 NB and PL introductions are analyzed using a multibreak model that quantifies the impact of product introductions on own share, rival NB and PL share, and category sales. Drawing on empirical generalizations, the author finds that products introduced by leading NBs, standard PLs, and premium PLs are more likely to increase category sales than products introduced by follower NBs or economy PLs. New products introduced by leader and follower NBs more often boost own share. Thus, new products help prevent the decline of NB shares. With respect to competitive impact, new products affect rival shares, with the exception of those launched by economy PLs. Still, NBs tend to hurt rival NBs more often than PLs, and only the leading NB is likely to steal share from all three PL tiers. Moreover, standard PLs tend to be harmed less often by rival new products, unless introduced by the leading NB. Overall, PLs are more likely to be affected by a NB that maintains a large price gap and offers new products with new intrinsic or usage benefits. To fight economy PLs successfully, however, NBs must maintain a smaller price gap, while offering products that focus less on intrinsic and usage benefits.
1	Many firms rely on choice experiment–based models to evaluate future marketing actions under various market conditions. This research investigates choice complexity (i.e., number of alternatives, number of attributes, and utility similarity between the most attractive alternatives) and individual differences in decision time as key factors that affect the predictive performance of models based on choice experiments, both within and between complexity conditions. The results show that complexity and individual decision time not only affect the error in consumer choice models but also consumers' decision strategy and systematic utilities. The authors introduce a complexity-adjusted mixed logit (CAM logit) model to capture the various influences of complexity in choice experiment–based models. They illustrate the consequences of complexity on choice behavior with market share predictions of the CAM logit model for different complexity conditions.
1	The authors propose a multicategory model of consumers' purchase incidence, quantity, and brand choice decisions. The model specification allows for cross-category promotion effects in both components of the primary demand (incidence and quantity decisions) and uses a flexible functional form of consumer's utility to accurately measure those cross-category effects. To demonstrate the importance of the methodology, the authors investigate two issues of relevance to retailers, namely, how retailers should (1) allocate promotional expenditures across brands in a category and (2) coordinate timing of promotions of brands across categories. The authors estimate the proposed model using consumers' purchases in pasta sauce and pasta categories. The results reveal that using restrictive functional forms of utilities or ignoring cross-category effects in incidence and quantity decisions leads to incorrect assessments on relative allocation of promotional expenditures across brands. Furthermore, retailers are better off contemporaneously promoting brands across the two categories than promoting them in different periods, and ignoring cross-category effects in quantity decisions leads to the opposite inference, namely, that retailers are better off promoting brands across the two categories in different periods.
1	Online content and products are presented as product networks, in which nodes are product pages linked by hyperlinks. These links are typically algorithmically induced recommendations based on aggregated data. Recently, websites have begun to offer social networks and user-generated links alongside the product network, creating a dual-network structure. The authors investigate the role of this dual-network structure in facilitating content exploration. They analyze YouTube's dual network and show that user pages have unique structural properties and act as content brokers. Next, the authors show that random rewiring of the product network cannot replicate this brokering effect. They present seven Internet studies in which participants browsing a YouTube-based website are exposed to different conditions of recommendations. The first set of studies shows that exposure to the dual network results in a more efficient (time to desirable outcome) and more effective (average product rating, overall satisfaction) exploration process. The next set of studies extends the previous ones to include dynamic structures, in which the network changes as a function of time or in response to participants' satisfaction. Furthermore, the results are replicated using data from another content site.
1	Firms in many industries release new products in sequential stages. They also launch separate advertising campaigns at each distribution stage. Thus, communication mix elements—advertising and word of mouth (WOM)—can play important, distinct, and yet interdependent roles in stimulating new product demand. Their effectiveness may fluctuate within and across stages and spill over from earlier to later stages. Thus, the authors construct a dynamic linear model to study the dynamic effects of advertising and WOM on demand for heterogeneous products across stages. They further apply the model to examine a canonical example, the theater-then-video sequential distribution of motion pictures, and estimate the parameters using Kalman filtering/smoothing and Markov chain Monte Carlo methods. The results show that advertising and WOM exert dynamic, yet diverse, influences on demand for new products. For example, while increased ad spending is more effective at an earlier stage due to repetition wear-in and synergy with WOM, increased WOM activities at a later stage could become more powerful in driving demand. Subsequent optimization exercises suggest that films of varied characteristics can potentially re-allocate their advertising budgets and reap additional revenues.
1	The question of how people should structure goal-directed activity to maximize the likelihood of goal attainment is one of theoretical and practical significance. In particular, should people begin by attempting relatively easy tasks or more difficult ones? How might these differing strategies affect the likelihood of completing the overarching goal? The authors examine this question in the context of an important goal for a large number of consumers—getting out of debt. Using a data set obtained from a debt settlement firm, they find that (1) closing debt accounts is predictive of debt elimination regardless of the dollar balance of the closed accounts, whereas (2) the dollar balance of closed accounts is not predictive of debt elimination when controlling for the fraction of accounts closed. These findings suggest that completing discrete subtasks might motivate consumers to persist in pursuit of a goal. The authors discuss implications for goal pursuit generally and for consumer debt management specifically.
1	Out-of-stock (OOS) is commonly observed in the retail environment with consumer packaged goods, but there have been few empirical studies of the effects of OOS on consumer product choice, because there is a lack of OOS information during households' purchase occasions. The authors study the effects of OOS on consumer stockkeeping unit (SKU) preference and price sensitivity, using a unique data set from multiple consumer packaged goods categories with information on recurring OOS incidents. They obtain several substantive findings: (1) Consumers' price sensitivity tends to be underestimated when OOS is not accounted for in a discrete choice model; (2) for consumers who have shorter interpurchase time, their preference for a SKU is attenuated when it is frequently OOS; and (3) for consumers who purchase from a small number of SKUs, their preference for a SKU is reinforced when facing OOS of other similar SKUs, whereas it is attenuated when facing OOS of other similar and also frequently purchased SKUs. The authors also illustrate that their findings can help retailers evaluate the effect of OOS on category revenue and predict time-varying market shares of SKUs in periods following OOS incidents.
1	Trendspotting has become an important marketing intelligence tool for identifying and tracking general tendencies in consumer interest and behavior. Currently, trendspotting is done either qualitatively by trend hunters, who comb through everyday life in search of signs indicating major shifts in consumer needs and wants, or quantitatively by analysts, who monitor individual indicators, such as how many times a keyword has been searched, blogged, or tweeted online. In this study, the authors demonstrate how the latter can be improved by uncovering common trajectories hidden behind the coevolution of a large array of indicators. The authors propose a structural dynamic factor-analytic model that can be applied for simultaneously analyzing tens or even hundreds of time series, distilling them into a few key latent dynamic factors that isolate seasonal cyclic movements from nonseasonal, nonstationary trend lines. The authors demonstrate this novel multivariate approach to quantitative trendspotting in one application involving a promising new source of marketing intelligence—online keyword search data from Google Insights for Search—in which they analyze search volume patterns across 38 major makes of light vehicles over an 81-month period to uncover key common trends in consumer vehicle shopping interest.
1	Does the number of funds offered in a defined contribution plan affect how many funds consumers choose to invest in or how they spread dollars across the funds they choose? Across three experiments and the analysis of defined contribution plan data, the authors explore these issues by examining investors' tendency to engage in the 1/n heuristic— that is, allocating their dollars evenly across all available investment options. The authors decompose this heuristic into its two underlying behavioral dimensions: the tendency to invest in all available funds (which they label “1/n#”) and the tendency to spread the invested dollars evenly across chosen funds (which they label “1/n$”). The authors argue that choosing from larger fund assortments taxes investors' cognitive resources, which leads to more simplified diversification strategies. They find that increasing the fund assortment size decreases the tendency to invest in all available funds (1/n#) but increases the tendency to spread the invested dollars evenly among the chosen alternatives (1/n$), provided that the number of funds chosen for investment allows for easy equal dollar allocations. The authors integrate their results with prior research regarding asset choice and allocation heuristics.
1	Previous research on word of mouth (WOM) has presented inconsistent evidence on whether consumers are more inclined to share positive or negative information about products and services. Some findings suggest that consumers are more inclined to engage in positive WOM, whereas others suggest that consumers are more inclined to engage in negative WOM. The present research offers a theoretical perspective that provides a means to resolve these seemingly contradictory findings. Specifically, the authors compare the generation of WOM (i.e., consumers sharing information about their own experiences) with the transmission of WOM (i.e., consumers passing on information about experiences they heard occurred to others). They suggest that a basic human motive to self-enhance leads consumers to generate positive WOM (i.e., share information about their own positive consumption experiences) but transmit negative WOM (i.e., pass on information they heard about others' negative consumption experiences). The authors present evidence for self-enhancement motives playing out in opposite ways for WOM generation versus WOM transmission across four experiments.
1	This study investigates the role of supervisors in implementing changes in marketing strategy. The authors propose that perceptions of outcome-oriented supervisory actions influence salespeople's primary appraisals of a strategic change (i.e., whether the change will affect them) and that perceptions of process-oriented supervisory actions influence salespeople's secondary appraisals (i.e., whether they can cope with the impact of the change on them). The results from a study of 828 salespeople in 204 branches of a large distributor of industrial goods provide evidence that perceived outcome risk containment and outcome reward emphasis enhance primary appraisals, whereas perceived process risk containment and process reward emphasis enhance secondary appraisals. In turn, the authors find that salespeople's primary and secondary appraisals influence their change implementation behaviors, leading to successful change implementation. Notably, they also find that (outcome and process) risk containment has a greater influence on appraisals of salespeople with a higher performance orientation, but the effects of (outcome and process) reward emphases are invariant across salespeople's performance orientation. The findings suggest that successful implementation of strategic change may depend not merely or even primarily on giving rewards to salespeople for implementing change but also on limiting salespeople's risks and recognizing them for their change-related efforts.
1	Prior research has identified several factors that influence brand extension evaluations. Extending this research, the authors suggest that external, situational factors can have an important influence on brand extension evaluations. This research focuses on mating mind-sets (i.e., thinking about a mate), which consumers commonly experience. Specifically, the authors propose that mating mind-sets triggered by the external situation can influence brand extension evaluations, particularly for men. Mating mind-sets induce male consumers (but not female consumers) to engage in relational processing, increasing fit perceptions and evaluations for moderately dissimilar brand extensions. These differences are more likely to emerge when a short-term mating mind-set is primed (vs. a long-term mating mind-set). Furthermore, using prestige brands (vs. functional brands) reduces the gap between men and women. In addition, subbrand architecture (vs. direct brand architecture) boosts the evaluations of female consumers but decreases those of male consumers. The authors find that the effects of mating mind-sets on brand extension evaluation are driven by male consumers' need to express creativity.
1	In an effort to establish and enhance the accuracy of key informant data, organizational survey studies are increasingly relying on triangulation techniques by including supplemental data sources that complement information acquired from key informants. Despite the growing popularity of triangulation, little guidance exists as to when and how it should be conducted. Addressing this gap, the authors develop hypotheses linking a comprehensive set of study characteristics at the construct, informant, organizational, and industry levels to key informant accuracy. Two studies test these hypotheses. The first study is a meta-analysis of triangulation applications. Using data from 127 studies published in six major marketing and management journals, the authors identify antecedents to key informant reliability. The second study, using eight multi-informant data sets, analyzes antecedents to key informant validity. The results from these studies inform survey researchers as to which conditions particularly call for the use of triangulation. The authors conclude by offering guidelines on when and how to employ triangulation techniques.
1	Consumers value status goods because of the impression status-product ownership makes on other consumers, and this impression depends on the actual distribution of ownership in population. Explicitly modeling consumer value of status products as coming from the information the product ownership conveys to other consumers, the authors show that a status-product manufacturer can benefit from a competitor's cost reduction because of the competitor's price reduction associated with it. In other words, they show that two status products that are (imperfect) substitutes in the consumer utility function may be complements in the profit function. As a consequence, competition could lead to higher prices than the optimal ones under monopoly ownership of both products. The authors confirm the assumptions that consumer value of a status good depends positively on the proportion of desirable type among owners and negatively on the proportion of the desirable type among nonowners in one experiment. Moreover, they find empirical support for the positive effect of a price reduction of one product on the demand for the other product from another experiment.
1	Marketers distinguish three types of media: paid (e.g., advertising), owned (e.g., company website), and earned (e.g., publicity). The effects of paid media on sales have been extensively covered in the marketing literature. The effects of earned media, however, have received limited attention. The authors examine how two types of earned media, traditional (e.g., publicity and press mentions) and social (e.g., blog and online community posts), affect sales and activity in each other. They analyze 14 months of daily sales and media activity data from a microlending marketplace website using a multivariate autoregressive time-series model. They find that (1) both traditional and social earned media affect sales; (2) the per-event sales impact of traditional earned media activity is larger than for social earned media; (3) because of the greater frequency of social earned media activity, after adjusting for event frequency, social earned media's sales elasticity is significantly greater than traditional earned media's; and (4) social earned media appears to play an important role in driving traditional earned media activity.
1	The authors study the role of reference price in a setting in which both the price and the quantity are set through personal interaction during the transaction process, such as in business-to-business markets. Most studies on reference price in the marketing research literature focus on consumer packaged goods, for which prices are typically fixed during the shopping trip and the transaction does not involve personal interaction with a salesperson. In this study, the authors study the effect of reference price on the quantity purchased and also on the pricing outcome of the transaction. They estimate a simultaneous equation system of both pricing and quantity purchased. The findings are as follows: (1) Reference price effects exist on quantity purchased and on the transaction pricing outcome in business-to-business market transactions, (2) business customers react asymmetrically to price increases and price decreases, and (3) salespeople have their own reference prices that affect the transaction price. The authors also find that customer experience with the salesperson might exacerbate the loss aversion effect. They conclude by discussing the underlying reasons behind these findings and their managerial implications.
1	Despite the powerful technologies that have enabled the assembly of transactional databases and the processing of information about individual customers and their buying patterns, database marketers have been limited by what is known as the “incomplete information problem”—that is, marketers have incomplete information about a customer's behavior in the product category of interest. A company's transactional database can only be built from customers' transactions with that company. Any transactions that have been made with other companies are missing. The authors present a modeling approach designed to solve the incomplete information problem. They use transactions conducted with a single supplier in that category to infer consumers' behavior with other suppliers in that category. In particular, armed with prior knowledge of the parametric form of consumer interpurchase time distributions, they uncover elements of the stochastic process that dictates which supplier a consumer chooses on a particular purchase occasion. The authors focus on interpurchase times because they form the core of the incomplete information problem. If a company uses its observed interpurchase times to estimate interpurchase times in the category for a specific consumer, its estimate will be biased upward. Combined with the model developed in this study, a familiar analysis of transaction profitability can be used to build a new type of lifetime value: lifetime category value of the customer.
1	Previous research in complaining behavior has focused on product or service failures in which the organization is responsible for the failure. In these cases, researchers have found that consumers feel better about the product failure after complaining about it. In contrast, the authors show that when consumers are to blame for product failure, complaining has a detrimental effect on consumer reactions to the product. In this context, self-threat from the product failure is shown to motivate defensive processing in both the content of complaints and the subsequent downstream product evaluations. The authors establish the role of self-threat in product failure in two ways: (1) by varying the intensity of the threat from product failure and (2) through mitigating negative outcomes through self-affirmation. The article concludes with discussion on how these findings can benefit companies and where this research could seed opportunities for subsequent investigation.
1	Firms are increasingly outsourcing new product development (NPD), yet little is known about the financial performance implications of this decision. An empirical test shows that there is considerable variation in the performance implications of NPD outsourcing. The authors develop a contingency framework to explain when taking a minority equity participation in the outsourcing provider versus selecting a provider to whom the outsourcing firm has outsourced NPD in the past (i.e., prior tie selection) may increase the outsourcing firm's performance. They find that the superior governance mechanism depends on two forms of uncertainty: technological uncertainty and cultural uncertainty.
1	The common finding that selling prices exceed buying prices (the so-called endowment effect) is typically explained by the assumptions that consumers evaluate potential transactions with respect to their current holdings and that the owners of a good regard its potential loss to be more significant than nonowners regard its potential acquisition. In contrast to this “pain-of-losing” account, the authors propose that the endowment effect reflects a reluctance to trade on terms that appear unfavorable with respect to salient reference prices. In six experiments (and eight more summarized in appendixes), the authors show that manipulations that reduce the gap between valuations and reference prices reduce or eliminate the endowment effect. These results suggest that the endowment effect is often best construed as an aversion to bad deals rather than an aversion to losing possessions.
1	Existing theory and prior research suggest that consumers perceive purchase prices more/less favorably when they are preceded by higher/lower prices. However, to date, researchers have found these effects in contexts in which the product, and thus perceived quality, is held constant. Given that consumers commonly believe price and quality are positively correlated and that price–quality perceptions have been shown to influence price evaluations and willingness to pay, the generalizability of existing research to commonly encountered contexts is questionable. In this research, the authors examine the influence of price order on consumer choice across differing brands in contexts in which consumer quality perceptions are free to covary with price and they are manipulated to be correlated or uncorrelated with price. Using reference dependence theory as a framework, they find that when differing brand options are presented in descending price order, consumers tend to choose higher-price options; when they are presented in ascending price order, consumers tend to choose lower-priced options (the price order effect). In addition, the authors show that consumers' price–quality perceptions are a necessary condition for this effect.
1	The location in which a product is presented can influence consumers' numerical estimates of product attributes (e.g., price). This effect can be driven by two alternative processes. First, people may have acquired a learned association between numerical magnitude and location as a consequence of frequently viewing larger numbers to the right of smaller ones. In addition, they may have learned a procedure of reading or writing numbers from left to right in increasing order of magnitude. The authors present six experiments that demonstrate the effect of location on numerical estimates and provide evidence that both number–location associations and number–order associations could drive this effect.
1	The authors propose a new Bayesian latent structure regression model with variable selection to solve various commonly encountered marketing problems related to market segmentation and heterogeneity. The proposed procedure simultaneously performs segmentation and regression analysis within the derived segments, in addition to determining the optimal subset of independent variables per derived segment. The authors present comparative analyses contrasting the performance of the proposed methodology against standard latent class regression and traditional Bayesian finite mixture regression. They demonstrate that their proposed Bayesian model compares favorably with these traditional benchmark models. They then present an actual commercial customer satisfaction study performed for an electric utility company in the southeastern United States, in which they examine the heterogeneous drivers of perceived quality. Finally, they discuss limitations of the research and provide several directions for further research.
1	There are important advantages to including reversed items in questionnaires (e.g., control of acquiescence, disruption of nonsubstantive responding, better coverage of the domain of content of a construct), but reversed items can also lead to measurement problems (e.g., low measure reliability, complex factor structures). The authors advocate the continued use of reversed items in measurement instruments but also argue that they should be used with caution. To help researchers improve their scale construction practices, the authors provide a comprehensive review of the literature on reversed and negated items and offer recommendations about their use in questionnaires. The theoretical discussion is supplemented with data on 1330 items from measurement scales that have appeared in Journal of Marketing Research and Journal of Consumer Research.
1	The authors measure the revenue and cost implications to supermarkets of changing their price positioning strategy in oligopolistic downstream retail markets. Their approach formally incorporates the dynamics induced by the repositioning in a model with strategic interaction. They exploit a unique data set containing the price format decisions of all U.S. supermarkets in the 1990s. The data contain the format change decisions of supermarkets in response to a large shock to their local market positions: the entry of Wal-Mart. The authors exploit the responses of retailers to Wal-Mart entry to infer the cost of changing pricing formats using a revealed-preference argument. The interaction between retailers and Wal-Mart in each market is modeled as a dynamic game. The authors find evidence that entry by Wal-Mart had a significant impact on the costs and incidence of switching pricing strategy. Their results add to the marketing literature on the organization of retail markets and have implications for long-term market structure in the supermarket industry. Their approach, which incorporates long-term dynamic consequences, strategic interaction, and sunk investment costs, may be used to empirically model firms’ positioning decisions in marketing more generally.
1	This article proposes a novel approach to assess the dynamic effect that advertising expenditures have on which products consumers include in their choice sets. In a discrete-choice model, consumers face choice sets that evolve according to their awareness of each product. Advertising expenditures have a dynamic effect in the sense that they raise consumer awareness of a product, increasing present and future sales. To estimate this effect, the authors explicitly model the firms’ dynamic advertising decisions and illustrate the model using data from the Spanish automobile market. The results show that the effect of advertising on awareness is dynamic and that accounting for it is crucial in explaining the evolution of product sales over its life cycle. Furthermore, the authors demonstrate that the awareness process can be significantly sped up by advertising. Thus, there is a great heterogeneity in the length of the awareness process among products, depending on the level of advertising expenditures; it may range from one to six years.
1	Advertising nudges consumers along the think–feel–do hierarchy of intermediate effects of advertising to induce sales. Because intermediate effects—cognition, affect, and experience—are unobservable constructs, brand managers use a battery of mind-set metrics to assess how advertising builds brands. However, extant sales response models explain how advertising grows sales but ignore the role of intermediate effects in building brands. To link these dual contributions of advertising, the authors propose an integrated framework that augments the dynamic advertising–sales response model by integrating the hierarchy, dynamic evolution, and purchase reinforcement of intermediate effects. Methodologically, the new approach incorporates the intermediate effects as factors from mind-set metrics while filtering out measurement noise, extracts the factor loadings, estimates the dynamic evolution of the factors, and infers their sequence in any hypothesized hierarchy by embedding their impact in a dynamic advertising–sales response model. The authors apply the proposed model and associated method to a major brand to discover the brand's operating hierarchy (advertising → experience → cognition → affect ↔ sales). The results provide the first empirical evidence that intermediate effects are indeed dynamic constructs, that purchase reinforcement effects exist not only for experience but also for other intermediate effects, and that advertising simultaneously contributes to both sales growth and brand building. Thus, both researchers and managers should consider using the proposed framework to capture advertising's dual contributions of building brands and growing sales.
1	The authors study physicians’ prescription choices when uncertainty about drug efficacy is resolved through two channels: firms’ marketing activities (e.g., detailing) and patients’ experiences with the drugs. They first provide empirical evidence that suggests that the well-understood information incentive for physicians to experiment with new drugs is reduced when physicians anticipate future detailing. Therefore, increased detailing activity triggers opposing forces: Adoption is hastened as physicians become informed (assuming prior knowledge is initially low) and slows as they reduce experimentation and instead obtain information from detailing at no cost. The authors then estimate a dynamic Bayesian learning model that embodies these trade-offs using physician-level data on prescription choices and detailing received in the months surrounding the introduction of two erectile dysfunction drugs, Levitra and Cialis. Detailing elasticities are lower when physicians anticipate changes in detailing activity than when such changes are unexpected. Accordingly, the authors conclude that to maximize the effect of detailing, firms should avoid announcing increases in detailing activities.
1	Because utility/profits, state transitions, and discount rates are confounded in dynamic models, discount rates are typically fixed for the purpose of identification. The authors propose a strategy of identifying discount rates. The identification rests on imputing the utility/profits using decisions made in a context in which the future is inconsequential, the objective function is concave, and the decision space is continuous. They then use these utilities/profits to identify discount rates in contexts in which dynamics become material. The authors exemplify this strategy using a field study in which cell phone users transitioned from a linear to a three-part-tariff pricing plan. They find that the estimated discount rate corresponds to a weekly discount factor (.90), lower than the value typically assumed in empirical research (.995). When using a standard .995 discount factor, they find that the price coefficient is underestimated by 16%. Moreover, the predicted intertemporal substitution pattern and demand elasticities are biased, leading to a 29% deterioration in model fit and suboptimal pricing recommendations that would lower potential revenue gains by 76%.
1	The authors develop a dynamic factor model of brand satiation to explain longitudinal variation in consumer purchases. Factor loadings are associated with a brand's position along a satiation dimension, and factor scores are associated with a household's sensitivity to satiation effects. The authors introduce dynamics by allowing the factor scores to evolve over time, reflecting variation in household satiation sensitivity. They embed the factor model in a direct utility model that allows for both corner and interior solutions and show that it fits the data better than alternative specifications. Analysis of a panel data set of corn chips purchases indicates that respondent satiation is better explained by a low-dimensional factor structure, while baseline utility and preferences are not. The authors explore implications for product line assortment in the face of quickening satiation.
1	New car purchases are among the largest and most expensive purchases consumers ever make. While functional and economic concerns are important, the authors examine whether visual influence also plays a role. Using a hierarchical Bayesian probability model and data on 1.6 million new cars sold over nine years, they examine how visual influence affects purchase volume, focusing on three questions: Are people more likely to buy a new car if others around them have recently done so? Are these effects moderated by visibility, the ease of seeing others’ behavior? Do they vary according to the identity (e.g., gender) of prior purchasers and the identity relevance of vehicle type? The authors perform an extensive set of tests to rule out alternatives to visual influence and find that visual effects are (1) present (one additional purchase for approximately every seven prior purchases), (2) larger in areas where others’ behavior should be more visible (i.e., more people commute in car-visible ways), (3) stronger for prior purchases by men than by women in male-oriented vehicle types, (4) extant only for cars of similar price tiers, and (5) subject to saturation effects.
1	Understanding how emotions can affect pleasure has important implications both for people and for firms’ communication strategies. Prior research has shown that experienced pleasure often assimilates to the valence of one's active emotions, such that negative emotions decrease pleasure. In contrast, the authors demonstrate that the activation of guilt, a negative emotion, enhances the pleasure experienced from hedonic consumption. The authors show that this effect occurs because of a cognitive association between guilt and pleasure, such that activating guilt can automatically activate cognitions related to pleasure. Furthermore, the authors show that this pattern of results is unique to guilt and cannot be explained by a contrast effect that generalizes to other negative emotions. The article concludes with a discussion of the implications of these findings for marketing and consumption behavior.
1	In many service industries, firms introduce three-part tariffs to replace or complement existing two-part tariffs. In contrast with two-part tariffs, three-part tariffs offer allowances, or “free” units of the service. Behavioral research suggests that the attributes of a pricing plan may affect behavior beyond their direct cost implications. Evidence suggests that customers value free units above and beyond what might be expected from the change in their budget constraint. Nonlinear pricing research, however, has not considered such an effect. The authors examine a market in which three-part tariffs were introduced for the first time. They analyze tariff choice and usage behavior for customers who switch from two-part to three-part tariffs. The findings show that switchers significantly “overuse” in comparison with their prior two-part tariff usage. That is, they attain a level of consumption that cannot be explained by a shift in the budget constraint. The authors estimate a discrete/continuous model of tariff choice and usage that accounts for the valuation of free units. The results show that the majority of three-part-tariff users value minutes under a three-part tariff more than they do under a two-part tariff. The authors derive recommendations for how the provider can exploit these insights to further increase revenues.
1	Marketers commonly assume that health claims attached to otherwise unhealthful food stimulate consumption because such claims offer justification for indulgence and reduce guilt. This article proposes a generalized theory of healthful indulgences, identifying when and why people overconsume versus regulate food intake in response to health claims. Four studies demonstrate that not all health claims are created equal. The authors suggest that the nature of the food attributes the claims emphasize—namely, functional versus hedonic—determines the extent of consumption of the indulgence. Health claims featuring functional attributes (e.g., “extra antioxidants”) trigger high levels of health-goal accessibility, which, together with simultaneously accessible indulgence goals attached to the indulgence, results in goal conflict. This conflict leads to reduced consumption of the food. In contrast, health claims featuring hedonic attributes (e.g., “low fat”) render health goals less accessible while accentuating the pleasure dimension of the food, resulting in lower goal conflict and increased consumption of the food. Implications for the food industry and public policy makers are discussed.
1	Companies increasingly employ cause-related marketing to enhance customer goodwill and improve their image. However, because these efforts have major implications for pricing strategy and firm profitability, understanding the relationship between the company's donation amount and customers’ willingness to pay is important. In particular, little is known about the moderating effects that influence this relationship or their underlying mechanisms. Study 1 confirms that two types of customer predispositions moderate the link between donation amount and willingness to pay: donation-related and cause-related predispositions. Three additional studies focus on the negative moderating effect of company–cause fit and provide insights into the underlying moderation process. Specifically, the motives customers attribute to the company mediate the moderating impact of fit on the donation amount–WTP link (Study 2), which occurs particularly in cases of utilitarian (Study 3) and privately consumed products (Study 4).
1	This article presents three studies on how the negative emotions of guilt and shame differentially influence the effectiveness of health messages framed as gains or losses. Guilt appeals are more effective when paired with gain frames, whereas shame appeals are more effective when paired with loss frames. These framing effects occur because gain frames facilitate the use of problem-focused coping strategies favored by guilt, whereas loss frames facilitate the use of emotion-focused coping strategies favored by shame. Frames that fit with the emotion facilitate the activation of coping strategies consistent with that emotion and consequently lead to greater fluency and message effectiveness. These effects manifest on intentions to binge drink and time spent viewing alcohol advertising.
1	Products can be described by different numbers of attributes, but can the mere number of attributes presented across a choice set influence what type of options people choose? This article demonstrates that attribute numerosity tends to benefit certain types of options more than others and consequently has systematic effects on choice. Because attributes often serve as a heuristic cue for product usefulness, they benefit options that people perceive as relatively inferior on this dimension. Consistent with this perspective, five studies demonstrate that attribute numerosity benefits hedonic more than utilitarian options by increasing the extent to which the former appear useful. Consequently, increasing attribute quantity equally across the choice set shifts choice toward hedonic options, regardless of whether the attributes are hedonic, utilitarian, or mixed in nature. Consistent with this conceptualization, these effects become amplified when decision makers engage in heuristic processing and when priming makes usefulness salient. The findings have important implications for how marketers present attribute information, for public policy and consumer welfare, and for understanding argument numerosity effects in persuasion more broadly.
1	Many firms attempt to enhance experience consumption by facilitating the consumption outcome (i.e., the end state achieved, such as the final score of a basketball game) and the consumption process (i.e., the course through which the end is achieved, such as how the game is played). The authors propose that the roles of outcome and process in the evaluation of experience consumption are dependent not only on consumers’ role in the experience (participant vs. spectator) but also on their self-construal (independent vs. interdependent). As a spectator (e.g., watching a game), independents’ (vs. interdependents') experience consumption evaluations are more likely to be influenced by outcome, while interdependent (vs. independent) consumers are more likely to be affected by process. The reverse is true when consumers assume the role of a participant in the experience (e.g., playing a game). The authors’ theorizing is supported across three studies.
1	This article introduces a new determinant of brand extension success, brand extension authenticity (BEA), as a complement to fit. The authors develop the BEA construct and a scale to measure it and then demonstrate that BEA captures consumer perceptions of brand extension legitimacy and cultural contiguity along four interrelated but distinct dimensions: maintaining brand standards and style, honoring brand heritage, preserving brand essence, and avoiding brand exploitation. They demonstrate the power of BEA in predicting consumer reactions to brand extensions, particularly among consumers with strong self–brand connections. Not only is BEA distinct from two conceptualizations of fit in brand extension literature—fit as similarity and fit as relevance—but it also moderates the effects of both fit dimensions on brand extension responses. By capturing a cultural and consumer relational perspective that shapes reactions to brand extensions, BEA provides an important, complementary construct for predicting brand extension success and enhancing brand value. Brand managers attentive to BEA may be able to stretch brands further than assessments of fit alone would suggest, but they risk failure in otherwise well-fitting extensions perceived as inauthentic.
1	Across five studies, the authors demonstrate that warm (vs. cool) temperatures deplete resources, increase System 1 processing, and influence performance on complex choice tasks. Real-world lottery data (pilot study) and a lab experiment (Study 1) demonstrate the effect of temperature on complex choices: People are less likely to make difficult gambles in warmer temperatures. Study 2 implicates resource depletion as the underlying process; warm temperatures lower cognitive performance for nondepleted people but do not affect the performance of depleted people. Study 3 illustrates the moderating role of task complexity to show that warm temperatures are depleting and decrease willingness to make a difficult product choice. Study 4 juxtaposes the effects of depletion and temperature to reveal that warm temperatures hamper performance on complex tasks because of the participants’ increased reliance on System 1 (heuristic) processing.
1	Whereas most existing self-control research and scales focus on singular self-control choice, the current work examines sequential self-control behavior. Specifically, this research focuses on behavior following initial self-control failure, identifying a set of key cognitive and emotional responses to initial failure that jointly underlie post-failure behavior. The tendency to experience these responses is captured in a new scale, the Response-to-Failure scale, which the authors develop and test in three consumer domains: eating, spending, and cheating. The results support the use of the same emotional and cognitive factors to predict post-failure behavior across these three domains, providing evidence of the generalizability of the scale structure. The data support the scale's structure, nomological and discriminant validity, and test–retest reliability across five studies. In five additional studies, the scale's predictive validity is demonstrated beyond other existing relevant scales. The authors also develop and test a short form of each domain scale. Finally, the authors discuss the implications for understanding post-failure behavior and suggest practical uses for the scale.
1	How does price sensitivity change with the macroeconomic environment? The authors explore this question by measuring price elasticity using household-level data across 19 grocery categories over 24 quarters. For each category, they estimate a separate random coefficients logit model with quarter-specific price response parameters and control functions to address endogeneity. This specification yields a novel set of 456 elasticities across categories and time that are generated using the same method and therefore can be directly compared. On average, price sensitivity is countercyclical: It rises when the macroeconomy weakens. However, substantial variation exists, and a handful of categories exhibit procyclical price sensitivity. The authors show that the relationship between price sensitivity and macroeconomic growth correlates strongly with the average level of price sensitivity in a category. They examine several explanations for this result and conclude that a category's share of wallet is the more likely driver versus alternative explanations based on product perishability, substitution across consumption channels, or market power.
1	Consumers' time allocation decisions among various activities are fundamental to marketing research and consumer behavior. The authors construct a dynamic panel data model to examine how consumers allocate time to a portfolio of leisure activities over time. The data comprise a longitudinal panel in which the authors tracked 287 U.S. consumers' time use, consumption motives, and expertise measures on a weekly basis from January to June 2011. This is the first empirical research to examine the underlying mechanisms that guide the dynamics of an individual's activity consumption. The authors demonstrate that expertise contributes to the perceived benefits of an activity, which in turn leads to high value associated with it. Expertise also directly influences value obtained from an activity. This expertise, in turn, is acquired over time through past consumption. This finding implies a chain from expertise to value to time use and back to expertise, which may lead consumers to form a lifestyle in which they specialize in a subset of activities they know well. Consequently, expertise can be regarded as a key variable that explains lifestyle choices.
1	The authors investigate whether consumers systematically consider feature usage before making multifunctional product purchase decisions. Across five studies and four product domains, the article shows that consumers fail to estimate their feature usage rate before purchasing multifunctional products, negatively affecting product satisfaction. The findings demonstrate that when consumers do estimate their feature usage before choice, preferences shift from many-feature products toward few-feature products. The authors show that this shift in preferences is due to a change in elaboration from having features to using features, and they identify three key moderators to the effect: need for cognition, feature trivialness, and materialism. Finally, the authors investigate the downstream consequences of usage estimation on product satisfaction, demonstrating that consumers who estimate usage before choice experience greater product satisfaction and are more likely to recommend their chosen product. These results point to the relative importance consumers place on having versus using product features.
1	Savvy consumers attribute a product's market performance to its intrinsic quality as well as the seller's marketing push. The authors study how sellers should optimize their marketing decisions in response. They find that a seller can benefit from “demarketing” its product, meaning visibly toning down its marketing efforts. Demarketing lowers expected sales ex ante but improves product quality image ex post, as consumers attribute good sales to superior quality and lackluster sales to insufficient marketing. The authors derive conditions under which demarketing can be a recommendable business strategy. A series of experiments confirm these predictions.
1	New empirical models of consumer demand that incorporate social effects seek to measure the causal effect of past adopter's behavior—the “installed-base”—on current adoption behavior. Identifying such causal effects is challenging due to several alternative confounds that generate correlation in agents' actions. In the absence of experimental variation, a preferred solution has been to control for these spurious correlations using a rich specification of fixed effects. The authors show that fixed-effects estimators of this sort are inconsistent in the presence of installed-base effects; in simulations, random-effects specifications perform even worse. The analysis reveals the tension the applied empiricist faces in this area: a rich control for unobservables increases the credibility of the reported causal effects, but the incorporation of these controls introduces biases of a new kind in this class of models. The authors present two solutions: a modified version of an instrumental variable approach and a new bias-correction approach, both of which deliver consistent estimates of causal installed-base effects. The empirical application to the adoption of the Toyota Prius Hybrid in California shows evidence for social influence in diffusion and reveals that implementing the bias correction reverses the sign of the measured installed-base effect. The authors also discuss implications of the results for identification of models in marketing involving state dependence in demand, and incorporating discrete games of strategic interaction.
1	With the growing popularity of online social networks, it is becoming more important for marketing researchers to understand and measure social intercorrelations among consumers. The authors show that the estimation of consumers' social intercorrelations can be significantly affected by the sampling method used in the study and the topology of the social network. Through a series of simulation studies using a spatial model, the authors find that the magnitude of social intercorrelations in consumer networks tends to be underestimated if samples of the networks are used (rather than using the entire population of the network). The authors further demonstrate that sampling methods that better preserve the network structure perform best in recovering the social intercorrelations. However, this advantage decreases in networks characterized by the scale-free power-law distribution for the number of connections of each member. The authors discuss the insights they glean from these findings and propose a method to obtain unbiased estimation of the magnitude of social intercorrelations.
1	The authors study how a person's evaluation of choice options influences his or her estimates of other people's evaluations when their choices are known. The study shows that people rely on the relationship between their own evaluations and their final decision to make sense of others, projecting their evaluations of the corresponding options. A person's liking of the option he or she chose between two alternatives influences the person's estimates of others' liking of the option they chose, regardless of whether it matches his or her own choice. Likewise, a person's evaluation of the rejected option affects his or her estimate of others' evaluations of the option they rejected. Across four studies, the authors provide evidence of conditional projection in political and consumer decisions, using across-people differences in ratings of choice options, within-person changes in ratings, and manipulated differences in participants' ratings. The authors also demonstrate that existing accounts of projection do not directly predict these findings and rule out other alternative explanations.
1	Many firms strive to create relationships with customers, but not all customers are motivated to build close commercial relationships. This article introduces a theoretical framework that explains how relationship-specific attachment styles account for customers' distinct preferences for closeness and how both attachment styles and preferences for closeness influence loyalty. The authors test their predictions with survey data from 1199 insurance customers and three years of purchase records for 975 of these customers. They find that attachment styles predict customers' preferences for closeness better than established marketing variables do. Moreover, attachment styles and preferences for closeness influence loyalty intentions and behavior, controlling for established antecedents (e.g., relationship quality). Finally, exploring the underlying process, the authors show that preference for closeness partially mediates the effect of attachment styles on cross-buying behavior. This research provides novel customer segmentation criteria and actionable guidelines that managers can use to improve their ability to tailor relationship marketing activities and more effectively allocate resources to match customer preferences.
1	For two reasons, marketers face significant challenges in measuring return on marketing investment in business-to-business (B2B) markets. First, buyers often have irregular purchase patterns, as the authors observe in the high-tech industry. Second, marketing efforts take considerable time to build a relationship with a customer. The authors attempt to precisely recover hidden buyer–seller relationship states to capture the effect of marketing contacts in B2B markets. The authors build a comprehensive hierarchical Bayesian bivariate Tobit hidden Markov model to assess the return on marketing in B2B markets. They use a recursive computing method—a forward–backward Gibbs sampler method—to retrieve the relationship states. The results suggest that marketing contacts have a heterogeneous long- and short-term impact on customers' purchasing behavior through changes in the buyer–seller relationship states. This study provides practical value to business marketers to measure the return on marketing investment in buyer–seller relationships.
1	In word-of-mouth seeding programs, customer word of mouth can generate value through market expansion; in other words, it can gain customers who would not otherwise have bought the product. Alternatively, word of mouth can generate value by accelerating the purchases of customers who would have purchased anyway. This article presents the first investigation exploring how acceleration and expansion combine to generate value in a word-of-mouth seeding program for a new product. The authors define a program's “social value” as the global change, over the entire social system, in customer equity that can be attributed to the word-of-mouth program participants. They compute programs’ social value in various scenarios using an agent-based simulation model and empirical connectivity data on 12 social networks in various markets as input to the simulation. The authors show how expansion and acceleration integrate to create programs’ social value and illustrate how the role of each is affected by factors such as competition, program targeting, profit decline, and retention. These results have substantial implications for the design and evaluation of word-of-mouth marketing programs and of the profit impact of word of mouth in general.
1	Firms are under increasing pressure to justify their marketing expenditures. This evolution toward greater accountability is reinforced in harsh economic times when marketing budgets are among the first to be reconsidered. To make such decisions, managers must know whether, and to what extent, marketing's effectiveness varies with the economic tide; however, surprisingly little research addresses this issue. Therefore, the authors conduct a systematic investigation of the business cycle's impact on the effectiveness of two important marketing instruments: price and advertising. To do so, they estimate time-varying short- and long-term advertising and price elasticities for 150 brands across 36 consumer packaged goods categories, using 18 years of monthly U.K. data from 1993 to 2010. The long-term price sensitivity tends to decrease during economic expansions, whereas long-term advertising elasticities increase. During contractions, the long-term own and cross price elasticities increase. Moreover, throughout the observation period, the short-term price elasticity became significantly stronger. Finally, patterns differ across categories and brands, which presents opportunities for firms that know how to ride the economic tide.
1	Different framing of the same duration (one year, 12 months, 365 days) can influence consumers’ impressions of subjective duration, thereby affecting their judgments and decisions. The authors propose that, ironically, self-relevance amplifies this duration framing effect. Consumers for whom a particular self-improvement domain is personally relevant are less likely to adopt a one-year self-improvement plan as compared with a 12-month plan because they perceive it as longer and more difficult. This bias is more likely to manifest in consumers who report that the task is highly personally relevant to them, who are making predictions for themselves (vs. others), and who have high (vs. low) task involvement. Personal relevance amplifies this effect because it prompts process-focused simulation of the plan, consequently increasing susceptibility to spurious duration and difficulty cues embedded in frames.
1	How do consumers’ needs and motivations influence their perceptions of external objects? For example, do hungry people perceive a cake to be larger or smaller than do satiated people? According to the New Look psychology literature, the answer is invariably “larger.” However, in this article, the authors demonstrate that the answer is more complex depending on whether the object belongs to the perceiver. If the cake does not belong to the perceiver, she will perceive it to be larger if she is hungry than if she is satiated. In contrast, if the cake already belongs to her, she will perceive it to be smaller if she is hungry than if she is satiated. The authors propose a two-process (wishful thinking vs. worryful thinking) hypothesis to explain the finding and discuss its theoretical and marketing implications.
1	In this research, the authors develop a theory addressing why people act opportunistically when the stakes (i.e., payoffs) are low. Transaction cost theory suggests that opportunistic behavior is more likely under high-stakes conditions. The authors identify rapport as an important moderator of this relationship. Through a series of three studies, they find that high-stakes opportunism appears to occur only when rapport is low. In contrast, when rapport is high, this relationship reverses, such that opportunism is actually more likely when the stakes are low than when they are high. The authors attribute these findings to differences in reasoning and find that when rapport is high and the stakes are low, people are better able to justify their actions by employing morally malleable reasoning. Thus, this research offers insights into an important form of opportunism that has been largely absent from transaction cost theory.
1	This research examines the effect of processing fluency on judgments of agent competence. In the context of service relationships, four studies reveal that the experience of processing difficulty, or disfluency, enhances expectations of agent-exerted effort and competence, which in turn increase expected service value. When reading information about a target service, consumers interpret the difficulty of processing information as a signal of the level of skill required to execute the task, which highlights the agent's expected utility. The authors explore several moderators of this positive effect of disfluency, showing that it is attenuated under conditions that decrease the relevance of consumers’ subjective experiences and it may be reversed on measures of experienced (vs. expected) service value.
1	Why do bidders in buyer-determined procurement auctions often bid above the lowest observed bid over the course of the auction? Are such bidding patterns meaningful? In this research, the authors propose that because bidders are differentiated in their value to the buyer and competition in these auctions is anonymous, bidders infer their potential quality advantage or disadvantage through their observation of competitive bids and incorporate this information into their responses and price bids. Using point-by-point bid data from two industrial procurement auctions, the authors show that bidders appear to be making inferences about their own implied quality differentials and adjust their bidding strategies and bidding aggression accordingly. Specifically, they find that high-quality bidders tend to be more aggressive in bidding against potentially higher-quality competition and less aggressive when bidding against potentially lower-quality competition. In contrast, low-quality bidders appear aggressive regardless of their implied quality in relation to the competition. The authors conclude with a discussion of implications for management and auction design.
1	Service innovativeness, or the propensity to introduce service innovations to satisfy customers and improve firm value at acceptable risk, has become a critical organizational capability. Service innovations are enabled primarily by the Internet or people, corresponding to two types of innovativeness: e- and p-innovativeness. The authors examine the determinants of service innovativeness and its interrelationships with firm-level customer satisfaction, firm value, and firm risk and investigate the differences between e- and p-innovativeness in these relationships. They develop a conceptual model and estimate a system of equations on a unique panel data set of 1049 innovations over five years, using zero-inflated negative binomial regression and seemingly unrelated regression approaches. The results reveal important asymmetries between e- and p-innovativeness. Whereas e-innovativeness has a positive and significant direct effect on firm value, p-innovativeness has an overall significantly positive effect on firm value through its positive effect on customer satisfaction but only in human-dominated industries. Both e- and p-innovativeness are positively associated with idiosyncratic risk, but customer satisfaction partially mediates this relationship for p-innovativeness to lower this risk in human-dominated industries. The findings suggest that firms should nurture e-innovativeness in most industries and p-innovativeness in human-dominated industries.
1	It is common for researchers discovering a significant interaction of a measured variable X with a manipulated variable Z to examine simple effects of Z at different levels of X. These “spotlight” tests are often misunderstood even in the simplest cases, and it appears that consumer researchers are unsure how to extend them to more complex designs. The authors explain the general principles of spotlight tests, show that they rely on familiar regression techniques, and provide a tutorial demonstrating how to apply these tests across an array of experimental designs. Rather than following the common practice of reporting spotlight tests at one standard deviation above and below the mean of X, it is recommended that when X has focal values, researchers should report spotlight tests at those focal values. When X does not have focal values, it is recommended that researchers report ranges of significance using a version of Johnson and Neyman's test the authors term a “floodlight.”
1	The Bayesian truth serum (BTS) is a survey scoring method that creates truth-telling incentives for respondents answering multiple-choice questions about intrinsically private matters, such as opinions, tastes, and behavior. The authors test BTS in several studies, primarily using recognition questionnaires that present items such as brand names and scientific terms. One-third of the items were nonexistent foils. The BTS mechanism, which mathematically rewards “surprisingly common” answers, both rewarded truth telling, by heavily penalizing foil recognition, and induced truth telling, in that participants who were paid according to their BTS scores claimed to recognize fewer foils than control groups, even when given competing incentives to exaggerate. Survey takers who received BTS-based payments without explanation became less likely to recognize foils as they progressed through the survey, suggesting that they learned to respond to BTS incentives despite the absence of guidance. The mechanism also outperformed the solemn oath, a competing truth-inducement mechanism. Finally, when applied to judgments about contributing to a public good, BTS eliminated the bias common in contingent valuation elicitations.
1	The authors propose that attempts to increase consumers’ objective knowledge (OK) regarding financial instruments can deter willingness to invest when such attempts diminish consumers’ subjective knowledge (SK). In four studies, the authors use different SK manipulations and investment products to show that investment decisions are influenced by SK, independent of OK. Specifically, they find that (1) willingness to pursue a risky investment increases when SK is high (vs. low) relative to a prior investment choice (Study 1); (2) willingness to enroll in a retirement saving program is enhanced by asking consumers an easy (vs. difficult) question about finance, thereby increasing SK (Study 2); (3) technically elaborating information about a mutual fund diminishes SK regarding that investment and decreases choice of that fund (Study 3); and (4) consumers invest less money in funds when missing information is made salient, holding the objective investment information constant (Study 4). Furthermore, the effects in Studies 2–4 are mediated by participants’ self-rated SK. The authors propose that effective financial education must focus not only on imparting relevant information and enhancing OK but also on promoting higher levels of SK.
1	Quota-based bonuses and commissions are the two most common incentive compensation plans. The authors uncover differential effects of these plans from a natural field-based experiment featuring 14,000 monthly observations over three years from 458 sales territories of a pharmaceutical firm that switched from a bonus plan to an equivalent commission plan. The intervention led to significant sales productivity improvement; this effect was heterogeneous across ability deciles, with much larger increases occurring at lower ability deciles. The authors find significant differences across these plans on (1) effort against nonincentivized tasks and (2) output fluctuations induced through “timing games.” At this firm, the bonus plan was strictly inferior to the implemented commission plan with respect to short-term revenues and timing games. In contrast, the commission plan induced greater neglect of nonincentivized tasks (tasks not directly affecting observable output). To organize their findings, the authors build a simple theoretical model in the personnel economics tradition. The novel result that multitasking concerns are reduced under bonus plans when the quota has been met provides a nuanced rationale for the widespread existence of lump-sum bonus plans.
1	Little empirical consumer research has focused on the decoding of conspicuous symbolism, that is, the inferences consumers make about others’ conspicuous consumption. Grounded in theory on social perception and role congruity, four experiments show that consumer inferences about and behavioral intentions toward conspicuous sellers are moderated by communal and exchange relationship norms. Specifically, conspicuous consumption by a seller decreases warmth inferences and, in turn, behavioral intentions toward the seller under the communal norm; conversely, it increases competence inferences and, in turn, behavioral intentions under the exchange norm. A seller's mere wealth triggers similar inferences, suggesting that conspicuous consumption is a surrogate for actual wealth. Priming consumers with persuasion knowledge inhibits the inferential benefits resulting from conspicuousness under the exchange norm. These findings reveal the theoretically meaningful role of the consumption context by showing that consumers’ warmth and competence inferences operate differentially in commercial relationships as a result of salient communal versus exchange norms, with important consequences for consumers’ behavioral intentions.
1	As firms collect greater amounts of data about their customers from an ever broader set of “touchpoints,” a new set of methodological challenges arises. Companies often collect data from these various platforms at differing levels of aggregation, and it is not clear how to merge these data sources to draw meaningful inferences about customer-level behavior patterns. In this article, the authors provide a method that firms can use, based on readily available data, to gauge and monitor multiplatform media usage. The key innovation in the method is a Bayesian data-fusion approach that enables researchers to combine individual-level usage data (readily available for most digital platforms) with aggregated data on usage over time (typically available for traditional platforms). This method enables the authors to disentangle the intraday correlations between platforms (i.e., the usage of one platform vs. another on a given day) from longer-term correlations across users (i.e., heavy/light usage of multiple platforms over time). The authors conclude with a discussion of how this method can be used in a variety of marketing contexts for which data have become readily available, such as gauging the interplay between online and brick-and-mortar purchasing behavior.
1	New brand extensions can push a brand outside its typical boundaries. In this article, the authors argue that people's acceptance of such extensions depends on their feelings of control. Across several studies, the authors demonstrate that when feelings of personal control are low, consumers and managers seek greater structure in brands and thus reject brand extensions that do not seem to fit well with the parent brand. The authors also identify important boundary conditions that illustrate when consumers are most likely to punish a brand for poor-fitting brand extensions and how the effect can be mitigated.
1	To manage marketing channels, subsidiaries of multinational corporations (MNCs) must balance mandates from headquarters (HQ) with the local realities of the foreign markets. The performance implications of subsidiary–distributor relationship efforts thus are contingent on the HQ–subsidiary relationship. Drawing on marketing channels, economics, and organization theory literature streams, the authors (1) describe the complex performance properties of output and process control mechanisms that MNC subsidiaries deploy to manage foreign distributors and (2) conceptualize the HQ–subsidiary nexus along three attributes that should moderate the performance effects of control mechanisms: task coordination, or HQ's central coordination of processes across subsidiaries; subsidiary decision involvement, or two-way communications and consensual decision making between HQ and the subsidiary; and relational disharmony, or the extent of the HQ–subsidiary conflict. The authors test the hypotheses using field data from German and Japanese MNCs in the United States and Bayesian models that account for measurement error, endogeneity in the control mechanisms, heterogeneity in country of origin, and nonlinear and interactive terms for the latent constructs. The results demonstrate the importance of the HQ–subsidiary relationship for managing the subsidiary–distributor relationship.
1	This study examines brand dispersion—variance in brand ratings across consumers—and its role in the translation of brand assets into firm value. Dispersion captures the covert heterogeneity in brand evaluations among consumers who like or dislike the brands, which would affect an investor's decision to buy or sell a stock. The higher the dispersion, the more inconsistent and polarized the brands’ cross-consumer ratings. Multiple analyses on 730,818 brand–day observations provide robust evidence that brand dispersion fluctuations affect stock prices. Brand dispersion has Januslike effects: it harms returns but reduces firm risk. Furthermore, downside dispersion has a stronger impact on abnormal returns than upside dispersion, indicating an asymmetry in brand dispersion's effects. Moreover, dispersion tempers the risk-reduction benefits of higher brand rating in both the short run and long run. Without modeling dispersion, brand rating's impact on firm value can be over- or underestimated. Managers should consider dispersion a vital brand-management metric and add it to the brand-performance dashboard.
1	Four studies investigate (1) whether a variety-seeking versus inertial environment activates a certain mind-set about risk propensity and (2) whether this mind-set influences preferences for immediate versus delayed promotions. Study 1 demonstrates that a variety-seeking environment activates a risk-taking mind-set, whereas an inertial environment activates a risk-averse mind-set and that such a difference in risk propensity makes a delayed (immediate) promotion relatively more appealing for consumers with a variety-seeking (inertial) tendency. Study 2 reveals that preferences for a brand offering a delayed promotion are stronger when consumers have a variety-seeking tendency and that preferences of consumers low (vs. high) in need for cognitive closure are more influenced by the difference in variety-seeking versus inertial tendency. Study 3 provides further insights by allowing participants to be variety seeking or inertial and by controlling for redemption effort and the hedonic/utilitarian aspects of categories. Finally, Study 4 highlights the impact of variety-seeking versus inertial tendency on real-world choices. The article concludes with a discussion of theoretical and managerial implications.
1	Brands and word of mouth (WOM) are cornerstones of the marketing field, and yet their relationship has received relatively little attention. This study aims to enhance understanding of brand characteristics as antecedents of WOM by executing a comprehensive empirical analysis. For this purpose, the authors constructed a unique data set on online and offline WOM and characteristics for more than 600 of the most talked-about U.S. brands. To guide this empirical analysis, they present a theoretical framework arguing that consumers spread WOM on brands as a result of social, emotional, and functional drivers. Using these drivers, the authors identify a set of 13 brand characteristics that stimulate WOM, including three (level of differentiation, excitement, and complexity) that have not been studied to date as WOM antecedents. The authors find that whereas the social and functional drivers are the most important for online WOM, the emotional driver is the most important for offline WOM. These results provide an insightful perspective on WOM and have meaningful managerial implications for brand management and investment in WOM campaigns.
1	Retailers and manufacturers are keenly interested in understanding unplanned consideration and purchase conversion, but data that capture in-store product consideration have been unavailable in the past. In the current research, the authors use in-store video tracking to collect a novel data set that records shopping behavior at the point of purchase, including product consideration. In conjunction with an entrance survey of purchase intentions, they conduct several descriptive analyses that focus on the incidence, category propensity, behavioral characteristics, and outcome of unplanned consideration. The results reveal several new empirical insights. First, the authors find significant category-level complementarities between planned items and unplanned considerations, which they capture using a latent category map. Second, planned consideration and unplanned consideration differ in key behavioral characteristics (e.g., likelihood of purchase, time of occurrence, number of product touches). Third, greater likelihood of purchase conversion is significantly associated with dynamic factors (e.g., remaining in-store slack, outcome of the previous consideration) and behavioral characteristics (e.g., number of displays viewed, distance to shelf, references to a shopping list). The authors conclude with a discussion of implications of these findings for research and shopper marketing.
1	Prior research shows that positive online reviews are less valued than negative reviews. The authors argue that this is due to differences in causal attributions for positive versus negative information such that positive reviews tend to be relatively more attributed to the reviewer (vs. product experience) than negative reviews. The presence of temporal contiguity cues, which indicate that review writing closely follows consumption, reduces the relative extent to which positive reviews are attributed to the reviewer and mitigates the negativity bias. An examination of 65,531 Yelp.com restaurant reviews shows that review value is negatively related to review valence but that this negative relationship is absent for reviews that contain temporal contiguity cues. A series of lab studies replicates these findings and suggests that temporal contiguity cues enhance the value of a positive review and increase the likelihood of choosing a product with a positive review by changing reader beliefs about the cause of the review.
1	Conspicuous brand usage, defined as attention-getting use of a brand, causes brand dilution under certain conditions. This research examines changes in observers' attitudes toward a brand after seeing a brand user engaged in conspicuous use of the brand. The authors propose that observers infer that a consumer engaged in conspicuous brand usage is driven by an ulterior motive of impression management. When observers have low self-brand connection, they exhibit less favorable attitudes toward both the brand user and the brand. In contrast, observers with high self-brand connection maintain their favorable view of the brand in the face of a conspicuous brand user. Three studies demonstrate the brand dilution effect of conspicuous brand usage.
1	Standard models of competition predict that firms will sell less when competitors target their customers with advertising. This is particularly true in mature markets with many competitors that sell relatively undifferentiated products. However, the authors present findings from a large-scale randomized field experiment that contrast sharply with this prediction. The field experiment measures the impact of competitors' advertising on sales at a private label apparel retailer. Surprisingly, for a substantial segment of customers, the competitors' advertisements increased sales at this retailer. This robust effect was obtained through experimental manipulation and by measuring actual purchases from large samples of randomly assigned customers. The effect size is also large, with customers ordering more than 4% more items in some categories in the treatment condition (vs. the control). The authors examine how these positive spillovers vary across product categories to illustrate the importance of product standards, customer learning, and switching costs. The findings have the potential to change our understanding of competition in mature markets.
1	Contrary to the general view that decision difficulty is a stable characteristic of specific choice sets, the authors propose that decision difficulty depends on how the choice set is mentally represented. Comparing the difficulty associated with comparable and noncomparable choice sets, the authors find that changes in mental representation can make the same choice feel more or less difficult. They propose that the representation level influences the type of decision criterion that becomes readily available; whether this available criterion is appropriate for comparing the options in turn affects choice difficulty. Four studies demonstrate the proposed effect of representation level on the difficulty of comparable and noncomparable choices and its downstream implications for decision satisfaction.
1	In this study, the authors develop an inexpensive method to help firms assess the relative effectiveness of multiple advertising media. Specifically, they use a firm's loyalty program database to capture media exposure, through an online media survey, for all the media in which the firm advertises. In turn, the exposure data are matched with the purchase history for these same respondents, thereby creating single-source data. The authors illustrate their method for a large retailer that undertook a short-term promotional sale by advertising in television, radio, newspaper, magazine, online display ad, sponsored search, social media, catalog, direct mail, and e-mail channels. In this case, seven of the ten media significantly influence purchase outcomes. Finally, the authors demonstrate how to use their advertising response model to determine the optimal budget allocation across each advertising media channel.
1	The authors propose that the physical sensation of balance can affect consumer judgments and decisions. A series of six experiments demonstrates that certain consumer behaviors, such as leaning back in a chair while shopping online, can activate the concept of balance and thereby affect the consumer decision-making process. Specifically, consumers experiencing a heightened sense of balance are more likely to choose compromise options. The authors propose and show evidence for the mechanism underlying these effects: that the concept of balance is metaphorically linked in the mind to the concept of parity and that activating balance increases the accessibility of the parity concept. The increased accessibility of parity changes consumer perceptions of the product offerings in a choice set, increasing the selection of compromise options because they provide parity on the described product attributes.
1	A recent meta-analysis has found that an increase in the size of an assortment has no reliable impact on choice difficulty. Building on a fundamental property of cognition, the authors investigate the link between mental representation and the choice overload effect based on the size of the assortment. They propose that the mental representation of a large assortment changes the perceived similarity of the assortment and consequently affects the degree of choice difficulty. Specifically, when choosing from a large assortment, consumers with an abstract representation perceive the options in the assortment as being more similar and accordingly experience less choice difficulty than those with a concrete representation of the assortment. The authors discuss theoretical and practical implications of the findings.
1	Firms can now offer personalized recommendations to consumers who return to their website, using consumers' previous browsing history on that website. In addition, online advertising has greatly improved in its use of external browsing data to target Internet ads. Dynamic retargeting integrates these two advances by using information from the browsing history on the firm's website to improve advertising content on external websites. When surfing the Internet, consumers who previously viewed products on the firm's website are shown ads with images of those same products. To examine whether this is more effective than simply showing generic brand ads, the authors use data from a field experiment conducted by an online travel firm. Surprisingly, the data suggest that dynamic retargeted ads are, on average, less effective than their generic equivalents. However, when consumers exhibit browsing behavior that suggests their product preferences have evolved (e.g., visiting review websites), dynamic retargeted ads no longer underperform. One explanation for this finding is that when consumers begin a product search, their preferences are initially construed at a high level. As a result, they respond best to higher-level product information. Only when they have narrowly construed preferences do they respond positively to ads that display detailed product information. This finding suggests that in evaluating how best to reach consumers through ads, managers should be aware of the multistage nature of consumers' decision processes and vary advertising content along these stages.
1	Franchise relationships are prone to conflict. To safeguard the rights of individual franchisees, several states have legislated greater franchisor disclosure (registration law) ex ante and/or franchisor “termination for good cause” (relationship law) ex post. The impact of regulatory oversight on franchisor–franchisee conflict, however, remains unclear. Relying on agency theory arguments, the authors first assess the influence of the regulatory context, both by itself and in combination with the franchise ownership structure, on the incidence of litigated conflict. Conditional on litigation, they also predict the impact of franchise regulation on both the parties’ litigation initiation and resolution choices and the resulting outcomes. The authors test the hypotheses using a unique multisource archival database of 411 instances of litigation across 75 franchise systems observed over 17 years. The results indicate that the regulatory context, by itself as well as in combination with the franchise ownership structure, significantly shapes parties’ conflict management choices. The authors also find evidence of a trade-off between prevailing in the particular conflict and achieving franchise system growth objectives.
1	“Hard discounters” (HDs) have become a considerable force in grocery retailing. With rock-bottom prices and minimal assortments, they differ greatly from “large discounters” such as Wal-Mart, constituting complements to, rather than substitutes for, more traditional supermarkets. Therefore, the authors propose that HD impact of entry on local incumbents is different as well. Using a store choice and spending model that explicitly accounts for interstore synergies and multiple-store shopping behavior, the authors study consumer responses to 194 HD openings. Although they find that HDs, like large discounters, especially appeal to private label–prone shoppers and lead to sizable incumbent losses, the results confirm that the nature of these losses is different. First, HDs do not cause incumbent chains to lose their best customers; instead, shoppers who have already visited other chains alongside the incumbent are lost. Second, the authors find that chains located in close proximity to new HDs do not suffer more from their entry. Third, losses are lower for upscale chains and incumbents that strongly complement the HD. The authors conclude by discussing implications for proper response to HD entry.
1	In a distribution network, a punishment event not only affects the disciplined distributor but also changes the attitudes and behaviors of others in the network (i.e., observers). By moving beyond a dyadic view of punishment, this article considers the effects of punishment on observers and integrates insights from social learning, fairness heuristic, and social network theories. The resulting framework of the observer effects of punishment in a distribution network, empirically tested with a survey in China, reveals two mechanisms through which punishment leads to reduced observer opportunism: (1) a direct deterrence effect and (2) a trust-building process. Moreover, two information-related constructs moderate the observer effects differently. The disciplined distributor's relational embeddedness, which motivates greater information flow to observers, aggravates the problem of information asymmetry against the manufacturer, making punishment less deterrent for observers. In contrast, the manufacturer's monitoring capability, which reduces information asymmetry, strengthens observer effects. The authors discuss both theoretical and managerial implications of using punishment to achieve collaboration from a wide network of channel members.
1	Managers, research administrators, and policy makers need a greater understanding of the factors that drive employment preferences of foreign science, technology, engineering, and mathematics (STEM) doctoral graduates of U.S. universities. To address this need, the authors report the results of a large multischool conjoint survey of return-migration preferences among U.S. STEM doctoral students from China. The survey presents the respondents with potential job offers and yields individual-level estimates of each respondent's indirect utility of a job as a function of location, job status, and salary. The authors use a delayed follow-up choice task to demonstrate stability of the preference estimates both over time and across response modalities. The estimated preferences imply that Chinese doctoral graduates tend to remain in the United States because of a large salary disparity between the two countries rather than because of an inherent preference for locating in the United States. Given these estimated preferences, the authors conduct several policy-relevant, counterfactual simulations of return-migration choice and outline effective targeting and positioning strategy for attracting Chinese STEM talent.
1	Researchers have recently introduced a finite mixture Bayesian regression model to simultaneously identify consumer market segments (heterogeneity) and determine how such segments differ with respect to active regression coefficients (variable selection). This article introduces three extensions of this model to incorporate managerial restrictions (constraints). The authors demonstrate with synthetic data that the new constrained finite mixture Bayesian regression models can be used to identify and represent several constrained heterogeneous response patterns commonly encountered in practice. In addition, they show that the proposed models are more robust against multicollinearity than traditional methods. Finally, to illustrate the proposed models' usefulness, the authors apply the proposed constrained models in the context of a service quality (SERVPERF) survey of National Insurance Company's customers.
1	Although the role of social networks and consumer interactions in new product diffusion is widely acknowledged, such networks and interactions are often unobservable to researchers. What may be observable, instead, are aggregate diffusion patterns for past products adopted within a particular social network. The authors propose an approach for identifying systematic conditions that are stable across diffusions and thus are “transferrable” to new product introductions within a given network. Using Facebook applications data, the authors show that incorporation of such systematic conditions improves prelaunch forecasts. This research bridges the gap between the disciplines of Bayesian statistics and agent-based modeling by demonstrating how researchers can use stochastic relationships simulated within complex systems as meaningful inputs for Bayesian inference models.
1	Consumers face many options that are presented to them as bargains, but in reality, they only subjectively construe a fraction of them as valuable. The authors propose that consumers are particularly attracted to offers they perceive as more valuable than the marketer presumably intended. Consistent with this analysis, six experiments indicate that consumers may perceive customized offers that are presented as tailored to their individual preferences or circumstances as less valuable than offers that seem to fit their preferences and provide value without the marketer's explicit intent. The experiments also suggest that the urge to exploit unintended value reflects a competitive desire to outsmart the market. The findings have theoretical implications for understanding consumers’ subjective perceptions of value as well as important practical implications for designing customized offers and targeted promotions.
1	News reports carrying positive or negative sentiment about a firm influence its stock market performance. This study examines how two firm-controllable marketing factors, advertising and marketing capability, moderate the relationship between news stories and firm stock returns. Analysis of a panel data set of more than 7,000 firm-month observations indicates asymmetric and complementary moderating roles of the two marketing variables: advertising reinforces the favorable impact of positive news on abnormal stock returns, and marketing capability mitigates the adverse impact of negative news. Moreover, these moderating effects operate through different stakeholders. Whereas the moderating effect of marketing capability is due to its influence on customers and thus affects the level and volatility of future cash flows, advertising moderates the effect of news through individual investors’ attention and response to the news. The econometric analysis accounts for potential endogeneity between news reports, stock returns, and marketing variables, and the results are robust to alternative measures and analysis approaches. The findings suggest the need for managers to broaden their stakeholder focus when evaluating advertising's returns and to communicate the value of marketing capability to investors.
1	The authors show, with real and hypothetical payoffs, that consumers are willing to pay substantially less for a risky prospect when it is called a “lottery ticket,” “raffle,” “coin flip,” or “gamble” than when it is labeled a “gift certificate” or “voucher.” Willingness to accept, in contrast, is not affected by these frames. This differential framing effect is the result of an aversion to bad deals, which causes buyers to focus on different aspects than sellers. Buyers’ willingness to pay is influenced by the extent to which a risky prospect's frame is associated with risk (Experiment 1) as well as the prospect's lowest (but not highest) possible outcome (Experiment 2). Sellers’ willingness to accept, in contrast, is influenced by a prospect's lowest and highest possible outcomes but not by the risk associated with its frame (Experiments 2 and 3). The framing effect on willingness to pay is independent of the objective level of uncertainty (Experiment 4) and can lead to the uncertainty effect. The findings have important implications for research on risk preferences and marketing practice.
1	Does the mere crowdedness of the environment affect people's choices and preferences? In six studies, the authors show that social crowdedness not only leads to greater accessibility of safety-related constructs but also results in greater preference for safety-oriented options (e.g., preferring to visit a pharmacy to a convenience store), being more receptive to prevention- (rather than promotion-) framed messages, and being more risk averse with real money gambles. In support of the authors’ underlying avoidance motivation perspective, these effects are mediated by participants’ net prevention focus and are attenuated when the crowd in question consists of in-group members. The authors close by discussing the practical and theoretical implications of the results.
1	This article argues that the structure of a choice set can influence the extent to which consumers weight a given attribute. The results of seven experiments suggest that the relationship between options under consideration can influence preference ordering by shifting the decision strategy people adopt when constructing their preference. In decisions in which people afford greater importance to one attribute versus another, preference for an option that scores high on this prominent attribute may decrease when decoy options that are clearly better or worse than the focal options are inserted into the choice set. The authors posit that this effect arises because decision makers initially (and spontaneously) use dominance cues rather than prominence when evaluating options, and they continue to use this strategy even when it does not enable them to differentiate the alternatives under consideration. The authors moderate this effect by prompting respondents to consider prominence and by manipulating the order in which respondents evaluate options in the choice set. This article has theoretical implications for research on context effects, contingent decision behavior, and choice architecture as well as practical implications for product-line management.
1	This study shows that people experiencing financial dissatisfaction may choose and consume food for its energy value. Because money and food are closely related, exchangeable resources, financially dissatisfied people may be motivated to replenish their need for financial resources by consuming caloric resources or food energy. Five experiments provide support for this hypothesis across various measures of caloric desire and actual eating behavior. The findings have notable implications for marketing and public policy. Whereas marketing researchers have increasingly investigated the interplay of taste and health considerations in food consumption, this research demonstrates the importance of investigating food energy considerations.
1	The authors propose a method and metric to quantify the consumer confusion between leading brands and copycat brands that results from the visual similarity of their packaging designs. The method has three components. First, image processing techniques establish the objective similarity of the packages of leading and copycat brands on the basis of their colors and textures. Second, a perceptual decision task (triangle test) assesses the accuracy and speed with which consumers can identify differences between brands from rapidly (300 milliseconds) flashed images of their packages. Third, a competing accumulator model describes the buildup of evidence on each of the alternative brands during consumers’ perceptual decisions and predicts the accuracy and response time of brand identification. Jointly, these components establish the impact of copycat packaging's visual features on consumer confusion. The method is applied in a test of experimentally designed copycats and market copycats in 15 product categories. A three-tiered metric (“copy alert,” “copy watch,” and “copy safe”) establishes the extent to which copycat brands imitate the package designs of target brands and identifies which visual features are responsible.
1	How important is the original conception of an idea—the “raw” idea— to an innovation's success? In this article, the authors explore whether raw ideas judged as “better” fare better in the market and also determine the strength of that relationship. The empirical context is Quirky.com, a community-driven product development company for household consumer products. The data include descriptions of the raw ideas as originally proposed, the ultimate product designs that resulted from those ideas, and sales figures. In addition, they contain two measures of idea quality: those from online consumer panelists and those from expert evaluators. The authors note the following findings: First, online consumer panels are a better way to determine a “good” idea than are ratings by experts. Second, predictions with samples as small as 20 consumers are reliable. Third, there is a stronger predictive link between raw ideas and consumers’ purchase intent of final product designs than there is between those intentions and market outcomes. Fourth, the commercial importance of the raw idea is large, with ideas one standard deviation better translating to an approximately 50% increase in sales rate.
1	Each month, millions of women experience an ovulatory cycle that regulates fertility. Previous consumer research has found that this cycle influences women's clothing and food preferences. The authors propose that the ovulatory cycle actually has a much broader effect on women's economic behavior. Drawing on theory in evolutionary psychology, the authors hypothesize that the week-long period near ovulation should boost women's desire for relative status, which should alter their economic decisions. Findings from three studies show that women near ovulation seek positional goods to improve their social standing. Additional findings reveal that ovulation leads women to pursue positional goods when doing so improves relative standing compared with other women but not compared with men. When playing the dictator game, for example, ovulating women gave smaller offers to a female partner but not to a male partner. Overall, women's monthly hormonal fluctuations seem to have a substantial effect on consumer behavior by systematically altering their positional concerns, a finding that has important implications for marketers, consumers, and researchers.
1	Technology enables a firm to produce a granular record of every touchpoint consumers make in their online purchase journey before they convert at the firm's website. However, firms still depend on aggregate measures to guide their marketing investments in multiple online channels (e.g., display, paid search, referral, e-mail). This article introduces a methodology to attribute the incremental value of each marketing channel in an online environment using individual-level data of customers’ touches. The authors propose a measurement model to analyze customers’ (1) consideration of online channels, (2) visits through these channels over time, and (3) subsequent purchases at the website to estimate the carryover and spillover effects of prior touches at both the visit and purchase stages. The authors use the estimated carryover and spillover effects to attribute the conversion credit to different channels and find that these channels’ relative contributions are significantly different from those found by other currently used metrics. A field study validates the proposed model's ability to estimate the incremental impact of a channel on conversions. In targeting customers with different patterns of touches in their purchase funnel, these estimates help identify cases in which retargeting strategies may actually decrease conversion probabilities.
1	This research demonstrates the effect of the completeness of a product's shape on size perceptions, preference, and consumption quantities. The authors show that people estimate an incompletely shaped product to be smaller and, therefore, prefer it less in general than a completely shaped one of equal size and weight. They also find that the reduced size estimations for incompletely shaped products lead to increased consumption quantities of this type of item. Finally, the authors demonstrate that the “completeness heuristic” operates even when the incompletely shaped item has a larger primary dimension than its completely shaped counterpart.
1	This research investigates how the relationships among pieces of numerical information in a price promotional offer (i.e., regular price, sale price, absolute discount, and relative discount) affect deal processing fluency. Across four studies (including a field study involving purchase data collected from an online group-buying website), the authors show that when the numbers constitute an approximation sequence or are multiples of one another, deal processing fluency is increased, which influences deal liking and ultimately has an impact on consumers’ price promotion predilection. In addition, this article demonstrates that when consumers are not highly motivated to process numerical information, they may choose deals that offer less economic value but feature a combination of numbers that they can more fluently process. This research has important implications for the type of numerical information marketers should include in price promotional offers.
1	Using subject indexes and text mining of author abstracts, the authors track the evolution of content in Journal of Marketing Research since its inception 50 years ago. These data reveal that the journal has expanded beyond its initial emphasis on marketing research methods and advertising to increase its coverage of other substantive topics and consumer behavior. Moreover, a joint space of topics and editors reveals that editorial orientations appear largely evolutionary rather than revolutionary and that a major shift in journal coverage occurs at the time Marketing Science began publication. The authors conclude their analysis with several policy recommendations.
1	The authors investigate the role of market orientation in advertising spending during economic contraction. They use the 2001 economic collapse in Turkey as the empirical context in which to test hypotheses regarding why some firms increase their advertising spending in a contraction period while the majority of firms cut back. Analyzing market orientation at the level of its intelligence and responsiveness facets, they find the responsiveness facet to be positively associated with increases in advertising spending but observe the intelligence facet to be negatively associated with advertising spending. Importantly, positive shifts in advertising spending during the economic contraction predict better subsequent business performance. The opposing roles of the intelligence and responsiveness facets disappear in a subsequent economic expansion period. These findings have managerial and theoretical implications. Firms that nurture the responsiveness facet of market orientation during economic contractions go against the tide to increase their advertising spending and experience the performance benefits that such countercyclical actions can amass.
1	People often use price as a proxy for quality, resulting in a positive correlation between prices and product liking, known as the “price– quality” (P–Q) heuristic. Using data from three experiments conducted at a winery, this article offers a more complex and complete reference-dependent model of the relationship between price and quality. The authors propose that higher prices set higher expectations, which serve as reference points. When expectations are met or exceeded, we observe the familiar P–Q relationship. However, when price is high and quality is relatively low, the product falls short of consumers’ reference point and the P–Q relationship is reversed; thus, people evaluate a low-quality product with a high price more negatively than a low-quality product with a low price. Using the results of a field experiment, the authors discuss implications for pricing considerations and profitability.
1	Two-thirds of adolescent and young adult smokers become lifetime smokers, and one-half of those lifetime smokers will die from this habit. The authors examine alternative persuasive pathways to thoughts of quitting taken by adolescent and young adult smokers when exposed to graphic visual health warnings on cigarette packages. For adolescent smokers, the authors find that graphic warnings and smoking frequency affect fear, and fear influences negative health beliefs about smoking, ultimately increasing thoughts of quitting. They also find that the graphic warning and a graphic warning × smoking frequency interaction have incremental effects on quit thoughts beyond the effects of fear and negative health beliefs. Using a longitudinal design with a sample of young adult smokers, the authors find support for many of the adolescent smoker findings, particularly the incremental effects of graphicness and its interaction with smoking frequency. These similar results from diverse samples support the use of graphic visual warnings but suggest that effects are attenuated for those who smoke the most. The authors offer implications for countermarketing programs and public health policy.
1	The authors propose that static visuals can evoke a perception of movement (i.e., dynamic imagery) and thereby affect consumer engagement and attitudes. Focusing on brand logos as the static visual element, the authors measure the perceived movement evoked by the logo and demonstrate that the evoked dynamic imagery affects the level of consumer engagement with the brand logo. They measure consumer engagement through both self-report measures and eye-tracking technology and find that engagement affects consumer attitudes toward the brand. The authors also show that the perceived movement–engagement–attitude effect is moderated by the congruence between perceived movement and brand characteristics. These findings suggest that dynamic imagery is an important aspect of logo design, and if used carefully, it can enhance brand attitudes.
1	The present research examines how the inclusion of consolation or token prizes influences consumers’ valuation of a promotional lottery. Results from four experiments show that consolation prizes lower consumers’ expectations of winning the big prize, their valuations of the lottery, and their intentions to participate in the lottery. Because of the high likelihood of attaining the consolation prizes, consumers shift their focus from the value of a big prize to the probability of attaining it. This shift increases the weight given to the probability dimension and results in lowered valuations of the lottery. The first two experiments demonstrate the effect in hypothetical and real choices. In Experiment 3, the authors propose and show a boundary condition for the effect. In Experiment 4, they conduct an exploratory test of the process. They conclude with a discussion of the theoretical and managerial implications.
1	In general, consumers enjoy products less with repeated consumption. Unfortunately, there are few known ways to slow such satiation. The authors show that consumers satiate more slowly on a product when it is available for consumption only at limited times. Specifically, they find that perceived limited availability made a product more enjoyable, and yet this effect largely emerged only after repeated consumption. The authors attribute this finding to an urge to take advantage of a rare consumption opportunity, which leads people to pay less attention to the quantity consumed and subsequently to experience less satiation. A series of studies establish the effect of perceived limited availability on the rate of satiation, show that it influences how much people eat, provide mediation evidence of the proposed theoretical account, and eliminate the effect by making salient the total amount consumed. The authors conclude with implications of these findings.
1	Marketers regularly remind consumers of valued social relationships (e.g., close friends, family, romantic couples) to influence choice and consumption. However, the author's research reveals that such relationship reminders can backfire when consumers lack or no longer have these highlighted relationships. The author shows that reminding consumers of relationships they lack reduces their perceptions of deservingness and causes them to restrict indulgent consumption. Five studies establish the effect of relationship reminders on indulgence and provide support for the underlying process by both measuring and manipulating perceptions of deservingness.
1	When consumers struggle with a difficult task, using a brand can help them perform better. The authors report four studies showing that brand use can enhance feelings of self-efficacy, which can lead to better task performance. Students scored higher on difficult Graduate Records Examination questions when they took the test using a Massachusetts Institute of Technology pen (Study 1) and showed better athletic performance when they drank water from a Gatorade cup during strenuous athletic exercise (Studies 2 and 3). These increases in task performance were mediated by feelings of self-efficacy (Studies 3 and 4). Furthermore, the results show that not everyone experiences the beneficial effect of brand use; it depends on the person's implicit self-theory. Across studies, users adopting entity theories (“entity theorists”) showed increased self-efficacy and better task performance, whereas users adopting incremental theories (“incremental theorists”) were unaffected by brand use.
1	The authors document that approximately 5% of product reviews on a large private label retailer's website are submitted by customers with no record of ever purchasing the product they are reviewing. These reviews are significantly more negative than other reviews. They are also less likely to contain expressions describing the fit or feel of the items and more likely to contain linguistic cues associated with deception. More than 12,000 of the firm's best customers have written reviews without confirmed transactions. On average, these customers have each made more than 150 purchases from the firm. This makes it unlikely that the reviews were written by the employees or agents of a competitor and suggests that deceptive reviews may not be limited to the strategic actions of firms. Instead, the phenomenon may be far more prevalent, extending to individual customers who have no financial incentive to influence product ratings.
1	Mobile advertising is one of the fastest-growing advertising formats. In 2013, global spending on mobile advertising was approximately $16.7 billion, and it is expected to exceed $62.8 billion by 2017. The most prevalent type of mobile advertising is mobile display advertising (MDA), which takes the form of banners on mobile web pages and in mobile applications. This article examines which product characteristics are likely to be associated with MDA campaigns that are effective in increasing consumers’ (1) favorable attitudes toward products and (2) purchase intentions. Data from a large-scale test-control field experiment covering 54 U.S. MDA campaigns that ran between 2007 and 2010 and involved 39,946 consumers show that MDA campaigns significantly increased consumers’ favorable attitudes and purchase intentions only when the campaigns advertised products that were higher (vs. lower) involvement and utilitarian (vs. hedonic). The authors explain this finding using established theories of information processing and persuasion and suggest that when MDAs work effectively, they do so by triggering consumers to recall and process previously stored product information.
1	Does the number of people with whom someone communicates influence what he or she discusses and shares? Six studies demonstrate that compared with narrowcasting (i.e., communicating with just one person), broadcasting (i.e., communicating with multiple people) leads consumers to avoid sharing content that makes them look bad. Narrowcasting, however, encourages people to share content that is useful to the message recipient. These effects are driven by communicators’ focus of attention. People naturally tend to focus on the self, but communicating with just one person heightens other-focus, which leads communicators to share less self-presenting content and more useful content. These findings shed light on the drivers of word of mouth and provide insight into when the communication sender (vs. receiver) plays a relatively larger role in what people share.
1	Unlike sales data, data on intermediate stages of the purchase funnel (e.g., how many consumers have searched for information about a product before purchase) are much more difficult to acquire. Consequently, most advertising response models have focused directly on sales and ignored other purchase funnel activities. The authors demonstrate, in the context of the U.S. automotive market, how consumer online search volume data from Google Trends can be combined with sales data to decompose advertising's overall impact into two underlying components: its impacts on (1) generating consumer interest in prepurchase information search and (2) converting that interest into sales. The authors show that this decompositional approach, implemented through a novel state-space model that simultaneously examines sales and search volumes, offers important advantages over a benchmark model that considers sales data alone. First, the approach improves goodness-of-fit, both in and out of sample. Second, it improves diagnosticity by distinguishing advertising effectiveness in interest generation from its effectiveness in interest conversion. Third, the authors find that overall advertising elasticity can be biased if researchers consider only sales data.
1	When should sales managers employ group incentives rather than individual incentives to motivate their sales force? Using economic experiments, the authors show that two-person group incentives can outperform individual incentives and that the relative efficacy of group incentives depends on three important factors. First, the strength of social ties among the group members matters. Effort decisions in group-based incentives increase significantly when members socialize briefly before committing effort. Second, the design of the group incentive matters. For the group incentive to work better than the individual incentive, the group-based component (i.e., how much the payment scheme weights the contribution of others) in the former cannot be too large. Third, the informational feedback that group members receive matters. When socialized group members can observe one another's true effort, rather than only their output, effort surprisingly decreases. The authors show that a model that accounts for social preferences and the psychological loss that occurs when teammates underestimate one's effort can explain salesperson behavior in group incentives well.
1	Extant preference measurement research, including conjoint analysis, has been silent on the explicit influence of others in the formation of consumer preferences. This article proposes a new holistic framework of preference, “PIE,” as well as a measurement method to remedy this problem. The new paradigm posits that consumers evaluate products on the basis of different “needs” determined by three sources: (1) P, the physical attributes of the product; (2) I, the individual characteristics of the choice maker; and (3) E, characteristics of an external peer group. To provide an empirically feasible method to capture all three sources of information, the authors propose and test an incentive-aligned approach, a “group-sourced mechanism,” which mimics a consumer's real-life consultation in the presence of his or her friends when making a purchase decision. The results provide support for the PIE framework, including superior predictive performance in a conjoint task that is “stacked against it.” The authors also show how firms can apply the PIE framework for product design. Practitioners, however, must carefully weigh the benefits of the group-sourced preference measurement with the heavier cognitive burden on the respondents in completing the task.
1	When patronizing stores, consumers may exhibit loyalty not only to a retail chain but also to a specific outlet. This distinction is important in a dynamic retail environment: if a store changes ownership, chain loyalty makes customers inclined to seek out another outlet of the former chain, whereas outlet loyalty enhances their stay rate after the takeover. This article distinguishes the two forms of loyalty conceptually and discusses how both can be identified empirically in a model of consumers’ reactions to store acquisitions. The authors estimate their model on unique scanner panel data covering ±200 local markets and takeovers. The results confirm that after an acquisition, consumers exhibit outlet loyalty, regardless of changes in chain and marketing mix. Counterfactual simulations point to important managerial implications. Acquiring outlets with a clientele in place leads to higher store traffic levels than the new owner could otherwise reach. Notably, these benefits cannot be reaped if the acquiring chain is a hard discounter, in which case customers’ previous store knowledge is less relevant, and incentives to seek out new outlets are greater.
1	It is well established that differences in manufacturing location can affect consumer preferences through lay inferences about production quality. In this article, the authors take a different approach to this topic by demonstrating how beliefs in contagion (the notion that objects may acquire a special aura or “essence” from their past) influence perceptions of authenticity for everyday consumer products and brands. Specifically, they find that due to a belief in contagion, products from a company's original manufacturing location are viewed as containing the essence of the brand. In turn, this belief in transferred essence leads consumers to view products from the original factory as more authentic and valuable than identical products made elsewhere. The authors further reveal that consumers who are higher in sensitivity to contagion are more likely to exhibit this effect and that activating the concept of contagion enhances preferences for products made in the brand's original factory. The authors close by discussing theoretical and practical implications of these findings.
1	In this research, the authors jointly model the sentiment expressed in social media posts and the venue format to which it was posted as two interrelated processes in an effort to provide a measure of underlying brand sentiment. Using social media data from firms in two distinct industries, they allow the content of the post and the underlying sentiment toward the brand to affect both processes. The results show that the inferences marketing researchers obtain from monitoring social media are dependent on where they “listen” and that common approaches that either focus on a single social media venue or ignore differences across venues in aggregated data can lead to misleading brand sentiment metrics. The authors validate the approach by comparing their model-based measure of brand sentiment with performance measures obtained from external data sets (stock prices for both brands and an offline brand-tracking study for one brand). They find that their measure of sentiment serves as a leading indicator of the changes observed in these external data sources and outperforms other social media metrics currently used.
1	The rise of new media is helping marketers evolve from digital to interactive marketing, which facilitates a two-way communication between marketers and customers without intruding on their privacy. However, while research has examined the drivers of customers’ opt-in and opt-out decisions, it has investigated neither the timing of the two decisions nor the influence of transactional activity on the length of time a customer stays with an e-mail program. In this study, the authors adopt a multivariate copula model using a pair-copula construction method to jointly model opt-in time (from a customer's first purchase to the opt-in decision), opt-out time (from the opt-in decision to the opt-out decision), and average transaction amount. Through such multivariate dependences, this model significantly improves the predictive performance of the opt-out time in comparison with several benchmark models. The study offers several important findings: (1) marketing intensity affects opt-in and opt-out times, (2) customers with certain characteristics are more or less likely to opt in or opt out, and (3) firms can extend customer opt-out time and increase customer spending level by strategically allocating resources.
1	First experiences are highly influential. Here, the authors show that nonfirst experiences can be made to seem like firsts and, consequently, to have a disproportionate influence on judgment. In six experiments, one piece of a series of information was framed to appear to have “first” status: For example, a weather report that appeared at the end of a sequence of weather reports happened to correspond to the first day of a vacation, and a customer review that appeared at the end of a sequence of hotel reviews happened to be the new year's first review. Such information had greater influence on subsequent judgments (e.g., of the next day's weather, of the hotel's quality) than identical information not framed as a first. This effect seems to arise largely because “phantom first” pieces of information receive greater weight, but not necessarily more attention, than other pieces of information.
1	Previous research has found that people tend to avoid products or behaviors that are linked to dissociative reference groups. The present research demonstrates conditions under which consumers exhibit similar behaviors to dissociative out-group members in the domain of positive consumption behaviors. In particular, when a consumer learns that a dissociative out-group performs comparatively well on a positive behavior, the consumer is more likely to respond with positive intentions and actions when the setting is public (vs. private). The authors suggest that this occurs because learning of the successful performance of a dissociative out-group under public conditions threatens the consumer's group image and activates the desire to present the group image in a positive light. The authors show that although group affirmation mitigates these effects, self-affirmation does not. They also examine the moderating role of the positivity of the behavior and the mediating role of group image motives. Taken together, the results highlight conditions under which communicating information about the behaviors of dissociative out-groups can be used to spur consumers to engage in positive actions.
1	Using data from an online hotel reservation site, the authors jointly examine consumers’ quality choice decision at the time of purchase and subsequent satisfaction with the hotel stay. They identify three circumstantial variables at the time of purchase that are likely to influence both the choice decisions and the postpurchase satisfaction: the time gap between purchase and consumption, distance between purchase and consumption, and time of purchase (business/nonbusiness hours). The authors incorporate these three circumstantial variables into a formal two-stage economic model and find that consumers who travel farther and make reservations during business hours are more likely to select higher-quality hotels but are less satisfied. Consumers who book earlier are more likely to select higher-quality hotels and are more satisfied. The findings suggest that incorporating circumstantial variables into formal choice models is useful in helping managers understand and predict consumer choices and satisfaction assessments.
1	Online chatter, or user-generated content, constitutes an excellent emerging source for marketers to mine meaning at a high temporal frequency. This article posits that this meaning consists of extracting the key latent dimensions of consumer satisfaction with quality and ascertaining the valence, labels, validity, importance, dynamics, and heterogeneity of those dimensions. The authors propose a unified framework for this purpose using unsupervised latent Dirichlet allocation. The sample of user-generated content consists of rich data on product reviews across 15 firms in five markets over four years. The results suggest that a few dimensions with good face validity and external validity are enough to capture quality. Dynamic analysis enables marketers to track dimensions’ importance over time and allows for dynamic mapping of competitive brand positions on those dimensions over time. For vertically differentiated markets (e.g., mobile phones, computers), objective dimensions dominate and are similar across markets, heterogeneity is low across dimensions, and stability is high over time. For horizontally differentiated markets (e.g., shoes, toys), subjective dimensions dominate but vary across markets, heterogeneity is high across dimensions, and stability is low over time.
1	The authors study consumers’ click behavior on organic and sponsored links after a keyword search on an Internet search engine. Using a data set of individual-level click activity after keyword searches from a leading search engine in Korea, the authors find that consumers’ click activity after a keyword search is low and heavily concentrated on the organic list. However, searches of less popular keywords (i.e., keywords with lower search volume) are associated with more clicks per search and a larger fraction of sponsored clicks. This indicates that, compared with more popular keywords, consumers who search for less popular keywords expend more effort in their search for information and are closer to a purchase, which makes them more targetable for sponsored search advertising.
1	Consumer research has documented dozens of instances in which the introduction of an “irrelevant” third option affects preferences between the remaining two. In nearly all such cases, the unattractive dominated option enhances the attractiveness of the option it most resembles—a phenomenon known as the “attraction effect.” In the studies presented here, however, the authors contend that this phenomenon may be restricted to stylized product representations in which every product dimension is represented by a number (e.g., a toaster oven that has a durability rating of 7.2 and ease of cleaning rating of 5.5). Such effects do not typically occur when consumers experience the product (e.g., taste a drink) or when even one of the product attributes is represented perceptually (e.g., differently priced hotel rooms whose quality is depicted with a photo). The authors posit that perceptual representations of attributes do not support the sorts of comparisons that drive the attraction effect with highly stylized examples, and they question the practical significance of the effect.
1	Ninety-one attempts to produce an attraction effect (involving a total of 23 product classes and 73 different decoyed choice sets) produced only 11 reliable effects—significantly fewer than expected given the statistical power of the studies. Cross-scenario analyses indicated that the use of meaningful qualitative-verbal descriptions, as well as pictorial depictions, to differentiate choice options substantially reduced the size of those effects. Indeed, the authors found attraction effects at only chance levels using these types of stimuli. The article concludes with a brief discussion of the implications of these findings for both marketing practice and research.
1	The likelihood of replicating an effect such as asymmetric dominance (AD) largely depends on other, usually more important choice drivers (e.g., attributes, values). Accordingly, it is not surprising that the AD effect is often not observed when other choice drivers have greater impact and/or when the AD configuration is unlikely to be perceived. However, when price is an attribute (and in many other cases), the AD effect is often observed in both properly designed studies and the real world. The author notes that Frederick, Lee, and Baskin (2014) raise important questions, but they would have made a greater contribution had they (1) systematically studied the drivers of AD perceptions, (2) more accurately tried to replicate previous AD effect demonstrations, and (3) systematically studied the repulsion effect. The author also briefly comments on the Yang and Lynn (2014) studies, which he asserts did not test the AD effect properly in most cases.
1	Frederick, Lee, and Baskin (2014) and Yang and Lynn (2014) argue that the conditions for obtaining the attraction effect are so restrictive that the practical validity of the attraction effect should be questioned. In this commentary, the authors first ground the attraction (asymmetric dominance) effect in its historical context as a test of an important theoretical assumption from rational choice theory. Drawing on the research reported by scholars from many fields of study, the authors argue that the finding of an asymmetric dominance effect remains robust because it holds when the conditions of the study are essentially replicated. Next, the authors identify some of the factors that mitigate (and amplify) the attraction effect and then position the effect into a larger theoretical debate involving the extent to which preferences are constructed versus merely revealed. The authors conclude by arguing that researchers who try to measure values as well as choice architects who attempt to shape values must be sensitive to the context-dependent properties of choice behavior, as illustrated by the attraction effect.
1	The current marketing environment is characterized by a surge in multichannel shopping and increasing choice of advertising channels. This situation requires firms to understand how advertising in one channel (e.g., online) influences sales in another channel (e.g., offline). This article studies the presence, magnitude, and carryover of these cross-channel effects for online advertising (display and search) and traditional media. The analysis considers how these advertising expenditures translate directly into sales, as well as indirectly through intermediate search advertising metrics—namely, impressions and click-through rate. For a high-end clothing and apparel retailer, the authors find that cross effects exist and are important and that cross-effect elasticities are almost as high as own-effect elasticities. Online display and, in particular, search advertising is more effective than traditional advertising. This result is primarily due to strong cross effects on the offline channel. Return-on-investment calculations suggest that by ignoring these cross effects, firms substantially miscalculate the effectiveness of online advertising. Notably, the authors find that traditional advertising decreases paid search click-through rates, thus reducing the net cross effect of traditional advertising.
1	This article investigates how Internet users’ perceptions of control over their personal information affect how likely they are to click on online advertising on a social networking website. The analysis uses data from a randomized field experiment that examined the effectiveness of personalizing ad text with user-posted personal information relative to generic text. The website gave users more control over their personally identifiable information in the middle of the field test. However, the website did not change how advertisers used data to target and personalize ads. Before the policy change, personalized ads did not perform particularly well. However, after this enhancement of perceived control over privacy, users were nearly twice as likely to click on personalized ads. Ads that targeted but did not use personalized text remained unchanged in effectiveness. The increase in effectiveness was larger for ads that used more unique private information to personalize their message and for target groups that were more likely to use opt-out privacy settings.
1	Marketers frequently offer a variety of communications, brands, and service encounters that customers evaluate sequentially. When customers make these evaluations, their previous experiences in the sequence influence their current evaluation. The authors propose that these prior experiences serve as multiple reference points against which the target stimulus is judged, creating rival co-occurring comparison effects. Using real-world and experimental data, they find that assimilation and contrast effects occur simultaneously: there is assimilation to the first score within a sequence and contrast with the immediate predecessor as well as with extremes experienced earlier in the sequence. The authors document the moderating effects of extreme first stimuli, domain similarity, and individual factors of mood and expertise. They provide different recommendations for sequence construction on the basis of whether the marketer's goal is fairness, accuracy, or influencing choice. This research is unique in (1) showing how several preceding evaluations can each have an impact on a subsequent evaluation at the same time and (2) using real-world data to do so.
1	Many advertisers are reluctant to shift a large proportion of their advertising budgets to the Internet because they still view television advertising as the main vehicle for building a brand. Using a unique and rich data set comprising 20 campaigns across a variety of industries, this study demonstrates that Internet ads perform on par with television ads on the brand-building metrics that advertisers use and trust. The authors extend traditional brand–message recall measurements to facilitate comparisons between Internet formats and television by supplementing brand–message surveys conducted during the campaign with a set of precampaign surveys to control for preexisting brand knowledge. They find that accounting for differences in preexisting brand knowledge is paramount in obtaining valid comparisons across advertising formats because people who are exposed to Internet display ads have significantly lower levels of preexisting brand knowledge than television viewers. Without considering the differences in these “initial conditions,” television advertising seems to be more effective than advertising on the Internet, but when the preexisting differences among media formats are taken into account, the brand recall lift measures for Internet ads are statistically indistinguishable from comparable television lift measures.
1	Research on sales force evaluation has mostly relied on reflective metrics such as sales volume, revenue, and manager evaluations to assess and manage a sales force. However, businesses are moving from a product-centric to a customer-centric view and from a backward-looking to a forward-looking strategic perspective, so sales organizations must adapt to the ever-changing marketplace to maximize performance. The authors propose a forward-looking and profit-oriented metric to evaluate and demonstrate the effects of training type and incentive type on a salesperson's future value. Using a latent class modeling approach, they identify two distinct segments in the sales force that exhibit different responses to varying levels of training and incentives. This suggests that a one-size-fits-all approach to sales force management may be suboptimal. Finally, the authors also evaluate the magnitude of the proposed effects in the short run as well as the long run and show that the magnitudes of the effects could vary depending on the time horizon being considered. The authors close with a discussion of the implications for research and practice, including sales force evaluation through customer relationship management–based heuristics and optimal training and incentive management.
1	Pay-for-performance (P4P) pricing schemes such as pay per click and pay per action have increased in popularity in Internet advertising. Meanwhile, pay-per-impression (PPI) schemes persist, and several publishers have begun to offer a hybrid mix of PPI and P4P schemes. Given the proliferation of pricing schemes, this study examines the optimal choices for publishers. The authors highlight two-sided information asymmetries in online advertising markets and the consequent trade-offs faced by a high-quality publisher using P4P schemes. Pay-for-performance schemes enable a high-quality publisher to reveal its superior quality; however, such schemes may incur allocative inefficiencies stemming from inaccurate estimates of advertiser qualities. The authors identify conditions under which a publisher may opt for a PPI, P4P, or hybrid scheme and, in doing so, provide theoretical explanations for the observed variations in the pricing schemes and the increasing popularity of hybrid schemes. Using a new “uncompromised” equilibrium refinement, the authors find that the hybrid scheme can emerge as an equilibrium choice in a variety of conditions. In addition, they provide prescriptive guidelines for firms choosing between different pricing schemes.
1	Research on new ventures has indicated that poorly conducted marketing is among the main reasons for new venture failure. To acquire urgently needed initial funding, new ventures strive to conform to investors’ expectations of appropriate marketing capabilities because these capabilities may endow them with legitimacy in the eyes of potential investors. Drawing on organizational legitimacy and human resource theory, the authors argue that the characteristics of the chief marketing officer (CMO) may endow new ventures with marketing legitimacy. Employing a two-stage selection hazard rate analysis to simultaneously account for potential selection bias and right-censored observations, the authors analyze a comprehensive data set of 2,945 high-technology new ventures. Bearing in mind that this research is a first exploratory attempt to illuminate the role of marketing for new venture funding using correlational secondary data, the results indicate that CMO education, marketing experience, and industry experience are positively related to the likelihood of funding. Moreover, the relationships between CMO characteristics and funding are contingent on task-related uncertainty and industry legitimacy. These findings provide initial insights for entrepreneurs, venture capitalists, and public policy makers.
1	The authors explore the effects of having a large dominant competitor and show conditions under which focusing on a competitive threat, rather than hiding it, can actually help a brand. Through lab and field studies, the authors demonstrate that highlighting a large competitor's size and close proximity can help smaller brands rather than harm them. The results show that support for small brands goes up when faced with a competitive threat from large brands versus when they are in competition with brands that are similar to them or when consumers view them outside a competitive context. This support translates into purchase intentions, real purchases, and more favorable online reviews in a study of more than 10,000 Yelp posts. The authors argue that this “framing-the-game effect” is mediated by consumers' motivation to express their views and have an impact in the marketplace through their purchase choices.
1	In brand imitation lawsuits, the primary focus is on demonstrating harm as it relates to brand confusion. The authors' premise is that such a narrow focus does not capture the complete impact of brand imitation. They offer a broader, theory-based perspective of harm by proposing that brand imitation affects both brand consideration and preference in the consumer choice process. The authors establish a formal nexus between brand consideration and the legal doctrine “initial interest confusion.” Using choice experiments and statistical modeling, they show that brand imitation harms the imitated brand even after controlling for brand confusion. Moreover, brand imitation harms not only the imitated brand but also other national brands in the category that are not being imitated. To quantify brand consideration and preference harm in financial terms, the authors propose two metrics: choice share shift and share-equalizing price cut. The results show that brand imitation harms the national brand more when the imitating brand is not associated with a well-known retailer and that brand imitation harm extends beyond brand confusion. The proposed framework provides both legal and managerial guidance.
1	This article examines the concept of employee-based brand equity—the value that a brand provides to a firm through its effects on the attitudes and behaviors of its employees—and empirically demonstrates its significance on executive pay. Executives value being associated with strong brands and, therefore, accept substantially lower pay at firms that own strong brands. Consistent with identity theory, this effect is stronger for chief executive officers and younger executives than for other executives. Data from a large, cross-industry sample of executives suggest that academics and practitioners should take a broader view of the contributions of brand-related investments to firm value and make use of strong brands in pay negotiations that are typically viewed as being outside the realm of marketing.
1	One of the key challenges in empirically modeling the total impact of marketing assets on financial performance is the limited availability of marketing metrics data over time. The author presents an approach for estimating the total financial impact of marketing assets with limited time-series data and demonstrates the approach with an application to brand equity research. Consistent with prior research, the aggregate analyses indicate that brand equity, as measured by customer mindset metrics, positively affects current financial performance. In addition, the author documents brand equity's significant and much greater impact on the firm's future financial performance: at the aggregate, only a small portion of the total financial impact of brand equity is reflected in current-year profits, whereas the bulk of the profitability impact is realized in the future. Most importantly, however, the analyses document significant heterogeneity of these effects: in some industries, the entire direct impact is contemporaneous, whereas in others, no contemporaneous effects are observed and all of the profitability impact occurs in the future.
1	Customizing a product by choosing each of its attributes individually tends to be onerous for consumers, and the benefits of product customization may thus be offset by an increase in choice complexity. As a remedy for this dilemma, the current research introduces the customization via starting solutions (CvSS) architecture, which substantially reduces the complexity of product customization while preserving all of its advantages. Under CvSS, consumers first select one starting solution from a set of prespecified products, which they then refine to create their final customized product. Evidence from nine studies (three of which were conducted in field settings) across a wide range of product domains (shirts, cars, vacation packages, jewelry, and financial products) shows that the CvSS architecture results in substantial benefits relative to the standard attribute-by-attribute product customization format for both consumers (increased satisfaction with their product choices, reduced choice complexity, and enhanced mental simulation of product use) and firms (purchases of more feature-rich, and thus higher-priced, products).
1	Do customers exhibit recurring behaviors beyond repeat purchases? If so, what are those behaviors, how are they formed, and why should marketers care? The authors apply the theory of habit to customer behavior in the context of a large customer data set of a national retailer. They find that (1) beyond repeat purchases, customers' recurring behavior with respect to returning products, purchasing on promotion, and purchasing low-margin items can be quantified along a continuum of habit strength; (2) marketing has a temporal impact on the formation of different customers' habits; and (3) customers' purchase and promotion habits positively affect firm performance (by $58 million), whereas return and low-margin purchase habits negatively affect firm performance (by $62 million). The findings underscore the need for managers to consider customer habits beyond repeat purchases, take stock of customers' habit measures before implementing policy changes, and leverage the habit measures (as compared with using only traditional behavioral measures) to strategically allocate resources at the customer level to maximize customer and firm profits.
1	Some online display advertisements are annoying. Although publishers know the payment they receive to run annoying ads, little is known about the cost that such ads incur (e.g., causing website abandonment). Across three empirical studies, the authors address two primary questions: (1) What is the economic cost of annoying ads to publishers? and (2) What is the cognitive impact of annoying ads to users? First, the authors conduct a preliminary study to identify sets of more and less annoying ads. Second, in a field experiment, they calculate the compensating differential, that is, the amount of money a publisher would need to pay users to generate the same number of impressions in the presence of annoying ads as it would generate in their absence. Third, the authors conduct a mouse-tracking study to investigate how annoying ads affect reading processes. They conclude that in plausible scenarios, the practice of running annoying ads can cost more money than it earns.
1	Featured price cuts are a popular tool among brand manufacturers and retailers. However, there is increasing concern about the net sales and revenue gains from these promotions, because retailers and manufacturers may simply be subsidizing consumers who shop around. Thus, the (co-)occurrence of a brand's promotions across retailers has been placed high on the promotion-planning agenda. This article examines the mechanisms underlying out-of-phase versus in-phase schedules and empirically demonstrates their sales and revenue implications in four product categories, covering purchases of a national panel of households across eight years. The results reveal that calendar effects primarily materialize in categories in which the chosen retailer is driven by brand promotions. In those categories, alternating the timing of featured price cuts across chains substantially increases the manufacturer and retailers' immediate sales lift. However, with regard to net gains, striving for out-of-phase promotions—the dominant approach among chains—is not necessarily the best practice, because retailers observe the revenue advantage diminish, and manufacturers may even earn less.
1	Three laboratory experiments and a field experiment in a restaurant demonstrate that neither a price surcharge nor an unhealthy label is enough on its own to curtail the demand for unhealthy food. However, when the two are combined as an unhealthy label surcharge, they reduce demand for unhealthy food. The authors also show that the unhealthy label is as effective for women as the unhealthy label surcharge, whereas it backfires for men, who order more unhealthy food when there is an unhealthy label alone. The authors demonstrate that an unhealthy surcharge, which highlights both the financial disincentive and potential health costs, can significantly drive healthier consumption choices. From a policy and government perspective, if the goal is to reduce demand for unhealthy food, increasing the transparency of the health rationale for any financial disincentive is necessary to effectively lower unhealthy food consumption.
1	The authors propose a new means by which nonprofits can induce donors to give today and commit to giving in the future: contingent match incentives, in which matching is made contingent on the percentage of others who give (e.g., “if X% of others give, we will match all donations”). A field experiment shows that a 75% contingent match (such that matches “kick in” only if 75% of others donate) is most effective in increasing commitment to recurring donations. An online experiment reveals that the 75% contingent match drives commitment to recurring donations because it simultaneously provides social proof while offering a low enough target to remain plausible that the match will occur. A final online experiment demonstrates that the effectiveness of the 75% contingent match extends to one-time donations. The authors discuss the practical and theoretical implications of contingent matches for managers and academics.
1	The authors examine online affiliate marketing programs in which merchants oversee thousands of affiliates they have never met. Some merchants hire outside specialists to set and enforce policies for affiliates, whereas other merchants ask their marketing staff to perform these functions. For clear violations of applicable rules, the authors find that outside specialists are the most effective at excluding the responsible affiliates, which can be interpreted as a benefit of specialization. However, in-house staff are more successful at identifying and excluding affiliates whose practices are viewed as “borderline” (albeit still contrary to merchants’ interests), forgoing the efficiencies of specialization in favor of the better incentives of a company's staff. The authors consider the implications for marketing of online affiliate programs and for online marketing more generally.
1	This research examines how the salience of scarcity influences choices of individual items from a product class. The authors propose that overall perception of scarcity versus overall perception of abundance increases choice share of the most-preferred item from a product class. They argue that this phenomenon occurs because scarcity induces arousal and the heightened arousal polarizes the evaluations of individual items contained in the choice set. The results from five experiments show that scarcity versus abundance broadens the discrepancy between the liking of the favorite and nonfavorite items and leads to a greater choice share of the favorite item. The findings provide support for the arousal-based explanation, showing that the effect of scarcity salience on choices is mediated by consumers’ reported arousal level and moderated by an experimentally induced arousal state.
1	The authors examine how a reference to an unrelated product in the choice context affects consumers’ likelihood of donating to charity. Building on research on self-signaling, the authors predict that consumers are more likely to give when the donation appeal references a hedonic product than when a utilitarian product is referenced or when no comparison is provided. They posit that this phenomenon occurs because referencing a hedonic product during a charitable appeal changes the self-attributions, or self-signaling utility, associated with the choice to donate. A series of hypothetical and actual choice experiments demonstrate the predicted effect and show that the increase in donation rates occurs because the self-attributions signaled by a choice not to donate are more negative in the context of a hedonic reference product. Finally, consistent with these experimental findings, a field experiment shows that referencing a hedonic product during a charitable appeal increases real donation rates in a nonlaboratory setting. The authors discuss the theoretical implications for both consumer decision making and the self-signaling motives behind prosocial choice.
1	In this article, the authors propose that in the long run, a nonprofit organization with supportively oriented positioning (e.g., promoting a cause) is likely to survive longer and achieve more donations compared with a nonprofit with a combative orientation (e.g., fighting against something). To test this proposition, the authors adopt a three-pronged approach that (1) uses publicly available financial data from nonprofits’ tax filings over a ten-year period, (2) measures annual donor pledges from a field study with a registered nonprofit organization, and (3) examines actual donation behavior of participants in a longitudinal lab study. Moreover, the authors test this proposition for donations of money as well as time. They consider various theoretical mechanisms that might cause the proposed effect, such as regulatory focus theory, inertia in giving, and the preponderance of supportive charities.
1	The authors study the market for factual content and examine whether competition increases or decreases its provision. Factual content is supplied by commercial media firms, which observe a set of facts depicting the state of the world and selectively decide how to report them. Consumers value content that matches their opinion, which incentivizes media firms to slant their reports by omitting certain facts. Novel features in the authors’ model include consumers’ ability to anticipate the media's incentives for slant and the requirement that all media stances must be supported by facts. Furthermore, consumers find reports with more facts to be more convincing. Despite consumers’ ability to detect slant and their demand for factual support, the research shows that competition results in consumers reading fewer facts and being unable to update their prior beliefs about the state of the world. The authors also find that a monopoly medium may be more polarizing than competitive media and that polarized reporting can be less biased.
1	This research investigates optimal selling strategies and equilibrium welfare implications in markets with buyer inequity aversion. When buyers care about their surplus relative to seller profits but are uncertain about seller costs, buyers’ fairness perceptions and, thus, their willingness to pay may be malleable and susceptible to seller influence. If a seller's optimal behavior (e.g., pricing) is not completely unvarying in variable costs, buyers can rationally make inferences about seller costs from observed seller behavior. Consequently, buyers’ fairness perceptions and their willingness to pay can influence, and be influenced by, optimal selling strategies. The study characterizes a fair selling equilibrium in which optimal seller behavior and buyer perceived fairness are interactively derived. The author shows that seller ex ante profit may increase as more buyers become inequity averse. In addition, buyer ex ante surplus can be nonmonotonically influenced by an increase in the number of fair-minded buyers or in the degree of inequity aversion. These counterintuitive results pinpoint the importance of investigating the strategic interaction between buyer fairness perceptions and selling strategies. Finally, the basic model is extended to examine how the fair selling equilibrium may be influenced by cost disclosure, buyer dynamic learning, and seller competition.
1	People often brag about, or advertise, their good deeds to others. Seven studies investigate how bragging about prosocial behavior affects perceived generosity. The authors propose that bragging conveys information about an actor's good deeds, leading to an attribution of generosity. However, bragging also signals a selfish motivation (a desire for credit) that undermines the attribution of generosity. Thus, bragging has a positive effect when prosocial behavior is unknown because it informs others that an actor has behaved generously. However, bragging does not help—and often hurts—when prosocial behavior is already known, because it signals a selfish motive. In addition, the authors demonstrate that conspicuous cause marketing products have effects akin to bragging by signaling an impure motive for doing good deeds. Finally, the authors argue that bragging about prosocial behavior is unique because it undermines the precise information that the braggart is trying to convey (generosity). In contrast, bragging about personal achievements does not affect perceptions of the focal trait conveyed in the brag. These findings underscore the strategic considerations inherent in signaling altruism.
1	Consumers are often mindless eaters. This research provides a framework for how consumers can become more mindful of their food choices. To do so, the authors develop an ability-based training program to strengthen people's ability to focus on goal-relevant emotional information. They demonstrate not only that emotional ability (EA) is trainable and that food choices can be enhanced (Study 1) but also that EA training improves food choices beyond a nutrition knowledge training program (Study 2). In Study 3, the authors test a conceptual model and find that EA training increases goal-relevant emotional thoughts and reduces reliance on the unhealthy = tasty intuition. Both factors mediate mindful eating effects. Last, Study 4 demonstrates the long-term benefits of EA training by showing that emotionally trained people lose more weight in a three-month period than a control group and a nutrition knowledge training group. Together, these findings suggest that consumers can gain control of their food choices through the enhancement of EA. The article concludes with a discussion of implications for policy officials, health care professionals, and marketers.
1	Drawing on research on grounded cognition and metaphorical representation, the authors propose and confirm in five studies that physical height, or even the mere concept of height, can affect the perceptual and conceptual levels of mental construal. As such, consumers who perceive themselves to be physically “high” or elevated are more likely to adopt a global perceptual processing and higher level of conceptual construal, whereas those who perceive themselves to be physically “low” are more likely to adopt a local perceptual processing and lower level of conceptual construal. This difference in construal level also affects product choices that involve trade-offs between long-term benefits and short-term effort. The authors address alternative accounts such as vertical distance, visual distance, and perceived power. By highlighting the novel relationship between height and construal level, these findings contribute to research on grounded cognition and construal-level theory while also providing practical suggestions to marketing managers across a variety of domains.
1	People have a lay notion of rationality—that is, the notion of using reason rather than feelings to guide decisions. Yet people differ in the degree to which they actually base their decisions on reason versus feelings. This individual difference variable is potentially general and important but is largely overlooked. The present research (1) introduces the construct of lay rationalism to capture this individual difference variable and distinguishes it from other individual difference variables; (2) develops a short, easy-to-implement scale to measure lay rationalism and demonstrates the validity and reliability of the scale; and (3) shows that lay rationalism, as measured by the scale, can predict a variety of consumer-relevant behaviors, including product preferences, savings decisions, and donation behaviors.
1	The extent to which a brand's individual products (relative to competing products) are available to consumers for purchase in a retail store can critically affect the brand's overall performance. However, store-level product availability information is lost in aggregate market-level data sets and has been ignored by extant demand studies in general, which can create the risk of misinformed managerial decision making. In this research, the authors propose a unique methodology to enable manufacturers to infer retailers’ joint stocking probability of products from aggregate data and, thus, enable consumers’ choices to be contingent on the assortment of products available in retail stores. The application of the proposed framework in the context of an emerging market results in unbiased demand parameter estimates, a significantly better model fit, and richer managerial insights (compared with conventional approaches) pertaining to how brand performance is affected by the (1) dynamics of retailers’ stocking preferences, (2) assortment of products that retailers are more (vs. less) likely to jointly stock, and (3) cannibalization of retailers’ shelf space resulting from product line extensions.
1	It is becoming increasingly easier for researchers and practitioners to collect eye-tracking data during online preference measurement tasks. The authors develop a dynamic discrete choice model of information search and choice under bounded rationality, which they calibrate using a combination of eye-tracking and choice data. Their model extends Gabaix et al.'s (2006) directed cognition model by capturing fatigue, proximity effects, and imperfect memory encoding and by estimating individual-level parameters and partworths within a likelihood-based hierarchical Bayesian framework. The authors show that modeling eye movements as the outcome of forward-looking utility maximization improves out-of-sample predictions, enables researchers and practitioners to use shorter questionnaires, and allows better discrimination between attributes.
1	Consumers often make product choices that involve the consideration of money and time. Building on dual-process models, the authors propose that these two basic resources activate qualitatively different modes of processing: while money is processed analytically, time is processed more affectively. Importantly, this distinction then influences the stability of consumer preferences. An initial set of three experiments demonstrates that, compared with a control condition free of the consideration of either resource, money consideration generates significantly more violations of transitivity in product choice, while time consideration has no such impact. The next three experiments use multiple approaches to demonstrate the role of different processing modes associated with money versus time consideration in this result. Finally, two additional experiments test ways in which the cognitive noise associated with the analytical processing that money consideration triggers could be reduced, resulting in more consistent preferences.
1	The authors empirically explore how consumers update beliefs about a store's overall expensiveness. They estimate a learning model of store price image (SPI) formation with the impact of actual prices linked to category characteristics on a unique data set combining consumers’ store visit and purchase information with their price perceptions. The results identify characteristics that drive categories’ store price signaling power for different store formats. “Big ticket” categories with a narrow price range strongly shape consumers’ store price beliefs, whereas (volatile) prices of frequently or deeply promoted categories are less influential. At traditional supermarkets, consumers anchor and elaborate on prices of storable categories bought in large quantities and for which quality differentiation is high. For hard discounters, however, SPI is mostly shaped by frequently bought categories with narrow assortments. Notably, categories’ SPI signaling power is not proportional to their share of wallet at either type of chain. Managers can use these results to identify “Lighthouse” categories that signal low prices, yet make up a small portion of store spending, and in which price cuts do not overly hurt revenue.
1	Many service firms acquire customers by offering free-trial promotions. However, a crucial challenge is to retain the customers acquired with these free trials. To address this challenge, firms need to understand how free-trial customers differ from regular customers in terms of their decisions to retain the service. This article conceptualizes how marketing communication and usage behavior drive customers’ retention decisions and develops hypotheses about the impact of free-trial acquisition on this process. To test the hypotheses, the authors model a customer's retention and usage decisions, distinguishing usage of a flat-rate service and usage of a pay-per-use service. The model allows for unobserved heterogeneity and corrects for selection effects and endogeneity. Using household panel data from a digital television service, the authors find systematic behavioral differences that cause the average customer lifetime value of free-trial customers to be 59% lower than that of regular customers. However, free-trial customers are more responsive to marketing communication and usage rates, which offers opportunities to target marketing efforts and enhance retention rates, customer lifetime value, and customer equity.
1	Common wisdom suggests that managerial empathy (i.e., the mental process of taking a consumer perspective) helps executives separate their personal consumption preferences from those of consumers, thereby preventing egocentric preference predictions. The results of the present investigation, however, show exactly the opposite. First, the authors find that managerial empathy ironically accelerates self-reference in predictions of consumer preferences. Second, managers’ self-referential tendencies increase with empathy because taking a consumer perspective activates managers’ private consumer identity and, thus, their personal consumption preferences. Third, empathic managers’ self-referential preference predictions make them less likely to use market research results. Fourth, the findings imply that when explicitly instructed to do so, managers are capable of suppressing their private consumer identity in the process of perspective taking, which helps them reduce self-referential preference predictions. To support their conclusions, the authors present four empirical studies with 480 experienced marketing managers and show that incautiously taking the perspective of consumers causes self-referential decisions in four contexts: product development, communication management, pricing, and celebrity endorsement.
1	Customers often stockpile reward points in linear loyalty programs (i.e., programs that do not explicitly reward stockpiling) despite several economic incentives against it (e.g., the time value of money). The authors develop a mathematical model of redemption choice that unites three explanations for why customers seem to be motivated to stockpile on their own, even though the retailer does not reward them for doing so. These motivations are economic (the value of forgone points), cognitive (nonmonetary transaction costs), and psychological (customers value points differently than cash). The authors capture the psychological motivation by allowing customers to book cash and point transactions in separate mental accounts. They estimate the model on data from an international retailer using Markov chain Monte Carlo methods and accurately forecast redemptions during an 11-month out-of-sample period. The results indicate substantial heterogeneity in how customers are motivated to redeem and suggest that the behavior in the data is driven mostly by cognitive and psychological incentives.
1	Relatively few retailers include metrics such as product returns in their customer selection and optimal resource allocation algorithms when measuring and maximizing customer value. Even when they do include this metric, increases in product return behavior are usually considered merely an economic cost that must be managed by decreasing the marketing resource allocations toward the customers making the returns. However, recent research has suggested that satisfactory product return experiences can actually benefit firms by lowering the customer's perceived risk of current and future purchases. To better understand the role of this perceived risk in the firm–customer exchange process, the authors conduct a large-scale customer selection and optimal resource allocation field experiment with 26,000 customers from an online retailer over six months. They find that the firm is able to increase both its short-and long-term profits when accounting for the perceived risk related to product returns in addition to managing product return costs. Furthermore, the authors find that by including this risk, rather than simply implementing traditional customer lifetime value–based models generically, the firm can target more profitable customers.
1	This article examines the temporal dynamics in 11 consumer traits in a broad sample of 1,411 Dutch consumers followed for 12 years. The traits encompass consumer-specific expressions of all Big Five personality dimensions and map responses to the 4 Ps. The authors examine measurement stability and individual and population trajectories over time, as well as test–retest correlations, profile stability, and structural stability. They find that consumer traits are almost as stable as Big Five personality traits, even when measured with only a few items. Yet stability does not preclude change. The authors report notable trends that reflect the process of aging and/or changing attitudes in society over time. They disentangle both effects and find that society's changes in consumer traits are largely consistent with dynamic cultural theory. In paying special attention to the effects of the Great Recession, they find that price consciousness is most susceptible to changes in environmental conditions. Theoretical and managerial implications of the findings conclude.
1	In four experimental studies, the authors investigate the effect of innovation locus—whether the innovation is integrated with the base product (the core locus) or offered as a detachable accessory (the peripheral locus)—on consumers’ adoption intentions. The findings show that offering a really new innovation (RNI) as a detachable peripheral component leads to higher adoption intentions than integrating the same innovation into the core. Innovation locus, however, does not have an effect on incrementally new innovations. The positive effect of peripheral locus (relative to core locus) for RNIs occurs through four mechanisms: (1) reduced schema incongruity, (2) lower risk perceptions, (3) increased benefit understanding, and (4) greater perceived usage flexibility associated with the new product. The authors demonstrate these effects by using stimuli from four product categories and including both attitudinal and behavioral measures of innovation adoption. The findings have implications for product design strategies for RNIs.
1	Firms with different management teams evidence different strategic capabilities. Some are able to reason through the reactions of their competitors, whereas others are less sophisticated in their thinking. In such cases, conventional wisdom suggests that the strategic firms will undercut their less sophisticated competitors’ prices and earn greater profits. The authors show that, under certain conditions, the strategic firms charge higher prices and accrue smaller equilibrium profits than their nonstrategic counterparts. Strategic firms’ efforts to capitalize on their loyal customers’ higher willingness to pay increases nonstrategic firms’ share of price-sensitive consumers. Furthermore, by raising prices, strategic firms help their nonstrategic counterparts more than themselves. This outcome arises when the proportion of consumers loyal to each firm is sufficiently large. A laboratory test for the main proposition's predictive accuracy provides empirical support.
1	How do consumers adjust their spending when their budget changes? A common view is that the allocation of one's current budget should not depend on previous budget allocations. Contrary to this, the authors find that when the budget contracts to a particular level, consumers select less variety (as measured by the number of different items with some of the budget allocated to them) than when their budget expands to that same level. This budget contraction effect stems from a reduction in variety under the contracting budget, not from variety expansion under the expanding budget. Evidence from five experiments indicates that the effect is driven by a desire to avoid feelings of loss associated with spreading allocation cuts (relative to reference quantities from previous allocations) across many items.
1	Investment decisions play a crucial role in the way consumers manage their wealth, and therefore, it is important to understand how consumers make these decisions. This research contributes to this attempt by examining consumers’ investment decisions in response to new information about changes in uncertainty in financial markets. The authors identify possible conditions under which consumers, despite having new information about changes in market uncertainty, are less likely to assimilate the new information and consequently do not make investment decisions that are in line with their risk-aversion levels. Specifically, in a series of studies, the authors show that high rather than low need for cognitive closure can lead to a lack of openness to new information and therefore may dilute consumers’ tendency to update their investment portfolios in a way that reflects their risk preferences. In addition, the authors address possible ways to influence consumers’ assimilation of new information, to help even those with high need for cognitive closure make investment decisions that are in line with their levels of risk aversion.
1	This article investigates the in-flight marketplace, using detailed data of in-flight purchases to understand social effects in purchase behavior and determine their potential for designing marketing promotions. On average, a passenger is approximately 30% more likely to buy an item after being exposed to a lateral purchase. Analyses on the underlying mechanisms reveal that the classical social influence theories do not suffice to explain all the patterns in the data. The author proposes omission neglect, product contagion, and goal balancing as complementary theories. Finally, consumers’ willingness to buy is shown to be positively correlated with responsiveness to social influence. This finding indicates that homophily and social feedback effects—classically viewed in the literature as nuisances—can provide targeting value for the firm. By taking these factors into account during behavior-based targeting, firms can double the social spillovers of marketing actions.
1	This study examines the effects of Internet display advertising using cookie-level data from a field experiment at a financial tools provider. The experiment randomized assignment of cookies to treatment (firm ads) and control conditions (charity ads), enabling the authors to handle different sources of selection bias, including targeting algorithms and browsing behavior. They analyze display ad effects for users at different stages of the company's purchase funnel (i.e., nonvisitor, visitor, authenticated user, and converted customer) and find that display advertising positively affects visitation to the firm's website for users in most stages of the purchase funnel, but not for those who previously visited the site without creating an account. Using a binary logit model, the authors calculate marginal effects and elasticities by funnel stage and analyze the potential value of reallocating display ad impressions across users at different stages. Expected visits increase almost 10% when display ad impressions are partially reallocated from nonvisitors and visitors to authenticated users. The authors also show that results from the controlled experiment data differ significantly from those computed using standard correlational approaches.
1	Why do consumers often feel pressed for time? This research provides a novel answer to this question: consumers’ subjective perceptions of goal conflict. The authors show that beyond the number of goals competing for consumers’ time, perceived conflict between goals makes them feel that they have less time. Five experiments demonstrate that perceiving greater conflict between goals makes people feel time constrained and that stress and anxiety drive this effect. These effects, which generalize across a variety of goals and types of conflict (both related and unrelated to demands on time), influence how consumers spend time as well as how much they are willing to pay to save time. The authors identify two simple interventions that can help consumers mitigate goal conflict's negative effects: slow breathing and anxiety reappraisal. Together, the findings shed light on the factors that drive how consumers perceive, spend, and value their time.
1	Platform companies such as Alibaba.com increasingly rely on search advertising as a revenue source. This study examines (1) the direct effect of new and existing buyers and sellers on platform advertising revenue, (2) their indirect effect through two intermediary performance variables (buyer's click rate and seller's click price), and (3) how the effects differ between launch and mature stages of the search advertising service. Unique data collected from a leading transactional business-to-business electronic platform suggest that new buyers click on more search advertisements than existing buyers, especially after the firm's buyers and sellers have learned and adapted to the service (mature stage). New sellers tend to outbid existing sellers in the mature stage, but the opposite is true when the service is newly introduced (launch stage). Because existing sellers can more effectively send quality signals in the launch stage, attracting existing, rather than new, sellers has a greater effect on click rate in the launch stage; however, the opposite is true in the mature stage. Attracting new buyers also has a greater effect on click rate and price, especially in the mature stage. Finally, using cost data from the platform, this article examines the economic returns of attracting new and existing buyers and sellers with respect to advertising revenue.
1	The first decade of consumer neuroscience research has produced groundbreaking work in identifying the basic neural processes underlying human judgment and decision making, with the majority of such studies published in neuroscience journals and influencing models of brain function. Yet for the field of consumer neuroscience to thrive in the next decade, the current emphasis on basic science research must be extended into marketing theory and practice. The authors suggest five concrete ways that neuroscientific methods can be fruitfully applied to marketing. They then outline three fundamental challenges facing consumer neuroscientists and offer potential solutions for addressing them. The authors conclude by describing how consumer neuroscience can become an important complement to research and practice in marketing.
1	In the past decade, there has been a tremendous increase in the use of neurophysiological methods to better understand marketing phenomena among academics and practitioners. However, the value of these methods in predicting advertising success remains underresearched. Using a unique experimental protocol to assess responses to 30-second television ads, the authors capture many measures of advertising effectiveness across six commonly used methods (traditional self-reports, implicit measures, eye tracking, biometrics, electroencephalography, and functional magnetic resonance imaging). These measures have been shown to reliably tap into higher-level constructs commonly used in advertising research: attention, affect, memory, and desirability. Using time-series data on sales and gross rating points, the authors attempt to relate individual-level response to television ads in the lab to the ads’ aggregate, market-level elasticities. The authors show that functional magnetic resonance imaging measures explain the most variance in advertising elasticities beyond the baseline traditional measures. Notably, activity in the ventral striatum is the strongest predictor of real-world, market-level response to advertising. The authors discuss the findings and their significant implications for theory, research, and practice.
1	Considerable attention has been given to the notion of a set of humanlike characteristics associated with brands, referred to as “brand personality.” The authors combine newly available machine learning techniques with functional neuroimaging data to characterize the set of processes that give rise to these associations. The authors show that brand personality traits can be captured by the weighted activity across a widely distributed set of brain regions previously implicated in reasoning, imagery, and affective processing. That is, as opposed to being constructed through reflective processes, brand personality traits seem to exist a priori inside consumers’ minds, such that the authors are able to predict what brand a person is thinking about solely on the basis of the relationship between brand personality associations and brain activity. These findings represent an important advance in the application of neuroscientific methods to consumer research, moving from work focused on cataloging brain regions associated with marketing stimuli to testing and refining constructs central to theories of consumer behavior.
1	Price is a key factor in most purchases, but it can be presented at different stages of decision making. The authors examine the sequence-dependent effects of price and product information on the decision-making process at both neural and behavioral levels. During functional magnetic resonance imaging, the price of a product was shown to participants either before or after the product itself was presented. Early exposure to price, or “price primacy,” altered the process of valuation, as observed in altered patterns of activity in the medial prefrontal cortex immediately before making a purchase decision. Specifically, whereas viewing products first resulted in evaluations strongly related to products’ attractiveness or desirability, viewing prices first appeared to promote overall evaluations related to products’ monetary worth. Consistent with this framework, the authors show that price primacy can increase purchase of bargain-priced products when their worth is easily recognized. Together, these results suggest that price primacy highlights considerations of product worth and can thereby influence purchasing.
1	Although much progress has been made in relating brain activations to choice behavior, evidence that neural measures could actually be useful for predicting the success of marketing actions remains limited. To be of added value, neural measures should significantly increase predictive power, beyond conventional measures. In the present study, the authors obtain both stated preference measures and neural measures (electroencephalography; EEG) in response to advertisements for commercially released movies (i.e., movie trailers) to probe their potential to provide insight into participants’ individual preferences as well as movie sales in the general population. The results show that EEG measures (beta and gamma oscillations), beyond stated preference measures, provide unique information regarding individual and population-wide preference and can thus, in principle, be used as a neural marker for commercial success. As such, these results provide the first evidence that EEG measures are related to real-world outcomes and that these neural measures can significantly add to models predicting choice behavior relative to models that include only stated preference measures.
1	A wealth of research has explored whether marketing-based expectancies such as price and brand quality beliefs influence the consumption experience and subsequent behavior, but almost no research has examined individual differences in “marketing placebo effects.” In this article, the authors suggest three moderators of the effect of marketing-based expectancies on the behavioral and neural measures of the consumption experience, based on previous findings from neuroscientific literature investigating traditional clinical pain placebo effects. They use a novel automated structural brain imaging approach to determine individual differences and combine this approach with traditional behavioral experiments. The findings show that consumers high in reward seeking, low in somatosensory awareness, and high in need for cognition are more responsive to marketing placebo effects.
1	It is well established that neural imaging technology can predict preferences for consumer products. However, the applicability of this method to consumer marketing research remains uncertain, partly because of the expense required. In this article, the authors demonstrate that neural measurements made with a relatively low-cost and widely available measurement method—electroencephalography (EEG)—can predict future choices of consumer products. In the experiment, participants viewed individual consumer products in isolation, without making any actual choices, while their neural activity was measured with EEG. At the end of the experiment, participants were offered choices between pairs of the same products. The authors find that neural activity measured from a midfrontal electrode displays an increase in the N200 component and a weaker theta band power that correlates with a more preferred product. Using recent techniques for relating neural measurements to choice prediction, they demonstrate that these measures predict subsequent choices. Moreover, the accuracy of prediction depends on both the ordinal and cardinal distance of the EEG data; the larger the difference in EEG activity between two products, the better the predictive accuracy.
1	This article introduces the method of single-neuron recording in humans to marketing and consumer researchers. First, the authors provide a general description of this methodology, discuss its advantages and disadvantages, and describe findings from previous single-neuron human research. Second, they discuss the relevance of this method for marketing and consumer behavior and, more specifically, how it can be used to gain insights into the areas of categorization, sensory discrimination, reactions to novel versus familiar stimuli, and recall of experiences. Third, they present a study designed to illustrate how single-neuron studies are conducted and how data from them are processed and analyzed. This study examines people's ability to up-regulate (i.e., enhance) the emotion of fear, which has implications for designing effective fear appeals. The study shows that the firing rates of neurons previously shown to respond selectively to fearful content increased with emotion enhancement instructions, but only for a video that did not automatically evoke substantial fear. The authors discuss how the findings help illustrate which conclusions can and cannot be drawn from single-neuron research.
1	Electrophysiological and hemodynamic studies provide substantial evidence of dissimilar brain responses when people view emotional compared with neutral pictures. This study investigates consumer brain responses underpinning passive viewing of luxury (high emotional value) versus basic (low emotional value) branded products when participants are alone or with another person. Conforming to social facilitation theory and using electroencephalogram methods, the authors recorded event-related potentials while female participants passively viewed pictures of luxury and basic branded products. They examined event-related-potential amplitudes in three time windows, corresponding to the P2 and P3 components and the late positive potential (LPP). Dissimilar brain responses occurred in the Together but not the Alone condition for the P2 and P3 components over visual cortex sites. The LPP amplitude was higher for luxury than for basic branded products, but only in the Together condition, suggesting that the presence of another person magnifies the emotional effect of brand type. Taken together, the results suggest that LPP amplitude during passive viewing of relevant marketing images reflects increased attention allocation and motivational significance, both enhanced by the presence of another person, to stimuli with higher emotional value.
1	The present study examines the relationship between social influence and recommendation decisions among adolescents in the new media environment. Participants completed the App Recommendation Task—a task that captures neural processes associated with making recommendations to others, with and without information about peer recommendations of the type commonly available online. The results demonstrate that increased activity in the striatum and orbitofrontal cortex in response to peer recommendations is significantly correlated with participants changing their recommendations to be consistent with this feedback within subjects. Furthermore, individual differences in activation of the temporoparietal junction during feedback that peer recommendations varied from those of the participant correlated with individual differences in susceptibility to influence on recommendation decisions between subjects. These brain regions have previously been implicated in social influence and the concept of being a “successful idea salesperson,” respectively. Together, they highlight a potential combination of internal preference shifts and consideration of the mental states of others in recommendation environments that include peer opinions.
1	The authors identify customers, termed “Harbingers of failure,” who systematically purchase new products that flop. Their early adoption of a new product is a strong signal that a product will fail—the more they buy, the less likely the product will succeed. Firms can identify these customers through past purchases of either new products that failed or existing products that few other customers purchase. The authors discuss how these insights can be readily incorporated into the new product development process. The findings challenge the conventional wisdom that positive customer feedback is always a signal of future success.
1	This article applies a two-sided matching model to investigate the consequences of banning controversial sponsors. Using a data set containing the shirt sponsorships from 43 English football clubs between 1990 and 2010, the authors' estimates suggest assortative matching between a club's attendance and a sponsor's revenue. In addition, sponsorships become less valuable as the distance between the club and the sponsor's head office grows, particularly for low-performing clubs and smaller domestic sponsors. The authors use these estimates to simulate the consequences of banning alcohol and gambling sponsors. Their estimates of counterfactual outcomes suggest that such bans may not have the largest impact on the clubs (particularly the relatively successful clubs) that currently have alcohol and gambling sponsors. Instead, clubs with low attendance and clubs in low-income areas will be most affected by a ban. More generally, the results demonstrate that when marketing relationships are viewed as the result of a matching process, actions that affect only some marketers may have substantial indirect effects on a variety of players in the market.
1	When pursuing goals that involve subgoals of varying levels of difficulty, consumers prefer to follow a difficult-to-easy sequence when completing the subgoals because they believe that such a sequence renders the overall goal easier to achieve. However, consumers are actually more successful when they follow an easy-to-difficult sequence when completing subgoals. In seven studies, the authors present consistent evidence for this mismatch and explore the value of subgoals as an important boundary condition.
1	As social media and virtual communities increase in popularity, the spread of word of mouth becomes easier, challenging firms to measure and manage the success of marketing initiatives in online community environments. This research examines how consumers react to firms' active participation in consumer-to-consumer conversations in an online community setting. The authors develop a tailored community-matched measure of consumer reaction (consumer sentiment) and analyze more than 115,000 consumer posts from ten online forums with active firm participation. The results indicate that consumers show diminishing returns to active firm engagement, which, at very high levels, can undermine consumer sentiment. Further subgroup analyses by conversation type indicate that these relationships hold for conversations that address consumers' functional needs but do not hold for conversations that address social needs. Finally, the results show diminishing returns to firm engagement for consumers primarily interested in product-related support but show no relationship for consumers primarily interested in inspiration and entertainment. These findings provide insights for marketing performance measurement and resource allocation in online communities.
1	Service providers sometimes face mass service failures. These problems occur across service industries, ranging from severe Internet outages to major delays for airlines or trains. The literature has not yet addressed the following key question: How do service crises affect perceived service quality (PSQ) over time? To answer this question, the authors introduce a Double-Asymmetric Structural Vector Autoregressive model. It captures not only the short- and long-term effects of objective service performance on PSQ but also the differential effects of service crises versus service restoration. The authors analyze a unique data set from a major European railway company, spanning seven years of monthly observations. During this period, severe winter weather caused dramatic service crises. The authors find that performance losses loom larger than gains in the short run and also have permanent negative effects on PSQ in the long run. Consequently, a crisis followed by a restoration will result in a net negative long-term effect on PSQ. The impact of a crisis also depends on the prior trend in objective service performance.
1	Marketers often employ a variety of positive emotions to encourage consumption or promote a particular behavior (e.g., buying, donating, recycling) to benefit an organization or cause. The authors show that specific positive emotions do not universally increase prosocial behavior but, rather, encourage different types of prosocial behavior. Four studies show that whereas positive emotions (i.e., love, hope, pride, and compassion) all induce prosocial behavior toward close entities (relative to a neutral emotional state), only love induces prosocial behavior toward distant others and international organizations. Love's effect is driven by a distinct form of broadening, characterized by extending feelings of social connection and the boundary of caring to be more inclusive of others regardless of relatedness. Love—as a trait and a momentary emotion—is unique among positive emotions in fostering connectedness that other positive emotions (hope and pride) do not and broadening behavior in a way that other connected emotions (compassion) do not. This research contributes to the broaden-and-build theory of positive emotion by demonstrating a distinct type of broadening for love and adds an important qualification to the general finding that positive emotions uniformly encourage prosocial behavior.
1	This article explores how media coverage of a price war affects customer, retailer, and investor reactions over time. Using data covering a Dutch supermarket price war (2003–2005), the authors find that price reductions, especially deep reductions, trigger media coverage of the price conflict. This sets off a chain of reactions. Press messages have a significant effect on market share and abnormal stock returns, beyond retailers' own price and advertising. Importantly, this study uncovers striking asymmetries regarding the kind of coverage to which stakeholders react: whereas consumers only respond to the tone of price-related press coverage, retailers and investors only react to its quantity. Next, media coverage feeds back into the retailers' pricing actions: more media coverage triggers new price cuts in addition to those dictated by competitive reactions. As such, media coverage triggers a deeper spiral of price cuts, intensifying the competitive price battle. However, as the price war progresses, media coverage becomes less frequent and less favorable, which decelerates the downward price spiral.
1	Although going public allows firms access to more financial capital that can fuel innovation, it also exposes them to a set of myopic incentives and disclosure requirements that constrain innovation. This tension is expected to produce a unique pattern of innovation strategies among firms going public, causing such firms to increase their innovation levels but reduce their innovation riskiness. Specifically, the authors predict that after going public, firms innovate at higher levels and introduce higher levels of variety with each innovation; however, these innovations are less risky, characterized by fewer breakthrough innovations and fewer innovations in new-to-the-firm categories. The authors compare 40,000 product introductions in the period 1980–2011 from a sample of consumer packaged goods firms that went public with a benchmark sample of firms that remained private, and the results support their predictions. Utilizing tests to resolve questions about endogeneity, including self-selection, reverse causality, and competing explanations, the authors demonstrate that initial public offering selection and dynamics do not drive this going-public effect. The authors also uncover a set of industry factors that mitigate the drop in breakthrough innovation by offering product-market incentives that counterbalance the documented effect of stock market incentives.
1	The use of coupons delivered by mobile phone, so-called “m-coupons,” is growing rapidly. In this study, the authors analyze consumer response to m-coupons for a two-year trial at a large shopping mall. Approximately 8,500 people were recruited to a panel and received three text-message m-coupons whenever they “swiped” their mobile phone at the mall entrances, with downstream redemption recorded. Almost 144,000 m-coupons were delivered during the trial, representing 38 stores that supplied 134 different coupons. The authors find that an important feature of m-coupons is where and when they are delivered, with location and time of delivery significantly influencing redemption. How long the m-coupons are valid (expiry length) is also important because redemption times for m-coupons are much shorter than for traditional coupons. This finding suggests that their expiration length should be shortened to help signal time urgency. Nevertheless, traditional coupon features, such as face value, still dominate m-coupon effectiveness, as does the product type, with snack food coupons being particularly effective.
1	As consumers spend more time on their mobile devices, a focal retailer's natural approach is to target potential customers in close proximity to its own location. Yet focal (own) location targeting may cannibalize profits on inframarginal sales. This study demonstrates the effectiveness of competitive locational targeting, the practice of promoting to consumers near a competitor's location. The analysis is based on a randomized field experiment in which mobile promotions were sent to customers at three similar shopping areas (competitive, focal, and benchmark locations). The results show that competitive locational targeting can take advantage of heightened demand that a focal retailer would not otherwise capture. Competitive locational targeting produced increasing returns to promotional discount depth, whereas targeting the focal location produced decreasing returns to deep discounts, indicating saturation effects and profit cannibalization. These findings are important for marketers, who can use competitive locational targeting to generate incremental sales without cannibalizing profits. Although the experiment focuses on the effects of unilateral promotions, it represents an initial step in understanding the competitive implications of mobile marketing technologies.
1	In recent years, marketing researchers have become increasingly interested in under- and overreporting. However, there are few suitable approaches to operationalize deviations from the truth, particularly in behavioral domains in which self-reports are usually the only viable method of choice to measure behavior or attitudes. An especially difficult situation arises if some people underreport while others overreport. This article proposes a Bayesian item response theory model to quantify under- and overreporting in surveys. The method utilizes within-person differences between answers obtained under direct questioning (no privacy protection) and randomized-response questioning (which ensures item-level privacy protection). This method has the important features of incorporating behavioral response-mode effects (e.g., privacy loss when switching from direct to randomized-response questioning, response-mode inertia effects) and allowing the direction of bias to differ across respondents. The authors provide an empirical application for excessive alcohol consumption involving 1,408 respondents from a commercial web panel. The results show that respondents are averse to decreases in privacy and that randomized response is less effective if respondents provide biased responses to earlier direct questions.
1	Multitier store brands are increasing in significance in retail outlets. In this article, the authors theoretically examine the rationale for the existence of multitier store brands, their optimal quality levels, and their implications for consumer welfare and channel profits. They show that despite the manufacturer's efforts to deter the entry of store brands by providing side payments and/or introducing additional national brands, the retailer will offer multitier store brands in equilibrium. Furthermore, the quality levels of store brands and national brands are interlaced, with a store brand taking the top-quality position unless national brands outnumber store brands. Even though the proliferation of store brands reduces product differentiation, it does not decrease consumer welfare or channel profits. However, store brands hurt the manufacturer's profits and make two-part tariffs ineffective in improving channel coordination. Nonetheless, the retailer can enhance channel coordination by procuring the store brand from the national brand manufacturer. The authors extend their model in several directions to capture additional features of retail markets and assess the robustness of their findings.
1	Tasks such as the elimination of all debts when faced with the immediate option to spend can be unpleasant but not conceptually difficult. Dividing these tasks into smaller parts and completing the parts from smallest size to largest size can help people realize quick motivational gains that increase their likelihood of completing the task. The authors more broadly define this idea as “small victories” and discuss, model, and empirically examine two related behavioral theories that might explain it. A laboratory experiment tests this prediction and provides data for model calibration. Consistent with the idea of small victories, when a task is broken down into parts of unequal size, participants perform faster when the parts are arranged in ascending order (i.e., from smallest to largest) rather than descending order (i.e., from largest to smallest). The calibrated model is consistent with the directional predictions of each theory. However, when participants are given choice over orderings, they choose the ascending ordering least often. The authors conclude with a discussion of the efficacy of this method in stylized debt-repayment scenarios.
1	During retailer-initiated price wars (PWs), hundreds of brands are involved simultaneously, affecting brands’ and retailers’ positioning and ultimately making the performance outcome for individual brands difficult to predict. Likewise, the impact on brand performance after the PW, when prices are restored, is unclear. The authors use a natural-experiment approach to track brand sales and shares before, during, and after a long-lasting supermarket PW in the Dutch grocery market. They find that PWs are not truly revenue, sales, or share generators for most brands unless prices remain reduced permanently by the retailer. Only after the PW, when rivals’ prices are restored and the focal brand's reduced retail price is maintained, can substantial sales, revenues, and share gains be realized. Moreover, restoring prices without additional price promotion support can severely damage brands’ performance. Overall, national brands can gain share, sales, and revenue, but at the cost of not restoring regular prices, while private labels can benefit even when prices are restored after the PW ends.
1	In markets, firms must compete following a set of rules determined by laws, regulations, and social practices or pressures. This article investigates the effect of the degree of competition on the extent to which firms invest in behaving according to the rules of the marketplace. The authors model investments in following these rules as increasing the firm's marginal costs of production and decreasing its probability of being caught violating the market rules (and thus losing profits). They show that greater competition leads to smaller investments in following the market rules. This leads to (1) the existence of a social optimum degree of competition that is less than perfect competition and (2) more competition in general, thus prompting greater optimal monitoring efforts. Stricter market rules can lead to greater investments in satisfying the market rules and to lower production. The authors also present results on the likelihood of firms having broken the market rules depending on relative market shares, optimal monitoring, and the effect of dynamics on the incentives to satisfy the market rules.
1	Grocery retailers are joining the fray against obesity by offering a wide range of health and wellness programs at the point of sale. However, the success of such programs in promoting healthy choices remains an open question. The authors examine the effectiveness of a growing health and wellness initiative: a simplified nutrition scoring system. They present a conceptual framework that predicts the effect of such a scoring system on shoppers’ food decisions and their sensitivity to price and promotion, as well as the moderating influence of category-level factors. Using a large-scale quasi experiment and panel data across eight product categories for more than 535,000 members of a grocery chain's frequent shopper program, the authors demonstrate that the point-of-sale nutrition scoring system helped consumers make healthier food choices, such that they switched to higher-scoring products in the postrollout period. The results also reveal that shoppers became less price sensitive and more promotion sensitive following the introduction of the food scoring system. The authors discuss implications for research and practice.
1	The authors develop an affect-as-information model to explain how targeted emotions used in persuasion can influence unrelated products and brands that are presented nearby. In Study 1, the presence of an emotion-eliciting image affected consumer spending on unrelated products in a simulated retail environment. In Study 2, emotional processing ability and whether consumers monitored their feelings moderated emotional transfers between unrelated advertisements, providing support for an affect-as-information model. In Studies 3 and 4, the authors use the context of evaluative conditioning to generalize the incidence of emotional contagion in persuasive communication. They manipulate salience of affect and whether brand attitudes were measured or primed to provide additional evidence for and extend affect-as-information theory.
1	Dynamic customer targeting is a common task for marketers actively managing customer relationships. Such efforts can be guided by insight into the return on investment from marketing interventions, which can be derived as the increase in the present value of a customer's expected future transactions. Using the popular latent attrition framework, one could estimate this value by manipulating the levels of a set of nonstationary covariates. The authors propose such a model that incorporates transaction-specific attributes and maintains standard assumptions of unobserved heterogeneity. They demonstrate how firms can approximate an upper bound on the appropriate amount to invest in retaining a customer and demonstrate that this amount depends on customers’ past purchase activity—namely, the recency and frequency of past customer purchases. Using data from a business-to-business service provider as their empirical application, the authors apply the model to estimate the revenue the service provider loses when it fails to deliver a customer's requested level of service. They also show that the lost revenue is larger than the corresponding expected gain that would result from exceeding a customer's requested level of service. The authors discuss the implications of their findings for marketers in terms of managing customer relationships.
1	In the context of the U.S. lodging industry (1994–2012), the authors empirically quantify the effects of the two main factors driving the rebranding effects identified by the theoretical branding literature—(1) the brand effect and (2) the interaction effect between the product (the hotel property) and the brands involved—on occupancy rate and other hotel performance indicators. They find that, on average, rebranding results in approximately a 6.31% increase in occupancy rates; 60% of this effect can be attributed to the brand identities (e.g., Holiday Inn) before and after rebranding while the remaining 40% is attributable to the interaction effect. The authors also find heterogeneity in the property–brand interaction effect of rebranding along various observable characteristics of the hotels. They assess the robustness of the results to various model assumptions and alternative instruments; in addition, they use matching estimators for analysis and exploit rebranding as a consequence of hotel mergers as a means of measuring rebranding effects. Finally, the authors consider the impact that rebranding might have on competitors’ properties. Their approach to measuring rebranding effects can be applied broadly to firms and industries experiencing a decoupling of the individual components of their value chain.
1	Choice-based conjoint is a popular technique for characterizing consumers’ choices. Three eye-tracking studies explore decision processes in conjoint choices that take less time and become more accurate with practice. These studies reveal two simplification processes that are associated with greater speed and reliability. Alternative focus gradually shifts attention toward options that represent promising choices, whereas attribute focus directs attention to important attributes that are most likely to alter or confirm a decision. Alternative and attribute focus increase in intensity with practice. In terms of biases, the authors detect a small but consistent focus on positive aspects of the item chosen and negative aspects of the items not chosen. They also show that incidental exposures arising from the first-examined alternative or from alternatives in a central horizontal location increase attention but have a much more modest and often insignificant impact on conjoint choices. Overall, conjoint choice is found to be a process that is (1) largely formed by goal-driven values that respondents bring to the task and (2) relatively free of distorting effects from task layout or random exposures.
1	Business leaders, governments, and scholars are increasingly recognizing the importance of creativity. Recent trends in technology and education, however, suggest that many people are facing fewer opportunities to engage in creative thought as they increasingly solve well-defined (vs. ill-defined) problems. Using three studies that involve real problem-solving activities (e.g., putting together a LEGO kit), the authors examine the mindset created by addressing such well-defined problems. The studies demonstrate the negative downstream impact of such a mindset on both creative task performance and tendency to choose to engage in creative tasks. The research has theoretical implications for the creativity and mindset literature streams as well as substantive insights for managers and public policy makers.
1	Consumers routinely rely on forecasters to make predictions about uncertain events (e.g., sporting contests, stock fluctuations). The authors demonstrate that when forecasts are higher versus lower (e.g., a 70% vs. 30% chance of team A winning a game), consumers infer that the forecaster is more confident in his or her prediction, has conducted more in-depth analyses, and is more trustworthy. Consumers also judge the prediction as more accurate. This occurs because people tend to evaluate forecasts on the basis of how well they predict a target event occurring (e.g., team A winning). Higher forecasts indicate greater likelihood of the target event occurring and signal a confident analyst, while lower forecasts indicate lower likelihood and lower confidence in the target event occurring. Yet because with lower forecasts, consumers still focus on the target event (rather than its complement), lower confidence in the target event occurring is erroneously interpreted as the forecaster being less confident in his or her overall prediction (instead of more confident in the complementary event occurring, i.e., team A losing). The authors identify boundary conditions, generalize to other prediction formats, and demonstrate consequences of their findings.
1	Facing the issue of increasing customer churn, many service firms have begun recommending pricing plans to their customers. One reason behind this type of retention campaign is that customers who subscribe to a plan suitable for them should be less likely to churn because they derive greater benefits from the service. In this article, the authors examine the effectiveness of such retention campaigns using a large-scale field experiment in which some customers are offered plan recommendations and some are not. They find that being proactive and encouraging customers to switch to cost-minimizing plans can, surprisingly, increase rather than decrease customer churn: whereas only 6% of customers in the control condition churned during the three months following the intervention, 10% did so in the treatment group. The authors propose two explanations for how the campaign increased churn, namely, (1) by lowering customers’ inertia to switch plans and (2) by increasing the salience of past-usage patterns among potential churners. The data provide support for both explanations. By leveraging the richness of their field experiment, the authors assess the impact of targeted encouragement campaigns on customer behavior and firm revenues and derive recommendations for service firms.
1	Previous studies have shown that a firm needs to rely on its customers and employees to achieve superior performance. In this study, the authors draw on signaling theory to develop and empirically test a cross-validation argument. They argue that how a firm treats one stakeholder group will be interpreted by investors in conjunction with how the firm treats another stakeholder group. Investors use consistency in stakeholder group treatment as a signal of complementarity in a firm's investments, which can improve the likelihood of competitive advantage. Specifically, the authors propose that a firm's achievements (lapses) directed at customers have a stronger positive (negative) impact on investors’ valuation of the firm if they are validated by the firm's achievements (lapses) directed at employees, and vice versa. Applying a multilevel model to a large sample of firms across various industries between 1994 and 2010, the authors find evidence to support these arguments. In addition, they find that cross-validation is more crucial for firms with a narrow than a broad business scope.
1	Product usage experiences have a significant impact on postpurchase evaluation and subsequent behavior. Consumers look to their own experiences, as well as those of others, when deciding what to buy and what to recommend. Contrary to the intuition that varied experiences should enhance evaluation, five studies demonstrate that in some situations, perceiving usage experiences as less—not more—varied improves postpurchase product evaluation. Less varied usage experiences make consumers think that products are used more frequently. As a result, perceiving usage experiences as less varied makes consumers more satisfied with their purchase, more likely to buy it again, and more likely to recommend it. In addition to their practical implications, the findings make important theoretical contributions to the variety literature and toward understanding frequency and numerosity judgments.
1	This study examines the relationship between customer satisfaction, loyalty intention, and shareholder value at the firm and individual customer levels. The authors also explore industry differences by using a multilevel and random-effects approach in which individual customer scores are nested within firm-level data and the estimated interrelationships are treated as random coefficients that are explained by industry characteristics. They compile a unique and detailed data set, which covers 10 years of information on 137 firms and includes a matched sample of 189,069 customers from multiple sources, such as the American Customer Satisfaction Index, the Center for Research in Security Prices, and Compustat, to yield three important insights. First, aggregate firm-level effects may overestimate the impact that satisfaction has at the individual customer level. Second, a consideration of loyalty intention or repurchase intention as the mediator can improve our understanding of the satisfaction–shareholder value relationship and the fact that this relationship can vary across firms. Finally, the influence of satisfaction and loyalty intentions on shareholder value varies by industry. The authors discuss implications of findings for researchers, managers, and investors.
1	Across six experiments, the authors demonstrate that superficial imperfections in the form of packaging damage can engender negative consumer reactions that shape subsequent attitudes and behaviors in ways that are not always objectively justified. Their findings show that these reactions function in a relatively automatic fashion, even emerging under conditions in which the packaging damage does not convey information about a health and safety threat from the product. The authors extend work on contagion to show that superficial packaging damage can act as a contamination cue, automatically activating thoughts of contamination and health and safety concerns. This tendency to avoid superficial packaging damage can be eliminated by counteracting these thoughts of contamination. This can be done with positive brand associations (i.e., by branding the product as organic) or by creating a physical buffer between the packaging damage and the product itself. The authors close with a discussion of implications for marketers, consumers, and public policy makers.
1	People who want to control their body weight often aim to regulate both energy intake (by reducing food consumption) and energy expenditure (by increasing physical activity), thus addressing both sides of the energy balance equation. Marketers have developed fitness-branded food that may lead restrained eaters (i.e., consumers who are chronically concerned about their body weight) to believe that they can achieve these two goals at the same time by consuming the food. The purpose of this research is to investigate the effects of fitness branding in food marketing (i.e., the integration of fitness into the branding of food) on consumption and physical activity in restrained (vs. unrestrained) eaters. The authors show that fitness branding increases consumption volumes for restrained eaters unless consumers view the food as dietary forbidden. Restrained eaters are also less physically active after consuming fitness-branded food, and food consumption volumes mediate this effect in restrained eaters. Fitness branding may therefore have undesirable effects on the weight-control behaviors of restrained eaters because it discourages physical activity despite an increase in consumption, which is contrary to the principle of energy balance.
1	The author provides a guide for effectively communicating and organizing ideas in an academic marketing paper in a manner that allows reviewers to best see their merit. He argues that it is often not the central idea of a piece that falters, but rather the mechanical and stylistic expression of that idea. He posits a straightforward methodology for eschewing ambiguity and appropriately highlighting salient research against a backdrop that is clean and well constructed.
1	Online chatter is important because it is spontaneous, passionate, information rich, granular, and live. Thus, it can forewarn and be diagnostic about potential problems with automobile models, known as nameplates. The authors define “perverse halo” (or negative spillover) as the phenomenon whereby negative chatter about one nameplate increases negative chatter for another nameplate. The authors test the existence of such a perverse halo for 48 nameplates from four different brands during a series of automobile recalls. The analysis is by individual and panel vector autoregressive models. The study finds that perverse halo is extensive. It occurs for nameplates within the same brand across segments and across brands within segments. It is strongest between brands of the same country. Perverse halo is asymmetric, being stronger from a dominant brand to a less dominant brand than vice versa. Apology advertising about recalls has harmful effects on both the recalled brand and its rivals. Furthermore, these halo effects affect downstream performance metrics such as sales and stock market performance. Online chatter amplifies the negative effect of recalls on downstream sales by about 4.5 times.
1	Observed contracts in the real world are often very simple, which partly reflects the constraints faced by contracting firms in making the contracts more complex. In this article, the authors focus on one such rigidity: the constraints faced by firms in fine-tuning contracts to the full distribution of heterogeneity of their employees. The authors explore the implication of these constraints for the provision of incentives within the firm. The study's application is to sales force compensation, wherein a firm maintains a sales force to market its products. Consistent with ubiquitous real-world business practice, the study assumes that a firm is restricted to fully or partially set uniform commissions across its agent pool. The authors show that this restriction implies an interaction between the composition of agent types in the contract and the compensation policy used to motivate them, leading to a “contractual externality” in the firm and generating gains to sorting. This article explains how this contractual externality arises; discusses a practical approach to endogenizing agents and incentives at a firm in its presence; and presents an empirical application to sales force compensation contracts at a U.S. Fortune 500 company that explores these considerations and assesses the gains from a sales force architecture that sorts agents into divisions to balance firmwide incentives. Empirically, the authors find that the restriction to homogeneous plans significantly reduces a firm's payoff, relative to a fully heterogeneous plan, when the firm is unable to optimize the composition of its agents. However, a firm's payoff under a homogeneous plan comes very close to that under a fully heterogeneous plan when the firm can optimize both composition and compensation. Thus, in the empirical setting of this study, the ability to choose agents mitigates the loss in incentives from the restriction to uniform contracts. The authors conjecture this result may hold more broadly.
1	Psychological and physiological states such as mood, hunger, stress, and sleep deprivation are known to affect decision-making processes and therefore crucially influence consumer behavior. A possible biological mechanism underlying the observed variability of consumer behavior is the context-sensitive variation in the levels of neuromodulators in the brain. In a series of four experimental studies, the authors pharmaceutically reduce the levels of the neurotransmitter serotonin in the brain to diminish the availability of subjects’ cognitive resources. In doing so, they study how serotonin brain levels influence (1) subjects’ tendency to avoid buying and (2) consumers' preference for product options positioned as a compromise in a given choice set rather than for more extreme alternatives (i.e., the compromise effect). Using realistic product choice scenarios in a binding decision framework, they find that a reduction of brain serotonin levels leads to choice deferral and decreases the compromise effect, both as a within-subjects and as a between-subjects choice phenomenon. As such, this study provides neurobiological evidence for the assumption that the compromise effect is the result of deliberate and demanding thought processes rather than intuitive decision making.
1	The authors add to the sales management literature in three ways. First, they demonstrate that a firm can benefit from higher sales uncertainty. This is contrary to the finding from the standard principal–agent models that more sales uncertainty hurts the firm when agents are risk-averse. Second, the authors find that the risk-averse agent's total pay can increase when there is high sales uncertainty, and this too is contrary to the standard principal–agent model. Third, they provide intuition for this surprising result by showing that it holds when the slope of the sales response function is random but not when the intercept is random. When the responsiveness (slope) of sales to a decision variable (of the firm or the agent) is random, information about randomness becomes decision-relevant and the firm can exploit learned information. In this study's model, the agent and firm can receive noisy signals of random demand. When the customers’ response to effort (or price) is random, the decision about effort (price) responds optimally to information in a way that benefits the firm. When uncertainty is high, there is more potential information for the firm to exploit profitably, owing to the convexity of the sales with respect to the uncertainty parameter. This is enough to dominate the negative impact of uncertainty owing to agents’ risk aversion. When randomness affects only baseline sales (intercept), received signals are not decision-relevant. In that case, higher uncertainty has only a negative impact, just as in standard principal–agent models.
1	Advertising's influence on firm sales and firm value has drawn early attention from economists and accountants and more recent attention from marketers. Most studies that have investigated a link between advertising and sales have found such a link. However, studies that have investigated a link between advertising and firm value have only sometimes found that link. Meta-analysis has failed to determine moderators that govern the link between advertising and firm value. In this article, the authors hypothesize that advertising influences firm value for a differentiator because advertising can elaborate the firm's point of difference into brand equity, thereby building firm value. Advertising cannot build brand equity for a cost leader because such a firm has no point of difference on which to build. Identifying differentiators and cost leaders on the basis of firms’ reactions to a change in accounting regulations, the authors confirm hypotheses: advertising is related to sales for all firms, but it is more strongly related to firm value for differentiators than for cost leaders. Beyond explaining differences in advertising effectiveness, this study's indicator of differentiation versus cost leadership should enhance future analyses of marketing's effect on firm-level outcomes using archival financial data.
1	This study investigates how the valence, channel, and social tie strength of a word-of-mouth (WOM) conversation about a brand relate to the purchase intentions and WOM retransmission intentions of WOM recipients. The analysis uses a nationally representative sample of 186,775 individual conversations about 804 different brands. The authors find insights linking WOM valence, WOM channel, and social tie strength that could not be revealed if the WOM conversations were analyzed in an aggregated form. The findings contribute to research that investigates differences between offline WOM and online WOM. The authors find that the relationship of WOM valence with purchase intentions is exacerbated when the conversation occurs offline, whereas offline conversations tend to be more strongly associated with WOM retransmission intentions regardless of the conversation's valence. The results also provide insights into how interpersonal characteristics influence WOM outcomes. Specifically, the authors find that the strength of the social tie relationship tends to influence a WOM receiver's intentions to purchase a brand; however, social tie strength has a much weaker association with a consumer's WOM retransmission intentions.
1	Decisions about life annuities are an important part of consumer decumulation of retirement assets, yet they are relatively underexplored by marketing researchers studying consumer financial decision making. In this article, the authors propose and estimate a model of individual preferences for life annuity attributes using a choice-based stated-preference survey. Annuities are presented in terms of consumer-relevant attributes such as monthly income, yearly adjustments, period certain guarantees, and company financial strength. The authors find that these attributes directly influence consumer preferences beyond their impact on the annuity's expected present value. The strength of the direct influence depends on how annuities are described: when annuities are represented only through basic attributes, consumers undervalue inflation protection, and preferences are not monotonically increasing in duration of period certain guarantees. When descriptions of annuities are enriched with cumulative payment information, consumers no longer undervalue inflation protection, but nonlinear preferences for period certain options remain. The authors find that among annuities with the same expected payout but different annual increases and period certain guarantees, the proportion of consumers who choose the annuity over self-management can vary by more than a factor of 2.
1	This article examines the popular marketing practice of interdependent ideation, whereby firms solicit ideas from customers through online platforms that enable customers to be exposed to or “inspired” by other customers’ ideas when generating their own. Although being exposed to others’ ideas means that customers are “connected” (at least implicitly) in a communication network that facilities the flow of ideas, the effect of network structure on individual innovativeness has not been considered in this context. The authors examine how, when, and why network structure—specifically, the clustering, or interconnectivity, of one's “inspirations” (other customers)—affects the innovativeness of individual customers’ product/service ideas in ideation tasks. Across five experiments, the authors show that (1) higher clustering/interconnectivity negatively affects the innovativeness of a customer's ideas; (2) this effect occurs because idea inspirations are more likely to be similar or redundant when their sources (i.e., other customers to which one is connected) are clustered; (3) greater redundancy among ideas used as inspirations is what causes lower innovativeness; and (4) this effect is attenuated when customers do not rely on other customers’ ideas for inspiration.
1	Third party–hosted consumer communities in general, and brand communities in particular, have been touted for their ability to generate value for firms by promoting consumer-to-consumer (C2C) helping. However, little research has examined whether consumer communities actually foster C2C helping, and who is helped. In contrast, the brand-community literature suggests community strategies may reduce the likelihood of community members helping non–community members. If so, strategies that promote third party–hosted brand or product-category communities may be counterproductive in fostering C2C helping. Should firms focus on promoting brand communities, promoting product-category communities, or both? On the basis of a hazard model analysis of 9,192 actual C2C helping events over a 25-month period, and supported by a second cross-sectional study, this article examines how participation in brand and product-category communities influences one's likelihood of helping others. We find that brand-community participation increases one's likelihood of helping fellow members while reducing the likelihood of helping members of rival brand communities. Surprisingly, product-category community participation reduces one's likelihood of helping members of brand communities. The authors discuss managerial recommendations.
1	The increasing amount of electronic word of mouth (eWOM) has significantly affected the way consumers make purchase decisions. Empirical studies have established an effect of eWOM on sales but disagree on which online platforms, products, and eWOM metrics moderate this effect. The authors conduct a meta-analysis of 1,532 effect sizes across 96 studies covering 40 platforms and 26 product categories. On average, eWOM is positively correlated with sales (.091), but its effectiveness differs across platform, product, and metric factors. For example, the effectiveness of eWOM on social media platforms is stronger when eWOM receivers can assess their own similarity to eWOM senders, whereas these homophily details do not influence the effectiveness of eWOM for e-commerce platforms. In addition, whereas eWOM has a stronger effect on sales for tangible goods new to the market, the product life cycle does not moderate the eWOM effectiveness for services. With respect to the eWOM metrics, eWOM volume has a stronger impact on sales than eWOM valence. In addition, negative eWOM does not always jeopardize sales, but high variability does.
1	Launching breakthrough and incremental new products is vital to firm performance; it also resonates with both ego (i.e., directly connected partners) and global (i.e., interconnected ties in an industry) network perspectives. Prior research has listed several ego network– and global network–level factors that affect innovations, but this study goes a step further, to reveal the interactions of these factors as critical product launch mechanisms. An analysis of alliance networks in the consumer packaged goods industry from 1990 to 2010 shows that a central position in a global network represents a double-edged sword: it improves a firm's incremental new product launches but harms its breakthrough new product launches. Furthermore, a firm's ego network (manifested as density and diversity) and R&D capability enable it to leverage its global network position by enhancing the benefits for incremental new products and mitigating its hazards for breakthrough new products. This study's findings thus offer new insights into the role of ego and global networks in facilitating or hindering new product launches.
1	Many marketing communications are carefully designed to cast a brand in its most favorable light. For example, marketers may prefer to highlight a brand's membership in the top 10 tier of a third-party list instead of disclosing the brand's exact rank. The authors propose that when marketers use these types of imprecise advertising claims, subtle differences in the selection of a tier boundary (e.g., top 9 vs. top 10) can influence consumers’ evaluations and willingness to pay. Specifically, the authors find a comfort tier effect in which a weaker claim that references a less exclusive but commonly used tier boundary can actually lead to higher brand evaluations than a stronger claim that references a more exclusive but less common tier boundary. This effect is attributed to a two-stage process by which consumers evaluate imprecise rank claims. The results demonstrate that consumers have specific expectations for how messages are constructed in marketing communications and may make negative inferences about a brand when these expectations are violated, thus attenuating the positive effect such claims might otherwise have on consumer responses.
1	Anchoring, the biasing of estimates toward a previously considered value, is a long-standing and oft-studied phenomenon in consumer research. However, most anchoring work has been in the lab, and the results from field work have been mixed. Here, the authors use real transactions from an empirically investigated and commercially-employed pricing scheme (“pay what you want”) to better understand how anchors influence payments. Sixteen field studies (N = 21,997) and four hypothetical studies (N = 3,174) reveal four main points: (1) Although anchoring replicates both with and without financial consequences (Studies 1–2), the percentile rank gap between anchors in the distribution of payments is a much stronger predictor of anchoring emerging than merely the absolute gap between the anchors on a number line (Studies 3–5). (2) Low anchors influence payments more than high anchors (Studies 6a–b). (3) Findings from the literature that should enhance anchoring effects—anchor precision, descriptive and injunctive norms, nonsuggestions—yield null results in payment (Studies 7–13). (4) The above patterns do not emerge in hypothetical settings (Studies 14a–d), in which anchoring is as big and reliable as the literature has previously suggested.
1	Encouraging consumers to select meals in advance rather than at mealtime has been proposed as a strategy to promote healthier eating decisions, taking advantage of the improved self-control that is thought to accompany decisions about the future. In two field studies at an employee cafeteria and a third in a university setting, we examine how time delays between placing a lunch order and picking it up affect the healthfulness of that lunch. The first study, a secondary data analysis, finds that longer delays between placing an order and picking up the meal are associated with reductions in calorie content. The second study tests the causality of this relationship by exogenously restricting some lunch orders to be substantially delayed, leading to a marginally significant (approximately 5%) reduction in calories among delayed orders. The third study compares orders for truly immediate consumption versus orders placed in advance and demonstrates a significant (100 calorie, or approximately 10%) reduction in lunch calories. We discuss evidence regarding possible theoretical mechanisms underlying this effect, as well as practical implications of our findings.
1	The authors focus on repeated distributive negotiations to investigate how expectations of role reversal in future transactions (i.e., a buyer [seller] in one transaction is the seller [buyer] in the next transaction) affect behaviors in the current negotiation. They demonstrate that when negotiators expect a role reversal, they are likely to make more concessions and reach agreement more quickly in the current negotiation. The authors find that this effect is driven by negotiators’ beliefs that they will be able to recover these concessions, because negotiators expect their counterparts to reciprocate in the later transaction when the parties reverse roles. However, when the two negotiations occur in different “accounting” periods (i.e., fiscal periods) or when the negotiating parties do not explicitly communicate their willingness to reverse roles in the future, role-reversal expectations do not affect concession making. Implications arise in both managerial and consumer contexts where the possibility of engaging in future negotiations—as well as reversing roles—exists.
1	The marketplace is replete with productivity metrics that put units of output in the numerator and one unit of time in the denominator (e.g., megabits per second [Mbps] to measure download speed). In this article, three studies examine how productivity metrics influence consumer decision making. Many consumers have incorrect intuitions about the impact of productivity increases on time savings: they do not sufficiently realize that productivity increases at the high end of the productivity range (e.g., from 40 to 50 Mbps) imply smaller time savings than productivity increases at the low end of the productivity range (e.g., from 10 to 20 Mbps). Consequently, the availability of productivity metrics increases willingness to pay for products and services that offer higher productivity levels. This tendency is smaller when consumers receive additional information about time savings through product experience or through metrics that are linearly related to time savings. Consumers’ intuitions about time savings are also more accurate when they estimate time savings than when they rank them. Estimates are based less on absolute than on proportional changes in productivity (and proportional changes correspond more with actual time savings).
1	Price transparency initiatives are typically undertaken by third parties to ensure that consumers can compare the prices of competing offers in markets in which obtaining such information is costly. Such practices have recently become widespread, yet it is unclear whether the increased price competition due to lower search costs overcomes the potential for collusion between competitors due to lower price coordination costs. Motivated by this question, the authors investigate the effect of mandatory price posting (on large electronic signs) on the pricing behavior of competing gas stations in the Italian highway system. The authors find that when prices are posted, the average price of gasoline decreases by 1 euro cent per liter, which represents about 20% of stations’ margins. About half the price decrease can be attributed to the introduction of a sign posting a station's own price and those of its nearest neighbors, with the other half due to the introduction of other signs posting the prices of other stations on the same road. Despite the price reduction, however, the introduction of signs seems to have little impact on price dispersion, suggesting that price uncertainty persists even after the policy is implemented. Analysis of customer transaction data confirms this finding, showing that less than 10% of consumers use the posted price information effectively.
1	Although yes/no response formats have been used to increase enrollment rates in several different types of programs, their effectiveness has generally been tested in forced-choice settings. The effects on postchoice engagement have not been measured. Across two field experiments in an e-mail context in which choice is not forced, the authors demonstrate a substantial advantage in click-through rates for a yes/no response format over traditional opt-in response formats. The increase in click-through rate does, under certain conditions, also persist through downstream program enrollment and participation. Finally, though noting that the yes/no format advantage is probably multidetermined, the authors discuss several potential psychological mechanisms, which are particularly relevant in non-forced-choice settings. The authors also discuss how the yes/no response format might operate in other settings, such as the implementation of mandated choice for organ donation.
1	In search of effective ways to encourage consumers to follow desired behaviors such as healthy eating, recycling, or financial planning, marketers sometimes use praise (e.g., “You are doing great”) and sometimes use scolding (e.g., “You are not doing enough”). However, the effectiveness of each approach in triggering behavior is not clear. A possible reason for the mixed results in this area is that it is not only what one says that matters but also how one says it: praising and scolding can be performed with a more or less assertive tone. This research introduces assertiveness as a moderator that can explain when praising or scolding would be more effective. Two field experiments in the context of hand hygiene and financial planning demonstrate that when communicators praise consumers, an assertive tone may be more effective in encouraging behavior, whereas scolding requires a nonassertive tone. The authors then replicate these field findings in a controlled laboratory experiment, which also provides click rates as an actual behavioral outcome.
1	People can be aware (conscious) or unaware (unconscious) of an active goal when making a choice. Being aware of a goal enables people to use conscious strategies to identify attributes that are relevant to goal pursuit and to assess the efficacy of the attributes of each choice alternative. For most people, this process encourages the choice of the most goal-consistent alternative. For some people, this process encourages the consideration of trade-offs, activates a competing goal, and encourages the choice of a goal-inconsistent alternative. With unconscious goal pursuit, people cannot devote resources to assessing the efficacy of the attributes of each alternative; therefore, they match the accessible goal to the attributes of the available alternatives. As a result, the unconscious selects an alternative with attributes that are consistent with the goal and not necessarily the alternative that is most efficacious for the goal. The authors investigate these processes by manipulating the conscious system's ability to assess the efficacy of product attributes and the unconscious system's ability to match the accessible goal to product attributes.
1	The author analyzes the impact of online ads on the advertiser's competitors, using data from randomized field experiments on a restaurant-search website. He finds that ads increase the chances of sales for nonadvertised restaurants significantly. The spillover benefits are concentrated on restaurants that serve the advertiser's cuisine and have a high rating on the restaurant-search website. The extent of spillovers also depends on the intensity of the advertising effort. The spillovers are largest when the intensity (frequency) of advertising is low. As the intensity increases, the spillovers disappear and the advertiser gains more sales. These patterns are consistent with the following mechanism: ads increase the chance of consumers buying the advertised product but also remind consumers of similar (nonadvertised) options. Higher ad intensity leads to a stronger direct effect favoring the advertiser and can offset the spillover caused by the broader reminder.
1	Sex is ubiquitous in advertising, yet little research has explored the effect of exposure to sexual imagery on preferences. Although sex and romance tend to go together in real-world relationships, the authors find that exposure to sex-based ads decreases preference for romantically linked products and services in men. Furthermore, the authors find that the effect is one directional, such that exposure to romantic imagery in ads does not decrease men's preference for sex-related products. Finally, the authors find that exposure to sex-based ads does not lead to a decreased preference for romantically linked products in women. The authors explain this pattern of results through the relatively opportunistic nature of the sex drive in men. They close with a discussion about implications for theories of fundamental motives and for the effect of sex-based advertisements on dating and relationships.
1	The authors highlight the need for and develop a framework for engagement by reviewing the relevant literature and analyzing popular-press articles. They discuss the definitions of the focal constructs—customer engagement (CE) and employee engagement (EE)—in the engagement framework, capture these constructs’ multidimensionality, and develop and refine items for measuring CE and EE. They validate the proposed framework with data from 120 companies over two time periods, and they develop strategies to help firms raise their levels of CE and EE to improve performance. They also observe that the influence of EE on CE is moderated by employee empowerment, type of firm (business-to-business [B2B] vs. business-to-consumer [B2C]), and nature of industry (manufacturing vs. service); in particular, this effect is stronger for B2B (vs. B2C) firms and service (vs. manufacturing) firms. The authors find that although both CE and EE positively influence firm performance, the effect of CE on firm performance is stronger. Furthermore, the effect of CE and EE on performance is enhanced for B2B (vs. B2C) and for service (vs. manufacturing) firms.
1	The interest in the value relevance of marketing investments has given rise to numerous studies on the marketing–finance interface. This study integrates extant research findings and establishes empirical generalizations on marketing's impact on firm value. Specifically, the authors conduct a meta-analysis of prior econometric elasticity estimates of the stock market impact of marketing actions and marketing assets. Analyses based on 488 elasticities drawn from 83 studies reveal a mean elasticity of .04 for advertising expenditure variables and of .54 for marketing asset variables. Among marketing assets, customer-related assets show a higher mean elasticity of .72, compared with .33 for brand-related assets. Further analyses show that advertising elasticities are lower in more concentrated industries and that marketing asset elasticities are higher during recession times. Researchers should also be aware that characteristics of the research design (e.g., the type of firm value metric used, the omission of control variables, or not accounting for endogeneity) may affect the estimation results.
1	This article examines how consumers forecast their future spare money, or “financial slack.” Although consumers generally think that both their income and expenses will rise in the future, they underweight the extent to which their expected expenses will cut into their spare money, a phenomenon the authors term “expense neglect.” The authors test and rule out several possible explanations for this phenomenon and conclude that expense neglect is due in part to insufficient attention toward expectations about future expenses relative to future income. “Tightwad” consumers, who are chronically attuned to costs, show less severe expense neglect than “spendthrifts,” who are less attuned to costs. The authors further find that expectations regarding changes in income (and not changes in expenses) predict responses to the Michigan Index of Consumer Sentiment, a leading macroeconomic indicator. Finally, the authors conduct a meta-analysis of their entire file drawer (27 studies, 8,418 participants) and find that (1) across studies, participants place 2.9 times greater weight on income change than they do on expense change when forecasting changes in their financial slack, and (2) expense neglect is stronger for distant than for near-future forecasts.
1	This research demonstrates that the synesthetic cross-modal correspondence between sound frequency and color lightness can guide visual attention: high-frequency (low-frequency) sounds guide visual attention toward light-colored (dark-colored) objects. Three eye-tracking studies indicate that this influence is automatic; it arises without goals or conscious awareness, it seems to take precedence over the influence from a simultaneously occurring semantic correspondence, and it even operates despite explicit instructions to the contrary. Two additional studies highlight the potential role for this influence in marketing contexts. In Study 4, the audio frequency in a soundtrack guides visual attention in a commercial, as evidenced by recall of marketing messages. In Study 5, customers in a supermarket exposed to high-frequency (vs. low-frequency) music are more likely to purchase products from a shelf with light (vs. dark) decor.
1	This research demonstrates the importance of thin slices of information in ad and brand evaluation, with important implications for advertising research and management. Three controlled experiments, two in the behavioral lab and one in the field, with exposure durations ranging from very brief (100 msec) to very long (30 sec), demonstrate that advertising evaluation critically depends on the duration of ad exposure and on how ads convey which product and brand they promote, but in surprising ways. The experiments show that upfront ads, which instantly convey what they promote, are evaluated positively after brief but also after longer exposure durations. Mystery ads, which suspend conveying what they promote, are evaluated negatively after brief but positively after longer exposure durations. False front ads, which initially convey another identity than what they promote, are evaluated positively after brief exposures but negatively after longer exposure durations. Bayesian mediation analysis demonstrates that the feeling of knowing what the ad promotes accounts for these ad-type effects on evaluation.
1	Because security analysts, who serve as brokers between public firms and investors, arrive at their forecasts by incorporating guidance from managers, there is immense pressure on the managers to meet or beat analyst earnings forecasts; moreover, investors reward (penalize) firms for exceeding (missing) analyst forecasts. Reasoning that decisions taken in response to analyst forecasts involve discretionary budgets, the authors study four contingent conditions under which quarterly analyst forecasts drive unanticipated adjustments to advertising and R&D budgets, and the long-term consequences of these budgetary changes. The choice of contingent conditions is related to agency theory–driven concepts of monitoring and bonding costs. Results from a panel data set of 515 firms and a hierarchical Bayesian model that provides firm-level coefficients show that both artificially imposed incentives on managers (monitoring costs) and personal career management concerns (bonding costs) moderate the extent to which managers react to analyst forecasts. Specifically, (1) bonus versus equity proportion of CEO compensation enhances the likelihood of managers reacting to analyst forecasts with unanticipated decreases in advertising and R&D budgets; (2) output experience of CEOs decreases this likelihood; (3) throughput experience of CEOs increases this likelihood; and (4) increasing marketing and R&D intensity decreases this likelihood. The authors also find that the unanticipated adjustments in advertising and R&D budgets adversely affect long-term firm returns and risk.
1	Consumers readily indicate that they like options that appear dissimilar—for example, enjoying both rustic lake vacations and chic city vacations, or liking both scholarly documentary films and action-packed thrillers. However, when predicting other consumers’ tastes for the same items, people believe that a preference for one precludes enjoyment of the dissimilar other. Five studies show that people sensibly expect others to like similar products, but erroneously expect others to dislike dissimilar ones. While people readily select dissimilar items for themselves (particularly if the dissimilar item is of higher quality than a similar one), they fail to predict this choice for others—even when monetary rewards are at stake. The tendency to infer dislike from dissimilarity is driven by a belief that others have narrow and homogeneous ranges of preferences.
1	Consumers frequently consume hedonic products together with other consumers and derive value from this shared experience. This article investigates the impact of shared consumption, a type of social influence that determines the enjoyment of joint experiences, in the context of a typical hedonic product: movies. The authors argue that this type of influence has important consequences for the diffusion curves of hedonic goods that are consumed together and the effectiveness of advertising in generating launch and postlaunch sales. An empirically validated agent-based model simulates the U.S. motion picture market, with new movies launching, competing, and exiting. The agent-based model serves as a means to demonstrate the essential role of shared consumption for explaining movie life cycles and tests how advertising expenditures accelerate and/or acquire movies’ demand in markets with varying levels of shared consumption. The results provide key theoretical insights for the new product diffusion of hedonic products and help managers predict the financial consequences of their strategic decisions.
1	A plural alliance structure involving multiple downstream partners has become increasingly popular, yet investigations of marketing alliances continue to address mainly dyadic structures. The authors present learning and dependence balancing as key mechanisms to understand the relative performance differences between plural and dyadic structures, as well as the determinants of effective collaboration in a plural structure. Two complementary studies test the performance of plural and dyadic structures in a wide range of high-tech industries. The analysis of both plural and dyadic structure alliances in an event study shows that plural structures outperform dyadic structures for the upstream firm when marketing alliances extend to product-related tasks, the upstream firm has more alliance experience, or the industry is growing fast; however, dyadic structures perform better when the upstream market is more competitive. A second study, focusing only on plural structure alliances, shows that horizontal relationship factors (i.e., market overlap and prior relationship between downstream partners) interact with the upstream firm's greater alliance experience and reputation to lead to better returns for the upstream firm.
1	The authors offer a new conceptualization and operational model of consumer choice that allows context-sensitive information usage and preference heterogeneity to be separately and simultaneously captured, thus transforming the axiom of full information use into a testable hypothesis. A key contribution of the proposed framework is the integration of two previously disjointed and often antagonistic research paradigms: (1) the economic rationality perspective, which assumes stable preferences and full information usage, and (2) the psychological bounded-rationality perspective, which allows context-sensitive preferences and information selectivity. The authors demonstrate that the two paradigms can and do coexist in the same decision-making space, even at the level of individual consumer choices. The proposed information archetype mixture model is tested in four studies that span different product categories and levels of task complexity. The findings have ramifications for choice modeling theory and implementation, beyond the disciplinary boundaries of marketing to applied economics and choice-focused social sciences.
1	The authors identify and examine the effect of space-to-product ratio on consumer response; very generally, consumers perceive products as more valuable when more space is devoted to their display. In both lab and field studies, the authors find that this phenomenon influences total sales, purchase likelihood, and even perceived product experience (taste perceptions). More interstitial space increases perceptions of individual products as more aesthetically pleasing and the store as more prestigious. The authors find these effects across a variety of product categories and rule out a number of competing alternative explanations that are based on perceptions of product popularity, scarcity, assortment search difficulty, and messiness.
1	The authors investigate how horizontal versus vertical displays of alternatives affect assortment processing, perceived variety, and subsequent choice. Horizontal (vs. vertical) displays are easier to process due to a match between the human binocular vision field (which is horizontal in direction) and the dominant direction of eye movements required for processing horizontal displays. It is demonstrated that this processing fluency allows people to browse information more efficiently, which increases perceived assortment variety and ultimately leads to more variety being chosen, and if the number of options chosen is allowed to vary, it leads to more options chosen. It is shown that because people see more variety in a horizontal (vs. vertical) display, they process a horizontal assortment more extensively. When more variety is positive, they find the choice task easier and have a higher level of satisfaction and confidence about their choices. When more variety is not necessarily positive, for example, in a choice of a single most-preferred option, these effects disappear. Two field studies, an eye-tracking study, and two lab studies support these conclusions.
1	In offline purchasing settings (e.g., retail stores), consumers often encounter reminders that product information can be found on the Internet. The authors refer to a reminder of the availability of online information as a “cue-of-the-cloud” and explore its unique consequences on offline consumer behavior. This research finds that when consumers are presented with relatively large amounts of information in offline purchasing situations, a cue-of-the-cloud can enhance purchase intentions and choice behaviors. This occurs because the cue increases consumers’ confidence in being able to retain and access the information seen in-store, which engenders positive feelings about the decision to purchase. Four studies, including two experiments in real brick-and-mortar field settings, demonstrate the consequences of a cue-of-the-cloud, along with some novel moderators of these effects.
1	How does interpersonal closeness (IC)—the perceived psychological proximity between a sender and a recipient—influence word-of-mouth (WOM) valence? The current research proposes that high levels of IC tend to increase the negativity of WOM shared, whereas low levels of IC tend to increase the positivity of WOM shared. The authors hypothesize that this effect is due to low versus high levels of IC triggering distinct psychological motives. Low IC activates the motive to self-enhance, and communicating positive information is typically more instrumental to this motive than communicating negative information. In contrast, high IC activates the motive to protect others, and communicating negative information is typically more instrumental to this motive than communicating positive information. Four experiments provide evidence for the basic effect and the underlying role of consumers’ motives to self-enhance and protect others through mediation and moderation. The authors discuss implications for understanding how WOM spreads across strongly versus weakly tied social networks.
1	Strategic resource allocation in growth markets is always a challenging task. This is especially true when it comes to determining the level of investments and expenditures for customer acquisition and retention in competitive and dynamic market environments. This study develops an analytical model to examine firms’ investments in customer acquisition and retention for a new service; it develops hypotheses drawing on analytical findings and tests them with firm-level operating data of wireless telecommunications markets from 41 countries during 1999–2007. The empirical investigation shows that a firm's acquisition cost per customer is more sensitive to market position and competition than retention cost per customer. Furthermore, whereas firms leading in market share, on average, do not have a cost advantage over other firms in retaining customers, they have a substantial cost advantage in acquiring customers, and this advantage tends to increase with market penetration. The study results provide guidelines for firms’ strategic resource allocation for customer acquisition and retention in competitive service markets.
1	People are able to order food using a variety of computer devices, such as desktops, laptops, and mobile phones. Even in restaurants, patrons can place orders on computer screens. Can the interface that consumers use affect their choice of food? The authors focus on the “direct-touch” aspect of touch interfaces, whereby users can touch the screen in an interactive manner. In a series of five studies, they show that a touch interface, such as that provided by an iPad, compared with a nontouch interface, such as that of a desktop computer with a mouse, facilitates the choice of an affect-laden alternative over a cognitively superior one—what the authors call the “direct-touch effect.” The studies provide some mediational support that the direct-touch effect is driven by the enhanced mental simulation of product interaction with the more affective choice alternative on touch interfaces. The authors also test the moderator of this effect. Using multiple product pairs as stimuli, the authors obtain consistent results, which have rich theoretical and managerial implications.
1	This research illustrates how perceived economic mobility moderates the linkage between materialism and impulsive spending. Using various data sources, four studies show that materialistic consumers do not easily engage in impulsive spending when they perceive high economic mobility, whereas they tend to spend impulsively when they perceive low economic mobility. However, perceived economic mobility functions in the opposite manner when the purchase is a means to achieve financial success. The authors trace this effect to the self-regulation process of materialistic consumers, such that when perceiving high economic mobility, these consumers regulate their behavior toward long-term financial success, sacrificing the pleasure of acquisitions in the present. By elucidating the important role that perceived economic mobility plays in impulsive spending, the current research sheds new light on consumer research and offers managerial and public policy implications.
1	In the modern advertising agency selection contest, each participating agency specifies not only its proposed creative campaign but also the budget required to purchase the agreed-on media. The advertiser selects the agency that offers the best combination of creative quality and media cost, similar to conducting a score auction. To participate in the contest, each agency needs to incur an up-front bid-preparation cost to cover the development of a customized creative campaign. Agency industry literature has called for the advertiser to fully reimburse such costs to all agencies that enter the contest. The authors analyze the optimal stipend policy of an advertiser facing agencies with asymmetric bid-preparation costs, such that the incumbent agency faces a lower bid-preparation cost than a competitor agency entering the contest. The authors show that reimbursing bid-preparation costs in full is never optimal, nor is reimbursing any part of the incumbent's bid-preparation cost. However, a stipend that is strictly lower than the competitor's bid-preparation cost can benefit the advertiser under certain conditions. The authors provide a sufficient condition (in terms of the distribution of agency values to the advertiser) for such a new-business stipend to benefit the advertiser.
1	Maintaining savings is an important financial goal. Yet there are times when savings should be spent, such as when people face unavoidable costs, and spending their savings allows them to avoid high interest rate debt. Existing behavioral research has focused on consumer decisions between savings and discretionary spending and has proposed interventions to promote savings in these contexts. However, when spending is not discretionary, such interventions could risk exacerbating a pattern found in economic research in which people borrow high interest rate debt while maintaining savings that earn low levels of interest. To examine how mental accounting interacts with considerations of personal responsibility and guilt to contribute to this pattern, this article explores whether people spend their savings when they need money most: during emergencies. Six studies reveal that people's tendency to preserve savings by borrowing from a high interest rate credit option varies as a function of the savings’ intended use. Paradoxically, people are most likely to turn to high interest rate credit with the belief that doing so is the responsible option.
1	Research on choice architecture is shaping policy around the world, touching on areas ranging from retirement economics to environmental issues. Recently, researchers and policy makers have begun paying more attention not just to choice architecture but also to information architecture, or the format in which information is presented to people. In this article, the authors investigate information architecture as it applies to consumption in retirement. Specifically, in three experiments, they examine how people react to lump sums versus equivalent streams of monthly income. Their primary question of interest is whether people exhibit more or less sensitivity to changes in retirement wealth expressed as lump sums (e.g., $100,000) or monthly equivalents (e.g., $500 per month for life). They also test whether people exhibit an “illusion of wealth,” by which lump sums seem more adequate than monthly amounts in certain conditions, as well as the opposite effect, in which lump sums seem less adequate. They conclude by discussing how format-dependent perceptions of wealth can affect policy and consumers’ financial decision making.
1	In examining how stress influences consumer saving and spending, the authors propose that consumers who experience a stressful situation allocate their resources strategically to gain control of their environment. A series of studies shows that this strategic allocation of resources occurs in two ways. Consumers experiencing stress may show increased saving behavior, which assures them that monetary resources will be available when needed. Alternatively, consumers experiencing stress may show increased spending behavior, directed specifically toward products that the consumer perceives to be necessities and that allow for control in an otherwise uncontrollable environment. This conceptualization and the related findings can inform assessments of when stress will lead to beneficial or impulsive consumer behaviors.
1	How does setting a donation option as the default in a charitable appeal affect people's decisions? In eight studies, comprising 11,508 participants making 2,423 donation decisions in both experimental settings and a large-scale natural field experiment, the authors investigate the effect of “choice-option” defaults on the donation rate, average donation amount, and the resulting revenue. They find (1) a “scale-back” effect, in which low defaults reduce average donation amounts; (2) a “lower-bar” effect, in which defaulting a low amount increases donation rate; and (3) a “default-distraction” effect, in which introducing any defaults reduces the effect of other cues, such as positive charity information. Contrary to the view that setting defaults will backfire, defaults increased revenue in the field study. However, the findings suggest that defaults can sometimes be a “self-canceling” intervention, with countervailing effects of default option magnitude on decisions and resulting in no net effect on revenue. The authors discuss the implications of the findings for research on fundraising specifically, for choice architecture and behavioral interventions more generally, and for the use of “nudges” in policy decisions.
1	Research on overeating assumes that pleasure must be sacrificed for the sake of good health. Contrary to this view, the authors show that focusing on sensory pleasure can make people happier and willing to spend more for less food, a triple win for public health, consumers, and companies alike. In five experiments, the authors ask U.S. and French adults and children to imagine vividly the taste, smell, and texture of three hedonic foods before choosing a portion size of another hedonic food. Compared with a control condition, this “multisensory imagery” intervention led hungry and nondieting people to choose smaller food portions, and they anticipated greater eating enjoyment and were willing to pay more for them. This occurred because multisensory imagery prompted participants to evaluate portions on the basis of expected sensory pleasure, which peaks with smaller portions, rather than hunger. In contrast, health-based interventions led people to choose a smaller portion than the one they expected to enjoy most—a hedonic cost for them and an economic cost for food marketers.
1	Defaults are extremely effective at covertly guiding choices, which raises concerns about how to employ them ethically and responsibly. Consumer advocates have proposed that disclosing how defaults are intended to influence choices could help protect consumers from being unknowingly manipulated. This research shows that consumers appreciate transparency, but disclosure does not make defaults less influential. Seven experiments demonstrate that disclosure alters how fair consumers perceive defaults to be but does not attenuate default effects because consumers do not understand how to counter the processes by which defaults bias their judgment. Given that defaults lead consumers to focus disproportionately on reasons to choose the default even with disclosure, debiasing default effects requires that consumers engage in a more balanced consideration of the default and its alternative. Encouraging people to articulate their preferences for the default or its alternative, as in a forced choice, shifts the focus away from the default and reduces default effects.
1	The authors examine prominent placement of search engines’ own services and effects on users’ choices. Evaluating a natural experiment in which different results were shown to users who performed similar searches, they find that Google's prominent placement of its Flight Search service increased the clicks on paid advertising listings by more than half while decreasing the clicks on organic search listings by about the same quantity. This effect appears to result from interactions between the design of search results and users’ decisions about where and how to focus their attention: users who decide what to click on the basis of relevance were more likely to select paid listings, whereas users who are influenced by visual presentation and page position were more likely to click on Google's own Flight Search listing. The authors consider implications of these findings for competition policy and for online marketing strategies.
1	In this study, the authors propose a flexible framework to assess customer lifetime value (CLV) in the consumer packaged goods (CPG) context. They address the substantive and modeling challenges that arise in this setting, namely, (1) multiple discreteness, (2) brand switching, and (3) budget-constrained consumption. Using a Bayesian estimation, the authors are also able to infer the consumer's latent budgetary constraint using only transaction information, thus enabling managers to understand the customer's budgetary constraint without having to survey or depend on aggregate measures of budget constraints. Using the proposed framework, CPG manufacturers can assess CLV at the focal brand level as well as at the category level, a departure from CLV literature, which has mostly been firm centric. The authors implement the proposed model on panel data in the carbonated beverages category and showcase the benefits of the proposed model over simpler heuristics and conventional CLV approaches. Finally, they conduct two policy simulations describing the role of the budget constraint on CLV, as well as the asymmetric effects of pricing in this setting, and develop managerial insights in this context.
1	Consumers often schedule their activities in an attempt to use their time more efficiently. Although the benefits of scheduling are well established, its potential downsides are not well understood. The authors examine whether scheduling uniquely undermines the benefits of leisure activities. In 13 studies using unambiguous leisure activities that consumers commonly schedule (e.g., movies, a coffee break), they find that scheduling a leisure activity (vs. experiencing it impromptu) makes it feel less free-flowing and more work-like. Furthermore, scheduling diminishes utility from leisure activities, in terms of both excitement in anticipation of the activities and experienced enjoyment. Importantly, the authors find that maintaining the free-flowing nature of the activity by “roughly scheduling” (without prespecified times) eliminates this effect, thus indicating that the effect is driven by a detriment from scheduling rather than by a boost from spontaneity. The reported findings highlight an important opportunity for marketers to improve consumers’ experiences and utility by leveraging scheduling behavior while also providing important implications for consumer well-being from leisure consumption.
1	Does explicit recall help or hurt memory-based comparisons? It is often assumed that attempting to recall information from memory should facilitate—or at least not disrupt—memory-based comparisons. Using the domain of price comparisons, the authors demonstrate that memory-based price comparisons are less accurate when consumers first attempt to recall the past price versus when they do not try to do so. Attempting—and failing at—explicit price recall focuses attention on metacognitive experience, resulting in a feeling-of-not-knowing, which then blocks the implicit memory that people could otherwise use to make accurate price comparisons. Drawing attention to this metacognitive feeling-of-not-knowing increases the blocking effect of recall on implicit memory, whereas drawing attention away from the feeling reduces the blocking effect. The results identify a new type of memory blocking—metacognitive memory blocking—in which the feeling-of-not-knowing blocks implicit memory during judgments. They also provide further evidence of dual representations of price memory and demonstrate that most memory-based price comparisons are based on implicit memory and do not entail explicit recall of the reference price.
1	Satiation frequently occurs from repeated consumption of the same items over time. However, results from five experiments show that when people anticipate consuming something different in the future, they satiate at a slower rate in the present. The authors find the effect in both food and nonfood consumption settings using different approaches to measure satiation. This effect is cognitive; specifically, anticipating variety in future consumption generates positive thoughts about that future experience. The authors find two boundary conditions: the future consumption outcome must be (1) in a related product category and (2) at least as attractive as the present consumption outcome. The authors rule out potential alternative explanations such as mere exposure to variety, the possibility that the future experience is more attractive (rather than just different) than the current one, and perceptions of scarcity associated with the item consumed in the present.
1	The authors analytically and experimentally evaluate how firms make decisions in a two-stage dyadic channel, in which firms decide on investments in the first stage and then on prices in the second stage. They find that firms’ behavior differs significantly from the predictions of the standard economic model and is consistent with the existence of fairness concerns. Using a quantal response equilibrium model, in which both manufacturer and retailer make noisy best responses, the authors show that fairness significantly affects channel pricing decisions. In addition, they investigate what affects the perceptions of fairness. More specifically, they analyze whether the notion of fairness is influenced by social norms of strict equality, by endogenous investments and contributions that are affected by players’ decisions, or by the exogenous game structure. To do so, the authors compare four principles of distributive fairness: strict egalitarianism; liberal egalitarianism; libertarianism; and a sequence-aligned ideal, which is studied for the first time in the literature. Surprisingly, the exogenous game structure reflected by the new ideal, whereby the sequence of moving determines the equitable payoff for players, significantly outperforms other fairness ideals, suggesting that equity in distribution channels can arise even in contests in which channel members have fairly different payoffs.
1	Consumers with inequity aversion experience some psychological disutility when buying products at unfair prices. Empirical evidence and behavioral research have suggested that consumers may perceive a firm's price as unfair when its profit margin is too high relative to consumers’ surplus. The authors develop an analytical framework to investigate the effects of the consumer's inequity aversion on a firm's optimal pricing and quality decisions. They highlight several findings. First, because of the consumer's uncertainty about the firm's cost, the firm's optimal quality may be nonmonotone with respect to the degree of the consumer's inequity aversion. Second, stronger inequity aversion makes an inefficient firm worse off but may benefit an efficient firm. Third, stronger inequity aversion by the consumer can actually lower the consumer's monetary payoff (economic surplus) because the firm may reduce its quality to a greater extent than it reduces its price. Finally, as the expected cost efficiency in the market decreases, both the expected quality and the social surplus may increase rather than decrease.
1	Gift givers balance their goal to please recipients with gifts that match recipient preferences against their own goal to signal relational closeness with gifts that demonstrate their knowledge of the recipient. Five studies in a gift registry context show that when close (vs. distant) givers receive attribution for the gifts they choose, they are more likely to diverge from the registry to choose items that signal their close relationships. The authors find that close givers’ divergence from the registry is not the result of their altruistic search for a “better” gift but is a strategic effort to express relational signals: it occurs only when givers will receive attribution for their choice. They show that close givers reconcile their goal conflict by engaging in motivated reasoning, which results in their perceptual distortion of the gift options in favor of relational-signaling gifts. Ironically, distant givers are more likely to choose gifts from the registry, resulting in the selection of items that better match recipient preferences.
1	Consumers often make choices for joint consumption with committed relationship partners, and these choices may include more or less variety. When planning a weekend for oneself and one's spouse, for example, a person could choose more varied activities (e.g., going out to dinner, to a movie, and to a concert) or less varied activities (e.g., seeing several different movies). What might affect how much variety people choose? Five experiments demonstrate that how much variety consumers prefer for joint consumption in committed relationships depends on their relationship time perspective (i.e., the perceived time ahead in the relationship). When consumers perceive more (vs. less) time ahead in a committed relationship, they prefer more variety for joint consumption with their partners. This increased preference for variety is driven by a shift in how much excitement is valued within the relationship and is unique to choices for joint consumption with the specific relationship partner. The findings demonstrate that variety preferences depend not just on personal or situational factors but also on aspects of consumers’ social relationships.
1	Eating a food reduces the desire to eat more of that food. General-process theories of motivation posit that eating a food also increases the motivation to eat other foods—an effect known as cross-stimulus sensitization. The authors propose that eating a food selectively sensitizes consumers to its complements rather than to all foods. Eating a food activates a goal to consume foods that consumers perceive to be well paired with the consumed food. In five experiments, imagined and actual consumption of a food sensitized participants to complementary foods but not to unrelated or semantically associated foods. These findings suggest that cross-stimulus sensitization is more specific and predictable than previously assumed. The authors identify goal activation as the process through which cross-stimulus sensitization occurs and can be instilled.
1	The conviction one holds about free will serves as a foundation for the views one holds about the consumption activities of other consumers, the nature of social support systems, and the constraints that should or should not be placed on industry. Across multiple paradigms and contexts, the authors assess people's beliefs about the control consumers have over consumption activities in the face of various constraints on agency. They find that beliefs regarding personal discretion are robust and resilient, consistent with their finding that free will is viewed as noncorporeal. Nonetheless, they also find that these beliefs are not monolithic but vary as a function of identifiable differences across individuals and the perceived cause of behavior, particularly with regard to physical causation. Taken together, the results support the general wisdom of libertarian paternalism as a framework for public policy and highlight current and emerging situations in which policy makers might be granted greater latitude.
1	Journal of Marketing Research (JMR) has a storied history as one of the preeminent journals in the marketing discipline. This position has enabled JMR to leverage and attract the best manuscripts from authors who seek a broad audience. Suggestions for improvements in five specific areas are discussed: competitive landscape, evolution in the theory and practice of marketing, stakeholder management, managing manuscripts, and improving credibility. Such improvements can be achieved with sustained effort and input from authors, reviewers, associate editors, and coeditors.
1	Fashions and conspicuous consumption play an important role in marketing. In this article, the author presents a three-pronged framework to analyze fashion cycles in data composed of (1) algorithmic methods for identifying cycles, (2) a statistical framework for identifying cycles, and (3) methods for examining the drivers of such cycles. In the first module, the author identifies cycles by pattern-matching the amplitude and length of cycles observed to a user-specified definition. In the second module, the author defines the conditional monotonicity property, derives conditions under which a data-generating process satisfies it, and demonstrates its role in generating cycles. A key challenge in estimating this model is the presence of endogenous lagged dependent variables, which the author addresses using system generalized method of moments estimators. Third, the author presents methods that exploit the longitudinal and geographic variations in agents' economic and cultural capital to examine the different theories of fashion. The author applies her framework to data on given names for infants, shows the presence of large-amplitude cycles both algorithmically and statistically, and confirms that the adoption patterns are consistent with Bourdieu's theory of fashion as a signal of cultural capital.
1	Marketing affects customer behavior, and customer behavior in turn drives a firm's cash flows and, ultimately, valuation. In this sequence of relationships, a commonly overlooked factor by marketers is the volatility of customers' cash flows. This study links different recurring customer behaviors to the future level and volatility of a customer's cash flows. Empirical analyses of the large customer database of a Fortune 500 retailer reveal that a 1% desired change in the different types of recurring customer behaviors corresponds to a future quarterly 4.61% decrease in the cash flow volatility and $39.42 million increase in the future cash flow level of the firm. Furthermore, firm-initiated marketing is 1.9–3.2 times more effective at managing the future cash flow level and volatility when it is selectively targeted to customers with certain characteristics. Overall, the study enables marketers to manage different customer behaviors that influence customers' future cash flow level and volatility and ultimately quantify the impact of these behaviors on the shareholder value of the firm.
1	When companies organize salespeople into teams to compete with one another, how does the ability composition of team members affect effort? And how may the effect of team composition on effort depend on the type of contest the teams are competing in? Using a game-theoretic model, we derive the very sharp predictions that when winning the contest depends on the average team sales, the ability composition of a team has no effect on team effort, and the stronger and weaker members will expend identical effort levels. However, when the contest winner is determined by the minimum or maximum output contribution within the team, heterogeneity in team composition exerts a deleterious effect on team effort, with the stronger and weaker members expending unequal effort. Our model also shows that in a contest between two symmetric teams, when team members are homogeneous, all three contest metrics yield identical team effort; however, when team members are heterogeneous, the average metric elicits the highest team effort. We also discuss cases when the minimum and maximum metrics are optimal. We test the model's empirical validity using two incentive-aligned experiments in which participants make effort decisions strategically. The experimental results exhibit broad support for the theoretical predictions.
1	Although consumers are concerned about their health, obesity statistics suggest that contextual factors often lead them to choose unhealthy alternatives (i.e., vices) rather than healthy ones (i.e., virtues). Noting the increasing prevalence of online grocery shopping, the authors focus on shopping channels as one such contextual factor and investigate how food choices made online differ from food choices made in a traditional brick-and-mortar store. A database study and three lab experiments demonstrate that consumers choose relatively fewer vices in the online shopping environment. Moreover, this shopping channel effect arises because online channels present products symbolically, whereas offline stores present them physically. A symbolic presentation mode decreases the products' vividness, which in turn diminishes consumers' desire to seek instant gratification and ultimately leads them to purchase fewer vices. These findings highlight several unexplored differences between online and offline shopping, with important implications for consumers, public policy makers, and retailers.
1	The authors investigate whether sellers treat consumers differently on the basis of how well informed consumers appear to be. They implement a large-scale field experiment in which callers request price quotes from automotive repair shops. The authors show that sellers alter their initial price quotes depending on whether consumers appear to be correctly informed, uninformed, or misinformed about market prices. The authors find that repair shops quote higher prices to callers who cite a higher benchmark price and that women are quoted higher prices than men when callers signal that they are uninformed about market prices. However, gender differences disappear when callers mention a benchmark price for the repair. Finally, the authors find that repair shops are more likely to offer a price concession if asked to do so by a woman than if asked by a man.
1	Firms often vie for competitive advantage by providing additional services (amenities)to their customers. Although extant research has focused on the effect of adding amenities on choice, return on service amenities may arise from two sources: increased initial choice and increased revenues from repurchase. The authors develop a return on investment (ROI) model to capture how service amenities produce financial return from these two sources. They apply the model to hotel amenities, using a discrete choice experiment and a large-scale customer database developed in collaboration with a multibrand global hotel company. The authors employ a hierarchical Bayesian formulation to estimate the parameters. They use the estimation results to compare ROI for three amenities for six brands and find that returns vary across amenities, and returns on a single amenity can vary substantially across brands. The authors validate the results for one amenity against the ROI from the actual historical implementation of that amenity using a natural experiment with a before/after design with controls. The present research demonstrates that the return on service amenities model provides a useful decision tool for managers deciding which amenities are most profitable.
1	Retail atmospherics is emerging as a major competitive tool, and it is especially notable in the restaurant industry, where lighting is used to create the overall ambience and influence consumer experience. In addition to influencing overall experience, can ambient light luminance have unintended consequences in terms of influencing what diners order? The results of a field study at multiple locations of a major restaurant chain and a series of lab studies robustly show that consumers tend to choose less healthy food options when ambient lighting is dim (vs. bright). Process evidence suggests that this phenomenon occurs because ambient light luminance influences mental alertness, which in turn influences food choices. While restaurant and grocery store managers can use these insights and their ambient light switches to nudge consumers toward targeted food choices, such as healthy or high-margin signature items, health-conscious consumers can opt for dining environments with bright ambient lighting.
1	This research investigates the conditions under which inequity in a buyer–supplier relationship influences a supplier's resource sharing with its buyer. More specifically, the authors examine the effects of both positive inequity and negative inequity under varying levels of interdependence magnitude and relative dependence. They further examine the effect of inequity on perceived relationship performance. The study includes a longitudinal survey design, with perceived relationship performance reported by a second informant. The study, conducted within the context of Japanese suppliers reporting on their relationship with U.S. buyers, takes a nuanced view of both inequity and relative dependence by employing spline variables to disentangle potentially different effects of positive and negative inequity and relative dependence of the supplier and buyer, respectively. The results reveal that firms' reactions to positive and negative inequity vary depending on the nature of the interdependence structure and that positive and negative inequity differentially influence perceived relationship performance. The findings are important for both further research and managerial action.
1	Despite increasing efforts to encourage the adoption of field experiments in marketing research (e.g., Campbell 1969; Cialdini 1980; Li et al. 2015), the majority of scholars continue to rely primarily on laboratory studies (Cialdini 2009). For example, of the 50 articles published in Journal of Marketing Research in 2013, only three (6%) were based on field experiments. The goal of this article is to motivate a methodological shift in marketing research and increase the proportion of empirical findings obtained using field experiments. The author begins by making a case for field experiments and offers a description of their defining features. She then demonstrates the unique value that field experiments can offer and concludes with a discussion of key considerations that researchers should be mindful of when designing, planning, and running field experiments.
1	Does “liking” a brand on Facebook cause a person to view it more favorably? Or is “liking” simply a symptom of being fond of a brand? The authors disentangle these possibilities and find evidence for the latter: brand attitudes and purchasing are predicted by consumers' preexisting fondness for brands, and these are the same regardless of when and whether consumers “like” brands on social media. In addition, we explore possible second-order effects by examining whether “liking” brands might cause consumers' friends to view that brand more favorably. When consumers see that a friend has “liked” a brand, they are less likely to buy the brand relative to when they learn that a friend genuinely likes the brand in the offline sense, which is a more meaningful social endorsement. Taken together, five experiments and two meta-analyses (N > 14,000) suggest that turning “liking” into improved brand attitudes and increased purchasing by consumers and their friends may require more than just the click of a button.
1	In two studies (a longitudinal field experiment with an established business-to-consumer national chain, and a field experiment with a business-to-business software manufacturer), the authors demonstrate that starting a survey with an open-ended positive solicitation increases customer purchase behavior. Study 1, a longitudinal field experiment, shows that one year after the completion of a survey that began by asking customers what went well during their purchase experience, those customers spent 8.25% more than customers who had completed a survey that did not include the positive solicitation. Study 2 utlizes multiple treatment groups to assess the stepwise gains of solicitation, measurement, and solicitation frame. The results demonstrate (1) a mere solicitation effect, (2) a traditional mere measurement effect, and (3) an additional “mere measurement plus” effect of an open-ended positive solicitation; all effects increased customer spending. Specifically, starting a survey with an open-ended positive solicitation resulted in a 32.88% increase in customer spending relative to a survey with no open-ended positive solicitation. The findings suggest that firms can proactively influence the feedback process. Soliciting open-ended positive feedback can create positively biased memories of an experience; the subsequent expression of those memories in an open-ended feedback format further reinforces them, making them more salient and accessible in guiding future purchase behavior.
1	Going beyond traditional seasonality, this research introduces the concept of the intrayear category demand cycle. This phenomenon reflects demand cycles most consumer packaged goods categories experience throughout the year, with periods of higher demand following periods of lower demand. The author argues that acknowledging the existence of these cycles and understanding their impact on both advertising and pricing effectiveness and practice is critical for marketers. Specifically, the author demonstrates how both advertising and price elasticities and observed advertising and prices evolve along these cycles for a unique set of 252 brands—ranging from high-advertising, high-priced “premium mass” brands to low-advertising, low-priced “value niche” brands—in 61 consumer packaged goods categories. Overall, both advertising efectiveness and observed advertising are found to be stronger at demand peaks. Surprisingly, consumer reactions to price decreases are weaker at demand peaks, whereas reactions to price increases remain unchanged. However, effectiveness evolutions and observed action patterns along these intrayear cycles are both markedly diverse across the different types of brands.
1	Seeding influential social network members is crucial for the success of a viral marketing campaign and product diffusion. In line with the assumption that connections between customers in social networks are binary (either present or absent), previous research has generally recommended seeding network members who are well-connected. However, the importance of connections between customers varies substantially depending on the relationship's characteristics, such as its type (i.e., friend, colleague, or acquaintance), duration, and interaction intensity. This research introduces a new Bayesian methodology to identify influential network members and takes into account the relative influence of different relationship characteristics on product diffusion. Two applications of the proposed methodology—the launch of a microfinance program across 43 Indian villages and information propagation in a large online social network—demonstrate the importance of weighting connections in social networks. Compared with traditional seeding strategies, the proposed methodology recommends substantially different sets of seeds that increased the reach by up to 10% in the first empirical application and up to 92% in the second.
1	The authors study the joint effects of creative format, message content, and targeting on the performance of digital ads over time. Specifically, they present a dynamic model to measure the effects of various sizes of static (GIF) and animated (Flash) display ad formats and consider whether different ad contents, related to the brand or a price offer, are more or less effective for different ad formats and targeted or retargeted customer segments. To this end, the authors obtain six months of data on daily impressions, clicks, targeting, and ad creative content from a major U.S. retailer, and they develop a dynamic zero-inflated count model. Given the sparse, nonlinear, and non-Gaussian nature of the data, the study designs a particle filter/Markov chain Monte Carlo scheme for estimation. Results show that carry-over rates for dynamic formats are greater than those for static formats; however, static formats can still be effective for price ads and retargeting. Most notably, results also show that retargeted ads are effective only if they offer price incentives. The study then considers the import of these results for the retailer's media schedules.
1	Acquisition is an important activity that enables firms to adapt their resource configurations. Acquisition literature has focused only on the levels of innovation or relational resources, the primary dimensions of organizational resource bases. The authors argue that in addition to the levels, the degree of overlap between the acquiring and target firms, in terms of innovation and relational resources, should influence acquisitions. Furthermore, they argue for the importance of the quality of the target's resources along with factors influencing the acquirer's absorptive capacity (e.g., the chief executive officer's [CEO's] functional background and the acquiring firm's marketing intensity and acquisition experience) as contingencies. Using 319 biopharmaceutical acquisitions and a random-effect regression model that accounts for unobserved heterogeneity and the endogeneity of relational and innovation overlap, the authors find that innovation overlap has a positive effect, whereas relational overlap has a negative effect, on acquisition outcomes. Furthermore, the acquirer CEO's throughput background and acquisition experience negatively moderate, whereas the target's innovation resource quality and the acquirer's marketing intensity positively moderate, the influence of innovation overlap. The target's relational resource quality and the acquirer CEO's throughput background positively moderate the influence of relational overlap.
1	Investors, analysts, and regulators frequently advocate greater disclosure of nonfinancial information, such as customer metrics. Managers, however, argue that such metrics are costly to report, reveal sensitive information to competitors, and therefore will lower future cash flows. To examine these counterarguments, this study presents the first empirical examination of the prevalence and consequences of backward- and forward-looking disclosures of customer metrics by manually coding 511 annual reports of firms in two industries, telecommunications (365 reports) and airlines (146 reports). The results reveal significant heterogeneity in the disclosure of customer metrics across firms and between industries. On average, in both industries, firms make more backward-looking than forward-looking disclosures. Notably, forward-looking disclosures of customer metrics are negatively associated with investors’ uncertainty in both industries and with analysts’ uncertainty in the telecommunications industry. Importantly, the results do not support the managerial thesis that such disclosures have a negative impact on future cash flows.
1	This research examines the influence of chief executive officers’ (CEOs’) political ideologies—specifically, their degree of political liberalism (i.e., support for the Democratic Party relative to the Republican Party)—on firms’ innovation propensity (i.e., rate of new product introductions [NPIs]). The authors propose that CEOs’ degree of political liberalism positively affects their firms’ rate of NPIs. This impact is weakened, however, when CEOs have low power, when a high proportion of their compensation comes from equity, when the marketing department has high influence in the top management team, and when the economy is growing. Liberal CEOs’ greater rate of NPIs is associated with superior Tobin's q but also higher stock return volatility. Findings based on observations of 421 publicly listed U.S. firms from 2006 to 2010 provide considerable support for the authors’ hypotheses. The authors also examine changes in firms’ rate of NPIs and performance around CEO turnovers and find corroborating evidence for their thesis. These results highlight the role of executives’ personal values in shaping firms’ innovation strategy as well as the risks and rewards associated with aggressive NPIs.
1	Conjoint analysis is a widely used method for determining how much certain attributes matter to consumers by observing a series of their choices. However, how those attributes are expressed has important consequences for their choices and thus for conclusions drawn by market researchers about attribute importance. Expanded attribute scales (e.g., expressing exercise time in minutes) leads consumers to perceive greater differences between scale levels than contracted scales (e.g., expressing exercise time in hours). The authors show in two domains that simply expanding an attribute's scale can shift choice toward alternatives that perform well on a scale that is expanded and thus can impact conjoint results such as attribute importance and screening. Thus, practitioners should take care when they choose precisely how to elicit preferences or how to describe their products: the extent of the scale's expansion will determine researchers’ inferences about the importance of the attribute it describes. By illustrating the curvilinear relationship between scale expansion and multiple measures, the authors also offer practitioners some insight into the limits of scale expansion.
1	This article proposes a utilitarian model in which recycling could reduce consumers’ negative emotions from wasting resources (i.e., taking more resources than what is being consumed) and increase consumers’ positive emotions from disposing of consumed resources. The authors provide evidence for each component of the utility function using a series of choice problems and formulate hypotheses on the basis of a parsimonious utilitarian model. Experiments with real disposal behavior support the model hypotheses. The findings suggest that the positive emotions associated with recycling can overpower the negative emotions associated with wasting. As a result, consumers could use a larger amount of resources when recycling is an option, and more strikingly, this amount could go beyond the point at which their marginal consumption utility becomes zero. The authors extend the theoretical model and introduce acquisition utility and the moderating effect of the costs of recycling (financial, physical, and mental). From a policy perspective, this research argues for a better understanding of consumers’ disposal behavior to increase the effectiveness of environmental policies and campaigns.
1	Despite the tremendous resources devoted to marketing on Facebook, little is known about its actual effect on customers. Specifically, can Facebook page likes affect offline customer behavior, and if so, how? To answer these questions, the authors conduct a field experiment on acquired Facebook page likes and find them to have a positive causal effect on offline customer behavior. Importantly, these likes are found to be most effective when the Facebook page is used as a platform for firm-initiated promotional communications. No effect of acquired page likes is found when customers interact organically with the firm's page, but a significant effect is found when the firm pays to boost its page posts and thus uses its Facebook page as a platform for paid advertising. These results demonstrate the value of likes beyond Facebook activity itself and highlight the conditions under which acquiring likes is most valuable for firms.
1	Given recent interest in social media, many brands now create content that they hope consumers will view and share with peers. While some campaigns indeed go “viral,” their value to the brand is limited if they do not boost brand evaluation or increase purchase. Consequently, a key question is how to create valuable virality, or content that is not only shared but also beneficial to the brand. Share data from hundreds of real online ads, as well as controlled laboratory experiments, demonstrate that compared with informative appeals (which focus on product features), emotional appeals (which use drama, mood, music, and other emotion-eliciting strategies) are more likely to be shared. Informative appeals, in contrast, boost brand evaluations and purchase because the brand is an integral part of the ad content. By combining the benefits of both approaches, emotional brand-integral ads boost sharing while also bolstering brand-related outcomes. The authors’ framework sheds light on how companies can generate valuable virality and the mechanisms underlying these effects.
1	There is meaning in sound that transcends language. Structural differences in the sound of a spokesperson's voice or a piece of background music can influence a consumer's perception of product attributes through cross-modal inference. This article examines how differences in acoustic pitch in marketing communications influence consumer's perceptions of product size. Through six studies, the authors find that when associated with a product, lower pitch in voice or music leads consumers to infer a larger product size. Furthermore, evidence shows that this pitch–size effect occurs through a process of visual mental imagery, which can be facilitated through stronger visualization cues delivered via auditory channels and reduced when size perceptions are assessed directly in the presence of visual product information. The cross-modal effects between auditory stimuli and physical products represent an unexplored influence on consumer perception and behavior with important managerial and theoretical implications.
1	Customer relationship management (CRM) campaigns have traditionally focused on maximizing the profitability of the targeted customers. The authors demonstrate that in business settings characterized by network externalities, a CRM campaign that is aimed at changing the behavior of specific customers propagates through the social network, thereby also affecting the behavior of nontargeted customers. Using a randomized field experiment involving nearly 6,000 customers of a mobile telecommunication provider, they find that the social connections of targeted customers increase their consumption and become less likely to churn, due to a campaign that was neither targeted at them nor offered them any direct incentives. The authors estimate a social multiplier of 1.28. That is, the effect of the campaign on first-degree connections of targeted customers is 28% of the effect of the campaign on the targeted customers. By further leveraging the randomized experimental design, the authors show that, consistent with a network externality account, the increase in activity among the nontargeted but connected customers is driven by the increase in communication between the targeted customers and their connections, making the local network of the nontargeted customers more valuable. These findings suggest that in targeting CRM marketing campaigns, firms should consider not only the profitability of the targeted customer but also the potential spillover of the campaign to nontargeted but connected customers.
1	Companies are eager to leverage social interactions among consumers by embedding social networking tools on their websites and actively integrating marketing actions with consumers' social activities. In this article, the authors investigate the impact of online social interactions on repeat usage behavior and the effectiveness of monetary incentives by formulating a model that parses out the effects of these individual factors. Using a unique data set from a wellness program, they find that online social interactions play a key role in driving repeat behavior, and after controlling for contemporaneous correlations from same-office friends, the social influence emanates even from distant friends working in different offices. The authors also find that monetary incentives have a significant impact on repeat usage and that ignoring this may overstate the impact of social influence. Furthermore, what-if scenario analyses show that social interactions are more effective than monetary incentives when both are present. The authors then explore social influence theories to understand the underlying process mechanisms that may operate in the repeat usage context. Using these findings, they offer strategic implications for marketing practice.
1	Salesperson turnover can have a negative overall effect on a firm. Research on salesperson turnover has conceptually studied the consequences of voluntary turnover on a firm. However, little empirical research has investigated the antecedents of salesperson turnover—specifically, the role of own effects (relative performance, customer satisfaction, and goal realization) and peer effects (peer performance variance and turnover). Therefore, the authors propose a framework to assess the influence of own factors (through identity theory) and of peer factors (through social identity theory) on salesperson turnover. Using a proportional hazard model implemented on data consisting of 6,727 salespeople over two years, the results suggest that in addition to own behaviors, managers need to pay attention to peer behaviors because peer turnover (voluntary and involuntary) greatly increases a salesperson's turnover probability. Furthermore, the results indicate that peer effects have a greater impact than own effects. This research has implications for sales force management because it helps managers (1) identify a salesperson's turnover risk, (2) diagnose the drivers of turnover behavior, and (3) build strategies to prevent salesperson turnover.
1	In a sorting task, consumers receive a set of representational items (e.g., products, brands) and sort them into piles such that the items in each pile “go together.” The sorting task is flexible in accommodating different instructions and has been used for decades in exploratory marketing research in brand positioning and categorization. However, no general analytic procedures yet exist for analyzing sorting task data without performing arbitrary transformations to the data that influence the results and insights obtained. This manuscript introduces a flexible framework for analyzing sorting task data, as well as a new optimization approach to identify summary piles, which provide an easy way to explore associations consumers make among a set of items. Using two Monte Carlo simulations and an empirical application of single-serving snacks from a local retailer, the authors demonstrate that the resulting procedure is scalable, can provide additional insights beyond those offered by existing procedures, and requires mere minutes of computational time.
1	Mortgage decisions have important consequences for consumers, lenders, and the state of the economy more generally. Mortgage decisions are also prototypical of consumer financial choices that involve a stream of expenditures and consumption occurring across time. The authors use heterogeneity in time preferences for both immediate (present bias) and long-term outcomes to explain a sequence of mortgage decisions, including mortgage choice and the decision to abandon a mortgage. The authors employ an analytic model and a survey of mortgaged households augmented by zip code–level house price and foreclosure data. The model suggests and data confirm that consumers with greater present bias and long-term discounting tend to choose mortgages that minimize up-front costs. However, greater present bias decreases homeowners' willingness to abandon a mortgage, locking them into the contract. Long-term patience increases mortgage abandonment. This reversal across mortgage decisions is difficult for alternative accounts to explain. These results suggest that a two-parameter model of time preferences is helpful for understanding how homeowners make mortgage decisions.
1	Variable selection is a decision heuristic that describes a selective choice process in which choices are made on the basis of only a subset of product attributes while the presence of other (“inactive”) attributes plays no active role in the decision. Within this context, the authors address two integrated topics that have received scant attention: the efficient design of choice experiments and the analysis of data that arises from a selective choice process. The authors propose a new dual-objective compound design criterion that incorporates prior information for the joint purpose of efficiently estimating the effects of the active attributes and detecting the effects of attributes labeled as inactive that may turn out to be active. The approach leverages self-stated auxiliary data as prior information both for individual-level customized design construction and in a heterogeneous variable selection model. The authors demonstrate the efficiency advantages of the approach relative to design benchmarks and highlight practical implications using both simulated data and actual data from a conjoint choice experiment in which individual designs were customized instantaneously using self-stated active-inactive attribute status.
1	This research explores how expressed emotional arousal in a consumer review affects reader perceptions of its helpfulness. Drawing from research on written communication and lay theories of emotion, the authors propose a pattern of diminishing returns, in which the marginal effect of arousal on perceived helpfulness is positive at low levels of arousal but diminishes at higher levels. Results of a field study using Apple's App Store, a follow-up survey, and two laboratory experiments provide consistent evidence for the predicted pattern. In addition, the results suggest that the nonlinear effect is explained in part by perceptions of reviewer effort and that the effect is stronger for products that are utilitarian in nature. By revealing a nuanced relationship between emotional expression and perceived helpfulness, these findings offer valuable implications for effective word-of-mouth communication.
1	There is a growing interest among marketing scholars to examine the evolutionary bases of a wide range of consumer phenomena. While specific evolutionary hypotheses are typically tested using tools familiar to marketing researchers (e.g., experiments, surveys), the method of evolutionary psychology is rooted in its unique epistemology (the manner in which knowledge is generated and organized), which comprises three elements: (1) the distinction between proximate and ultimate explanations, (2) the building of nomolological networks of cumulative evidence (triangulation of convergent lines of evidence), and (3) an organizing tree of knowledge. The purpose of this article is to describe this process using marketing-relevant examples as a means of providing a framework of best practices to marketing scholars aiming to incorporate the evolutionary lens within their research programs.
1	Firms can save considerable money if consumers conserve resources (e.g., if hotel patrons turn off the lights when leaving the room, if restaurant patrons use fewer paper napkins, if airline passengers clean up after themselves). In two studies conducted in real-world hotels, the authors show that consumers' conservation behavior is affected by the extent to which consumers perceive the firm as being green. Furthermore, consumer perceptions of firms' greenness and consumer conservation behavior depend on (1) whether the firm asks consumers to conserve resources, (2) the firm's own commitment to the environment, and (3) the firm's price image. In addition, firm requests to consumers to save resources can create consumer reactance and can backfire when firms themselves do not engage in visible costly environmental efforts. Such reactance is more likely for firms with a high-price image. Finally, the authors show that by spending a little money to signal environmental commitment, firms can save even more money through consumers' conservation of resources, resulting in wins for the firm, the consumer, and the environment.
1	Marketers of programs that are designed to help consumers reach goals face dual challenges of making the program attractive enough to encourage consumer signup while still motivating consumers to reach desirable goals and thus stay satisfied with the program. The authors offer a possible solution to this challenge: the emergency reserve, or slack with a cost. They demonstrate how an explicitly defined emergency reserve not only is preferred over other options for goal-related programs but can also lead to increased persistence. Study 1 demonstrates that consumers prefer programs with emergency reserves to programs that do not have them, and Study 2 further clarifies that consumers' preference for an emergency reserve depends on the presence of a superordinate goal. Study 3 reveals that consumers prefer goals with emergency reserves because they perceive them to have both higher attainability and value than other goals. Study 4 demonstrates that reserves can lead to increased goal persistence in a realistic task that involves persistence over time. Finally, Studies 5 and 6 reveal that consumers persist more with reserve goals because they want to avoid using the “emergency” reserve.
1	The authors conduct a field experiment in which they vary the sales force compensation scheme at an Asian enterprise that sells consumer durable goods. With variation generated by the experimental treatments, the authors model sales force performance to identify the effectiveness of various forms of conditional and unconditional compensation. They account for salesperson heterogeneity using a hierarchical Bayesian framework to estimate the model. They find conditional compensation in the form of quota bonus incentives to improve performance; however, such compensation may lead to lower future performance. The authors find little difference in effectiveness between a quota bonus plan and punitive bonus plans framed as a penalty for not achieving quota. They find that unconditional compensation, in the form of reciprocity, is effective at improving sales force performance only when it is given as a delayed reward; however, the effectiveness of this plan decreases with repeated exposure. The authors also find heterogeneity in the impact of compensation on performance across salespeople, such that unconditional compensation is more effective for salespeople with high base performance, whereas conditional compensation is equally effective across all types of salespeople.
1	To complement their in-house, designer-driven efforts, companies are increasingly experimenting with crowdsourcing initiatives in which they invite their user communities to generate new product ideas. Although innovation scholars have begun to analyze the objective promise of crowdsourcing, the current research is unique in pointing out that merely marketing the source of design to customers might bring about an incremental increase in product sales. The findings from two randomized field experiments reveal that labeling crowdsourced new products as such—that is, marketing the product as “customer-ideated” at the point of purchase versus not mentioning the specific source of design—increased the product's actual market performance by up to 20%. Two controlled follow-up studies reveal that the effect observed in two distinct consumer goods domains (food and electronics) can be attributed to a quality inference: consumers perceive “customer-ideated” products to be based on ideas that address their needs more effectively, and the corresponding design mode is considered superior in generating promising new products.
1	When consumers post questions online, who influences the content of the discussion more: the consumer posting the question or those who respond to the post? Analyses of data from real online discussion forums and four experiments show that early responses to a post tend to drive the content of the discussion as much as or more than the content of the initial query. Although advice seekers posting to online discussion forums often explicitly tell respondents which attributes are most important to them, the authors demonstrate that one common online posting goal, affiliation, makes respondents more likely to repeat attributes mentioned by previous respondents, even if those attributes are less important to the advice seeker or support a suboptimal choice given the advice seeker's decision criteria. Firms “listening in” on social media should account for this systematic bias when making decisions on the basis of the discussion content.
1	To properly evaluate a potential product upgrade, consumers should compare the upgraded option with the product they already own to assess the upgrade's added utility. However, although consumers explicitly and spontaneously acknowledge the importance of comparing the upgrade with the status quo, the authors find that they often fail to do so. Consequently, consumers frequently buy product upgrades that they would not have bought had they followed their own advice. Five experiments, involving both real and hypothetical upgrade decisions, show that even when the status quo option is represented in the decision context, if consumers are not explicitly prompted to reflect on it or compare it with the upgraded option, they often do not compare it with the upgrade and thus show an elevated likelihood of upgrading. The experiments suggest that this “comparison neglect” increases upgrade likelihood by making people overlook the similarities between the upgraded and status quo options and that it persists even when deliberation effort is high. The findings have important implications for theory, marketing practice, and consumer welfare.
1	Word of mouth affects consumer behavior, but how does the language used in word of mouth shape that impact? Might certain types of consumers be more likely to use certain types of language, affecting whose words have more influence? Five studies, including textual analysis of more than 1,000 online reviews, demonstrate that compared to more implicit endorsements (e.g., “I liked it,” “I enjoyed it”), explicit endorsements (e.g., “I recommend it”) are more persuasive and increase purchase intent. This occurs because explicit endorsers are perceived to like the product more and have more expertise. Looking at the endorsement language consumers actually use, however, shows that while consumer knowledge does affect endorsement style, its effect actually works in the opposite direction. Because novices are less aware that others have heterogeneous product preferences, they are more likely to use explicit endorsements. Consequently, the endorsement styles novices and experts tend to use may lead to greater persuasion by novices. These findings highlight the important role that language, and endorsement styles in particular, plays in shaping the effects of word of mouth.
1	Five experiments show that less physical involvement in obtaining food leads to less healthy food choices. The authors find that when participants are given the choice of whether to consume snacks that they perceive as relatively unhealthy, they have a greater inclination to consume them when less (vs. more) physical involvement is required to help themselves to the food; this is not the case for snacks that they perceive as relatively healthy. Further, when participants are given the opportunity to choose their portion size, they select larger portions of unhealthy foods when less (vs. more) physical involvement is required to help themselves to the food; again, this is not the case for healthy foods. The authors suggest that this behavior occurs because being less physically involved in serving one's food allows participants to reject responsibility for unhealthy eating and thus to feel better about themselves after indulgent consumption. These findings add to the research on consumers’ self-serving attributions and to the growing literature on factors that nudge consumers toward healthier eating decisions.
1	Despite widespread conviction that neediness should be a top priority for charitable giving, this research documents a “charity beauty premium” in which donors often choose beautiful, but less needy, charity recipients instead. The authors propose that donors hold simultaneous yet incongruent preferences of wanting to support beautiful recipients (who tend to be judged as less needy), but believing they should support needy recipients. The authors also posit that preferences for beautiful recipients are most likely to emerge when decisions are intuitive, whereas preferences for needy recipients are most likely to emerge when decisions are deliberative. These propositions are tested in several ways. First, when a beautiful recipient is included in basic choice sets, this recipient becomes the most popular option and increases donor satisfaction. Second, heightening deliberation steers choices away from beautiful recipients and toward needier ones. Third, donors explicitly state that they “want” to give to beautiful recipients but “should” give to less beautiful, needier ones. Taken together, these findings reconcile and extend previous and sometimes conflicting results about beauty and generosity.
1	Prior research has indicated that greater dietary variety in a single eating episode increases caloric intake, contributing to weight gain and obesity. This study presents a novel conceptual framework for investigating dietary variety across the entire diet according to time frame (cumulative vs. daily), aggregation level (overall vs. episode-specific), and categorization level (individual foods vs. food groups). This framework is used to assess how naturally occurring dietary variety relates to weight loss among overweight/obese women enrolled in a 16-week trial to achieve weight loss. Acknowledging this is a first exploratory attempt to test such relationships using correlational analyses, the authors uncover several key findings. First, whereas cumulative overall variety was not associated with weight loss, daily overall variety was positively associated with weight loss. Second, this relationship was strongest for variety during breakfast and afternoon snacks and was particularly driven by consuming greater vegetable variety. Overall, the authors develop a novel conceptual framework for investigating variety, and through application to a unique data set, uncover novel findings countering some existing theories about how dietary variety relates to weight loss success.
1	People often assume that costlier means lead to better outcomes, even in the absence of an objective relationship in the specific context. Such cost–benefit heuristics in goal pursuit have been observed across several domains, but their antecedents have not been fully explored. In this research, the authors propose that a person's tendency to use cost–benefit heuristics depends on the extent to which that person subscribes to the Protestant Work Ethic (PWE), an influential concept originally introduced to explain the rise of capitalism. The PWE is a core value predicated on the work-specific belief that hard work leads to success, but people who subscribe strongly to it tend to overgeneralize and align other work-unrelated cognitions for consistency. Across ten studies (N = 1,917) measuring and manipulating PWE, robust findings show that people who are high (vs. low) in PWE are more likely to use cost–benefit heuristics and are more likely to choose costlier means in pursuit of superior outcomes. Suggestions are provided for how marketers may identify consumers high versus low in PWE and tailor their offerings accordingly.
1	In business-to-business markets, top marketing and sales executives (TMSEs) have considerable influence on their organizations’ customer strategies. When TMSEs switch firms, a pattern of informal organizational connections results; this pattern reflects the flow of information and knowledge among firms and creates managerial social capital in the process. To model this information flow, the current study considers information reach and richness, conceptualized according to the network position (i.e., centrality and brokerage) of the firm in the TMSE mobility network, which can be constructed by tracing executive movements through the work experience records of TMSEs in an industry. TMSE tenure (i.e., time with the firm) and firm market orientation constitute critical moderators, which capture motivation and ability at the individual and firm level, respectively. Data from the semiconductor industry and a model that corrects for unobserved heterogeneity and endogeneity suggest that managerial social capital enhances firm performance; however, TMSE tenure and firm market orientation are essential for absorbing the benefits of managerial social capital.
1	This study investigates time lapses that interrupt product consumption. Preeminent examples are commercial breaks during television or radio programming. The authors suggest that breaks facilitate consumers’ search for alternatives. Specifically, when there is so much uncertainty that consumers are unclear about utility levels of different products, they engage in costly search to resolve the uncertainty. For TV programming, breaks lower the opportunity cost of search, allowing the consumer to sample alternative channels without further interrupting the viewing experience on the current channel. Using data from the Chinese TV market, the authors estimate a sequential search model to study consumer TV channel choice behavior. The data contain a quasi-natural experiment due to a Chinese government policy change on commercial breaks. The natural experiment creates exogenous variations in the data that enable the empirical identification of heterogeneous consumer preference and search cost. The data patterns support the idea that viewers search for alternatives during commercial breaks. Drawing on the estimates, the authors investigate how the timing of breaks affects TV channels’ viewership, offering insights about how to strategically adjust the timing of breaks.
1	Peer-to-peer markets, collectively known as the sharing economy, have emerged as alternative suppliers of goods and services traditionally provided by long-established industries. The authors explore the economic impact of the sharing economy on incumbent firms by studying the case of Airbnb, a prominent platform for short-term accommodations. They analyze Airbnb's entry into the state of Texas and quantify its impact on the Texas hotel industry over the subsequent decade. In Austin, where Airbnb supply is highest, the causal impact on hotel revenue is in the 8%–10% range; moreover, the impact is nonuniform, with lower-priced hotels and hotels that do not cater to business travelers being the most affected. The impact manifests itself primarily through less aggressive hotel room pricing, benefiting all consumers, not just participants in the sharing economy. The price response is especially pronounced during periods of peak demand, such as during the South by Southwest festival, and is due to a differentiating feature of peer-to-peer platforms—enabling instantaneous supply to scale to meet demand.
1	The authors use evidence from store openings by a bricks-and-clicks retailer to examine the drivers of substitution and complementarity between online and offline retail channels. The evidence supports the coexistence of substitution across channels and complementarity in demand. In places where the retailer has a strong presence, the opening of an offline store is associated with a decrease in online sales and search; however, in places where the retailer does not have a strong presence, the opening of an offline store is associated with an increase in online sales and search. The evidence suggests that whereas online and offline channels may be substitutes in distribution, they are complements in marketing communications. Specifically, the type of marketing communication driving complementarity seems to be information about the existence of the brand. For example, the authors observe a large increase in new customer acquisition and sales, and little difference between fit and feel products and other products. Thus, it is the presence of the store, rather than information about the attributes of the products in the store, that drives complementarity.
1	This article introduces a new data enrichment method that combines revealed data on consumer demand and competitive reactions with stated data on competitive reactions to yet-to-be-enacted, unprecedented marketing policy changes. The authors extend the data enrichment literature to include stated competitive reactions, collected from subject-matter experts through a conjoint experiment. The authors apply their method to investigate hypothetical and unprecedented sales force policy changes of pharmaceutical companies. The results from the data enrichment method have high face validity and lead to various unique insights compared with using revealed data only. The authors find that only a very large sales force decrease initiated by the market leader triggers all competitors to decrease their sales force as well, leading to substantial profit increases for each firm. With respect to sales force allocation, when competitors decrease their sales force, they mainly decrease the reach of detailing across doctors, rather than decreasing the number of details to the most-visited doctors. The proposed data enrichment method provides managers with a powerful tool to, ex ante, predict the consequences of unprecedented marketing policy changes.
1	How does bundling affect valuation? This research proposes the asymmetry hypothesis in the valuation of bundles: consumers demand more compensation for the loss of items from bundles, compared with the loss of the same items in isolation, yet they express lower willingness to pay for items added to bundles, compared with the same items purchased separately. This asymmetry persists because bundling causes consumers to perceive multiple items as a single, inseparable gestalt unit. Thus, consumers resist altering the “whole” of the bundle by removing or adding items. Six studies demonstrate this asymmetry across judgments of monetary value (Studies 1 and 2) and (dis)satisfaction (Study 3). Moreover, bundle composition—the ability of different items to create the impression of a “whole”—moderates the effect of bundling on valuation (Study 4), and the need to replace missing items (i.e., restoring the “whole”) mediates the effect of bundling on compensation demanded for losses (Study 5). Finally, the authors explore a boundary condition: the effect is attenuated for items that complete a set (Study 6).
1	Marketers must constantly decide how to implement word-of-mouth (WOM) programs, and a well-developed decision support system (DSS) can provide them valuable assistance in doing so. The authors propose an agent-based framework that aggregates social network–level individual interactions to guide the construction of a successful DSS for WOM. The framework presents a set of guidelines and recommendations to (1) involve stakeholders, (2) follow a data-driven iterative modeling approach, (3) increase validity through automated calibration, and (4) understand the DSS behavior. This framework is applied to build a DSS for a freemium app in which premium users discuss the product with their social network and promote its viral adoption. After its validation, the agent-based DSS forecasts the aggregate number of premium sales over time and the most likely users to become premium in the near future. The experiments show how the DSS can help managers by forecasting premium conversions and increasing the number of premiums through targeting and implementing reward policies.
1	Consumers are often faced with the opportunity to purchase a new, enhanced product, such as a new phone, even though the product they currently own is still fully functional. The authors propose that consumers act more recklessly with their current products when in the presence of appealing, though not yet attained, product upgrades (not just mere replacements). Carelessness and neglect toward currently owned products stem from a desire to justify the attainment of upgrades without appearing wasteful. A series of studies with actual owners of a wide range of different goods (durable, consumable, functional, and hedonic products) and evidence from a real-word data set of lost Apple iPhones demonstrate how the availability of product upgrades increases cavalier behavior toward possessions. Moreover, the authors demonstrate that product neglect in the presence of attractive upgrades can occur without deliberate intentions. Finally, theoretical and managerial implications of these findings are discussed.
1	This research explores how the experience of a jilt—the anticipation and subsequent inaccessibility of a highly desirable, aspirant option—influences preference for incumbent and non-incumbent options. The authors conceptualize jilting as a multi-stage process, which consists of a pre-jilt anticipatory phase that is initiated on the introduction of an aspirant option and a post-jilt phase that is initiated when the aspirant option becomes inaccessible. They show that during the anticipatory phase, a process of denigration specific to the incumbent option is engendered. The subsequent jilt elicits a negative emotional response. During the affectively charged post-jilt phase, preference shifts away from the now-denigrated incumbent option, yielding a jilting effect. In four field and laboratory studies, the authors establish this jilting effect, rule out alternative accounts, and discuss the theoretical and managerial implications of the findings.
1	Investors routinely follow firms’ communications and actions to form expectations about the firms’ future performance. The authors propose a set of firm and industry characteristics that influence the formation of investor expectations in the context of new product announcements. Specifically, they argue that positive expectations of future innovation output should cause an ex-ante increase in stock prices and a smaller ex-post market reaction when an actual new product is announced. Using a sample of 4,865 new product announcements made by 826 publicly traded U.S. firms, the authors show that the stock market reaction to a new product announcement measured in a five-day window around the announcement is negatively related to (1) the number of new products previously announced by the firm, (2) the average number of new products previously announced by the firm's competitors, and (3) the average sentiment of past public news about the firm. These three factors are also positively related to the market value of the firm measured immediately before each new announcement, controlling for increases in firm value directly attributable to past new product announcements. In contrast to many articles in the marketing literature that imply that the added value of a marketing event can be fully assessed from the stock market reaction to the announcement of the event, the authors clarify that for recurrent events or events that are part of a firm's broader strategy, this reaction reflects only an update of investors’ expectations of future firm performance.
1	This research demonstrates that the self-expressive customization of a product can improve performance on tasks performed using the customized product. Five studies show that the effect is robust across different types of tasks (e.g., persistence tasks, concentration tasks, agility tasks). The evidence further shows that the effect is not due to changes in product efficacy beliefs, feelings of competence, feelings of accomplishment, mood, task desirability, goal activation, or goal attainability. Instead, the self-expressive customization of a product extends an identity (e.g., personal identity, group identity) into the product. When the product is subsequently used to pursue a goal whose desired outcome can affirm the extended identity, performance improves.
1	Many businesses today have adopted tweeting as a new form of product marketing. However, whether and how tweeting affects product demand remains inconclusive. The authors explore this question using a randomized field experiment on Sina Weibo, the top tweeting website in China. The authors collaborate with a major global media company and examine how the viewing of its TV shows is affected by (1) the media company's tweets about its shows, and (2) recruited Weibo influentials’ retweets of the company tweets. The authors find that both company tweets and influential retweets increase show viewing, but in different ways. Company tweets directly boost viewing, whereas influential retweets increase viewing if the show tweet is informative. Meanwhile, influential retweets are more effective than company tweets in bringing new Weibo followers to the company, which indirectly increases viewing. The authors discuss recommendations on how to manage tweeting as a marketing tool.
1	Although email marketing is highly profitable and widely used by marketers, it has received limited attention in the marketing literature. Extant research has focused on either customers’ email responses or the “average” effect of emails on purchases. In this article, the authors use data from a U.S. home improvement retailer to study customers’ email open and purchase behaviors by using a unified hidden Markov and copula framework. Contrary to conventional wisdom, the authors find that email-active customers are not necessarily active in purchases, and vice versa. Furthermore, the number of emails sent by the retailer has a nonlinear effect on both the retailer's short- and long-term profitability. Through a counterfactual study, the authors provide a decision support system to guide retailers in making optimal email contact decisions. This study shows that sending the right number of emails is vital for long-term profitability. For example, sending four (ten) emails instead of the optimal number of seven emails can cause the retailer to lose 32% (16%) of its lifetime profit per customer.
1	To measure the effects of advertising, marketers must know how consumers would behave had they not seen the ads. The authors develop a methodology they call “ghost ads,” which facilitates this comparison by identifying the control group counterparts of the exposed consumers in a randomized experiment. The authors show that, relative to public service announcement and intent-to-treat A/B tests, ghost ads can reduce the cost of experimentation, improve measurement precision, deliver the relevant strategic baseline, and work with modern ad platforms that optimize ad delivery in real time. The authors also describe a variant, “predicted ghost ad” methodology, which is compatible with online display advertising platforms; their implementation records more than 100 million predicted ghost ads per day. The authors demonstrate the methodology with an online retailer's display retargeting campaign. They show novel evidence that retargeting can work: the ads lifted website visits by 17.2% and purchases by 10.5%. Compared with intent-to-treat and public service announcement experiments, advertisers can measure ad lift just as precisely while spending at least an order of magnitude less.
1	The authors propose a new approach to evaluate the perceptions and performance of a large set of paid search ads. This approach consists of two parts. First, primary data on hundreds of ads are collected through paired comparisons of their relative ability to generate awareness, interest, desire, action, and click performance. The authors use the Elo algorithm, a statistical model calibrated on paired comparisons, to score the full set of ads on relative perceptions and click performance. The estimated scores validate the theoretical link between perceptions and performance. Second, the authors predict the perceptions and performance of new ads relative to the existing set using textual content metrics. The predictive model allows for direct effects and interactions of the text metrics, resulting in a “large p, small n” problem. They address this problem with a novel Bayesian implementation of the VANISH model, a penalized regression approach that allows for differential treatment of main and interaction effects, in a system of equations. The authors demonstrate that this approach ably forecasts relative ad performance by leveraging perceptions inferred from content alone.
1	Many firms are allocating increasing parts of their advertising budgets to banner advertising. Yet, for firms that predominantly sell offline, existing research provides little guidance on online advertising decisions. In this study, the authors analyze the impact of banner advertising on consumers’ online and offline behavior across multiple distinct campaigns for one focal firm, which predominantly sells through the offline channel. Results suggest that banner and TV advertising increase website visit incidence for consumers who have not visited the focal firm's website in the previous four weeks (nonrecent online consumers). For these consumers, banner and TV advertisements indirectly increase offline sales through website visits. For consumers who have visited the firm's website in the previous four weeks (recent online consumers), the authors find evidence for a cross-campaign, brand-building effect of banner advertising, and TV ads also directly affect offline purchases. Overall, the findings indicate that for firms that predominantly (or even exclusively) sell offline, banner advertising is most suitable to generate awareness for a firm's new products among nonrecent online consumers, and to build their brand(s) among recent online consumers.
1	Building on cultural values research, the authors identify specific image attributes on which multicountry brands should position themselves consistently across markets. Leveraging prior research, they identify three life values that are most equal (benevolence, universalism, and self-direction) and two that are least equal (power and hedonism) in cross-national importance. The authors link specific brand image attributes (e.g., friendly, social, elite style, arrogant) to these life values through empirical data and semantic analysis. Using an extensive field data set on consumer perceptions and preferences from 22 countries regarding more than 1,700 brands, the authors then show that greater global consistency of a brand's image decreases overall brand attitudes if the specific image attribute is one that is not equally desired worldwide. They also find that the attitudinal impact of a multicountry brand's positioning consistency on commonly valued image attributes is greater when the set of competitors the brand faces across its markets is more homogeneous. The authors discuss implications for global brand management theory and practice.
1	While a time-based segmentation approach to customer segmentation for new products allows firms to identify consumers in the innovator and early adopter segments, this study adds a profitability-based perspective to generate new insights. Using six years of data on the adoption of technology services over three generations from a large technology manufacturer–service provider across seven countries, the authors provide empirical evidence that the short-term and long-term profitability per period of clients in the early majority segment is the highest, followed by the late majority, the innovators, the early adopters, and the laggards, respectively. While a time-based segmentation approach enables firms to identify consumers who are likely to adopt new products sooner than others, a profitability-based perspective can complement their targeting strategy and enhance overall profits. Managers can make informed decisions on investments required to develop new markets with better estimates of the profitability of consumers from later segments. Our study offers managers the necessary insights to develop a road map for identifying and targeting the most profitable clients.
1	Franchisors’ long-term viability is tied to the ongoing operations of their franchisees. To ensure the ongoing performance of franchisees, franchisors deploy multiple governance mechanisms. This study assesses how governance mechanisms deployed to enhance franchisee ability (via selection and socialization) and motivation (via incentives and monitoring) impact franchisee bankruptcy. The authors examine the individual and joint effects of deploying governance mechanisms that share the same underlying objective, namely, to enhance franchisee ability and motivation. They also assess how motivation-inducing mechanisms may serve to counter the motivation-dampening effect of an increased royalty rate. Relying on data from multiple archival sources, the authors identify all bankruptcy filings by franchisees and their franchisors across 1,115 franchise systems over a 13-year observation window. Their findings document a positive and significant relationship between franchisee and franchisor bankruptcy. They also find main and interaction effects of the ability- and motivation-influencing governance mechanisms on the likelihood of franchisee bankruptcy, and the existence of significant bankruptcy spillovers among franchisees within the same franchise system. They discuss implications for franchise theory and management.
1	Firms sometimes make selective or deceptive claims, which can have negative consequences for consumers, especially if consumers are not fully informed and the claims are hard to verify. This study aims to measure the decline in demand that a firm making such claims faces when caught. In addition, it seeks to understand which type of consumer these claims primarily affect. Using a panel data set of consumer purchases and firm advertising, the authors measure this impact by exploiting the fact that four popular products settled charges raised by the Federal Trade Commission. They further control for and document firm responses in terms of price and advertisement changes around the date of the settlement. Findings indicate a significant decline in demand following the termination of the claims, resulting in a 12%–67% monthly loss in revenue across the four products, which amounts to a $.40 million–$3.82 million loss in monthly revenue. They also find that these claims primarily affect consumers who are newcomers.
1	To optimally set marketing communication (“marcom”) budgets, reliable estimates of short-term elasticities and carryover effects are required. Empirical generalizations from meta-analyses of prior field studies can help guide these decisions. However, the last such meta-analysis of marcom carryover effects was performed on Koyck model–based estimates collected before 1984 and was confined to mass media advertising. The authors update and extend extant empirical generalizations via two meta-analyses of carryover estimates compiled from studies encompassing personal selling, targeted advertising, and mass media advertising, using diverse model forms, until 2015. The first is focused on and utilizes 918 estimates of the carryover proportion of the total effect, termed long-term share of the total effect, and the second focuses on 863 derivable estimates of 90% implied duration intervals. The authors find the mean long-term shares of the total effect for personal selling (.687) and targeted advertising (.650) are distinctly larger than that for mass media advertising (.523) and the corresponding median 90% implied duration intervals are 12.6, 2, and 3.4 months, respectively. The authors conclude by discussing differences by model type and the implications for marcom budget-setting and analyses.
1	Whereas many theories of decision making predict that presenting or not presenting common features of choice alternatives should not affect choice, in this research, the authors show that common features can be a powerful driver of choice behavior. They conjecture that consumers often hold expectations about the features that choice alternatives have in common, and they demonstrate that presenting (vs. omitting information about) a common feature increases the choice probability of the alternative that would have been expected to perform worse on the common feature, given its performance on differentiating features. This effect occurs because performance on the common feature is judged not at face value but relative to an expectation about which product should perform best on that feature. The effect holds even though performance on the common feature is clearly the same when alternatives are presented side by side. Finally, the authors demonstrate four boundary conditions of this effect.
1	Using data on advertising and sales of an innovative piece of golf equipment and the performance of its celebrity endorsers, the authors build a discrete-choice model that incorporates consumer awareness and preferences. They empirically investigate how celebrity endorsements affect consumer choices during new product introductions, the roles of planned advertising and unplanned media exposure, and how firms can strategically leverage the unplanned component. Model estimates reveal that wins in professional golf tournaments (which proxy for unplanned television exposure during weekly golf tournaments) affect awareness and that paid planned advertising impacts awareness and preferences. Focusing on Titleist equipment, counterfactual analysis demonstrates that the unplanned media exposure and planned advertising account for 22% and 24% of sales, respectively. The results also suggest that firms would benefit from coordinating the two. The planned portion should serve as a substitute for unplanned media exposure in the early stage and a complement as products age.
1	Distributional assumptions for random utility models play an important role in relating observed product attributes to choice probabilities. Choice probabilities derived with independent errors have the property of independence of irrelevant alternatives, which often does not match observed substitution behavior and leads to inaccurate calculations of source of volume when new entrants are introduced. In this article, the authors parameterize the covariance matrix for a probit model so that similar brands in the preference space have higher correlation than dissimilar brands, resulting in higher rates of substitution. They find across multiple data sets that similarity based on overall utility, not just attributes, defines products as similar with heightened rates of substitution. The proposed model results in better in-sample and predictive fits to the data and more realistic measures of substitution for a new product introduction.
1	Business-to-business firms spend significant resources in direct marketing to manage close relationships with their customers. Nevertheless, there is limited understanding of how the effectiveness of direct marketing communications varies by value propositions. Typically, direct marketing efforts are geared toward explicitly featuring economic or relational values. To implement an effective communication strategy catering to customers' preferences, firms should understand how customers consistently evaluate these organizational marketing communications, which ultimately affect their buying behaviors. Therefore, the authors analyze marketing messages and employ content analysis to capture the two distinct types of direct marketing communications. Using data from a Fortune 500 business-to-business service firm and a robust econometric model, they find that the (1) effects of economic and relational marketing communication on customer purchase behaviors interplay and vary over time, (2) latent stock of direct marketing communication affects customer purchase behaviors, and (3) evolution of customers' perceived importance can be recovered using transaction data. Overall, the authors provide a marketing resource reallocation strategy that enables marketers to customize marketing communications and improve a firm's financial performance.
1	Consumer lay theory suggests that women will spend more money than men in the presence of a physically dominant male employee, whereas theories of intrasexual competition from evolutionary psychology predict the opposite outcome. A retail field study demonstrates that male customers spend more money and purchase more expensive products than their female counterparts in the presence (vs. absence) of a physically dominant male employee. This effect has a more powerful impact on male customers who lack bodily markers of dominance (shorter stature or measures linked to lower levels of testosterone). When confronted with other physically dominant (vs. nondominant) men, these male customers are particularly prone to signal status through price or logo size. Their elevated feelings of intrasexual (male-to-male) competitiveness drive them to spend more money on status-signaling, but not functional, products and to prefer and draw larger brand logos. Because pictorial exposure is sufficient for the effect to occur, these findings are not limited to in-store interactions with dominant male employees but have broad implications for marketing and advertising.
1	Companies in a variety of sectors are increasingly managing customer churn proactively, generally by detecting customers at the highest risk of churning and targeting retention efforts towards them. While there is a vast literature on developing churn prediction models that identify customers at the highest risk of churning, no research has investigated whether it is indeed optimal to target those individuals. Combining two field experiments with machine learning techniques, the author demonstrates that customers identified as having the highest risk of churning are not necessarily the best targets for proactive churn programs. This finding is not only contrary to common wisdom but also suggests that retention programs are sometimes futile not because firms offer the wrong incentives but because they do not apply the right targeting rules. Accordingly, firms should focus their modeling efforts on identifying the observed heterogeneity in response to the intervention and to target customers on the basis of their sensitivity to the intervention, regardless of their risk of churning. This approach is empirically demonstrated to be significantly more effective than the standard practice of targeting customers with the highest risk of churning. More broadly, the author encourages firms and researchers using randomized trials (or A/B tests) to look beyond the average effect of interventions and leverage the observed heterogeneity in customers' response to select customer targets.
1	Television, the predominant advertising medium, is being transformed by the microtargeting capabilities of set-top boxes (STBs). By procuring impressions at the STB level (often denoted “programmatic television”), advertisers can now lower per-exposure costs and/or reach viewers most responsive to advertising creatives. Accordingly, this study uses a proprietary, household-level, single-source data set to develop an instantaneous show and advertisement viewing model to forecast consumers' exposure to advertising and the downstream consequences for impressions and sales. Viewing data suggest that person-specific factors dwarf brand- or show-specific factors in explaining advertising avoidance, thereby suggesting that device-level advertising targeting can be more effective than existing show-level targeting. Consistent with this observation, the model indicates that microtargeting lowers advertising costs and raises incremental profits considerably relative to show-level targeting. Further, these advantages are amplified when advertisers can buy in real time as opposed to up front.
1	To begin building an understanding of how thoughts about God influence consumer persuasion processes and outcomes, the current research explores how reminders of God affect consumer compliance with fear-based advertising. Results across seven studies demonstrate that when the concept of God is salient, consumer compliance and persuasion in response to fear appeals is dampened. Importantly, the results suggest that one reason for this persuasion-dampening effect of God salience is the fact that consumers associate the concept of God with the idea of unlimited support. Consistent with this, the results reveal that when God is not associated with the idea of support, the dampening effect of God salience on fear appeal compliance is eliminated.
1	Customers acquired through a referral program have been observed to exhibit higher margins and lower churn than customers acquired through other means. Theory suggests two likely mechanisms for this phenomenon: (1) better matching between referred customers and the firm and (2) social enrichment by the referrer. The present study is the first to provide evidence of these two mechanisms in a customer referral program. Consistent with the theory that better matching affects contribution margins, (1) referrer–referral dyads exhibit shared unobservables in customer contribution margins, (2) referrers with more extensive experience bring in higher-margin referrals, and (3) this association between the referrer's experience and margin gap becomes smaller over the referral's lifetime. Consistent with the theory that social enrichment affects retention, referrals exhibit lower churn only as long as their referrer has not churned. These findings indicate that better matching and social enrichment are two mechanisms through which firms can leverage their customers' networks to gain new customers with higher customer lifetime value and convert social capital into economic capital. One recommendation for the managers of the firm studied is to recruit referrers among their customers who have been acquired at least six months ago, exhibit high margins, and are unlikely to churn.
1	Enacted in an effort to discourage negative political advertising, American regulations mandate that candidates endorse their ads (“My name is ___, and I approve this message.”). Four studies suggest that mandatory endorsements enhance the perceived credibility of some ads these regulations were designed to discourage. This research tests for what types of messages mandatory endorsements have this effect, and why. Mandatory endorsements boosted evaluations of policy-focused attack ads—those typically plagued by overcomeable skepticism—but had no consistent effect on positive or character-focused ads. Mandatory endorsements boost ad believability—largely outside of participants' awareness—for two reasons: (1) the tagline offers a legitimizing association with regulation and (2) the candidates' own personally delivered endorsement language offers an implicit promise of the ads' truth value. The authors discuss how these findings bring order to and extend previous work on mandatory endorsements and ironic effects of communications requirements. Finally, they consider how regulations could be reformed to promote the public good by informing (without misleading) the electorate.
1	In this study, the authors investigate the externalities of managers' responses (MRs) to online reviews on popular travel websites. Specifically, the authors examine the effect of publicly responding to hotel guests' reviews on subsequent reviewer ratings. The authors find that manager responses to negative reviews (MR-N) can significantly influence subsequent opinion in a positive way if those responses are observable at the time of reviewing. Notably, the findings show this externality to be negative for manager responses to positive reviews (MR-P). The authors conduct a topic analysis on review texts and corresponding MRs to study the moderating role of response tailoring on the opinion externalities of MR. The authors show that tailored MR amplifies the positive (negative) impact of MR-N (MR-P) on subsequent opinion. Intuitively, tailoring an MR-N adds specificity to the hotel's complaint management strategy, bolstering the positive effects of MR-N on subsequent opinion. However, by highlighting specific positive elements of a review, managers' intent for responding is brought into question as they take advantage of reviewers' positive feedback to promote their hotel.
1	This study examines the impact of frontline employees' problem solving on customer satisfaction (CSAT) during ongoing interactions prompted by service failures and complaints. Using outsourced regulation theory, the authors predict negative moderating effects of frontline relational work and displayed affect on the dynamic influence of frontline solving work on CSAT. Frontline employees' verbal (nonverbal) cues provide the basis to identify solving and relational work (displayed affect). The authors test hypotheses with data from video recordings of real-life problem-solving interactions involving airline customers as well as a controlled experimental study. They find that frontline solving work has a positive effect on CSAT, and it increases in magnitude as the interaction unfolds. However, this positive effect becomes weaker for relatively higher levels of frontline relational work or displayed affect and, conversely, stronger for relatively lower levels over time. In summary, overdoing relational work and overdisplaying positive affect diminish the efficacy of problem-solving interactions, a finding that provides implications for theory and practice.
1	This article studies the impact of shopping at the warehouse club format on households' purchases of packaged food for the home. In addition to low prices, this format has several unique characteristics that can influence packaged food purchases. The empirical analysis uses a combination of households' longitudinal grocery purchase information, rich survey data, and detailed item-level nutrition information. After accounting for selection on observables and unobservables, the authors find a substantial increase in the total quantity (servings per capita) of packaged food purchases attributable to shopping at this format. Because there is no effect on the nutritional quality of purchases, this translates into a substantial increase in calories, sugar, and saturated fat per capita. The increase comes primarily from storable and impulse foods, and it is drawn equally from foods that have positive and negative health halos. The results have important implications for how marketers can create win–win opportunities for themselves and for consumers.
1	Service firms develop win-back strategies to rectify issues that cause customer churn and rebuild relationships with lost customers. To better support retention, it is important to understand how the revived relationship evolves and possibly ends again. To examine customers' repeat churn behavior, we develop a “mixture cure-competing risks” model, jointly estimating the duration of second lifetimes, multiple reasons for churn, and heterogeneity of customers in exhibiting a related churn reason. The proposed model is tested using a data set from a large telecommunications provider including information on customer behavior and marketing activities during customers' first and second lifetimes. We find support for the existence of a “cured” group of returning customers, defined as those who are not susceptible to churn for the same reason they churned previously. Our findings suggest that mitigating repeat churn behavior can extend customers' second lifetime tenure and increase profitability by $150,000 over the lifetime of the customers in the sample (leading to gains of over $15 million for deferring second-lifetime churn in a million returning customers), depending on the type of churn.
1	This research finds that when a single gain has strong associations with multiple costs, consumers often mentally deduct that gain from perceived costs multiple times. For example, with some price promotions (e.g., spend $200 now and receive a $50 gift card to spend in the future), consumers mentally deduct the value of the price promotion from the cost of the first purchase when they receive the promotion, as well as from the cost of the second purchase when they use the promotion. Multiple mental deductions based on a single gain result in consumers' perceptions that their costs are lower than they actually are, which can trigger higher expenditures. This mental accounting phenomenon, referred to as “double mental discounting,” is driven by the extent to which gains feel associated, or coupled, with multiple purchases. This article also documents methods to decouple promotional gains from purchases, thus mitigating double mental discounting.
1	Despite substantial prior research regarding the effect of context on choices, uncertainty remains regarding when particular context effects will be observed. In this article, the authors advance a new perspective on context-dependent choices, according to which context effects are a function of the relative advantage of one option over another and of the different strategies that decision makers evoke when making a choice. They propose that context effects resulting from the addition of a third option to a two-option set are more frequently observed when the added option is relatively similar (adjacent) to the “disadvantaged” alternative (i.e., the lower-share option) in the set. The authors conduct a series of studies to analyze the occurrence of context effects and find support for predictions related to asymmetric dominance and extremeness aversion.
1	This research draws from the psychodynamic perspective of social identity theory to examine the causal effect of mergers and acquisitions involving a mismatch of external images on the sales force. Study 1, a natural longitudinal experiment, shows that a merger with a poorer-image firm immediately dilutes salespeople's organizational identification (OI), which in turn impairs their performance. As sense makers, salespeople who are more tenured experience stronger OI dilution, whereas those who perceive a high level of social inclusion experience weaker OI dilution. As sense givers, managers who emphasize the firm's strategic intent in their communication buffer the OI-dilution effect, whereas those who emphasize the firm's organizational culture aggravate the effect. Study 2, a scenario-based experiment, further demonstrates that the OI-dilution effect is stronger than the OI-enhancement effect from merging with a better-image firm. Furthermore, both studies confirm that the adverse effect of mergers and acquisitions that involve a mismatch of external image stems from image uncertainty rather than job uncertainty.
1	The authors tested whether image-based information is more effective than text in changing implicit attitudes from positive to negative, even when both forms similarly change explicit attitudes. They studied corrective information (i.e., warnings about misleading advertising and product recall notices) because it is a common, important effort to change consumer attitudes. Corrective information in the form of pictures or imagery-evoking text, as well as direct instructions to imagine the scene, changed implicit attitudes more than plain, descriptive text, which is currently the most common warning method. Image-based stimuli can change implicit attitudes because they evoke vivid visual mental imagery of counterattitudinal valence (Experiments 1–2). Conditions that hindered the formation of visual mental imagery blocked implicit attitude change, whereas cognitive busyness did not (Experiment 3). In short, imagery-based information changed both explicit and implicit attitudes, whereas materials not based on imagery changed only explicit attitudes. Managers and regulators who aim to protect consumers from claims and products that could do harm should use image-based campaigns to best convey the message effectively.
1	Although the actions of others can influence a consumer's behavior, these actions are often at odds with performance norms. For example, charities can experience relatively low rates of support (resulting in a negative deviation from a performance norm) or relatively high rates of support (resulting in a positive deviation from a performance norm). Previous research provides evidence of the equivocal effects of these deviations, with both positive and negative deviations motivating prosocial behaviors. The current research reconciles these competing findings by introducing construal as a moderator. Across four studies, the authors find that positive deviations from performance norms motivate prosocial behavior for independent donors, whereas negative deviations from performance norms motivate prosocial behavior for interdependent donors. They further show that these effects are driven by a prevention focus associated with interdependent consumers and a promotion focus associated with independent consumers. The article concludes with implications for the marketing of charities and prosocial behaviors.
1	Consumers pursue numerous goals that are linked to particular time frames. Might one's likelihood of agreeing to pursue a goal fluctuate even if nothing about the goal's objective features changes, but if instead the only change is in how the time allotted for goal pursuit is described? Seven experiments show that consumers are more likely to agree to pursue goals when the completion interval is described by duration (e.g., “within exactly two weeks from now”) instead of date (e.g., “between today and November 17”). This pattern may arise because dates, which may make it easier to retrieve competing obligations falling within the interval, lead people to focus more on the (unenjoyable) goal-pursuit process, whereas durations, which present the interval in isolation, allow people to focus more on the goal's (beneficial) outcome. These findings suggest that although how a time interval is described seems inconsequential, it has striking effects on goal-pursuit decisions and therefore has important implications for the marketing of products and actions designed to assist consumers in achieving their goals.
1	Information frictions play a key role in an array of economic activities and are frequently incorporated into formal models as search costs. However, little is known about the underlying source of consumer search costs and how heterogeneous they are across markets. This study analyzes the sources and magnitude of heterogeneity in consumer search costs in retail gasoline markets. In doing so, the authors also investigate the extent to which retail gasoline stations employ mixed pricing strategies. They identify hundreds of geographically isolated markets and are the first to estimate the distribution of consumer search costs for many geographic markets. They directly recover the distribution of consumer search costs, market by market, using price data for retail gasoline in the United States. They find that the distribution of consumer search costs varies significantly across geographic markets and that distribution of household income is closely associated with search cost distribution.
1	This article proposes a framework for studying how a brand, firm, or individual can use networking activities to manage a social network and drive its success. Using data from ego networks of music artists, the article models how artists can enhance their social networking presence and stimulate relationships between fans to achieve long-term benefits in terms of music plays. The authors use a Bayesian modeling framework to model the heterogeneous and dynamic impact of networking activities on network structure and on music popularity, while relying on instrumental variables from another independent online social network to handle potential endogeneity. The results imply that artists can shape network structure via marketing activities and thereby achieve a long-term impact on success that far exceeds the direct and short-term impact in magnitude. Specifically, improving the density of ego networks enables long-term effects beyond those that stem from growth in network size.
1	Sales induced through price promotions depend heavily on discount depth, so firms create mechanisms to influence perceptions of discount depth. Typically, consumers compute discount depth as the difference between the sale price and the original price, with this difference compared against the original price. But thus far, no research has examined the effect of reframing this difference by comparing the discount depth against the sale price. Multiple studies, including a field study across four grocery stores, show that framing the discount depth by comparing it against the sale price increases consumers’ discount depth perceptions and thus increases purchase intentions. As evidence of the underlying process, the authors identify boundary conditions related to both individual differences (numeracy) and managerially relevant factors (discount depth size). In addition to contributing to research on price promotions, behavioral pricing, and numeric processing, the article offers implications for both practitioners and policy makers focused on consumer welfare.
1	Goals are constructs that direct choice behavior by guiding a decision maker toward desirable (or away from undesirable) end states. Often, consumers are motivated to satisfy multiple goals within a single choice. Although previous research has recognized this possibility, it has not directly formulated models of choice as a multigoal problem. The authors develop such a model, referred to as the multiple-goal-based choice model, which incorporates (1) simultaneous multiple goal pursuit and (2) context-driven goal adaptation but (3) does not require a priori identification of the number or nature of the goals. Goal adaptation within a single choice instance, allied to repeated choices, is the key to empirical identification of multiple latent goals. The proposed model is tested and supported using discrete choice experimental data on digital cameras through multiple validation exercises. The model can lead to significantly different policy implications with regard to consumers’ valuation for new product designs, compared with extant utility-based choice models.
1	The pervasive use of merchandise (i.e., noncash) incentives in sales compensation plans is an empirical and theoretical puzzle given the supposed superiority of cash incentives in the standard theory (i.e., principal–agent models) as well as the scant, and contradictory empirical evidence. The authors conducted a large-scale field intervention that switched 580 salespeople at a large frozen food manufacturer away from their cash plus “merchandise points” bonus to a commensurate all-cash bonus. After controlling for salesperson, seasonality, year, and target effects, the authors estimate that sales, on average, dropped by 4.36%. Furthermore, they estimated individual-level sales changes and effort changes to validate the incentive–effort–sales causal chain. The results show that the top salespeople experienced the largest drops in sales. A post-intervention survey of social and individual difference variables reveals that salespeople from households with more discretionary financial resources and those who think more abstractly about the uses of cash income exhibited smaller reductions in effort and sales. Although the absence of a control group prevents the authors from making strong causal inferences, this set of results nevertheless provides descriptive and suggestive evidence for separate mental accounts as the most promising explanation for the greater utility provided by merchandise incentives.
1	Consumers nowadays have easy and rich access to information about social others who are pursuing goals similar to their own (e.g., through a Fitbit device, the Endomondo mobile app, stickK.com). This research focuses on objective social information during goal striving (e.g., performance data and progress information of others) and shows that this information may not always be welcome. The author finds that when people are in the middle of a goal pursuit journey (vs. when they have just begun or are about to complete their goal), to circumvent potentially negative comparisons, they avoid information about social referents who are relevant (pursuing the same goal), proximal (in the same stage of goal pursuit), and superior. Head turn frequency, eye movements, and consumers’ direct choices in the lab and in the field are used to document a U-shaped pattern of information avoidance behavior, which paradoxically contributes to the phenomenon whereby goal pursuers become “stuck in the middle” of their pursuits. These findings connect the information avoidance literature with the psychophysics of goal pursuit and shed light on the questions of when and why people may be undermining their goal striving by avoiding relevant, motivating social information.
1	Matchmaking is a complex process that requires considerable expertise. Matchmakers in various industries often advertise proprietary technologies that presumably help users find an ideal match in a short time. However, matchmakers may have incentives to provide suboptimal matchmaking services so that users remain clients longer and pay more fees. This article considers a matchmaking market with network effects and strategic consumers and analyzes under what conditions matchmakers would offer more effective versus less effective matchmaking services. The authors find that stronger pricing power paradoxically leads to lower technology provision when consumers have high valuation for the matchmaking service. Moreover, network effects typically encourage matchmakers to retain the users in the market to create positive externalities, which can result in less precise matchmaking. In addition, consumer patience prompts competing matchmakers to implement ineffective technology. In two extensions, the authors explore asymmetric two-sided markets and discuss the impacts of alternative pricing schemes on technology provision.
1	Movies vary widely in appeal, star power, cost, and other elements, and therefore, each might be expected to charge a different price. Multiplexes, however, typically charge the same price for all movies, except for such premium formats as 3D, a choice that has puzzled managers and researchers. Because of data limitations, minimal empirical work has directly addressed this issue. In Hong Kong, however, prices vary both within and across multiplexes. Using daily ticket prices and attendance by theater and movie, the authors empirically examine the potential gains from differentiated movie-specific pricing as well as the increasingly common two-tier (2D/3D) uniform pricing, as compared with a full uniform pricing strategy in which a theater charges the same price for all its movies. Their results show that differential pricing leads to higher profits than the two-tier uniform pricing practice, but that the improvement is limited. In contrast, the gains are substantial when compared with the full uniform pricing strategy, suggesting that only minimal differentiation (2D/3D) may obtain most of the gains available from fully differentiated prices.
1	This research examines how incidental exposure to death-related information in the media affects consumers’ value orientation and scope sensitivity to marketing stimuli. Five studies demonstrate that, in contrast to thoughts about one's own mortality, exposure to death-related information in the media can shift consumers’ focus from extrinsic to intrinsic values. This leads them to pay less attention to the marketing stimuli, which are generally associated with extrinsic values, and consequently results in lower sensitivity to the magnitude of products and services. These effects are reversed when the marketing stimuli are associated with intrinsic values. Moreover, we found that exposure to death-related media information will generate effects similar to those of mortality salience when the information is perceived to be self-relevant and thus could induce death anxiety. The authors discuss implications and possible extensions.
1	Satiation is an ongoing marketing challenge as it continually reduces a consumer's ability to enjoy a favored experience. The prevailing notion is that satiation increases with similarity; hence, consumers can best slow satiation by consuming stimuli that are as different as possible. We challenge this traditional (and intuitive) view and instead propose that stimuli can be so inherently different that consumers no longer spontaneously consider them together as part of the same experience. In such cases, promoting the similarity of the stimuli can counterintuitively slow satiation. We propose that this reversal happens because finding similarities leads the consumer to place these episodes into a single ad hoc category for the ongoing experience, thereby helping the consumer fully realize the overall variety inherent across all stimuli. Five studies establish this finding across multiple domains (music, art, and food) and provide process evidence that an ad hoc categorization for the overall experience underlies our effect. Our theory and findings provide insight into how and when similarity can help or hinder satiation, and they clarify the role of ad hoc categorization in this relationship.
1	Showrooming, the phenomenon of consumers visiting a brick-and-mortar (B&M) store to learn about products but then buying online to obtain lower prices, is attracting increased attention both in business practice and in academic literature. It is considered a major threat to the B&M retailers, and determining “how to fight it” seems to be the only consideration. However, the manufacturer's need for retail informational services has always been one of the essential reasons for retailers to exist and is a means for retailers to achieve profitability. The popular arguments about the threat of showrooming ignore the strategic role of the manufacturer in the distribution channel. This article analytically shows that when the manufacturer's decisions are considered (i.e., when the manufacturer–retailer contract is endogenous), consumers’ ability to engage in showrooming may lead to increased, rather than decreased, profitability for B&M retailer(s). Thus, retail efforts to restrict showrooming behavior may be misguided. This result holds even if the manufacturer is restricted to wholesale-only contracts and is not allowed to price discriminate between channels.
1	Consumers incorrectly rely on their sense of understanding of what a company does to evaluate investment risk. In three correlational studies, greater sense of understanding was associated with lower risk ratings (Study 1) and with prediction distributions of future stock performance that had lower standard deviations and higher means (Studies 2 and 3). In all studies, sense of understanding was unassociated with objective risk measures. Risk perceptions increased when the authors degraded sense of understanding by presenting company information in an unstructured versus structured format (Study 4). Sense of understanding also influenced downstream investment decisions. In a portfolio construction task, both novices and seasoned investors allocated more money to hard-to-understand companies for a risk-tolerant client relative to a risk-averse one (Study 5). Study 3 ruled out an alternative explanation based on familiarity. The results may explain both the enduring popularity and common misinterpretation of the “invest in what you know” philosophy.
1	In today's digital market, the number of websites available for advertising has ballooned into the millions. Consequently, firms often turn to ad agencies and demand-side platforms (DSPs) to decide how to allocate their Internet display advertising budgets. Nevertheless, most extant DSP algorithms are rule-based and strictly proprietary. This article is among the first efforts in marketing to develop a nonproprietary algorithm for optimal budget allocation of Internet display ads within the context of programmatic advertising. Unlike many DSP algorithms that treat each ad impression independently, this method explicitly accounts for viewership correlations across websites. Consequently, campaign managers can make optimal bidding decisions over the entire set of advertising opportunities. More Importantly, they can avoid overbidding for impressions from high-cost publishers, unless such sites reach an otherwise unreachable audience. The proposed method can also be used as a budget-setting tool, because it readily provides optimal bidding guidelines for a range of campaign budgets. Finally, this method can accommodate several practical considerations including consumer targeting, target frequency of ad exposure, and mandatory media coverage to matched content websites.
1	While previous research has investigated various drivers of electronic word of mouth (eWOM), the firm's offline competitive environment has not been considered. The authors explore this new horizon and examine the different effects of firms’ geographic concentration, or agglomeration, on the volume of eWOM received. They distinguish three types of agglomeration—density agglomeration (number of firms in the industry in an area), product agglomeration (overlap in product types offered by the firms in the area), and temporal agglomeration (overlap in moment of consumption). The authors develop hypotheses pertaining to the main effects of the three agglomeration types on eWOM volume, which take the form of an inverted U-shape, and the moderating effect of vertical quality differentiation. The authors test the hypotheses on the volume of eWOM generated per month on Yelp for restaurants in Phoenix, generated by 23,526 users for 2,885 restaurants over an eight-year period. The empirical results broadly support the hypotheses.
1	Gift giving generates high revenues for retailers. It is also marked with significant welfare, or deadweight, loss in that givers tend to pay more than the receivers’ valuation. Previous research has attributed this discrepancy to givers’ inaccurate predictions of the receivers’ preferences. This research demonstrates that reduced price sensitivity is another important source of the deadweight loss: givers use gift prices to signal the importance of their relationship with the receiver. In order to demonstrate this mechanism, the authors develop a new Bayesian gift-choice model that captures both preference predictions as well as the signaling value of price. The model is estimated on two choice-based conjoint studies for gift giving that allow for the manipulation of the giver's uncertainty about the receiver's preferences. Both studies show the strong signaling value of price, especially when givers are uncertain about receivers’ preferences. Decomposition of the deadweight loss shows that the signaling value of price is an important source of welfare loss, especially in markets with heterogeneous prices. These findings have key implications for the gift industry.
1	In responding to customer questions or complaints, should marketing agents linguistically “put the customer first” by using certain personal pronouns? Customer orientation theory, managerial literature, and surveys of managers, customer service representatives, and consumers suggest that firm agents should emphasize how “we” (the firm) serve “you” (the customer), while de-emphasizing “I” (the agent) in these customer–firm interactions. The authors find evidence of this language pattern in use at over 40 firms. However, they theorize and demonstrate that these personal pronoun emphases are often suboptimal. Five studies using lab experiments and field data reveal that firm agents who refer to themselves using “I” rather than “we” pronouns increase customers’ perceptions that the agent feels and acts on their behalf. In turn, these positive perceptions of empathy and agency lead to increased customer satisfaction, purchase intentions, and purchase behavior. Furthermore, the authors find that customer-referencing “you” pronouns have little impact on these outcomes and can sometimes have negative consequences. These findings enhance understanding of how, when, and why language use affects social perception and behavior and provide valuable insights for marketers.
1	Seven studies covering diverse contexts show an underappreciated benefit of teasing in information acquisition: first creating and then resolving an uncertainty can generate a net positive experience, yet laypeople do not seek out this process. For example, trivia readers report better hedonic experiences if they are first teased with some missing information and then given that information than if they receive all the information at the same time; however, when given a choice, readers prefer to receive all information at the same time. The authors further show that teasing is hedonically beneficial because uncertainty engenders curiosity and thereby builds a potential for a positive experience, whereas uncertainty resolution satisfies the curiosity and thereby realizes that potential. This research yields practical implications by demonstrating that imbuing an ad with an uncertainty creation–resolution process improves the viewer's attitude toward and increases the viewer's willingness to try the advertised product.
1	Improving content sharing on social media platforms helps firms enhance the efficacy of their marketing campaigns. The authors study the impact of network overlap—the overlap in network connections between two users—on content sharing in directed social media platforms. The authors propose a hazards model that flexibly captures the impact of three measures of network overlap (i.e., common followees, common followers, and common mutual followers) on content sharing. Using data on content sharing from two directed social media platforms (Twitter and Digg), the authors establish that a receiver is more likely to share content from a sender with whom they share more common followees, common followers, or common mutual followers even after accounting for other measures. In addition, common followers have a higher effect than common mutual followers on the sharing propensity of the receiver. Finally, the effect of common followers and common mutual followers is positive when the content is novel but decreases, and may even become negative, when many others have already shared it. Collectively, these results have a bearing for marketers to more effectively target users for spreading content on social media platforms.
1	The authors propose that consumers’ increased self-focused attention promotes their relative reliance on affective feelings when they make decisions. The authors test this hypothesis in a variety of consumption domains and decision tasks, including real-life, consequential charitable donations. Consistent support from five experiments with more than 1,770 participants shows that (1) valuations of the decision outcome increase when consumers with high (low) self-focus adopt a feeling-based (reason-based) strategy. The hypothesized effect of self-focus on relative reliance on feelings in decision making is (2) moderated by self-construal. Furthermore, greater attention to the self (3) increases evaluations of products that are affectively superior but (4) decreases evaluations of products that are affectively inferior and (5) exerts little influence on evaluations of products that are less affective in nature (i.e., utilitarian products). Finally, self-focused attention (6) amplifies a decision bias typically attributed to feeling-based judgments, known as scope-insensitivity bias, in a hypothetical laboratory study and in a real-life, consequential charitable donation. Theoretical and marketing implications are discussed.
1	The authors demonstrate a novel template-based approach to profiling brand image using functional magnetic resonance imaging. They compare consumers’ brain responses during passive viewing of visual templates (photos depicting various social scenarios) and brain responses during active visualizing of a brand's image, and then they generate individual neural profiles of brand image that correlate with the participant's own self-report perception of those consumer brands. In aggregate, these neural profiles of brand image are associated with perceived cobranding suitability and reflect brand image strength rated by a separate and bigger sample of consumers. This neural profiling approach offers a customizable tool for inspecting and comparing brand-specific mental associations, both across brands and across consumers. It also demonstrates the potential of using pattern analysis of neuroimaging data to study multisensory, nonverbal consumer knowledge and experience.
1	There is growing interest in “customer-based corporate valuation”—that is, explicitly tying the value of a firm’s customer base to its overall financial valuation using publicly disclosed data. While much progress has been made in building a well-validated customer-based valuation model for contractual (or subscription-based) firms, there has been little progress for noncontractual firms. Noncontractual businesses have more complex transactional patterns because customer churn is not observed, and customer purchase timing and spend amounts are more irregular. Furthermore, publicly disclosed data are aggregated over time and across customers, are often censored, and may vary from firm to firm, making it harder to estimate models for customer acquisition, ordering, and spend per order. The authors develop a general customer-based valuation methodology for noncontractual firms that accounts for these issues. They apply this methodology to publicly disclosed data from e-commerce retailers Overstock.com and Wayfair, provide valuation point estimates and valuation intervals for the firms, and compare the unit economics of newly acquired customers.
1	This article investigates the value of business format franchising and how it is changing in response to a large increase in consumer information provided by online reputation mechanisms. Theory has suggested that much of the value of chain affiliation to firms comes from the ability of chain partners to use the same name, imagery, logo, and marketing to create a common brand reputation and signal specific qualities in settings with asymmetric information between buyers and sellers. As more information becomes available, consumers should rely less on branding for quality signals, and firms’ ability to extend reputations across heterogeneous outlets should decrease. To examine this empirically, the author combines a large panel of hotel revenues with millions of online reviews from multiple platforms. Chain-affiliated hotels earn substantially higher revenues than equivalent independent hotels, but this premium has declined by over 50% from 2000 to 2015. This can be largely attributed to an increase in online reputation mechanisms, and this effect is largest for low-quality and small-market firms. Measures of the information content of online reviews show that as information has increased, independent hotel revenue has grown substantially more than chain hotel revenue. This result should be viewed as descriptive, with attempts to come to near causality including the use of machine learning to derive latent dimensions of firm quality from the text of online reviews. Finally, the correlation between firm revenue and chain-wide reputation is decreasing, whereas the correlation with individual hotel reputation is increasing.
1	To advance theory, this study details how consumers evaluate multiple percentage price changes (discounts or surcharges). If they consider two discounts—for example, take 18% off the list price, then take an additional 12% off—consumers weight the two percentages to make their evaluations. Cues endogenous to the communication of those percentages also influence the weights applied, according to whether the two percentages appear presented at the same time (simultaneously) or temporally separated (sequentially) and whether the first percentage is larger or smaller. Depending on both the presentation mode and the ordering, consumers use different processes. In addition to providing practical guidance, this article extends understanding of anchoring and adjustment processes; information presented simultaneously leads consumers to anchor on the first piece of information. Sequential presentation instead induces surprise and shifts attention to the latter percentage change, which serves as the anchor in subsequent judgments. In addition to the underlying theory for these effects, this article delineates some boundary conditions and reveals the effects on consumers’ evaluations and choices, with findings from 11 studies.
1	Media attribution is the assignment of a percentage weight to each media touchpoint a consumer is exposed to prior to purchasing. Many firms consider using attribution to allocate media budgets, particularly for digital media, but an important question is whether this is appropriate. An initial hurdle when answering this question is that, despite the surge in interest for media attribution in marketing academia and practice, attribution does not have an agreed-on formal definition. Therefore, this article proposes an attribution formulation based on the relative incremental contribution that each medium makes to a purchase, taking into account advertising carryover and interaction effects. The formulation shows that attribution is proportional to the marginal effectiveness of a medium times its number of exposures. This means that often-used media will have high attribution weights. However, the profit-maximizing allocation for a fixed budget is a function of advertising effectiveness, but not a function of past exposure levels. By offering analytical derivations and studying simulated and empirical data, the paper shows how attribution can offer misleading insights on how to allocate resources across media. Moreover, the empirical example demonstrates that substantial gains in purchase probability can be made using profit-maximizing allocation compared with attribution-based allocation.
1	Evidence of the impact of partitioned pricing is contradictory. Research indicates that partitioning a price into multiple components can result in more favorable preferences, due to a lower recalled price, or less favorable preferences, due to unfavorable surcharge evaluations. To explain these divergent effects, the authors examine the role of price presentation moderators, which reflect how managers convey prices to consumers (e.g., Is the total price present or absent?), magnitude moderators, which reflect the actual prices charged (e.g., What is the surcharge magnitude?), and contextual moderators, which reflect nonprice transaction characteristics (e.g., Is the product category hedonic or utilitarian?). A meta-analysis of 17 years of partitioned pricing research examining 149 observations in 27 papers (N = 12,878) suggests that consumers respond more favorably to partitioned pricing than to all-inclusive pricing when the total price is absent, as the price level increases, when the surcharges are typical for the product category, when the surcharges are perceived as offering high benefit, and when the product category is utilitarian.
1	The article studies interfirm governance in the context of supplier–reseller relationships. Using a longitudinal study, the authors examine the roles of supplier selection efforts and mutual specific investments with respect to (1) motivating a supplier to make incremental investments and (2) safeguarding these investments from supplier ex post transaction costs. The authors also examine the joint effects of selection efforts and mutual investments on supplier ex post transaction costs. From a practical standpoint, the findings suggest guidelines for channel strategy. Theoretically, they provide new insights into relationship dynamics, including evidence regarding the effects of a firm’s governance choices over time.
1	Companies spend billions of dollars annually on sales force training, often carried out as off-site, multiday training events. However, the numerous challenges involved in training an entire sales group force many retailers to selectively train only a subset of their salespeople. It is crucial to know when selective training can be more effective and what composition of salespeople should be trained to benefit the entire group. This study addresses these questions using data from several stores of a retailer with different sales force training policies (full, selective, and no training [control]). The authors track the degree to which salespeople applied a customer relationship—building strategy taught in the training, along with more than 30 store- and salesperson-level covariates, and perform various analyses to correct for selection issues. They find that (1) selective training can be highly effective in stores with low performance diversity, (2) training salespeople with diverse tenures helps the spillover of training to the untrained, and (3) untrained salespeople with performance that is similar to the trained group are more likely to adopt the training-related behavior.
1	This article explores how intercompetitor licensing between an incumbent and an entrant affects market competition and the entrant’s optimal product quality. In the model, the incumbent has a noncore technology that is used for the noncore attribute of the final product, and the entrant has a new core technology to introduce a new, higher-quality product. For the noncore technology of its product, the entrant can either license it from the incumbent or develop it in-house. The authors show that a royalty licensing contract of the noncore technology between the incumbent and the entrant has a competition-alleviating effect. More important, the effect of such licensing on the entrant’s optimal quality depends on whether its core technology can significantly or only incrementally increase its product quality over the incumbent’s product quality. The royalty contract will tend to increase the entrant’s optimal quality when the entrant’s core technology can offer a significant quality improvement over the incumbent’s. By contrast, if the entrant’s technology can raise its product quality only incrementally over the incumbent’s product quality, the royalty contract will tend to reduce the entrant’s optimal quality. A wide range of royalty licensing contracts are mutually acceptable; the incumbent (entrant) can benefit from such a contract even when the entrant pays a total royalty fee that is lower (higher) than its alternative research-and-development cost. These results hold even when the incumbent endogenously chooses its royalty licensing fee. The main results are robust to several alternative modeling assumptions (e.g., alternative game sequence, endogenous quality decision by the incumbent, alternative licensing contract).
1	Research has shown that possessions have the power to change consumers’ self-construal and activate different aspects of the self. Building on this literature, the authors suggest that the salience of product ownership not only activates the product-related self but also simultaneously deactivates product-unrelated selves, resulting in impaired performance on tasks unrelated to the activated self. In five experiments, we first elicit feelings of ownership over a product (e.g., a calculator) to activate a product-related identity (e.g., the math self). Participants then engage in a task that is labeled as being a product-related task (e.g., a math task) or a product-unrelated task (e.g., a visual task). Although the task is the same, participants in the ownership condition perform worse on a task labeled as product-unrelated than those in the baseline condition do. Support for the underlying identity activation process comes from the finding that performance impairment is more likely to hold under conditions of low self-concept clarity, in which identity is malleable. The authors discuss the theoretical and practical implications of this finding.
1	Automated fabrication, home services, and premade goods pervade the modern consumer landscape. Against this backdrop, this research explores how the emotion of awe might motivate a consumer to partake instead in experiential creation (i.e., activities in which they actively produce an outcome) by enhancing their willingness to learn. Across eight experiments, experiencing awe (vs. happiness, excitement, pride, amusement, or neutrality) increases people’s likelihood of choosing an experiential creation gift (vs. one not involving experiential creation), willingness to pay for experiential creation products (vs. comparable ready-made products), likelihood of creating a bespoke snack (vs. taking a premade one), preference for experiential creation solutions (vs. solutions without experiential creation), likelihood of purchasing a product when it is framed as high (vs. low) in experiential creation, preference for high (vs. low) experiential creation meals, and likelihood of creating a knickknack (vs. taking a premade one). This greater desire for experiential creation is mediated by openness to learning and moderated by the need for closure. These findings, relevant for firms encouraging creation-oriented products and behaviors, offer fresh insights for engaging consumers.
1	In incentive-aligned choice experiments, each decision is realized with some probability, Prob. In three eye-tracking experiments, we study the impact of varying Prob from 0 (as in purely hypothetical choices) to 1 (as in real-life choices) on attention, information processing, and choice. Consistent with the bounded rationality literature, we find that as Prob increases from 0 to 1, consumers process the choice-relevant information more carefully and more comprehensively. Consistent with the psychological distance literature, we find that as Prob increases from 0 to 1, consumers become less novelty seeking and more price sensitive. These findings underscore that even with incentive alignment, preference measurement choice experiments such as choice-based conjoint analyses only represent an approximation of real-life choices. Although it is not feasible to systematically use questions with high Prob in the field, we predict and find that placing a higher probability question (such as an external validity task) at the beginning rather than the end of a questionnaire has a carryover effect on attention and information processing throughout the questionnaire, and it influences preference estimates as well.
1	Before their launch, many new products generate word of mouth (WOM) on social media. Such WOM typically increases toward the release date and contains sudden spikes. These spikes capture manifestations of peak consumer attention and are therefore of managerial importance, yet they have not received research attention. This article is the first to provide a comprehensive descriptive treatment of WOM spikes. The authors propose a conceptual framework to present spikes as a standalone WOM dimension and explain their emergence. They employ a robust filtering procedure to detect spikes and apply it in a data set of 90,000 prerelease online WOM messages on 157 Hollywood movies. The results indicate that prerelease spikes are widely prevalent: While some of them are event-driven, emerging in response to firm-created communications (e.g., trailer release), they are far more likely to emerge spontaneously. Content analysis reveals that WOM in spikes is more positive in sentiment and is more likely to deal with factual details than is WOM outside spikes. Prerelease WOM spikes also contribute significantly to the predictability of future product sales.
1	Automation is transforming many consumption domains, including everyday activities such as cooking or driving, as well as recreational activities like fishing or cycling. Yet little research in marketing examines consumer preferences for automated products. Automation often provides obvious consumption benefits, but six studies spanning a variety of product categories show that automation may not be desirable when identity motives are important drivers of consumption. Using both correlational and experimental designs, these studies demonstrate that people who strongly identify with a particular social category resist automated features that hinder the attribution of identity-relevant consumption outcomes to themselves. The findings have substantial theoretical implications for research on identity and technology, as well as managerial implications for targeting, product innovation, and communication.
1	Retailers routinely allow consumers to negotiate a discount off the posted price for big-ticket items such as home appliances and automobiles, and on online platforms such as Amazon and eBay. The profitability of such a strategy, relative to selling only at posted prices, depends on consumers’ willingness to initiate a negotiation and ability to negotiate a discount. In this article, the authors incorporate consumers’ decision of whether to negotiate into a demand model. The decision to negotiate hinges on how the expected discount from negotiation compares with the magnitude of a nonpecuniary cost that the consumer incurs by initiating the negotiation. The current study shows how this cost can be nonparametrically identified, separately from consumers’ ability to get a discount and marginal utility of income. The application of this model to individual-level data on refrigerator transactions reveals that, conditional on negotiating, consumers get, on average, 41% of the available surplus and incur an average cost of $28 to initiate a negotiation. The magnitude of these nonpecuniary costs’ not only affects retailer profits but also has implications for pricing strategy and consumer surplus. Ignoring these costs results in biased estimates of consumers’ willingness to pay, translating to annual losses of $1.6 million in the current study setting.
1	The authors examine purchase behavior in the context of cashback shopping—a novel form of price promotion online in which consumers initiate transactions at the website of a cashback company and, after a significant delay, receive the savings promised to them. Specifically, they analyze panel data from a large cashback company and show that, independent of the predictable effect of cashback offers on initial demand, cashback payments (1) increase the probability that consumers will make an additional purchase via the website of the cashback company and (2) increase the size of that purchase. These effects pass several robustness checks and are also meaningful: At average values in the data, an additional $1.00 in cashback payment increases the likelihood of a future transaction by .02% and spending by $.32—figures that represent 10.03% of the overall impact of a given promotion. Moreover, the authors find that consumers are more likely to spend the money returned to them at generalist retailers, such as department stores, than at other retailers. They consider three explanations for these findings; the leading hypothesis is that consumers fail to treat money as a fungible resource. They also discuss implications for cashback companies and retailers.
1	The authors propose a conceptual framework of misresponse to multi-item scales in surveys in which misresponse to items that are reversed relative to other items (reversal misresponse) is differentiated from misresponse to items that are negated (negation misresponse) and from misresponse to items whose core concept is the opposite of the core concept in regular items (polar opposite misresponse). The framework specifies two broad mechanisms to account for the three forms of misresponse: lack of motivation to process items in detail (“inattention”) and lack of ability to comprehend items accurately (“difficulty”). The authors propose a procedure to identify potential misresponse effects on the observed item responses and factor loadings, and they report two empirical studies to test the framework; the second study uses eye movement recordings to examine the underlying process. The findings reveal that polar opposite, reversed, and negated items contribute to misresponse to varying degrees and that difficulty rather than inattention may be a more potent cause of misresponse in surveys than has traditionally been acknowledged.
1	Nonprofit health care organizations in low- and middle-income countries often pursue a cross-subsidization business model wherein services are offered to poor patients for free through surpluses generated by serving some patients at market prices. This approach allows such organizations to fulfill their mission-oriented and revenue-generation goals. Conventional wisdom holds that mission activities need financial subsidies from revenue-generating activities. The authors examine this dependence in the context of Aravind Eye Hospitals, which delivers eye care services in India. They measure whether the marketing activities (outreach camps) of Aravind that are targeted only to poor patients produce the spillover benefit of attracting paying patients to its hospitals. Using nine years of patient-level historical data, the authors find that camps increase the flow of paying patients. These effects are comparable to the camps acting as advertising for Aravind. Using model estimates, the authors compute the incremental revenue accruing to Aravind from a camp and find that it exceeds the incremental cost of a camp. The findings challenge conventional beliefs about the subsidies required by mission activities.
1	The authors examine how consumers respond to pseudo-free offers—offers that are presented to consumers as free but that require consumers to make a nonmonetary payment (such as completing a survey or providing personal information) in order to receive the “free” good or service. Across six studies, the authors find that consumers are generally just as likely to accept pseudo-free offers (with nonmonetary costs) as comparable truly free offers (with no costs), as long as the costs of the pseudo-free offers are below some threshold. Additionally, they find that consumers are significantly more likely to accept pseudo-free offers (with nonmonetary costs) than comparable nonfree offers (with monetary costs). The authors provide evidence that consumers respond to pseudo-free offers in this way because, in general, consumers generate neutral or positive attributions for why firms make these offers, and these attributions, in turn, lead consumers to perceive the pseudo-free offers as fair. However, when contextual influences, characteristics of the pseudo-free offer, or individual dispositions increase the likelihood of negative attributions, consumers’ preference for pseudo-free offers is attenuated.
1	The authors examine how perceived similarity between sequential risks affects individuals’ risk-taking intentions. Specifically, in six studies, the authors find that, in sequential choice settings, individuals exhibit significant positive state dependence in risk-taking preferences, such that they are more likely to take a risk when it is similar to a previously taken risk than when it is dissimilar. For example, if an individual has previously taken a health/safety risk, that individual is more likely to take a second health/safety risk than a second risk that is in the financial domain. The authors show that because similarity between risks is malleable and can be determined by situational and contextual variables, subsequent risk-taking intentions can be changed in a predictable manner when similarity is manipulated through framing. The authors establish that increased feelings of self-efficacy and self-signaling through the prior risk-taking experience drive state-dependent risk-taking preferences. The authors further show that the effect of similarity on preferences is not moderated by the outcome received in the prior risk and holds when controlling for individual-level and domain-specific heterogeneity. Taken together, the results demonstrate that the similarity structures that exist between risks have a significant effect on risk-taking preferences in dynamic choice settings.
1	Prior research has identified product improvement perceptions as critical to consumers’ product upgrade decisions (e.g., upgrading to a new iPhone), but little work has examined factors influencing these improvement perceptions. This research shows that drawing consumers’ attention to their global self-improvement can increase product improvement judgments and upgrade intentions when self–brand connection is high, a phenomenon the authors refer to as egocentric improvement evaluation. In line with egocentric categorization theory, which identifies the self as a dominant reference category in product judgment, the authors demonstrate cognitive drivers of the effect. Specifically, egocentric improvement evaluations are moderated by self-focus, which determines whether the self is an accessible reference category. Furthermore, the authors propose that egocentric improvement evaluations also have a motivational driver: consumers project their self-improvement onto self-connected brands to satisfy self-enhancement motives. The core effect is moderated by self-affirmation, which quells the need for self-enhancement, and by self-threat, which heightens the need for self-enhancement. The authors investigate this effect in five studies and discuss the theoretical and practical implications.
1	Intertemporal savings strategies, such as bulk buying or accelerating purchase timing to take advantage of a good deal, provide long-term savings in exchange for an increase in immediate spending. Although households with limited financial resources stand to benefit the most from these strategies, they are less likely to make use of them. The authors provide causal evidence that liquidity constraints impede low-income households’ ability to use these strategies, above and beyond the impact of other constraints. Exploiting recurring variation in household liquidity, this study shows that when low-income households have more liquidity, they partially catch up to higher-income households’ ability to use intertemporal savings strategies. The findings provide guidance to marketing managers and researchers regarding targeted promotional design and measurement of deal-proneness. For policy makers, they suggest a new path for decreasing the higher prices low-income households have been documented to pay for everyday goods. Policies have traditionally focused on increasing financial literacy or access to supermarkets. Our work suggests that providing greater liquidity can help low-income households make better use of savings opportunities already available to them.
1	The authors propose a quantitative approach for describing entertainment products, in a way that allows for improving the predictive performance of consumer choice models for these products. Their approach is based on the media psychology literature, which suggests that people’s consumption of entertainment products is influenced by the psychological themes featured in these products. They classify psychological themes on the basis of the “character strengths” taxonomy from the positive psychology literature (Peterson and Seligman 2004). They develop a natural language processing tool, guided latent Dirichlet allocation (LDA), that automatically extracts a set of features of entertainment products from their descriptions. Guided LDA is flexible enough to allow features to be informed by psychological themes while allowing other relevant dimensions to emerge. The authors apply this tool to movies and show that guided LDA features help better predict movie-watching behavior at the individual level. They find this result with both award-winning movies and blockbuster movies. They illustrate the potential of the proposed approach in pure content-based predictive models of consumer behavior, as well as in hybrid predictive models that combine content-based models with collaborative filtering. They also show that guided LDA can improve the performance of models that predict aggregate outcomes.
1	The authors aim to answer the following question: If the capital market reacts with abnormal stock returns to new product development success events, do these returns influence subsequent marketing decisions? Drawing on informational market feedback and managerial learning theories, the authors posit that when firms are uncertain about how responsive the product market will be to their marketing activities, signals received from the capital market help them update their beliefs about the product market’s responsiveness. In the pharmaceutical context, the authors decompose the abnormal returns at a new drug approval event into components that the firm can and cannot predict (i.e., predicted and unpredicted abnormal returns) and find that the postapproval advertising budget is larger when unpredicted abnormal approval returns are higher. Furthermore, this tendency is more pronounced for spending on detailing than for direct-to-consumer advertising. Consistent with these higher budgets, the authors find that postlaunch advertising is more effective when unpredicted abnormal approval returns are higher, particularly for detailing spending (vs. direct-to-consumer advertising). Overall, this study suggests that information flows from the capital market’s initial perceptions at new product introduction play an important role in subsequent marketing decisions in the product market.
1	The global importance of online advertising calls for a detailed understanding of consumer-specific responses to online ad repetitions. A key concern for advertisers is not only whether some consumers display degrees of “wearout” but also whether they can surpass a point at which additional exposures have a negative marginal effect: “weariness.” The authors examine a large-scale advertising campaign aimed at driving viewers to a target website, which comprises more than 12,000 users across over 400 websites. These data are analyzed using a flexible discrete mixture specification that accommodates different response shapes over ad stock and timing and parcels ad viewers into response classes based on their internet usage metrics. The resulting classes display varying degrees of wearout, with one subgroup, accounting for about 24% of the sample, evincing weariness. The model also estimates differential publisher effectiveness, with the most effective publisher being nine times more effective than the one 26 places down. The authors demonstrate that the finding of weariness is robust to all the model’s main components, with one key exception: heterogeneity in users’ ad response. Analysis further suggests that an appropriate “profiling and capping” strategy can improve ad deployment by as much as 15% overall for these data.
1	Although firms are increasingly launching branded mobile apps, an understanding of their influence on firm value remains elusive. Using stock market returns to assess firm value, the authors investigate the impact of branded mobile app announcements on such value. Moreover, recognizing that mobile apps generate various touchpoints in the customer journey, the authors also investigate how an app’s design shifts the effects of mobile apps on firm value. In particular, they investigate effects from whether an app emphasizes features related to peer-to-peer interactions about the brand, personal-oriented interactions between a customer and the brand, or the purchase phase itself. They find that the launch of a mobile app increases firm value and that the features emphasized in app design play an important role in such value creation. The study offers important implications regarding the accountability of branded mobile apps and provides direction for marketing theory and practice.
1	Five studies using a variety of experimental approaches and secondary data sets show that a visual property present in all brand logos—the degree of (a)symmetry—can interact with brand personality to affect brand equity. Specifically, compared with symmetrical logos, asymmetrical logos tend to be more arousing, leading to increased perceptions of excitement. As such, consumers tend to perceive asymmetrical logos as more congruent with brands that have an exciting personality. This can boost consumers’ evaluations and the market’s financial valuations of such brands, a phenomenon referred to as the “visual asymmetry effect.” The studies also show that this interplay between brand personality and logo design occurs only for the personality of excitement and the visual property of asymmetry. These findings add to theories of visual design and branding and offer actionable insights to marketing practitioners.
1	Despite the ubiquity of numerical information in consumers’ lives, prior research has provided limited insights to marketers about when numerical information exerts greater impact on decisions. This study offers evidence that judgments involving numerical information can be affected by consumers’ sense of personal control over the environment. A numerical attribute’s format communicates the extent to which the magnitude of a benefit is predictable (Study 1a), such that people who experience a control threat and want to see their external environment as predictable (Study 1b) rely on point value (vs. range) information as a general signal that the environment is predictable (Study 2). A personal control threat changes consumers’ preferences as a function of whether the numerical information appears as a point value or a range (Studies 3–4). This heightened focus on format may lessen the impact of a product benefit’s predicted magnitude, if a lower magnitude is specified in a more precise format (Study 5). Study 6 provides first evidence that the interactive effect of personal control levels and numerical formats can affect consequential choices.
1	Managers are using ambient scent as an important strategic element in various service settings, with food-related scents being especially common. This research examines the effects of food-related ambient scents on children’s and adults’ food purchases/choices. The results of a series of experiments, including field studies at a supermarket and at a middle school cafeteria, show that extended exposure (of more than two minutes) to an indulgent food–related ambient scent (e.g., cookie scent) leads to lower purchases of unhealthy foods compared with no ambient scent or a nonindulgent food–related ambient scent (e.g., strawberry scent). The effects seem to be driven by cross-modal sensory compensation, whereby prolonged exposure to an indulgent/rewarding food scent induces pleasure in the reward circuitry, which in turn diminishes the desire for actual consumption of indulgent foods. Notably, the effects reverse with brief (<30 seconds) exposure to the scent. Whereas prior research has examined cross-modal effects, this research adopts the novel approach of examining cross-modal sensory compensation effects, whereby stimuli in one sensory modality (olfactory) can compensate/satisfy the desire related to another sensory modality (gustatory).
1	Researchers have found that consumers abandon and avoid products when they feel threatened by the presence of dissimilar groups who also use the product. In this article, the authors propose a different strategy for responding to dissimilar users, upgrading to a brand’s more exclusive products. Results from six studies show that upgrading is a preferred strategy for consumers with strong self–brand connections, who are unlikely to avoid or abandon the brand because their self-identity is tied to the brand. Among these consumers, the act of upgrading to a brand’s more exclusive products is driven by their feelings of self-threat when exposed to dissimilar users, which triggers a desire to attain a higher-status position among brand users that is fulfilled by upgrading. The authors also identify moderators of the upgrading effect for consumers with strong self–brand connections. These consumers’ increased interest in upgrading is dampened when a brand’s exclusive products are made more readily available and when a brand has already conferred higher status to them through a customer status tier program.
1	Brand naming challenges are more complex in logographic languages (e.g., Chinese), compared with phonographic languages (e.g., English) because the former languages feature looser correspondence between sound and meaning. With these two dimensions of sound and meaning, the authors propose a four-way categorization of brand name types for logographic languages: alphanumeric, phonetic, phonosemantic, or semantic. Using automobile sales data from China and a discrete choice model for differentiated products, the authors relate brand name types to demand, with evidence showing that Chinese consumers preferred vehicle models with semantic brand names (7.64% more sales than alphanumeric) but exhibited the least preference for phonosemantic names (4.92% lower sales than alphanumeric). Domestic Chinese firms benefited from semantic brand names, whereas foreign firms gained from using foreign-sounding brand names. Entry-level products performed better with semantic brand names, and high-end products excelled when they had foreign-sounding brand names. Thus, the four-way categorization of brand name types should help multinational firms and domestic Chinese firms understand and leverage the association between brand name types and consumer demand.
1	Sleepiness, the subjective feeling of the propensity to fall asleep, is a common, everyday experience that can be induced by various factors, such as sleep quality, sleep deprivation, ingestion of certain substances, or belief about how much sleep a person needs. Despite its prevalence, sleepiness and its influence on consumption behavior have rarely been linked in the research to date. The present research helps fill this void by uncovering the novel impact of sleepiness on consumer variety-seeking behavior. The studies, using various methods and all involving consequential choices, revealed that sleepier consumers tended to seek more variety. The driver of this effect was found to be a need for arousal to maintain wakefulness. The authors also show that variety-seeking behavior is effective in partially reducing sleepiness. The effect of sleepiness on variety seeking uncovered in this research is somewhat nonintuitive, in the sense that, a priori, one might expect sleepiness to be more likely to decrease rather than increase exploratory behavior. The authors discuss implications of the findings for different research areas and for marketing practice.
1	Consumers frequently express themselves by posting about products on social media. Because consumers can use physical products to signal their identities, posting about products on social media may be a way for consumers to virtually signal identity. The authors propose that there are conditions in which this action can paradoxically reduce a consumer’s subsequent purchase intentions. Five experiments demonstrate that posting products on social media that are framed as being identity-relevant can reduce a consumer’s subsequent purchase intentions for the same and similar products, as this action allows consumers to virtually signal their identity, fulfilling identity-signaling needs. Fortunately for retailers, the authors suggest theoretically and managerially relevant moderators that attenuate this negative effect on intent to purchase. These findings have important implications for how firms can conduct social media marketing to minimize negative purchase outcomes.
1	Bayesian methods for dynamic models in marketing have so far been parametric. For instance, it is invariably assumed that model errors emerge from normal distributions. Yet using arbitrary distributional assumptions can result in false inference, which in turn misleads managers. The author therefore presents a set of flexible Bayesian nonparametric (NP) dynamic models that treat error densities as unknown but assume that they emerge from Dirichlet process mixtures. Although the methods address misspecification in dynamic linear models, the main innovation is a particle filter algorithm for nonlinear state-space models. The author used two advertising studies to confirm the benefits of the methods when strict error assumptions are untenable. In both studies, NP models markedly outperformed benchmarks in terms of fit and forecast results. In the first study, the benchmarks understated the effects of competitive advertising on own brand awareness. In the second study, the benchmark inflated ad quality, and consequently, the effects of past advertising appeared 36% higher than that predicted by the NP model. In general, these methods should be valuable wherever state-space models appear (e.g., brand and advertising dynamics, diffusion of innovation, dynamic discrete choice).
1	This article examines how consumers’ exposure to the viewpoint of high versus low vertical position changes their risk-taking behavior. The author proposes that consumers’ views of scenery from a high physical elevation induce an illusory sense of control, which in turn intensifies risk taking. Multiple studies show that exposure to the viewpoint of high vertical positions increases risk taking in both laboratory settings (Studies 1a, 1b, and 1c) and real settings (Study 4). In addition, the author demonstrates that an illusory sense of control mediates the effect of an elevated viewpoint on risk taking (Study 2) and that the effect of elevation on risk taking is attenuated when people use a low-level mental construal to process visual information (Study 3).
1	If consumers believe that stores offering price-matching guarantees (PMGs) charge low prices, high-search-cost consumers will purchase from PMG stores. This leads PMG stores’ demand to be less price sensitive, which drives these stores to charge higher prices. The belief that PMG stores charge low prices paradoxically leads them to charge high prices. For this reason, the literature finds that PMGs can only signal low prices when firm heterogeneity is sufficiently large. Because PMGs are offered by retailers that purchase the same product from the same producer, large firm heterogeneity may be a strong assumption. This article proposes a theory that explains how homogeneous firms may signal their low prices through PMGs: consumers perceive PMG stores to have lower prices not because they expect them to have low marginal costs or service quality, but simply because they offer a PMG.
1	User-generated content has become ubiquitous and very influential in the marketplace. Increasingly, this content is generated on smartphones rather than personal computers (PCs). This article argues that because of its physically constrained nature, smartphone (vs. PC) use leads consumers to generate briefer content, which encourages them to focus on the overall gist of their experiences. This focus on gist, in turn, tends to manifest as reviews that emphasize the emotional aspects of an experience in lieu of more specific details. Across five studies—two field studies and three controlled experiments—the authors use natural language processing tools and human assessments to analyze the linguistic characteristics of user-generated content. The findings support the thesis that smartphone use results in the creation of content that is less specific and privileges affect—especially positive affect—relative to PC-generated content. The findings also show that differences in emotional content are driven by the tendency to generate briefer content on smartphones rather than user self-selection, differences in topical content, or timing of writing. Implications for research and practice are discussed.
1	This research examines drivers of consumer word of mouth (WOM) in free-product settings, revealing fundamental differences with traditional, paid-product settings. The authors build and investigate a theoretical model that highlights two unique characteristics of free products (reciprocity motivation and diminished adoption risk) and considers their implications for WOM sharing. Results of a retrospective survey, two controlled experiments, and an analysis of more than 5,000 mobile apps at Google Play and Apple’s App Store reveal that consumers are generally more likely to share their opinions of free products than paid products, because of feelings of reciprocity toward the producer. However, this difference is reduced when prior consumer WOM is low in volume and highly disperse, signaling greater adoption risk. These findings contribute to nascent understanding of free-product marketing while offering new insights for catalyzing consumer WOM.
1	Value-appropriation activities enable a firm to extract more profits from existing customers. The authors examine how investments in two types of value-appropriation activities—advertising and receivables—are jointly associated with abnormal stock returns and idiosyncratic risk. Using data from 1,375 firms over the period of 2003–2015, the authors find that advertising investments and receivables investments interact as substitutes, such that increasing advertising (receivables) investments is detrimental to the beneficial effect of receivables (advertising) investments on firm shareholder value. They find that this association is contingent on firm business scope, such that the joint effect of advertising investments and receivables investments becomes weaker when firms have a broader business scope compared with a narrower business scope.
1	Targeted promotions based on individual purchase history can increase sales. However, the opportunity costs of targeting to optimize promoted product sales are poorly understood. A series of randomized field experiments with a large e-book platform shows that although targeted promotions increase promoted product sales and purchases of similar products, they can crowd out purchases of dissimilar products (i.e., e-books from nontargeted genres) by decreasing search activities of nontargeted goods on the same platform. The effects on total sales are heterogeneous, ranging from net decreases to insignificant drops, motivating a targeting exercise comparing strategies that optimize promoted product sales versus total sales. Targeting for promoted product sales tends to assign promotions to customers who purchased similar products, whereas targeting for total sales assigns promotions on the basis of other user characteristics. Targeting for promoted product sales generated incremental total sales that amounted to approximately 29% of the optimal incremental total sales when targeting for total sales (an opportunity cost of 71%). The optimal targeting exercise highlights how maximizing promotional lift can incur opportunity costs in terms of other forgone sales.
1	Following subgoal success at a long-term goal, consumers can persist, behaving consistently with the long-term goal, or license, behaving consistently with competing shorter-term goals. The authors extend prior work by proposing that this choice is driven, in part, by chronic differences in consumers’ cognitive and emotional responses to subgoal success. In Studies 1–5, they develop and validate a measure that captures these individual differences: the Persistence–Licensing Response Measure (PLRM). They demonstrate that the PLRM predicts persistence and licensing where existing constructs and measures do not, showing that consumers’ responses to subgoal success represent a unique dimension of self-regulation. In Studies 6–10, the authors demonstrate that the PLRM moderates the effect of subgoal success on persistence and licensing behavior, revealing that the same subgoal success situation can lead to systematically different behaviors. The authors also examine how marketing interventions can be used to increase consumers’ persistence following subgoal success. This work furthers the understanding of the determinants of persistence and licensing, improves prediction of behavior, and offers marketers tools to segment and target consumers, increasing persistence in key goal domains.
1	The authors propose a new truth-telling technique and statistical model called “item count response technique” (ICRT) to assess the prevalence and drivers of sensitive consumer behavior. Monte Carlo simulations and a large-scale application to self-reported cigarette consumption among pregnant women (n = 1,315) demonstrate the effectiveness of the procedure. The ICRT provides more valid and precise prevalence estimates and is more efficient than direct self-reports and previous item count techniques. It accomplishes this by (1) incentivizing participants to provide truthful answers, (2) accounting for procedural nonadherence and differential list functioning, and (3) obviating the need for a control group. The ICRT also facilitates the use of multivariate regression analysis to relate the prevalence of the sensitive behavior to individual-level covariates for theory testing and policy analysis. The empirical application reveals a significant downward bias in prevalence estimates when questions about cigarette consumption were asked directly to pregnant women, or when standard item count techniques were used. The authors find lower smoking prevalence among women with higher levels of education and who are further along in their pregnancy, and a much higher prevalence among unmarried respondents.
1	This article addresses seeding policies in user-generated content networks by challenging the role of influencers in a setting of unpaid endorsements. On such platforms, the content is generated by individuals and firms interested in self-promotion. The authors use data from a worldwide leading music platform to study unknown music creators who aim to increase exposure of their content by expanding their follower base through directing outbound activities to other users. The authors find that the responsiveness of seeding targets strongly declines with status difference; thus, unknown music creators (the majority) do not generally benefit at all from seeding influencers. Instead, they should gradually build their status by targeting low-status users rather than attempt to “jump” by targeting high-status ones. This research extends the seeding literature by introducing the concept of risk to dissemination dynamics in online communications, showing that unknown music creators do not seed specific status levels but rather choose a portfolio of seeding targets while solving a risk versus return trade-off. The authors discuss various managerial implications for optimal seeding in user-generated content networks.
1	Several industries have recently been criticized by parents, think tanks, and governments for creating product environments that lead to excessive screen usage. If firms do not properly manage product usage, demand may drop, and public policy makers may intervene. The authors of this study test alternative ways to manage the use of such products: redesigning the timing of rewards, introducing notifications to users, and imposing time limits. A continuous-time demand model is proposed and empirically estimated with high-frequency data. The methodology is flexible enough to simultaneously explain multiple usage decisions that happen in quick succession, such as when to start and stop usage and how to respond to rewards or messages from the firm. The approach is implemented on a data set from the online gaming industry that includes usage decisions of a large sample of individuals. The authors find that improving reward schedules and imposing time limits leads to shorter usage sessions and longer product subscriptions—a win-win outcome. Notifications are found not to be useful to manage product usage.
1	In collaboration with an online seller of home-improvement products, the authors conduct a large-scale randomized field experiment to study the effects of retargeted advertising, a form of internet advertising in which banner ads are displayed to users after they visit the advertiser’s website. They find that switching on experimental retargeting causes 14.6% more users to return to the website within four weeks. The impact of retargeting decreases as the time since the consumer first visited the website increases—indeed, 33% of the effect of the first week’s advertising occurs on the first day. Furthermore, the authors find evidence of the existence of complementarities in advertising over time: the effect of advertising in week 2 of the campaign is higher when the user was assigned to a nonzero level of advertising in week 1. The authors discuss mechanisms that can explain their findings and demonstrate a novel low-cost method that can be applied generally to conduct valid online advertising experiments.
1	Marketers of digital content such as books, news, video, music, and mobile games often provide free samples of the content for consumers to try out before buying the product or signing up for subscription. Similarly, firms selling software (such as software as a service), and cloud-based services may provide free limited-version products or a free-trial period for the service. In this article, the authors focus on how firms should design such free samples to maximize their revenue. They examine in an analytical setting how quality and other design parameters of the sample affect profit generated by the product or service. They then test the normative implications in the application context of a book publisher that provides free samples for the books it sells online. Using a field experiment, the authors vary the design parameters of the sample and, based on the demand estimates, provide recommendations for the firm on the optimal design of the sample. They find that, rather than being substitutes, free samples of the entire content can be very effective in increasing revenues. Furthermore, they find that higher-quality samples have a greater impact on the sales of popular content. This has important implications for freemium and free-trial business models.
1	In emerging markets, the effective implementation of distribution strategies is challenged by underdeveloped road infrastructure and a low penetration of retail stores that are insufficient in meeting customer needs. In addition, products are typically distributed in multiple forms through multiple retail channels. Given the competitive landscape, manufacturers’ distribution strategies should be based on anticipation of competitor reactions. Accordingly, the authors develop a manufacturer-level competition model to study the distribution and price decisions of insecticide manufacturers competing across multiple product forms and retail channels. Their study shows that both consumer preferences and estimated production and distribution costs vary across brands, product forms, and retail channels; that ignoring distribution and solely focusing on price competition results in up to a 55% overestimation of manufacturer profit margins; and that observed pricing and distribution patterns support competition rather than collusion among manufacturers. Through counterfactual studies, the authors find that manufacturers respond to decreases in distribution costs and to the exclusive distribution of more preferred manufacturers by asymmetrically changing their price and distribution decisions across different retail channels.
1	This article examines whether and why organizing product categories according to the consumption goal they serve (i.e., complement-based assortment organization) may increase purchases compared with organizing product categories according to their attributes or physical characteristics (i.e., substitute-based assortment organization). Across two field experiments, a virtual reality experiment, and a lab experiment, the authors show that a complement-based assortment organization, compared with a substitute-based assortment organization, leads to increased numbers of purchases and increased expenditures. Ease of visualization of the consumption process mediates the results. The impact of the complement-based organization on purchases is more pronounced for less involved consumers and for consumers with a less specific shopping goal. These findings have both theoretical and practical implications.
1	As business-to-business customers increasingly use online channels, sellers must reconsider strategic investments in at least two areas: the salesperson channel, which faces the threat of substitution, and customer-specific discounts, which may be more precisely targeted. The authors draw on communication theory to posit that a customer’s search and purchasing in the seller’s online channels interact positively with both salesperson contact and customer-specific discounts to drive the seller’s customer-level sales and profit return on these investments. A multimethod approach using a complex data set from a large industrial seller provides broad support for hypothesized effects. Two post hoc experiments reveal how online and salesperson channels are complementary, together improving customer–seller communication such that the seller is better able to fulfill customer needs and reduce customer perceived risk. This research advances the multichannel and pricing literatures and offers actionable insights for business-to-business marketers, revealing how online channels can complement traditional seller investments in salespeople and customer-specific discounts.
1	In reward-based crowdfunding, creators of entrepreneurial projects solicit capital from potential consumers to reach a funding goal and offer future products/services in return. The authors examine consumers’ contribution patterns using a novel data set of 28,591 projects collected at 30-minute resolution from Kickstarter. Extending prior research that assumes that economic considerations (e.g., project quality, campaign success likelihood) drive backers’ decisions, the authors provide the cleanest field evidence so far that consumers also have prosocial motives to help creators reach their funding goals. They find that projects collect funding faster right before (vs. right after) meeting their funding goals because consumers not only are more likely to fund projects but also contribute greater amounts of money prior to goal attainment. This effect is amplified when the nature of a project tends to evoke consumers’ prosocial motivation and when a project’s creator is a single person. These results suggest that consumers’ prosocial motives not only play a role in reward-based crowdfunding but also can outweigh the opposing effects of economic factors including rational herding and certainty about campaign success.
1	Consumers often fail to utilize desirable offers they had originally selected and planned to use and thus later regret missing out on them. This failure to follow through induces an opportunity cost. In contrast to prior research findings that opportunity costs tend to be underestimated, the authors propose that in situations where the need to choose arises from external rather than internal constraints, opportunity costs may actually be overestimated. Consumers view choice constraints as external when the necessity to trade off one option for another relates to extraneous resource limitations (e.g., whenever time, budget, or space constraints necessitate choosing between two desirable offers). Conversely, consumers perceive choice constraints as internal when that trade-off is “built-in” (e.g., when a marketing incentive requires choosing between two desirable offers). Five studies demonstrate that choosing on the basis of an external constraint induces consumers to imagine ways in which they can utilize all of the competing options in the choice set. Consequently, consumers feel that by failing to utilize their chosen option, they simultaneously miss out on all options (although in actuality they could have realized only one of those options). Consistent with this conceptualization, only consumers who want to use all of the choice set options simultaneously demonstrate opportunity cost overestimation.
1	Interactions between consumers and humanoid service robots (HSRs; i.e., robots with a human-like morphology such as a face, arms, and legs) will soon be part of routine marketplace experiences. It is unclear, however, whether these humanoid robots (compared with human employees) will trigger positive or negative consequences for consumers and companies. Seven experimental studies reveal that consumers display compensatory responses when they interact with an HSR rather than a human employee (e.g., they favor purchasing status goods, seek social affiliation, and order and eat more food). The authors investigate the underlying process driving these effects, and they find that HSRs elicit greater consumer discomfort (i.e., eeriness and a threat to human identity), which in turn results in the enhancement of compensatory consumption. Moreover, this research identifies boundary conditions of the effects such that the compensatory responses that HSRs elicit are (1) mitigated when consumer-perceived social belongingness is high, (2) attenuated when food is perceived as more healthful, and (3) buffered when the robot is machinized (rather than anthropomorphized).
1	The authors propose a new, exploratory approach for analyzing market structures that leverages two recent methodological advances in natural language processing and machine learning. They customize a neural network language model to derive latent product attributes by analyzing the co-occurrences of products in shopping baskets. Applying dimensionality reduction to the latent attributes yields a two-dimensional product map. This method is well-suited to retailers because it relies on data that are readily available from their checkout systems and facilitates their analyses of cross-category product complementarity, in addition to within-category substitution. The approach has high usability because it is automated, is scalable and does not require a priori assumptions. Its results are easy to interpret and update as new market basket data are collected. The authors validate their approach both by conducting an extensive simulation study and by comparing their results with those of state-of-the-art, econometric methods for modeling product relationships. The application of this approach using data collected at a leading German grocery retailer underlines its usefulness and provides novel findings that are relevant to assortment-related decisions.
1	This article investigates how people’s political identity is associated with their financial risk taking. The authors argue that conservatives’ financial risk taking increases as their self-efficacy increases because of their greater social dominance orientation, whereas liberals’ financial risk taking is invariant to their self-efficacy. This central hypothesis is verified in six studies using different measures of political identity, self-efficacy, and financial risk taking. The studies also use different samples of U.S. consumers, including online panels, a large-scale data set spanning five election cycles, and a secondary data set of political donations made by managers at companies. Finally, the authors articulate and demonstrate the mediating effect of individuals’ focus on the upside potential of a decision among conservatives but not liberals.
1	This article provides insight into the management of brands that are also people by unpacking the interdependencies that exist between people and brands and focusing on the qualities that make person-brands human rather than on the qualities that make them brands. Using the extended case method to examine 20 years of public data about the Martha Stewart brand, the authors highlight the interdependent relationship between the person and the brand—in particular, consistency and balance—and identify four aspects of the person that can upset these interdependencies: mortality, hubris, unpredictability, and social embeddedness. Mortality and hubris can cause imbalance, but with the right skills and structures, these factors can be proactively managed. Inconsistency in the meanings of the person versus the brand can derive from the person’s unpredictability and social embeddedness and compromise brand value, but it may also enhance brand value by adding needed intimacy and authenticity. This two-bodied conceptualization suggests renewed management principles and contributes to branding theory through identification of the doppelgänger within, new brand strength facets, and emphasis on risk versus returns.
1	Cognitive linguistic studies have found that people perceive time to be intertwined with space. Western consumers, in particular, visualize time on a horizontal spatial axis, with past events on the left and future events on the right. Underexplored, however, is whether and how space-time associations influence future time-related judgments and decisions. For instance, can spatial location cues affect intertemporal decisions? Integrating cognitive linguistics, time psychology, and intertemporal choice, the authors demonstrate across five studies that when choices are displayed horizontally (vs. vertically), consumers more steeply discount future outcomes. Furthermore, this effect is serially mediated by attention to time and anticipated duration estimates. Specifically, the authors propose and demonstrate that horizontal (vs. vertical) temporal displays enhance the amount of attention devoted to considering the time delay and lead consumers to overestimate how long it will take to receive benefits. This research has important implications for consumers who want to forgo immediate gratification and for firms that need to manage consumers’ time perceptions.
1	One of the traditional tenets of marketing is that managers considering whether to develop and launch a new product should adopt a customer orientation and consider whether the product would satisfy the needs of customers. This research discovers that adopting a customer orientation causes managers to experience undesirable cognitive effects. The authors find that when considering customers’ needs during the screening phase of the new product development process, managers often voluntarily engage in mental imagery (i.e., cognitive simulation) that biases their evaluation of a new product idea toward unrealistic optimism—even for a flawed product that would not satisfy customer needs. Furthermore, the authors find that managers who exert greater vigilance to achieve more accurate evaluations of the new product idea are especially vulnerable to the biasing effect, leading to less accurate evaluations. The authors test an analytical technique (i.e., a theory-based approach to analyzing the new product) that successfully allows a manager to adopt a customer orientation without an attendant bias toward optimism.
1	Sales leaders often use threats of punishment to manage poor performers (i.e., laggards), but little research has examined the effect of these threats. The current research addresses this gap by investigating an intervention termed the “bench program” with a field-based quasi experiment and a randomized lab experiment. In the field, the company under study told salespeople in treatment districts that a trainee would replace them at the end of the year if they failed to hit their quota and placed last in their district. Difference-in-differences analyses of matched treatment and control groups show that the bench program had an immediate and sustained impact on performance. Moreover, laggards improved their performance more than higher performers, and salespeople with larger advice networks improved their performance more than salespeople with smaller advice networks. A lab experiment compares the bench program with a program that had the same threat of firing but did not have replacements in sight. Performance in the bench program exceeded that in the firing condition, indicating that the vividness of a threat can increase its deterrent value.
1	The authors study the use of sales agents for network mobilization in a two-sided market platform that connects buyers and sellers, and they examine how the presence of direct and indirect network effects influences the design of the sales compensation plan. They employ a principal–agent model in which the firm tasks sales agents to mobilize the side of the platform that it monetizes (i.e., sellers). Specifically, the presence of network effects alters the agency relationship between the firm and the sales agent, requiring the platform firm to alter the compensation design, and the nature of the alteration depends on whether the network effects are direct or indirect and positive or negative. The authors first show how the agent’s compensation plan should account for different types of network effects. They then establish that when the platform firm compensates the agent solely on the basis of network mobilization on the side cultivated by the agent (sellers), as intuition would suggest, it will not fully capitalize on the advantage of positive network effects; that is, profit can be lower under stronger network effects. To overcome this limitation, the platform should link the agent’s pay to a second metric, specifically, network mobilization on the buyer side, even though the agent is not assigned to that side. This design induces a positive relation between the strength of network effects and profit. This research underlines the complexity and richness of network effects and provides managers with new insights regarding the design of sales agents’ compensation plans for platforms.
1	Conventional wisdom suggests that more intense competition will lower firms’ profits. The authors show that this may not hold in a channel setting with exclusive retailers. They find that a manufacturer and its retailer can both become worse off if their competing manufacturer and retailer with quality-differentiated products exit the market. Put differently, in a channel setting, more intense competition can be all-win for the manufacturer, the retailer, and the consumers. Interestingly, a high-quality manufacturer can benefit from an increase in its competitor’s perceived quality (e.g., due to favorable product reviews from consumers or third-party rating agencies). In other words, a manufacturer may prefer a strong rather than a weak enemy, and the manufacturer can have an incentive to help its competitor improve product quality or remain in the market. Furthermore, the authors show that a multiproduct monopolist manufacturer with an exclusive retailer may make higher profits by spinning off a product into a competing manufacturer that has its own retail channel, even without accounting for any proceeds from the spinoff.
1	A multiperiod, theoretical model characterizes the relationship between a publication that ranks universities and prospective students who might use this ranking to decide which university to attend. The published ranking offers information about the universities’ objective quality but also affects their prestige, which may increase student utility. This prestige effect gives the commercial publication incentive to act contrary to the best interest of the students. If a ranking created with the commonly used attribute-and-aggregate methodology creates prestige, then to maximize profit the publication needs to (1) choose attribute score weights that do not match student preferences and (2) alter those attribute score weights over time, even in the absence of changes to student preferences and/or education technology. Without a prestige effect, the publication should choose attribute score weights that match student preferences. This model also defines a student-optimal ranking methodology that maximizes the sum of the students’ utilities. The results offer insights for prospective students who use existing rankings to choose a university, as well as which ranking designs would better align with students’ preferences.
1	The authors propose and empirically evaluate a new hybrid estimation approach that integrates choice-based conjoint with repeated purchase data for a dense consumer panel, and they show that it increases the accuracy of conjoint predictions for actual purchases observed months later. The key innovation lies in combining conjoint data with a long and detailed panel of actual choices for a random sample of the target population. By linking the actual purchase and conjoint data, researchers can estimate preferences for attributes not yet present in the marketplace, while also addressing many of the key limitations of conjoint analysis, including sample selection and contextual differences. Counterfactual product and pricing exercises illustrate the managerial relevance of the approach.
1	The authors study how faster delivery in the online channel affects sales within and across channels in omnichannel retailing. The authors leverage a quasi-experiment involving the opening of a new distribution center by a U.S. apparel retailer, which resulted in unannounced faster deliveries to western U.S. states through its online channel. Using a difference-in-differences approach, the authors show that online store sales increased, on average, by 1.45% per business-day reduction in delivery time, from a baseline of seven business days. The authors also find a positive spillover effect to the retailer’s offline stores. These effects increase gradually in the short-to-medium run as the result of higher order count. The authors identify two main drivers of the observed effect: (1) customer learning through service interactions with the retailer and (2) existing brand presence in terms of online store penetration rate and offline store presence. Customers with less online store experience are more responsive to faster deliveries in the short run, whereas experienced online store customers are more responsive in the long run.
1	At many firms, incentivized salespeople with private information about customers are responsible for customer relationship management. Although incentives motivate sales performance, private information can induce moral hazard by salespeople to gain compensation at the expense of the firm. The authors investigate the sales performance–moral hazard trade-off in response to multidimensional performance (acquisition and maintenance) incentives in the presence of private information. Using unique panel data on customer loan acquisition and repayments linked to salespeople from a microfinance bank, the authors detect evidence of salesperson private information. Acquisition incentives induce salesperson moral hazard, leading to adverse customer selection, but maintenance incentives moderate it as salespeople recognize the negative effects of acquiring low-quality customers on future payoffs. Critically, without the moderating effect of maintenance incentives, the adverse selection effect of acquisition incentives overwhelms the sales-enhancing effects, clarifying the importance of multidimensional incentives for customer relationship management. Reducing private information (through job transfers) hurts customer maintenance but has greater impact on productivity by moderating adverse selection at acquisition. This article also contributes to the recent literature on detecting and disentangling customer adverse selection and customer moral hazard (defaults) with a new identification strategy that exploits the time-varying effects of salesperson incentives.
1	Technology is making it easier for firms to track consumers’ purchase history and leverage the information when setting prices. This article explores the practice of behavior-based pricing (BBP) in a horizontally differentiated market where consumers’ taste is diverse and the consideration set is limited. The analysis identifies a novel mechanism that can help firms earn more profits with BBP than without it. Prior research shows that BBP intensifies price competition for new consumers. The authors show that if consumer valuation is low, the lower price can help expand sales to consumers for whom only the second preferred product is available, and the resulting increase in revenue more than offsets the loss in revenue because of the intensified price competition. The opposite result occurs if product valuation is high. Moreover, the difference in the price charged for old and new consumers under BBP decreases with the diversity in consumers’ taste if consumer valuation is low. The result, however, is reversed if consumer valuation is high.
1	In the context of user-generated content (UGC), mobile devices have made it easier for consumers to review products and services in a timely manner. In practice, some UGC sites indicate if a review was posted from a mobile device. For example, TripAdvisor uses a “via mobile” label to denote reviews from mobile devices. However, the extent to which such information affects consumers is unknown. To address this gap, the authors use TripAdvisor data and five experiments to examine how mobile devices influence consumers’ perceptions of online reviews and their purchase intentions. They find that knowing a review was posted from a mobile device can lead consumers to have higher purchase intentions. Interestingly, this is due to a process in which consumers assume mobile reviews are more physically effortful to craft and subsequently equate this greater perceived effort with the credibility of the review.
1	Research suggests that consumers are averse to relying on algorithms to perform tasks that are typically done by humans, despite the fact that algorithms often perform better. The authors explore when and why this is true in a wide variety of domains. They find that algorithms are trusted and relied on less for tasks that seem subjective (vs. objective) in nature. However, they show that perceived task objectivity is malleable and that increasing a task’s perceived objectivity increases trust in and use of algorithms for that task. Consumers mistakenly believe that algorithms lack the abilities required to perform subjective tasks. Increasing algorithms’ perceived affective human-likeness is therefore effective at increasing the use of algorithms for subjective tasks. These findings are supported by the results of four online lab studies with over 1,400 participants and two online field studies with over 56,000 participants. The results provide insights into when and why consumers are likely to use algorithms and how marketers can increase their use when they outperform humans.
1	Four experiments supported by six supplemental studies show that premium but higher-priced products (e.g., direct flights, larger-capacity data storage devices) are more popular when the additional cost is made explicit using differential price framing (DPF; e.g., “for $20 more”) rather than being left implicit, as in standard inclusive price framing (IPF; e.g., “for $60 total”). The DPF effect is driven by pricing focalism: relative to IPF, DPF creates a focus on the price difference, which, because it is smaller than the total price, leads to lower perceived expensiveness and thus greater choice share for the premium option. This price framing effect is robust to displaying the total cost of the purchase, bad deals, and easy-to-compute price differences, and it appears to be uniquely effective in pricing contexts. However, DPF effects are reduced among consumers who adopt a slow and effortful decision process. These findings have implications for research on price partitioning, the design of effective pricing strategy, the sources of expensiveness perceptions in the marketplace, and consumer welfare.
1	Many service providers offer supplementary products related to their ongoing services (e.g., fitness centers offer fitness smartwatches). In seven studies, the authors show that the payment method for such supplementary products (multiple payments vs. a single lump sum) affects customers’ tendency to defect from the provider’s core service over time. Specifically, when customers pay for add-ons in multiple payments—provided that (1) they perceive the add-on as being bundled with the core service and (2) the payment period has an end point—they are initially less likely to defect from the service provider than when they pay in a single payment. Over time, however, as payments are made, this gap closes, such that defection intentions under the two payment methods eventually become similar. The authors propose that this phenomenon reflects “commitment projection,” wherein a decrease in customers’ commitment to the add-on product over time is projected onto their commitment to the service provider. These findings carry important managerial implications, given that many service providers offer add-on products in multiple-payment plans and that customers’ defection decisions substantially affect firms’ profitability.
1	Logos frequently include textual and/or visual design elements that are descriptive of the type of product/service that brands market. However, knowledge about how and when logo descriptiveness can influence brand equity is limited. Using a multimethod research approach across six studies, the authors demonstrate that more (vs. less) descriptive logos can positively influence brand evaluations, purchase intentions, and brand performance. They also demonstrate that these effects occur because more (vs. less) descriptive logos are easier to process and thus elicit stronger impressions of authenticity, which consumers value. Furthermore, two important moderators are identified: the positive effects of logo descriptiveness are considerably attenuated for brands that are familiar (vs. unfamiliar) to consumers and reversed (i.e., negative) for brands that market a type of product/service linked with negatively (vs. positively) valenced associations in consumers’ minds. Finally, an analysis of 597 brand logos suggests that marketing practitioners might not fully take advantage of the potential benefits of logo descriptiveness. The theoretical contributions and managerial implications of these findings are discussed.
1	Companies frequently allow customers to customize products by assembling different product features or ingredients. Whereas existing research has demonstrated that customers assign greater overall value to customized products, this research focuses on the effect of customization on customers’ perceptions of specific product attributes (e.g., how healthy a product is). The findings of six studies—in the field, laboratory, and online—demonstrate that customizers and noncustomizers differ in their product perceptions even if the product is objectively the same. This is because customization leads customers to perceive the product in line with their own self-image (e.g., as an unhealthy eater), a phenomenon that the authors term “self-image-consistent product perceptions.” Essentially, customization may influence product perceptions depending on the product and individuals’ self-image; this can have downstream consequences on recommendations and social media communication. The authors test this theory for different product categories (clothing, food, and vacation packages) and attributes (fashionable, healthy, and adventurous) and demonstrate that framing customization as a simple choice or strengthening product positioning through labeling mitigates negative effects of customization.
1	In this research, the authors study the process by which social media posts are created and shared during live political debates. Using data from over 9.5 million tweets posted during and shortly after four key debates leading up to the 2016 U.S. presidential election, the authors test a series of hypotheses about how tweeting evolves over time during such events. They find that (1) as the debates progressed, the content of the “Twittersphere” became increasingly decoupled from the live event, and (2) the drivers of the success of tweets during the debates differed from the drivers of success observed after the debates. During the debates, users acted akin to narrators, posting shorter tweets that commented on unfolding events, with linguistic emotionality playing a limited role in sharing. However, when the debates were over, users acted more like interpreters, with successful posts being more elaborate and visually and emotionally rich accounts of the event. Evidence for the generalizability of the findings is provided by an analysis of Barack Obama’s last State of the Union Address, where similar dynamics are observed.
1	How consumers use review content has remained opaque due to the unstructured nature of text and the lack of review-reading behavior data. The authors overcome this challenge by applying deep learning–based natural language processing on data that tracks individual-level review reading, searching, and purchasing behaviors on an e-commerce site to investigate how consumers use review content. They extract quality and price content from 500,000 reviews of 600 product categories and achieve two objectives. First, the authors describe consumers’ review-content-reading behaviors. Although consumers do not read review content all the time, they do rely on it for products that are expensive or of uncertain quality. Second, the authors quantify the causal impact of read-review content on sales by using supervised deep learning to tag six theory-driven content dimensions and applying a regression discontinuity in time design. They find that aesthetics and price content significantly increase conversion across almost all product categories. Review content has a higher impact on sales when the average rating is higher, ratings variance is lower, the market is more competitive or immature, or brand information is not accessible. A counterfactual simulation suggests that reordering reviews based on content can have the same effect as a 1.6% price cut for boosting conversion.
1	Many online stores are designed such that shoppers can easily access any available discounted products. The authors propose that deliberately increasing search frictions by placing obstacles to locating discounted items can improve online retailers’ margins and even increase conversion. The authors demonstrate this using a simple theoretical framework that suggests inducing consumers to inspect higher-priced items first may simultaneously increase the average price of items sold and the overall expected purchase probability by inducing consumers to search more products. The authors test and confirm these predictions in a series of field experiments conducted with a dominant online fashion and apparel retailer. Furthermore, using information in historical transaction data about each consumer, the authors demonstrate that price-sensitive shoppers are more likely to willingly incur search costs when locating discounted items. Our results show that increasing search frictions can be used as a self-selecting price discrimination tool to match high discounts with price-sensitive consumers and full-priced offerings with price-insensitive consumers.
1	The authors present empirical evidence that borrowers, consciously or not, leave traces of their intentions, circumstances, and personality traits in the text they write when applying for a loan. This textual information has a substantial and significant ability to predict whether borrowers will pay back the loan above and beyond the financial and demographic variables commonly used in models predicting default. The authors use text-mining and machine learning tools to automatically process and analyze the raw text in over 120,000 loan requests from Prosper, an online crowdfunding platform. Including in the predictive model the textual information in the loan significantly helps predict loan default and can have substantial financial implications. The authors find that loan requests written by defaulting borrowers are more likely to include words related to their family, mentions of God, the borrower’s financial and general hardship, pleading lenders for help, and short-term-focused words. The authors further observe that defaulting loan requests are written in a manner consistent with the writing styles of extroverts and liars.
1	This article studies how the incentive structures and disclosure schemes of a contest affect the contestants’ intrinsic motivations. Specifically, the authors measure the effects of these design decisions on two types of nonmonetary rank-based utility: self-generated and peer-induced. They run a set of laboratory experiments involving contests under various reward spreads and disclosure schemes. First, they find that virtually all commonly adopted disclosure schemes generate positive peer-induced rank-based utility. However, the relative performances of alternative disclosure schemes can depend on the spread of contest rewards and the number of contestants. Second, being recognized as a winner confers positive peer-induced rank-based utility; moreover, being recognized as the sole first-place winner or as one among multiple winners does not produce significantly different peer-induced utility. Third, “shaming” by disclosing the identity of contestants ranked at the bottom leads to negative peer-induced rank-based utility, but the effect is marginally insignificant. Finally, a smaller spread of contest rewards consistently results in higher levels of self-generated rank-based utility. These results underscore the importance of jointly choosing incentive structures and disclosure schemes.
1	As sustainable consumption becomes increasingly important, firms must better understand the drivers behind the consumption of these products. This article examines the effects of mass media in the context of the U.S. hybrid vehicle market. Drawing on monthly sales data, the authors provide evidence that the general coverage of climate change or global warming by major media outlets exerts an overall positive impact on the sales of hybrid vehicles. This impact mainly comes from the media reports that assert that climate change is occurring. In contrast, media coverage that either denies climate change or holds a neutral stance on the issue has little impact. The authors provide preliminary evidence that a social norm advocating for environmentally friendly consumption plays an important role in how media coverage affects consumer purchase. They provide implications for theory and practice and call for future research that examines the causal impact of media in general on consumer decisions, especially in domains that are crucial for the society.
1	Research on customer participation (CP) has focused on its benefits for customers. However, recent research suggests that CP is beneficial to both customers and firms. The literature is also sparse on the economic (e.g., profitability) and customer (e.g., customer retention) impact of CP. This research introduces the concept of customer empowerment and develops and tests a model of customer empowerment as a parallel mediator, along with customer satisfaction, to explain the linkage between CP and bank branch performance. Furthermore, the authors draw on a broader set of moderators beyond customer characteristics to examine when CP affects empowerment and satisfaction. Using triadic matched data from a multiwave design and a three-level model in which customers are nested within employees, who are, in turn, nested within bank branches, the authors show that customer empowerment and satisfaction fully mediate the effect of CP on branch performance. The findings also show that CP results in greater customer empowerment and satisfaction when there is fit between participation and the context in which it is used. The authors discuss implications for advancing CP research and suggest actionable steps for reaping the economic and customer benefits of CP.
1	Previous research has shown that there exist “harbinger customers” who systematically purchase new products that fail (and are discontinued by retailers). This article extends this result in two ways. First, the findings document the existence of “harbinger zip codes.” If households in these zip codes adopt a new product, this is a signal that the new product will fail. Second, a series of comparisons reveal that households in harbinger zip codes make other decisions that differ from other households. The first comparison identifies harbinger zip codes using purchases from one retailer and then evaluates purchases at a different retailer. Households in harbinger zip codes purchase products from the second retailer that other households are less likely to purchase. The analysis next compares donations to congressional election candidates; households in harbinger zip codes donate to different candidates than households in neighboring zip codes, and they donate to candidates who are less likely to win. House prices in harbinger zip codes also increase at slower rates than in neighboring zip codes. Investigation of households that change zip codes indicates that the harbinger zip code effect is more due to where customers choose to live, rather than households influencing their neighbors’ tendencies.
1	Mass customization interfaces typically guide consumers through the configuration process in a sequential manner, focusing on one product attribute after the other. What if this standardized customization experience were personalized for consumers on the basis of how they process information? A series of large-scale field and experimental studies, conducted with Western and Eastern consumers, shows that matching the interface to consumers’ culture-specific processing style enhances the effectiveness of mass customization. Specifically, presenting the same information isolated (by attribute) to Western consumers but contextualized (by alternative) to Eastern consumers increases satisfaction with and likelihood of purchasing the configured product, along with the amount of money spent on the product. These positive consumer responses emerge because of an increase in “interface fluency”—consumers’ subjective experience of ease when using the interface. The authors advise firms to personalize the customization experience by employing processing-congruent interfaces across consumer markets.
1	In many markets, such as video streaming or information services, a consumer may purchase multiple competing products or services. The existing theoretical literature typically assumes that each consumer can buy only one product. This article explicitly models the consumer’s multipurchase behavior and examines an upstream content creator’s content production and selling strategies as well as competing downstream distributors’ content acquisition and pricing strategies. The authors find that in contrast to the case of single-product purchase, under multiproduct purchase, only one distributor will acquire the creator’s new content in equilibrium. Furthermore, when the content distributors are not highly differentiated (each having a limited amount of unique content), the content creator will reduce new content production, leading to lower profits for both the content creator and the content distributors. By contrast, when the distributors are already highly differentiated with a substantial amount of unique content, the content creator will increase its content production, leading to higher profits for both the content creator and the distributors. The authors show that their main results and insights are robust to several alternative assumptions.
1	Are social media posts with pictures more popular than those without? Why do pictures with certain characteristics induce higher engagement than some other pictures? Using data sets of social media posts about major airlines and sport utility vehicle brands collected from Twitter and Instagram, the authors empirically examine the influence of image content on social media engagement. After accounting for selection bias on the inclusion of image content, the authors find a significant and robust positive mere presence effect of image content on user engagement in both product categories on Twitter. They also find that high-quality and professionally shot pictures consistently lead to higher engagement on both platforms for both product categories. However, the effect of colorfulness varies by product category, while the presence of human face and image–text fit can induce higher user engagement on Twitter but not on Instagram. These findings shed light on how to improve social media engagement using image content.
1	Native advertising is a type of online advertising that matches the form and function of the platform on which it appears. In practice, the choice between display and in-feed native advertising presents brand advertisers and online news publishers with conflicting objectives. Advertisers face a trade-off between ad clicks and brand recognition, whereas publishers need to strike a balance between ad clicks and the platform’s trustworthiness. For policy makers, concerns that native advertising confuses customers prompted the U.S. Federal Trade Commission to issue guidelines for disclosing native ads. This research aims to understand how consumers respond to native ads versus display ads and to different styles of native ad disclosures, using randomized online and field experiments combining behavioral clickstream, eye movement, and survey response data. The results show that when the position of an ad on a news page is controlled for, a native ad generates a higher click-through rate because it better resembles the surrounding editorial content. However, a display ad leads to more visual attention, brand recognition, and trustworthiness for the website than a native ad.
1	Previous research has demonstrated that consumers evaluate products according to their perceived benefits when making a choice. This article extends prior work by proposing a method that evaluates the degree to which multiple a priori defined benefits mediate product choices. The model is the first to consider process heterogeneity—that is, heterogeneity in how consumers perceive multiple attributes to positively or negatively affect multiple benefits simultaneously and the contribution of each benefit to product utility. The authors propose discrete choice experiments to holistically measure the link between attributes and benefits, as well as between attributes and choice, resulting in data that can be analyzed with a generalized probit model. The approach contributes to mediation research by offering an alternative method of handling multiple multinomial mediators and dichotomous outcome variables. An empirical illustration of bread choices shows how consumer judgments about health and value perceptions of products mediate purchase decisions. The authors demonstrate how the method can help managers (1) confirm and test existing knowledge about latent benefits, including whether they explain all the variation in choice, and (2) consider process heterogeneity to inform market segmentation strategies.
1	Marketing research relies on individual-level estimates to understand the rich heterogeneity of consumers, firms, and products. While much of the literature focuses on capturing static cross-sectional heterogeneity, little research has been done on modeling dynamic heterogeneity, or the heterogeneous evolution of individual-level model parameters. In this work, the authors propose a novel framework for capturing the dynamics of heterogeneity, using individual-level, latent, Bayesian nonparametric Gaussian processes. Similar to standard heterogeneity specifications, this Gaussian process dynamic heterogeneity (GPDH) specification models individual-level parameters as flexible variations around population-level trends, allowing for sharing of statistical information both across individuals and within individuals over time. This hierarchical structure provides precise individual-level insights regarding parameter dynamics. The authors show that GPDH nests existing heterogeneity specifications and that not flexibly capturing individual-level dynamics may result in biased parameter estimates. Substantively, they apply GPDH to understand preference dynamics and to model the evolution of online reviews. Across both applications, they find robust evidence of dynamic heterogeneity and illustrate GPDH’s rich managerial insights, with implications for targeting, pricing, and market structure analysis.
1	Behavior-based pricing (BBP) refers to the practice in which firms collect consumers’ purchase history data, recognize repeat and new consumers from the data, and offer them different prices. This is a prevalent practice for firms and a worldwide concern for consumers. Extant research has examined BBP under the assumption that consumers observe firms’ practice of BBP. However, consumers do not know that specific firms are doing this and are often unaware of how firms collect and use their data. In this article, the authors examine (1) how firms make BBP decisions when consumers do not observe whether firms perform BBP and (2) how the transparency of firms’ BBP practice affects firms and consumers. They find that when consumers do not observe firms’ practice of BBP and the cost of implementing BBP is low, a firm indeed practices BBP, even though BBP is a dominated strategy when consumers observe it. When the cost is moderate, the firm does not use BBP; however, it must distort its first-period price downward to signal and convince consumers of its choice. A high cost of implementing BBP serves as a commitment device that the firm will forfeit BBP, thereby improving firm profit. By comparing regimes in which consumers do and do not observe a firm’s practice of BBP, the authors find that transparency of BBP increases firm profit but decreases consumer surplus and social welfare. Therefore, requiring firms to disclose collection and usage of consumer data could hurt consumers and lead to unintended consequences.
1	People like graspable objects more when the objects are located on the dominant-hand side of their body or when the handles point toward their dominant-hand side. However, many products do not have handles or are not graspable (e.g., services, objects hanging on the wall). Can nongraspable products nevertheless benefit from the effects of appealing to viewers’ dominant hands? The present research shows that, yes, consumers respond more positively to nongraspable products if a haptic cue (an object that is graspable or suggestive of hand action) is located within the same visual field as the target and is positioned to appeal to the viewer’s dominant hand. This result is driven by the creation and transfer of perceived ownership from cue to target. These findings extend the use of haptic cues to nongraspable products and uncover the critical role played by perceived ownership, including its ability to transfer from one object to another located in the same visual field. Moreover, the current research demonstrates situations in which the use of haptic cues will not enhance response.
1	In this research, the authors propose that incidental exposure to price promotions can cause downstream impatience in an unrelated domain. Specifically, price promotions trigger reward seeking—a general motivational state—and reward seeking, in turn, yields impatience. Seven experiments (N = 1,795) demonstrate how incidental exposure to price promotions can cause greater willingness to pay to avoid waiting (Experiments 1a and 1b), shorter actual wait times (Experiments 2, 3b, and 5), greater propensity to break a rule to save time (Experiment 3a), and greater discounting in a consequential intertemporal choice (Experiment 4). Consistent with this account, the effect is both more pronounced for people with greater reward sensitivity (Experiments 3a and 3b) and mediated by reward seeking (Experiment 4). Finally, a conceptual replication in a field setting underscores the external validity and managerial relevance of the findings (Experiment 5).
1	The authors propose a new conceptual basis for predicting when and why consumers match others’ consumption choices. Specifically, they distinguish between ordinal (“ranked”) versus nominal (“unranked”) attributes and propose that consumers are more likely to match others on ordinal than on nominal attributes. Eleven studies involving a range of different ways of operationalizing ordinal versus nominal attributes collectively support this hypothesis. The authors’ conceptualization helps resolve divergent findings in prior literature and provides guidance to managers on how to leverage information about prior customers’ choices and employees’ recommendations to shape and predict future customers’ choices. Furthermore, the authors find process evidence that this effect is driven in part by consumers’ beliefs that a failure to match on ordinal (but not nominal) attributes will lead to social discomfort for one or both parties. Although the primary focus is on food choices, the effects are also demonstrated in other domains, extending the generalizability of the findings and implications for managerial practice and theory. Finally, the conceptual framework offers additional paths for future research.
1	Selfish incentives typically outperform prosocial incentives, and customer referral programs frequently use such “selfish” (i.e., sender-benefiting) incentives to incentivize current customers to recruit new customers. However, in two field experiments and a fully incentivized lab experiment, this research finds that “prosocial” (i.e., recipient-benefiting) referral incentives recruit more new customers. Five subsequent experiments test a process account for this effect, identifying two key psychological mechanisms: reputational benefits and action costs. First, at the referral stage, senders (existing customers) anticipate reputational benefits for referring recipients (potential new customers), who receive a reward for signing up. These reputational benefits render recipient-benefiting referrals just as effective as sender-benefiting referrals at the relatively low-cost referral stage. Second, at the uptake stage, recipient-benefiting referrals are more effective than sender-benefiting referrals: recipient-benefiting referrals directly incentivize recipients to sign up, providing a clear reward for an otherwise costly uptake decision. The preponderance of selfish, or sender-benefiting, referral incentives in the marketplace suggests these effects are unanticipated by marketers who design incentive schemes.
1	The authors examine how gender identity and local–global identity—two important consumer identities and segmentation bases—affect consumer price sensitivity. These identities may be associated with similar behavioral norms such that a female identity and a local identity are associated with the norms of small groups and intimate relationships, whereas a male identity and a global identity are associated with the norms of large groups and broad relationships. Thus, a female identity and a global (vs. local) identity, as is the case with a male identity and a local (vs. global) identity, results in identity incongruence (vs. congruence). The authors argue that identity incongruence depletes cognitive resources and induces affective processing, which in turn activates a sacrifice mindset and results in lower price sensitivity. Five studies with different research designs, divergent measures of key constructs, and diverse samples support the theories and identify boundary conditions of the focal relationship. The findings expand the literature on multiple consumer identities and price sensitivity and provide useful guidelines for global companies’ pricing strategies.
1	This article studies the intersection between the largest U.S. industry—health care—and the $1 trillion nonprofit sector. Using analytical and empirical analyses, the authors reveal the marketing strategies helping private nonprofit hospitals achieve higher output, prices, and profits than for-profit hospitals. Nonprofit hospitals, focusing on both profits and output, obtain these outcomes by expanding their service mix with high-priced premium specialty medical services (PSMS), whereas for-profit hospitals can be more profitable with higher prices for basic services. Competition increases the differences between nonprofit and for-profit hospitals in PSMS breadth, output, and prices. Nonprofit hospitals lose their competitive advantage when competing with other nonprofits; that is, presence of a for-profit competitor broadens available nonprofit PSMS. With broader service mixes, nonprofits focus more on national advertising than for-profits because PSMS (e.g., pediatric trauma, neurosurgery, heart transplants, oncology) require larger geographic markets than local basic services (e.g., laboratory, diagnostics, nursing, pharmaceutics). Exogenous, heterogeneous state regulations restricting for-profit hospital entry help econometric identification (i.e., markets prohibiting for-profits act as controls). Service mix may be a key difference between nonprofit and for-profit hospitals.
1	The authors study multiperiod sales force incentive contracting in which salespeople can engage in effort gaming, a phenomenon that has extensive empirical support. Focusing on a repeated moral hazard scenario with two independent periods and a risk-neutral agent with limited liability, the authors conduct a theoretical investigation to understand which effort profiles the firm can expect under the optimal contract. The authors show that various effort profiles that may give the appearance of being suboptimal, such as postponing effort exertion (“hockey stick”) and not exerting effort after a bad or a good initial demand outcome (“giving up” and “resting on laurels,” respectively) may indeed be induced optimally by the firm. This is because, under certain conditions that depend on how severe the contracting frictions are and how effective effort exertion is in increasing demand, the firm wants to concentrate rewards on extreme demand outcomes. Doing this induces gaming and reduces expected demand but also makes motivating effort cheaper, thus saving on incentive payments. On introducing dependence between time periods, such as when the agent can transfer demands between periods, this insight continues to hold and, furthermore, “hockey stick,” “giving up,” and “resting on laurels” can be optimal for the firm even under repeated short time horizon contracting. The results imply that one must carefully consider the setting and environmental factors when making inferences about contract effectiveness from dynamic effort profiles of agents.
1	Many studies have quantified the effects of TV ad spending or gross rating points on brand sales. Yet this effect is likely moderated by the different types of brand-related messages or cues (e.g., logo, brand attributes) embedded in the ads and by the ways (e.g., explicitly or implicitly) these cues are conveyed to TV audiences. The authors thus measure 17 cues often used within ads to build brand awareness (or salience) and brand image and investigate their influence on ad effectiveness. Technically, the study builds a dynamic model to quantify the effects of advertising on sales; builds a robust and interpretable (i.e., nonparametric and sparse) factor model that integrates correlated, left-censored branding cues; and then models the effects of advertising as a function of the factors identified by these cues. An analysis of 177 campaigns aired by 62 brands finds that salience cues (e.g., logo) and benefit and attribute messages moderate ad effectiveness. It also finds that explicit cues are more effective than implicit ones; nonetheless, the primary drivers of ad effectiveness are visual salience cues: the duration and frequency with which the logo and the duration with which the product are displayed. The study can thus suggest ways brand and ad agency managers can improve the effects of creative ad content on sales.
1	For many consumer goods, the visual appearance is a vital determinant of market success. Although there is an emerging literature on how objective design characteristics drive consumer preferences, this literature has not yet taken into account that product design happens in the context of a brand’s equity. This research addresses the question of how to leverage brand equity when designing the visual appearance of a product. Specifically, it investigates the role of two key strategic visual design decisions: brand typicality (similarity within the brand’s range) and segment typicality (similarity to the competitive set). Drawing on fluency theory, the authors argue that high-equity brands benefit more from brand typicality but less from segment typicality than low-equity brands. Using data from the U.S. car market tracking market shares of 456 car models of 39 major brands operating in seven market segments across 13 years, the study provides empirical evidence for this conjecture and, thereby, implications for strategic product design and visual design theory.
1	Ethics has long been, and continues to be, a central topic among marketing scholars and practitioners. When providing complex services—multiple interactions over time that are predicated on the evolving needs of customers—service providers face ethical dilemmas, which are often resolved by engaging an ethics committee (EC). Despite the prevalence of ECs, research on service providers’ preference to engage with an EC is sparse. This study examines whether the role that health care providers play, as either task manager or relationship manager, makes a difference in their preference for engaging with and utilizing an EC for resolving ethical dilemmas. Results based on 1,440 observations collected from health care service providers show that service providers’ task or relationship management role, as well as prior experience with an ethics consultation, influences their preference both for engaging an EC and for having the EC prescribe a specific outcome to resolve an ethical dilemma. This study extends prior work on conceptual models examining ethical decision-making processes in marketing.
1	The authors develop and test a theory of consumer inaction traps in the domain of decisions to either address or endure product malfunctions. According to this theory, the magnitude of product malfunctions can have a paradoxical effect on consumption experience. In particular, the less severe a product malfunction is, the more inclined consumers are to defer the initial decision about whether to take corrective action. Subsequent opportunities for corrective action are devalued relative to previously forgone ones. This dynamic tends to trap consumers in a state of inaction, resulting in their enduring smaller malfunctions longer than larger ones. A consequence of these inaction traps is that minor product malfunctions may result in less enjoyable overall consumption experiences than more severe defects. Evidence from eight experiments and a survey provides support for this theorizing by demonstrating the inaction-trap phenomenon, examining its downstream consequences, shedding light on the psychological dynamics of inaction, and identifying boundary conditions that suggest interventions for counteracting consumers’ vulnerability to suffering disproportionately from relatively minor product malfunctions.
1	Prior research on the use of scent in advertising has shown that scent can enhance the memorability of and engagement with an ad. However, can scenting an ad also change the way consumers perceive and react to the advertised product? This research provides new insights for this question and demonstrates an additional facet of scent: its ability to physically represent the essence of a target product and thus induce a sense of proximity. Through six studies, the authors show that scented ads enhance consumers’ sense of proximity of the advertised product and consequently increase product appeal. In line with the proposed visceral nature of the effect, this effect holds even for unpleasant scents but is contingent on the scent’s ability to represent the advertised product. The effect is weakened when the product is physically close. The findings of this research have implications for when and why firms should use scented ads.
1	Researchers, marketers, and consumers often believe that amplifying emotional content is impactful for the spread of information and purchasing decisions. However, there is little systematic investigation of when emotionality backfires. This research demonstrates when and why positive emotion can have enhancing versus backfiring effects. The authors find that reviewers who express greater positive emotion are indeed more positive toward their products, regardless of product type. In addition, expressed emotion for hedonic products has a positive impact when read by others, but this emotion backfires for utilitarian products, leading others to be less positive. The authors construct a conceptual model of these effects and show that violated expectations leading to decreased trust underlie this divergence between reviewers and readers. The effects occur in well-controlled experiments as well as computational linguistic analysis of 100,000 Amazon reviews across 500 products. Indeed, emotional reviews of utilitarian products are less likely to become popular and be displayed on the product’s front page on Amazon. This work also introduces a novel tool for quantifying natural language in marketing: the Evaluative Lexicon.
1	Every day, consumers share word of mouth (WOM) on how products and behaviors are commonly adopted through the use of consensus language. Consensus language refers to words and expressions that suggest general agreement among a group of people regarding an opinion, product, or behavior (e.g., “everyone likes this movie”). In a series of online and field experiments, the authors demonstrate that the interpretation and persuasiveness of consensus language depends on the tie strength between the communicator and the receiver of WOM. Although abundant literature highlights the advantage of strong ties (e.g., close friends, family) in influence and persuasion, the authors find that weak ties (e.g., distant friends, acquaintances) are more influential than strong ties when using consensus language. The authors theorize and demonstrate that this effect occurs because weak ties evoke perceptions of a larger and more diverse group in consensus, which signals greater validity for the issue at hand. These findings contribute to research on WOM, tie strength, and descriptive norms and provide practical implications for marketers on ways to analyze and encourage consumer discourse.
1	A demonstration field experiment in a live-radio fund drive shows that women (but not men) primed with moral traits give about 20% more. The authors test one understudied explanation for this finding: gender differences in how market behavior (e.g., giving and supporting a nonprofit) shrinks moral identity discrepancy (i.e., the gap between actual and ideal moral identity). Field Survey 1 demonstrates the basic effect: the less money women (but not men) have historically given on average to a nonprofit, the larger their moral identity discrepancy. Field Experiment 2 shows a managerial implication of this basic effect: when primed with moral identity, women (but not men) who have supported the nonprofit less frequently in the past are more likely to follow an emailed link to help the nonprofit again. Study 3 tests one possible pathway underpinning this finding: even though giving makes women and men experience similar feelings of encouragement and uplift and similar reinforcement of their moral identity, only women with larger prebehavior moral identity discrepancy consequently shrink this discrepancy.
1	Internet news and search sites often excerpt content from and link to competing news outlets. On the one hand, providing outbound links can make the linking site more attractive, even to the point of stealing traffic from the linked sites. Regulatory policy, such as the European Union’s Copyright Directive Article 15 taxing links, is predicated in part on this idea. On the other hand, receiving inbound links can increase a linked site’s audience by informing readers about its news content that day. To explore these opposing perspectives, the authors develop a dynamic learning model and fit it to browsing and link data from celebrity news sites. They then simulate how banning links affects consumer browsing and find that linking increases celebrity news consumption, especially among consumers who browse the least. On average, linking benefits both the linking and linked sites. The authors estimate that exposure to a link increases the likelihood of visiting the linked site by .14%. This increase is approximately three times the commonly reported click-through rate for paid display advertisements.
1	The authors develop an incentive-aligned experimental paradigm to study how consumer purchase dynamics are affected by the interplay between competing firms’ loyalty programs and their pricing and promotional strategies. In this experiment, participants made sequential choices between two competing airlines in a stylized frequent traveler task for which an optimal dynamic decision policy can be numerically computed. The authors find that, on average, participants are able to partially realize the long-term benefits from loyalty programs, though most are sensitive to price. They also find that participants’ preferences and levels of bounded rationality depend on the nature of the competitive environment, the particular state of each decision scenario, and the type of optimal action. Accordingly, the authors use an approximate dynamic programming model to incorporate boundedly rational decision making. The model classifies participants into five segments that exhibit variation in their performance and decision strategies. Importantly, they find that participants are able to adapt their decision strategies to the environment they face, and thus the overall market outcome and the performance of each firm are influenced by both the competitive environment and the assumption on the extent of consumer optimality.
1	An important aspect of multimedia advertising effectiveness that remains unexplored is a customer-level analysis of the relative importance of each medium for multiple retailer-brands within a product category. The increasing availability of customer databases for parent companies containing multimedia ad exposures, sales transactions in several purchase channels, and information across multiple retailer-brands now allows for a broader examination of advertising effectiveness. In this research, the authors monitor 4,000 customers over two years, linking their exposure to three media (email, catalogs, and paid search) to their in-store and online purchases for three retailer-brands in the clothing category. They develop a Tobit model for sales response to multimedia advertising that captures within-brand and within-channel correlations and accommodates individual-level advertising response parameters. Due to the very large number of observations (2.4 million) and random effects (60), the authors employ an emerging machine learning technique, variational Bayes, to estimate the model parameters. They find that email and sometimes catalogs from a focal retailer-brand have a negative influence on other retailer-brands in the category, whereas paid search influences only the focal retailer-brand. However, competitor catalogs often positively influence focal retailer-brand sales, but only among omnichannel customers. They segment customers by retailer-brand and channel usage, revealing a sizeable group of customers who shop across multiple retailer-brands and both purchase channels. Moreover, this segment is the most responsive to multimedia advertising.
1	The authors investigate how heuristics and analytics contribute to the advertising budget decision by decomposing it into four components: (1) baseline spending, (2) adaptive experimentation, (3) advertising-to-sales ratio, and (4) competitive parity. They propose a methodology to estimate and infer the weights of these four components. Applying this methodology to sales and advertising data across eight brands from three categories substantiates for the first time, and uniformly across all brands, that managers depart from optimality through adaptive experimentation, which is in line with dual control theory that suggests they do so to learn about advertising effectiveness. The adaptive experimentation finding, combined with evidence on the use of heuristic methods, suggests that budget decision making is characterized by bounded rationality. Furthermore, budgeting decisions are brand-specific, reflecting the considerations of a brand’s market position and performance. Finally, simulation studies show that brands from categories with high uncertainty in advertising effectiveness can benefit from double-digit revenue lifts by placing higher emphasis on adaptive experimentation.
1	Ads promising a desired change are ubiquitous in the marketplace. These ads typically include visuals of the starting and ending point of the promised change (“before/after” ads). “Progression” ads, which include intermediate steps in addition to starting and ending points, are much rarer in the marketplace. Across several consumer domains, the authors show an ad-type effect: progression ads foster spontaneous simulation of the process through which the change will happen, which makes these ads more credible and, in turn, more persuasive than before/after ads (Studies 1–3). The authors also show that impairing process simulation and high skepticism moderate the ad-type effect (Studies 4–5). Finally, they show effect reversals: if consumers focus on achieving the desired results quickly, and it is possible to do so, progression ads and the associated process simulation backfire in terms of credibility and persuasion (Studies 6–7). These findings contribute to existing research by identifying conditions under which progression ads have beneficial or disadvantageous effects. These findings have managerial implications because they run counter to current marketing practices, which favor before/after over progression ads.
1	This research examines how the unsystematic (vs. systematic) spatial arrangement of a set of alternatives affects consumers’ product choices. The key hypothesis is that an unsystematic product arrangement—in which an assortment consisting of several alternatives is arranged in an apparently arbitrary manner—causes greater perceptual disfluency, which in turn triggers more extensive exploratory product search, ultimately promoting the choice of unfamiliar products. This sequence of effects is particularly pronounced when consumers do not have a strong prior preference for specific alternatives in the assortment. Evidence from five studies, including a large-scale field experiment, provides support for this theorizing across various display formats and product domains. The findings advance our understanding of how the spatial arrangement of a product assortment influences consumer choice, and they shed light on the psychological mechanism that governs this effect.
1	Anecdotal evidence is mixed regarding whether handheld scanners used in stores increase or decrease consumer sales. This article reports on three field studies, supported by eye-tracking technology and matched sales receipts, as well as two laboratory studies that show that handheld scanner use increases sales, notably through unplanned, healthier, and impulsive purchases. The findings highlight that these effects may be limited by factors such as not having a budget; for those without a budget, use of scanners can decrease sales. Building on embodied cognition and cognitive appraisal theories, the authors predict that scanners, as a bodily extension, influence sales through both cognitive (shelf attention, perceived control) and affective (number of products touched, shopping experience) mechanisms. The results offer implications for retailers considering whether to integrate scanners into their store environments.
1	Previous studies of in-store decision making have assumed that motivations for unplanned purchases are homogeneous throughout a shopping trip. In response to this assumption, the authors develop a conceptual framework to explain how consumers’ internal (i.e., intrinsic) and external (i.e., extrinsic) motivations for unplanned purchases actually vary during a shopping trip. Two field studies and five online experiments provide evidence that the personality trait of buying impulsivity predicts differences in whether a shopper initially focuses on internal motivations (e.g., “because I love it”) or external motivations (e.g., “because it is on sale”) for unplanned purchases at the beginning of a shopping trip and, consistent with a mechanism of motivation balancing, that motivations for unplanned purchases change as a shopper satisfies their initial motivations. The studies also demonstrate how the level of buying impulsivity influences the effectiveness of point-of-purchase messages at stimulating unplanned purchases and consumers’ relative spending on unplanned purchases. Overall, these findings address conflicting results in previous shopping studies, advance the literature streams on consumer motivation and sequential choice, and contribute insights to enhance shopper-marketing programs.
1	Previous research on the effects of celebrity endorsement has focused on the transfer of positive properties (likeableness, credibility, symbolic meanings, etc.) from the endorser to the product. Taking a different perspective, this study suggests that the way in which consumers evaluate an endorser (i.e., the cognitive process, such as applying family origin or achievement as the basis for evaluation) will carry over to the evaluation of the endorsed product (e.g., applying country of origin or performance as the criteria). Five experiments support this process-transfer account and show that it can be induced by subtle verbal/visual cues in advertisements. Because the process transfer is not inherently associated with positive/negative valence, it provides a theoretical rationale for explaining successful endorsements involving endorsers who are less favorable/credible, less of a “fit” with the product, or associated with some negative meanings, in addition to those involving positively or neutrally evaluated endorsers. The process-transfer model supplements existing models and provides a more comprehensive understanding of endorser effects. It provides marketers with a set of less stringent guidelines for selecting endorsers as well as valuable damage control tools for brands when an endorser screws up.
1	This research examines the psychological processes and factors that shape illness-detection versus illness-prevention health actions. Four experiments using contexts of mental health, skin cancer, and breast cancer show that illness detection evokes fear, which undermines engagement in detection behaviors. Considering detection at low (vs. high) levels of thought reduced fear and increased health persuasion. Illness prevention is driven by self-efficacy perceptions and considering prevention at high (vs. low) levels of thought increases persuasion. In further evidence of process, trait fear moderated the detection effects, and dispositional self-efficacy moderated the prevention effects. As an intervention, framing a detection action as serving illness-prevention goals increased people’s likelihood of engaging with an online breast cancer detection tool. These findings illuminate the psychology of detection as being distinct from the psychology of prevention, identify the role of fear in the consideration of health behaviors, and show contexts in which construal levels have divergent effects on health persuasion.
1	Although product advertising has been widely studied and understood in relation to the consumer’s purchase decision, advertising may also have unintended but important societal and economic consequences. In this article, the authors examine a public health outcome—birth rate—associated with advertisements for erectile dysfunction (ED) drugs. Since the United States loosened regulations on direct-to-consumer television advertising for prescription drugs in 1997, ED drug makers have consistently been top spenders. By comparing advertising data with multiple birth data sets (patient-level hospital data from Massachusetts between 2001 and 2010 and micro birth certificate data from the United States between 2000 and 2004), the authors demonstrate that increased ED drug television advertising leads to a higher birth rate. Their results, which are robust with respect to different functional forms and falsification tests, show that a 1% increase in ED drug advertising contributes to an increase of .04%–.08% of total births. Their findings suggest that beyond the customer purchase decision, advertising can have important public health outcomes, with resulting implications for managerial decision making and policy formulation.
1	Consumers and marketers use facial information to make important inferences about others in many business contexts. However, consumers and firms are increasingly concerned about privacy and discrimination. To address privacy–perception trade-offs, the authors propose a novel contour-as-face (CaF) framework that transforms face images into contour images incorporating both the nonoutline and outline features of facial parts. In three empirical studies, the authors (1) compare human perceptions of face and contour images along 15 dimensions commonly assessed in marketing contexts; (2) investigate the effectiveness of contour images for protecting anonymity related to identity, age, and gender; and (3) implement the CaF framework in a real-life online dating context. Results show that the CaF framework effectively resolves privacy–perception trade-off problems by preserving the information that is useful for humans to make inferences about many relevant perceptual dimensions in marketing while making it virtually impossible for humans to infer identity and very difficult to infer age and gender accurately—two critical discrimination factors. Results from the field implementation demonstrate the feasibility and value of using the CaF framework for real-life decision making.
1	Free shipping promotions have become popular among online retailers. However, little is known about their influence on consumers’ purchases, return behavior, and, ultimately, firm profit. The authors propose that free shipping promotions encourage customers to make riskier purchases, leading to more product returns. They estimate the impact of these promotions on purchase incidence, high-risk and low-risk spend, and return share. The results show that free shipping promotions increase expenditure for high-risk products, expanding their share of the consumer’s market basket and thus increasing the overall return rate. This is validated in a field experiment. A field test and an online lab experiment analyze the mechanism linking free shipping and returns. The results suggest that the free shipping effect occurs through consumers’ perceptions that free shipping serves as a risk premium compensating them for potential returns and through positive affect generated by the promotion. A simulation shows that for the focal firm, free shipping promotions increase net sales volume, but higher product returns and lost shipping revenue render these promotions unprofitable.
1	Consumers can buy concert tickets from primary platforms (e.g., Ticketmaster) or from consumer-to-consumer resale platforms (e.g., StubHub). Recently, Ticketmaster has entered and been trying to control the resale market by prohibiting consumers from reselling on competing resale platforms. Several states in the United States have passed or are discussing laws requiring tickets to be transferrable on any resale sites, worrying that platform integration—Ticketmaster controlling both the primary and the resale platforms—will increase ticket service fees and harm musicians and consumers. This article establishes a game-theoretic framework and shows that the opposite can happen: platform integration can lower the service fees in both markets, alleviating double marginalization in the primary market and benefiting the musician and consumers. Moreover, with platform integration, the presence of a small number of scalpers can counterintuitively reduce the ticket price and benefit the musicians and consumers. In addition, platform competition in the resale market may harm consumers. This article further shows that these insights apply in other markets (e.g. used goods, peer-to-peer product-sharing markets) and provides suggestive empirical support for the theoretical results.
1	The authors study the consequences of rebranding multiple category-specific private-label (PL) brands by “opening the umbrella” and unifying them under a common brand name. Retailers expect positive consequences that may manifest themselves in two ways: (1) an increased intrinsic brand strength and (2) an improved marketing-mix effectiveness. The authors analyze three substantially different retailers that rebranded one of their PL tiers. Consistent with the national-brand literature on umbrella rebranding, all three retailers realized an increase in the rebranded PL tier’s intrinsic brand strength, along with a reduced price elasticity. However, and in contrast to the national-brand literature, the effectiveness of both price-promoting and assortment size dropped for all three retailers after they unified their category-specific PLs under a common umbrella name.
1	Manufacturers frequently face the challenge of motivating distributor salespeople to focus efforts on their products rather than competitors’ products. The present research explores two mechanisms that manufacturers use to address this challenge: training and incentives (spiffs). The authors find that the impact of these mechanisms on distributor salespeople’s efforts (toward a manufacturer’s products) largely depends on the extent to which manufacturers also provide training and incentives to distributor sales managers. More specifically, providing greater incentives to distributor sales managers undermines the relationship between their salespeople’s training and effort but enhances the relationship between their salespeople’s incentives and effort. Furthermore, greater sales manager training enhances the impact of salespeople’s incentives on effort; however, greater salesperson training undermines the relationship between salesperson incentives and effort. Thus, this research shows that the combination of mechanisms (training and incentives) and the levels at which manufacturers provide them (distributor salespeople and sales managers) can have different implications for distributor salespeople’s efforts.
1	How and why does the association between weather and hedonic consumption differ between men and women? This article theorizes that women have a stronger affective response to weather conditions, which subsequently induces a larger increase in their hedonic consumption as compared to men. Seven studies show that the relationship between weather conditions and hedonic consumption (food and nonfood items) is differentially mediated by affect for women and men. The studies achieve triangulation by using diverse methodologies (census data, surveys, and experiments), participants (students and nonstudents), measures of independent variables (weather conditions as measured and manipulated), dependent measures (consumption preference and choice), and consumption modalities (food and nonfood).
1	To boost sales, marketers often conduct promotions that offer gifts conditional on the purchase of a focal product. Such promotions can present gifts in different formats: customers could be informed that they will receive a gift directly or that they will receive a voucher entitling them to a gift. Normatively speaking, the two formats are equivalent, as a voucher’s value is identical to that of the gift it represents. Yet this article suggests that promotion format (voucher vs. direct gift) influences consumers’ intention to purchase the focal product. Five lab experiments and one field experiment reveal that, compared with presenting a gift directly, introducing a voucher attenuates the influence of gift value on purchase decisions, decreasing purchase intentions for promotions offering high-value gifts but increasing purchase intentions for promotions offering low-value gifts. This effect occurs because vouchers break the direct association between the focal product and the gift, reducing people’s tendency to compare the gift’s value with the focal product’s value. The effect observed can be attenuated by increasing the salience of gift value.
1	Both the total amount to be donated and the way it is communicated can influence consumer reactions to cause-related marketing (CM) campaigns. While companies often choose not to explicate any donation limit, this study argues that donation frames (e.g., minimum or maximum total donation) can enhance the likelihood of consumer purchases associated with CM campaigns. In a series of four studies, the authors find that consumers often respond more favorably to minimum-frame CM campaigns with a relatively low donation amount (e.g., at least $100,000 will be donated) than those with a high donation amount (e.g., at least $10 million will be donated) despite the superiority of the latter for the recipient cause. This effect is inverted for maximum donation frames, such that a high donation amount leads to greater consumer participation. This research also demonstrates that this effect is driven by the consumer desire to make a personal contribution to a cause, which is more likely to be observed when consumers endow it with high importance. These effects are obtained with attitudes, behavioral intentions, and actual expenditures.
1	Consumers’ price evaluations are influenced by the left-digit bias, wherein consumers judge the difference between $4.00 and $2.99 to be larger than that between $4.01 and $3.00, even though the numeric differences are identical. This research examines when and why consumers are more likely to fall prey to the left-digit bias. The authors propose that the left-digit bias is stronger in stimulus-based price evaluations, wherein people see the focal price and the reference price side by side, and weaker in memory-based price evaluations, wherein people have to retrieve at least one price from memory. This is because in stimulus-based price evaluations, people tend to rely on perceptual representations of prices without rounding them. In memory-based price evaluations, they rely more on conceptual representations, which makes them more likely to round the prices. Results from six studies—five experiments and a scanner panel study—support the hypothesis that the left-digit bias is stronger in stimulus-based evaluations. These results inform managers about when to use left-digit pricing and characterize fundamental differences between stimulus-based and memory-based evaluations.
1	The authors analyze the initial conditions bias in the estimation of brand choice models with structural state dependence. Using a combination of Monte Carlo simulations and empirical case studies of shopping panels, they show that popular, simple solutions that misspecify the initial conditions are likely to lead to bias even in relatively long panel data sets. The magnitude of the bias in the state dependence parameter can be as large as a factor of 2–2.5. The authors propose a solution to the initial conditions problem that samples the initial states as auxiliary variables in a Markov chain Monte Carlo procedure. The approach assumes that the joint distribution of prices and consumer choices is in equilibrium, which is plausible for the mature consumer packaged goods products commonly used in empirical applications. In Monte Carlo simulations, the approach recovers the true parameter values even in relatively short panels. Finally, the authors propose a diagnostic tool that uses common, biased approaches to bound the values of the state dependence and construct a computationally light test for state dependence.
1	Retail expansion is led by multistore firms, which often mix two organizational forms: franchised and company-owned outlets (“franchising decisions”). The authors examine whether strategic considerations in entry and expansion play a role in organizational-form decisions (e.g., franchising) in retailing. The authors utilize store count and revenues for franchised and company-owned outlets of nationwide convenience store chains in 47 geographical markets in Japan between 1984 and 2010. The empirical analyses show that strategic considerations in entry and expansion, ignored in the literature on franchising, appear to influence an organizational-form decision: firms rely more on company-owned outlets for expansion when the threat of entry from competitor firms in adjacent markets increases. The authors examine two interpretations: the convenience of quick deployment and a credible signal. Numerical analyses of a simple dynamic model of entry and franchising confirm that company-owned-outlet-based expansion arises under heightened entry threat. The simulation analysis highlights how franchising decisions in response to an elevated threat of entry may be beneficial (or harmful) for an incumbent firm, which yields key implications for firms, consumers, and policy makers.
1	The authors propose a new Bayesian synthetic control framework to overcome limitations of extant synthetic control methods (SCMs). The proposed Bayesian synthetic control methods (BSCMs) do not impose any restrictive constraints on the parameter space a priori. Moreover, they provide statistical inference in a straightforward manner as well as a natural mechanism to deal with the “large p, small n” and sparsity problems through Markov chain Monte Carlo procedures. Using simulations, the authors find that for a variety of data-generating processes, the proposed BSCMs almost always provide better predictive accuracy and parameter precision than extant SCMs. They demonstrate an application of the proposed BSCMs to a real-world context of a tax imposed on soda sales in Washington state in 2010. As in the simulations, the proposed models outperform extant models, as measured by predictive accuracy in the posttreatment periods. The authors find that the tax led to an increase of 5.7% in retail price and a decrease of 5.5%∼5.8% in sales. They also find that retailers in Washington overshifted the tax to consumers, leading to a pass-through rate of approximately 121%.
1	In this research, the authors investigate the prevalence, robustness, and possible reasons underlying the polarity of online review distributions, with the majority of the reviews at the positive end of the rating scale, a few reviews in the midrange, and some reviews at the negative end of the scale. Compiling a large data set of online reviews—over 280 million reviews from 25 major online platforms—the authors find that most reviews on most platforms exhibit a high degree of polarity, but the platforms vary in the degree of polarity on the basis of how selective customers are in reviewing products on the platform. Using cross-platform and multimethod analyses, including secondary data, experiments, and survey data, the authors empirically confirm polarity self-selection, described as the higher tendency of consumers with extreme evaluations to provide a review as an important driver of the polarity of review distributions. In addition, they describe and demonstrate that polarity self-selection and the polarity of the review distribution reduce the informativeness of online reviews.
1	This article proposes a measurement approach to determine how consumers prefer to locate themselves in proximity to others during consumption experiences, such as when they purchase reserved seating tickets to a performance. Applied to data from locational choice experiments that simulate reserved seating assortments, administered to more than 2,000 participants, this approach reveals the importance of modeling proximity to others when studying locational choices. It also emphasizes the degree to which consumers are heterogeneous in their preferences for proximity to both focal elements (e.g., stage, screen, aisles) and other consumers. Therefore, event operators should collect data beyond purchase ticket logs and also include consumers who did not purchase. Furthermore, this study illustrates how managers can use fitted, individual-level parameters and an optimization model to make more effective seat-level availability decisions. In addition to these recommendations for managers of reserved seating venues, this article offers novel contributions to research related to advance selling, spatial models, and personal space.
1	This article examines how the consumer’s search cost and filtering on a retail platform affect the platform, the third-party sellers, and the consumers. The authors show that, given the platform’s percentage referral fee, a lower search cost can either increase or decrease the platform’s profit. By contrast, if the platform optimally adjusts its referral fee, a lower search cost will increase the platform’s profit. As the consumer’s search cost decreases, if the platform’s demand elasticity increases significantly, the platform should reduce its fee, potentially resulting in an all-win outcome for the platform, the sellers, and the consumers; otherwise, a lower search cost will increase the platform’s optimal fee percentage, potentially leading to higher equilibrium retail prices. Furthermore, the availability of filtering on the platform will in expectation induce consumers to search fewer products but buy products with higher match values, and filtering can either increase or decrease equilibrium retail prices. When filtering reveals only a small amount of the products’ match-value variations, it will benefit the platform, the sellers, and the consumers. This article shows that the effects of filtering and those of a decrease in search cost are qualitatively different.
1	Confronted with increasing digitalization, service firms are challenged to sustain customer loyalty. A promising means to do so is to leverage the digital presence of service employees on their website. A large-scale field study and several experimental studies show that the digital presence of service employees on the firm website increases current website service quality perceptions and positively shapes memories related to employee service quality perceptions from past service encounters. Both effects indirectly increase customer loyalty and, in turn, financial performance, and are amplified by employee accessibility and a service firm’s customer orientation. The authors examine further boundary conditions for the memory process: only service employees evoke the beneficial spillover effect to employee service quality perceptions, and the spillover effect does not generalize to evaluations of product quality. Remarkably, an employee’s digital presence, although factually unrelated, augments customer perceptions of service employees’ competence and commitment and thus strengthens rather than erodes service employees’ role in customer–firm relationships. Theoretical and managerial implications deepen the understanding of how to add a human touch to digital channels.
1	Luxury brands have started to offer consumers the opportunity to customize their exclusive products by making certain aesthetic decisions, such as the color, fabric, or cut of their products. A robust finding in the marketing literature is that consumers place a greater value on customized than on standard products because these unique products better fit and communicate their tastes, preferences, and identity. However, the majority of focal products in these studies fall outside the luxury segment. The authors demonstrate that consumers’ customization preferences differ between mainstream and luxury brands. In the luxury segment, consumers pay a premium for the designer’s expertise and the status that it can convey. As such, the consumers’ desire for self-expression can potentially erode the product’s signaling value. Through a series of four experiments, the authors demonstrate that luxury brands can benefit from customization but they can also take customization too far. Their findings suggest that brand managers should allow consumers to make fewer design decisions for luxury versus mainstream brands to preserve the signal value created by the designer.
1	The marketplace includes many attractive high-calorie indulgent food offerings. Despite their appeal, consumers may often be prompted to consider lower-calorie-package offerings instead (e.g., 100-calorie packs). The question thus arises: What predicts consumers’ preferences between different kinds of lower-calorie offerings? The authors conceptualize two different routes to lower-calorie versions of indulgent foods: a lower-caloric-density version (e.g., baked potato chips) or a smaller-portion-size version (e.g., a smaller bag of potato chips). The authors examine how such versions are differentially preferred and why, focusing on the key role of dietary restraint. The authors show that as dietary restraint increases, the preference for a lower-calorie version created via lower caloric density (vs. a smaller portion size) increases. Differential weights placed on health and fullness goals help explain differing preferences across dietary restraint (as the lower-caloric-density version is perceived as healthier and more filling, albeit less tasty, than the smaller-portion-size version). This framework offers theoretical implications for understanding two routes to cutting calories, practical implications for food marketers, and methodological implications for studying food choices.
1	The authors investigate the role of political ideology in consumer reactions to consumption regulations. First, they demonstrate via a natural experiment that conservatives (but not liberals) increase usage of mobile phones in cars after a law was enacted prohibiting that activity (Study 1). Then, through three lab experiments the authors illustrate that after consumers are exposed to consumption regulations from the government (e.g., laws that restrict consumption, warning labels designed by the Food and Drug Administration), conservatives (vs. liberals) are more likely to (1) use phones when restricted (Study 2), (2) purchase unhealthy foods (Study 3), and (3) view smoking e-cigarettes more favorably (Study 4). No such effects are observed when a nongovernment source is used, or when the message from the government is framed as a notification (vs. warning). These findings point to the important roles of political ideology and the message source in increasing reactance to consumption regulations, thereby mitigating the effectiveness of public policy initiatives undertaken by the government.
1	The authors study the nature of articles published in the Journal of Marketing Research (JMR) during the seven-year period 2013–2019. Consistent with the broad positioning of JMR, they find substantial diversity in domains, topics, methods, and sources of data among the published articles. They observe the emergence of new substantive topics, such as social media, social networks, and prosocial behavior, which reinforce the continued relevance of JMR. Notably, they observe increasing convergence across articles in the behavioral, quantitative, and strategy domains, reflecting more shared substantive topics of interest and common use of methods and sources of data. This trend bodes well for JMR, given its historical position as a diverse journal in the field.
1	Many problems in marketing and economics require firms to make targeted consumer-specific decisions, but current estimation methods are not designed to scale to the size of modern data sets. In this article, the authors propose a new algorithm to close that gap. They develop a distributed Markov chain Monte Carlo (MCMC) algorithm for estimating Bayesian hierarchical models when the number of consumers is very large and the objects of interest are the consumer-level parameters. The two-stage and embarrassingly parallel algorithm is asymptotically unbiased in the number of consumers, retains the flexibility of a standard MCMC algorithm, and is easy to implement. The authors show that the distributed MCMC algorithm is faster and more efficient than a single-machine algorithm by at least an order of magnitude. They illustrate the approach with simulations with up to 100 million consumers, and with data on 1,088,310 donors to a charitable organization. The algorithm enables an increase of between $1.6 million and $4.6 million in additional donations when applied to a large modern-size data set compared with a typical-size data set.
1	In gathering information for an intended purchase decision, consumers submit search phrases to online search engines. These search phrases directly express the consumers’ needs in their own words and thus provide valuable information to marketing managers. Interpreting consumers’ search phrases renders a better understanding of their purchase intentions, which is critical for marketing success. In this article, the authors develop an integrated model to connect the latent topics embedded in consumers’ search phrases to their website visits and purchase decisions. Using a unique data set containing more than 8,000 search phrases submitted by consumers, the model identifies latent topics underlying the searches that led consumers to the firm’s website. Compared with a model lacking any textual information from consumers’ search phrases, a model using textual data in a heuristic approach, and a model based on the latent Dirichlet allocation, the proposed model provides a better evaluation of a consumer’s position on the path to purchase and achieves much better predictive accuracy, which could in turn substantially increase the firm’s revenue. The authors also extend the discussion to aggregators, affiliated websites, and segments of consumers who are exposed to the firm’s outbound ads. Marketing managers can use this method to extract structured information from consumers’ search phrases to facilitate their inference of consumers’ latent purchase states and thereby improve marketing efficiency.
1	This research introduces a framework wherein consumers take on “requestor” or “responder” roles in making joint consumption decisions. The authors document a robust preference expression asymmetry wherein “requestors” soliciting others’ consumption preferences (e.g., “Where do you want to go for dinner?”) desire preference expressions (e.g., “Let’s go to this restaurant”), whereas “responders” instead do not express preferences (e.g., “Anywhere is fine with me”). This asymmetry generalizes under a broad set of situations and occurs because the requestor and responder roles differ in their foci. Compared to responders, requestors are more focused on mitigating the difficulty of arriving at a decision, whereas compared to requestors, responders are more focused on conveying likability by appearing easygoing. Responders thus behave suboptimally, incurring a “preference cost” (when masking preferences) and a “social friction cost” (requestors favor responders who express preferences). Requestors can elicit preference expression by conveying their own dislike of decision making, which increases responders’ focus on mitigating decision difficulty. The authors conclude by discussing the framework’s contributions to looking “under the hood” of joint consumption decisions.
1	Although previous studies have established a direct link between customer-based metrics and stock returns, research is unclear on the mediated nature of their association. The authors examine the association of customer satisfaction and abnormal stock returns, as mediated by the trading behavior of short sellers. Using quarterly data from 273 firms over 2007–2017, the authors find that short interest—a measure of short seller activity—mediates the impact of customer satisfaction and dissatisfaction on abnormal stock returns. Customer dissatisfaction has a more pronounced effect on short selling compared with customer satisfaction. In addition, customer satisfaction and dissatisfaction are more relevant for firms with low capital intensity and firms that face lower competitive intensity. The results show that a one-unit increase in customer satisfaction is associated with a .56 percentage point increase in abnormal returns, while a one-unit increase in customer dissatisfaction is associated with a 1.34 percentage point decrease in abnormal returns.
1	Negotiations today are less likely to be characterized by information asymmetry—the notion that buyers are less informed than sellers—due to the amount of information available to buyers. A number of industries have reacted to this change by shifting their attention to earning profits in aftermarkets: products and services that augment the main purchase (e.g., add-ons, insurance, financing, service and maintenance). In these aftermarkets, firms often retain an information advantage, even if information asymmetries are eliminated from the main purchase. This has given rise to an interesting setting untapped by prior research: information “symmetry” in the front end (main purchase) and information “asymmetry” in the back end (aftermarket). The authors argue that symmetry in the front end provides an opportunity to build trust, as the knowledgeable customer can verify the information disclosed by the seller. In an observational study in the automotive industry, the authors find that customers to whom the salesperson revealed the cost of a car at the beginning of the negotiation spent significantly more in the back end than others. As corroborated in subsequent studies, this effect holds only when cost is disclosed at the beginning of the negotiation and when customers can verify the cost information.
1	This research examines the impact of defaults on product choice in sequential-decision settings. Whereas prior research has shown that a default can affect what consumers purchase by promoting choice of the preselected option, the influence of defaults is more nuanced when consumers make a series of related choices. In such a setting, consumer preferences may evolve across choices due to “spillover” effects from one choice to subsequent choices. The authors hypothesize that defaults systematically attenuate choice spillover effects because accepting a default is a more passive process than either choosing a nondefault option in the presence of a default or making a choice in the absence of a default. Three experiments and a field study provide compelling evidence for such default-induced changes in choice spillover effects. The findings show that firms’ setting of high-price defaults with the aim of influencing consumers to choose more expensive products can backfire through the attenuation of spillover. In addition to advancing the understanding of the interplay between defaults and preference dynamics, insights from this research have important practical implications for firms applying defaults in sequential choices.
1	Multichannel sales systems in business-to-business markets vary substantially in their designs and thereby either attenuate or aggravate agency conflicts between manufacturers and sales partners. Drawing on multiple agency theory, the authors introduce direct and indirect channel usage as focal design dimensions of multichannel sales systems and investigate each channel’s performance effects using a matched manufacturer–sales partner data set. Whereas direct channel usage predominantly lowers agency conflicts in terms of information asymmetry and sales partner moral hazard, indirect channel usage amplifies moral hazard concerns. How those sales partner effects translate into manufacturer performance outcomes critically depends on governance mechanisms, confirming predictions from governance value analysis: formalization enhances performance outcomes for manufacturers in the case of indirect channel usage but diminishes performance in the case of direct channel usage. The authors observe converse effects for centralization and information exchange: centralization and information exchange enhance outcomes of direct channel usage but diminish outcomes of indirect channel usage. The focal managerial implication is that managers must align the design of their multichannel sales systems with effective governance mechanisms.
1	Consumers increasingly expect brands to “pick a side” on divisive sociopolitical issues, but managers are reluctant to risk alienating customers who oppose their position. Moreover, research on identity-based consumption and negativity bias suggests that corporate political advocacy (CPA) is more likely to repel existing customers who oppose the CPA than to attract new customers who support it, implying that the net effect will be negative even if consumers overall are evenly divided in their support/opposition. In this research, the authors posit that despite this negativity bias in individual-level choice, the net effect of CPA at the market level is determined by a sorting process that benefits small-share brands and hurts large-share brands. This is because having few customers to lose and many to gain can offset the risk of the negativity bias in consumers’ identity-driven responses to CPA, potentially leading to a net influx of customers for small-share brands. Five experiments provide support for this theorizing and identify authenticity as a necessary condition for small share brands to benefit.
1	Marketers commonly seed information about products and brands through individuals believed to be influential on social media, which often involves enlisting micro influencers, users who have accumulated thousands as opposed to millions of followers (i.e., other users who have subscribed to see that individual’s posts). Given an abundance of micro influencers to choose from, cues that help distinguish more versus less effective influencers on social media are of increasing interest to marketers. The authors identify one such cue: the number of users the prospective influencer is following. Using a combination of real-world data analysis and controlled lab experiments, they show that following fewer others, conditional on having a substantial number of followers, has a positive effect on a social media user’s perceived influence. Further, the authors find greater perceived influence impacts engagement with the content shared in terms of other users exhibiting more favorable attitudes toward it (i.e., likes) and a greater propensity to spread it (i.e., retweets). They identify a theoretically important mechanism underlying the effect: following fewer others conveys greater autonomy, a signal of influence in the eyes of others.
1	This article introduces a near-optimal bidding algorithm for use in real-time display advertising auctions. These auctions constitute a dominant distribution channel for internet display advertising and a potential funding model for addressable media. The proposed efficient, implementable learning algorithm is proven to rapidly converge to the optimal strategy while achieving zero regret and constituting a competitive equilibrium. This is the first algorithmic solution to the online knapsack problem to offer such theoretical guarantees without assuming a priori knowledge of object values or costs. Furthermore, it meets advertiser requirements by accommodating any valuation metric while satisfying budget constraints. Across a series of 100 simulated and 10 real-world campaigns, the algorithm delivers 98% of the value achievable with perfect foresight and outperforms the best available alternative by 11%. Finally, we show how the algorithm can be augmented to simultaneously estimate impression values and learn the bidding policy. Across a series of simulations, we show that the total regret delivered under this dual objective is less than that from any competing algorithm required only to learn the bidding policy.
1	The authors analyze the impact of a tax on sweetened beverages using a unique data set of prices, quantities sold, and nutritional information across several thousand taxed and untaxed beverages for a large set of stores in Philadelphia and its surrounding area. The tax is passed through at an average rate of 97%, leading to a 34% price increase. Demand in the taxed area decreases by 46% in response to the tax. Cross-shopping to stores outside of Philadelphia offsets more than half of the reduction in sales in the city and decreases the net reduction in sales of taxed beverages to only 22%. There is no significant substitution to bottled water and modest substitution to untaxed natural juices. The authors show that tax avoidance through cross-shopping severely constrains revenue generation and nutritional improvement, thus making geographic coverage an important policy decision.
1	Using one field and two online lab experiments, this article shows that congruency between shelf layout and a consumer’s internal product categorization increases the perceived variety of the assortment and reduces the perceived complexity of the shelf layout. These assortment perceptions, in turn, heighten purchase intention and satisfaction toward the chosen item. Results are robust across internal categorization measurements (planogram design vs. sorting tasks), congruency measures (distance- vs. matching-based), and products (biscuits vs. yogurt snacks). In the field study, familiarity—operationalized as either consumption frequency or subjective product knowledge—increased the overall effect of categorization congruency and strengthened its pathway through perceived variety (vs. the one through complexity). The authors show how their research can be exploited to improve shelf layouts by optimizing the external categorization. They demonstrate the value of a unifying Bayesian framework for research on behavioral decision making that uses the same set of posterior parameter draws for parameter inference, moderated mediation analysis, and optimization under uncertainty.
1	Empirical demand functions, such as those from choice-based conjoint analyses, are critical to many aspects of marketing. Approaches have been developed to ensure that research subjects provide honest and thoughtful responses. However, to reduce costs, researchers increasingly collect data online, under conditions that compromise the value of the information provided. Objective measures related to how the study is completed, such as latency (how quickly answers are given), can only be tied to other objective measures (such as the consistency of the answers), but ultimately their relationship to the subject’s utility function is questionable. To address this problem, the authors introduce a mixture modeling framework that clusters subjects based on variances. The proposed model naturally groups subjects based on their internal consistency. The authors argue that a higher level of internal consistency (i.e., lower variance) reflects more engaged consumers who have sufficient experience with the product category and choice task. “Gremlins,” in contrast, behave such that the noise in their responses overwhelms any signal, leading to a lack of predictive power. This approach provides an automated way to determine which respondents are relevant. The authors discuss the conceptual and modeling framework and illustrate the method using both simulated and commercial data.
1	Open innovation contests that display all submitted ideas to participants are a popular way for firms to generate ideas. In such contest-based ideation, the authors show that seeing numerous competitive ideas of others harms, rather than stimulates, creative performance (Study 1). Others’ competitive prior ideas interfere with idea generation, as new ideas need to be differentiated from the preceding ones to be original. Exposure to an increasing number of prior ideas thus heightens individuals’ perceived constraints of expressing ideas and harms creative performance (Studies 2 and 3). Furthermore, creative performance monotonically reduces with an increasing number of prior ideas (Study 4). A final study demonstrates that showing only a limited number of ideas as well as grouping prior ideas offer actionable ways to reduce prior ideas’ harmful influence (Study 5). These results illustrate viable ways to improve contest-based ideation outcomes merely by changing how competitive prior ideas are presented.
1	In 2019, U.S. pharmaceutical companies paid $3.6 billion to physicians in the form of gifts to promote their drugs. To curb inappropriate financial relationships between health care providers and firms, several state laws require firms to publicly declare the payments they make to physicians. In 2013, this disclosure law was rolled out to all 50 states. The authors investigate the causal impact of this increased transparency on subsequent payments between firms and physicians. While firms and physicians were informed of the disclosure regulation at data collection, complete transparency did not occur until the data were published online. The authors estimate the heterogeneous treatment effects of the online data disclosure exploiting the phased rollout of the disclosure laws across states, facilitated by recent advances in machine learning methods. Using a 29-month national panel covering $100 million in payments between 16 antidiabetic brands and 50,000 physicians, the authors find that the monthly payments changed insignificantly, on average, due to disclosure. However, the average null effect masks some unintended consequences of disclosure, wherein payments may have increased for more expensive drugs and among physicians who prescribed more heavily. The authors further explore potential mechanisms that can parsimoniously describe the data pattern.
1	Public school districts not only make strategic investments in internet access as a means to attract and retain students but also communicate the value of these investments with parents as part of their marketing programs. While it helps attract more customers, how does school district internet access spending (SDIAS) affect academic performance and disciplinary problems among students? Using a longitudinal data set that combines SDIAS of 1,243 school districts with academic performance and disciplinary records of more than 9,000 Texas public schools between 2000 and 2014, the authors find that a one-standard-deviation increase in SDIAS (an average increase of $.6 million) is associated with an improvement in eight academic performance indicators, with effect sizes ranging from 2% to 5% of a standard deviation, amounting to a $.8 million to $1.8 million increase in cumulative income for the current students of a school district. Furthermore, a one-standard-deviation increase in SDIAS is associated with a 5% increase in Part II offense–related school disciplinary problems, amounting to a yearly cost of $25,800 to $53,440 for a school district. The positive and negative consequences of SDIAS are more pronounced among schools in regions with a higher level of household internet access.
1	Anecdotal evidence and extant research show that consumers can prefer both user-designed and designer-designed products. However, the factors that moderate such preferences are not well understood. The authors posit power distance belief (PDB) as a moderator such that low-PDB consumers prefer user-designed to designer-designed products because they identify more with user-driven companies. In contrast, high-PDB consumers prefer designer-designed to user-designed products due to their stronger trust in designer-driven companies. Six studies examining power distance belief at both the country and individual levels provide convergent support for the proposed moderating effect of PDB and underlying mechanisms. Furthermore, the authors demonstrate that the interaction between design philosophy and PDB is more likely for low-complexity than high-complexity products.
1	Sales force incentive design often involves significant participation by sales managers in designing the compensation plans of salespeople who report to them. Although sales managers hold valuable territory-level information, they may benefit from misrepresenting that information given their own incentives. The author uses a game theoretic model to show (1) how a firm can efficiently leverage a manager’s true knowledge and (2) the conditions under which involving the manager is optimal. Under the proposed approach, the firm delegates sales incentive decisions to the manager within restrictive constraints. She can then request relaxed constraints by fulfilling certain requirements. The author shows how these constraints and requirements can be set to ensure the firm’s best possible outcome given the manager’s information. Thus, this “request mechanism” offers an efficient, reliable alternative to approaches often used in practice to incorporate managerial input, such as internal negotiations and behind-the-scenes lobbying. The author then identifies the conditions under which this mechanism outperforms the well-established theoretical approach of offering the salesperson a menu of contracts to reveal territory-level information.
1	There is a growing body of evidence that customer satisfaction is predictive of firms’ future financial performance. However, studies of this relationship have been limited to competitive markets, and monopolistic markets have been largely ignored. This study explores the large and important utilities market and exploits its unique regulatory requirements that generate detailed and reliable operating and accounting data to examine the overall relationship between customer satisfaction and utility profit and establish the causal mechanisms involved. Using data from U.S. public utility firms, the authors show that even when customer satisfaction does not affect future revenues, it does positively predict future profitability by reducing utility firm operating costs. More specifically, they find that higher satisfaction reduces the costs of utility firm distribution, customer service, and sales and general administration expenses. These findings and additional post hoc evidence are consistent with the notion that customer satisfaction (1) generates efficiency-enhancing benefits for utility firms by lowering the direct and employee engagement costs of dealing with dissatisfied customers and (2) fosters greater trust and cooperation from customers. This study has important implications for both managers and regulators and provides important new insights for market-based asset theory and regulatory economic theory.
1	A consumer’s personal attribute (e.g., disease, body weight) can assume the qualities of a stigma (i.e., become a source of devaluation by others) in the presence of certain audiences, which can affect consumption and represent a major hurdle to marketers in many industries (e.g., health care). Two field experiments manipulating the marketing communications sent to 1,453 consumers diagnosed with 87 diseases of varying stigma potential, as well as two Amazon Mechanical Turk studies, reveal that consumers with potentially stigmatizing attributes distinctly decode aspects of marketing communications as audience cues, to infer how (un)favorable observers of their consumption will be in light of the potential stigma. When consumers possess potentially stigmatizing attributes, audience cues influence social devaluation inferences, which influence their beneficial consumption (program enrollment, long-term engagement in health care program; e.g., 64% click-through decrease) and their interest in detrimental consumption (products that promise to alleviate the stigma but are associated with considerable risks). Anticipated empowerment may increase beneficial consumption among consumers managing stigmatizing attributes.
1	Firms use mobile applications to engage with customers, provide services, and encourage customer purchase. Conventional wisdom and previous research indicate that apps have a positive effect on customer spending. The authors critically examine this premise in a highly competitive institutional context by estimating how customers’ multichannel spending changes after adopting a hotel group’s app and identifying the factors contributing to such change. Exploiting the variation in customers’ timing of app adoption and using a difference-in-differences approach, the authors find that the effect of app adoption on customers’ overall spending is significant and negative. Additional analyses suggest the possibility that customers who adopt the focal app also adopt competitor apps and therefore search more and shop around, leading to decreased share of wallet with the focal hotel group. The authors also find that the negative effect on spending is smaller for customers who use the app for mobile check-in service than those who use the app for only searching, highlighting the role of app engagement in mitigating the negative effect.
1	This research highlights how gender shapes consumer payments in pay-what-you-want contexts. Four studies involving hypothetical and real payments show that men typically pay less than women in pay-what-you-want settings, due to gender differences in agentic versus communal orientation. Men approach the payment decision with an agentic orientation, and women approach it with a communal orientation. These orientations then shape payment motives and ultimately affect payment behavior. Because agentic men are more self-focused, their payment decisions are motivated by economic factors, resulting in lower payments. Conversely, communal women are more other-focused, and their payment decisions are motivated by both social and economic factors, resulting in higher payments. The findings additionally highlight how sellers can use marketing communications to increase the salience of social payment motives and demonstrate that by doing so, marketers can increase how much men pay without altering how much women pay in pay-what-you-want settings.
1	Despite the prevalence of both chronic and transient loneliness and the detrimental consequences associated with them, as a negatively valenced response to social exclusion, loneliness has received surprisingly little attention in the marketing literature. Drawing on research showing that lonely people often lack meaning in their life, the authors propose that ritualistic behavior that involves consumer products may reduce loneliness by increasing meaning in life. Specifically, a series of studies finds that engaging in even minimal, unfamiliar rituals reduces loneliness among lonely consumers. The results support the important role of meaningfulness. The authors find that the effect of rituals on loneliness is mediated by meaning in life via perceived product meaningfulness. They also find that ritualistic behavior no longer affects loneliness when the experience of meaningfulness can be derived incidentally.
1	This article documents how informational and emotional appeals in more than 2,000 television ads for 144 car models, aired over four years, influence online search and sales. Increasing the emotional content of ads leads to increases in online search, but increasing the informational content does not. Both informational and emotional content positively influence sales. However, increases in informational content lead to more incremental sales for low-price and low-quality cars than for high-price and high-quality cars. In turn, increases in emotional content generate more incremental sales for high-price cars than for low-price cars. Analyses of the results suggest that managers of high-price and high-quality cars should prioritize emotional rather than informational content in ads. However, managers of low-price and low-quality cars should emphasize emotional content if their objective is to increase online search and informational content if their objective is to increase sales.
1	A consumer’s decision to engage in search depends on the beliefs the consumer has about an unknown product characteristic (e.g., price). Because beliefs are rarely observed, researchers typically assume that consumers have rational expectations or update beliefs consistent with Bayesian updating. These assumptions are restrictive and do not enable the researcher or the retailer to price discriminate among consumers on the basis of heterogeneity in beliefs. The authors use Monte Carlo experiments to show how these assumptions affect estimates of search cost. Next, they design an incentive-aligned online study in which participants search over the price of a homogeneous good, and the authors elicit distributions of price beliefs before and after each search. Drawing on data collected from a nationally representative panel, they find substantial heterogeneity in prior price beliefs, such that participants update their beliefs in response to search outcomes but deviate from Bayesian updating in that they underreact to new information. Importantly, the authors show that (1) assuming Bayesian updating does not significantly bias search cost estimates at the aggregate level if the researcher accounts for heterogeneous prior beliefs, (2) eliciting heterogeneity in prior expected prices is much more important than eliciting heterogeneity in prior price uncertainty, and (3) a retailer can increase profits through third-degree price discrimination by recognizing the heterogeneity in prior beliefs.
1	The authors study how a consumer optimally allocates attention to favorable and unfavorable product-related information before making the purchase decision, when information processing is costly. They find that attention allocation depends on, among other factors, the consumer’s prior belief about whether the product matches their needs and their unit information processing cost. A consumer processes both “confirmatory” and “disconfirmatory” information to their prior belief, but to different degrees under different conditions. In general, if the consumer has an extreme prior, or if the unit cost of processing information is high such that only a small amount of information is optimally processed, they process more confirmatory than disconfirmatory information; this offers a rational explanation for the phenomenon known as “confirmation bias.” The authors also find that a seller can benefit by influencing the consumer’s attention allocation by strategically choosing how much favorable and unfavorable information to make available for the consumer to process and by influencing the information processing cost, where the optimal strategy depends on the seller’s ability to adjust product price. Surprisingly, a seller has a lower incentive to suppress unfavorable information when the consumer has a worse prior belief about product fit. The authors illustrate their model with an application to information provision in product reviews.
1	Crowdfunding has emerged as an alternative means of financing new ventures wherein a large number of individuals collectively back a project. This research specifically examines reward-based crowdfunding, in which those who take part in the crowdfunding process receive the new product for which funding is sought in return for their financial support. This work illustrates that consumers make fundamentally different decisions when considering whether to contribute their money to crowdfund versus purchase a product. Six studies demonstrate that compared with a traditional purchase, crowdfunding more strongly activates an interdependent mindset and, as a result, increases consumer demand for social-good products (i.e., products with positive social and/or environmental impact). The research further highlights that an active involvement in the crowdfunding process is necessary to increase demand for social-good products: when a previously crowdfunded product is already to market, the effect is eliminated. Finally, it is demonstrated that crowdfunding participants exhibit an increased demand for social-good products only when collective efficacy (i.e., one’s belief in the collective’s ability to bring about change) is high.
1	To understand the impact of competition on organizational service reliability decisions, this study investigates whether firms in the airline industry consider competitors’ actions when making their service reliability decisions. Using data from the U.S. Bureau of Transportation Statistics on flight cancellation rates and average length of flight delays, the authors use two complementary approaches, a simultaneous equation model and a discrete game framework, to examine competitive influence on firm decisions on the level of service reliability. The authors find that competitive effects are asymmetric and differ by the type of firm and its competitors—full-service versus low-cost airlines—as well by level of market concentration. The authors show that internal initiatives, such as on-time bonuses, can substantially improve service reliability but require the firm to account for competitive reactions. Ignoring competitive effects leads to an overestimation of the impact of these programs on service reliability levels.
1	This research demonstrates that under states of certainty, consumers with a relatively stronger global (local) identity prefer global (local) brands, whereas under states of uncertainty, consumers with a relatively stronger global (local) identity prefer local (global) brands. This effect occurs because uncertainty (certainty) activates a divergent (convergent) thinking style, which results in a preference for options that are more distant from (closer to) the identity to which consumers associate more strongly. The effect holds both when individuals’ global–local citizenship identity is measured and when it is manipulated. The research further establishes an important boundary condition for the effect. The effect holds in the citizenship identity context because people normally associate themselves with both local and global citizenship identities, and situational or dispositional factors only influence the degree to which they associate with each identity. The effect does not surface when individuals construe their local–global citizenship identities as interfering, meaning they conceive that holding one identity conflicts with holding the other.
1	Live streaming offers an unprecedented opportunity for content creators (broadcasters) to deliver their content to consumers (viewers) in real time. In a live stream, viewers may send virtual gifts (tips) to the broadcaster and engage with likes and chats free of charge. These activities reflect viewers’ underlying emotion and are likely to be affected by the broadcaster’s emotion. This article examines the role of emotion in interactive and dynamic business settings such as live streaming. To account for the possibility that broadcaster emotion, viewer emotion, and viewer activities influence each other, the authors estimate a panel vector autoregression model on data at the minute level from 1,450 live streams. The results suggest that a happier broadcaster makes the audience happier and begets intensified viewer activities, in particular tips. In addition, broadcasters reciprocate viewer engagement with more smiles. Further analyses suggest that these effects are pronounced only after a live stream has been active for a while, and they manifest only in streams by broadcasters who have more experience, receive more tips, or are more popular in past live streams. These results help platforms and broadcasters optimize marketing interventions such as broadcaster emotion enhancement in live streaming and quantify the financial returns.
1	Consumers frequently engage in activities with others, such as visiting an art gallery with a friend or going to a sports match with a family member, and they tend to assume that sharing experiences with another person will make these activities more enjoyable. However, navigating a shared experience—making decisions about pacing, sequencing, and interacting with another person as the experience unfolds—can take consumers’ attention away from the activity, potentially reducing their enjoyment. In a series of studies in which consumers engage in real consumption experiences, the authors show that lack of clarity about a partner’s interests can distract consumers, making it difficult for them to focus on the shared activity and reducing their enjoyment of shared experiences relative to solo experiences. Notably, simple interventions can increase clarity of a partner’s interests and consumers’ enjoyment of shared activities, providing tools for service providers who want to retain customers and benefit from positive word of mouth.
1	This research explores how marketers can avoid the so-called “false consensus effect”—the egocentric tendency to project personal preferences onto consumers. Two pilot studies show that most marketers have a surprisingly strong lay intuition about the existence of this inference bias, admit that they are frequently affected by it, and try to avoid it when predicting consumer preferences. Moreover, the pilot studies indicate that most marketers use a very natural and straightforward approach to avoid the false consensus effect in practice, that is, they simply try to “suppress” (i.e., ignore) their personal preferences when predicting consumer preferences. Ironically, four subsequent studies show that this frequently used tactic can backfire and increase marketers’ susceptibility to the false consensus effect. Specifically, the results suggest that these backfire effects are most likely to occur for marketers with a low level of preference certainty. In contrast, the results imply that preference suppression does not backfire but instead decreases the false consensus effect for marketers with a high level of preference certainty. Finally, the studies explore the mechanism behind these results and show how marketers can ultimately avoid the false consensus effect—regardless of their level of preference certainty and without risking backfire effects.
1	This article studies how receiving a bonus changes consumers’ demand for auto loans and their risk of future delinquency. Unlike traditional consumer products, auto loans have a long-term impact on consumers’ financial state because of the monthly payment obligation. Using a large consumer panel data set of credit and employment information, the authors find that receiving a bonus increases auto loan demand by 21%. These loans, however, are associated with higher risk, as the delinquency rate increases by 18.5%–31.4% depending on different measures. In contrast, an increase in consumers’ base salary will increase their demand for auto loans but not their delinquency. By comparing consumers with bonuses with those without bonuses, the authors find that bonus payments lead to both demand expansion and demand shifting on auto loans. The empirical findings help shed light on how consumers make financial decisions and have important implications for financial institutions on when demand for auto loans and the associated risk arise.
1	The current research introduces the concept of psychological ownership of borrowed money, a construct that represents how much consumers feel that borrowed money is their own. The authors observe both individual-level and contextual-level variation in the degree to which consumers feel psychological ownership of borrowed money, and variation on this dimension predicts willingness to borrow money for discretionary purchases. At an individual level, psychological ownership of borrowed money is distinct from other individual factors such as debt aversion, financial literacy, income, intertemporal discounting, materialism, propensity to plan, self-control, spare money, and tightwad–spendthrift tendencies, and it predicts willingness to borrow above and beyond these factors. At a contextual level, the authors document systematic differences in psychological ownership between different debt types. They show that these differences in psychological ownership manifest in consumers’ online search behavior and explain consumers’ differential interest in borrowing across debt types. Finally, the authors demonstrate that psychological ownership of borrowed money is malleable, such that framing debt using language lower in psychological ownership can reduce consumers’ propensity to borrow.
1	As firms use advertising to gain product market advantages and increase their valuation in financial markets, disclosing their advertising spending is influential—whether it erodes organizational competitive advantages in product markets or signals quality in financial markets. The authors argue that firms learn from peers’ decisions to reduce the uncertainty in their own advertising disclosure, and they empirically investigate information-based organizational herding in the context of advertising spending disclosure, where a 1994 reporting rule made advertising spending disclosures voluntary in the United States. The authors examine whether a firm relies on information from benchmark leaders or similar peers to resolve disclosure uncertainty. A novel identification strategy, which uses partially overlapping strategic groups to mitigate simultaneity and correlated unobservables, shows robust evidence for herding effects among peer firms in the same strategic group. Moreover, firms are more likely to resolve disclosure uncertainty from similar peers rather than from benchmark leaders. The authors discuss how firms can use knowledge of competitors’ predicted advertising disclosure decisions conditional on their disclosure to their strategic advantage in product and financial markets.
1	A series of controlled experiments examine how the strategy of incentivizing reviews influences consumers’ expressions of positivity. Incentivized (vs. unincentivized) reviews contained a greater proportion of positive relative to negative emotion across a variety of product and service experiences (e.g., videos, service providers, consumer packaged goods companies). This effect occurred for both financial and nonfinancial incentives and when assessing review content across multiple natural language processing tools and human judgments. Incentives influence review content by modifying the experience of writing reviews. That is, when incentives are associated with review writing, they cause the positive affect that results from receiving an incentive to transfer to the review-writing experience, making review writing more enjoyable. In line with this process, the effect of an incentive on review positivity attenuates when incentives are weakly (vs. strongly) associated with review writing (i.e., incentive for “participating in an experiment” vs. “writing a review”) and when the incentive does not transfer positive affect (i.e., when an incentive is provided by a disliked company). By examining when incentives do (vs. do not) adjust the relative positivity of written reviews, this research offers theoretical insight into the literature on incentives, motivation, and word of mouth, with practical implications for managers.
1	The market for supplemental educational programs (SEPs)—tutorials, educational materials, summer programs—has burgeoned. Thus, it is important to understand factors that may influence parents’ choices for SEPs. This article examines how parents’ political identity affects their preference for SEPs contingent on their focus on self. Using two main educational orientations—conformance orientation and independence orientation—the authors argue that SEPs with conformance-oriented pedagogy may be preferred more by conservative parents due to their higher need for structure. This association of political identity with preference for SEPs is moderated by self-focus. Counterintuitively, when using political orientation to target messages for SEPs, firms should frame messages to focus parents on themselves for identity-consistent effects to manifest. Five studies, including a field study, test this theorizing and replicate key results using different measures of political identity and self-focus.
1	Life-role transition is a state wherein people pass through different life stages, involving changes in identities, roles, and responsibilities. Across six studies, the current research shows that consumers under life-role transition have more favorable attitudes toward distant (i.e., low- or moderate-fit) brand extensions than consumers who are not under life-role transition. The effect is driven by a sense of self-concept ambiguity associated with life-role transition, which subsequently prompts dialectical thinking that helps improve perceived fit between a parent brand and its extension, finally resulting in more favorable brand extension evaluation. This effect diminishes for (1) near (i.e., high-fit) brand extensions that do not require dialectical thinking for perceiving fit; (2) for sub-brand (vs. direct brand) architecture, for which there is less of a need to use dialectical thinking to reconcile the inconsistencies between a parent brand and its extension; and (3) when consumers perceive they have resources to cope with the life-role transition, which attenuates self-concept ambiguity. This research offers important theoretical and managerial insights by focusing on life-role transition—an important aspect of consumers’ lives that has been largely underresearched—and by demonstrating how and why it elicits more favorable attitudes toward brand extensions.
1	This article shows that animated display of time-varying data (e.g., stock or commodity prices) enhances risk judgments. We outline a process whereby animated display enhances the visual salience of transitions in a trajectory (i.e., successive changes in data values), which leads to transitions being utilized more to form cognitive inferences about risk. In turn, this leads to inflated risk judgments. The studies reported in this article provide converging evidence via eye tracking (Study 1), serial mediation analyses (Studies 2 and 3), and experimental manipulations of transition salience (graph type; Study 3) and utilization of transitions (global trend; Study 4 and investment goals; Study 5) and, in the process, outline boundary conditions. The studies also demonstrate the effect of animated display on consequential investment decisions and behavior. This article adds to the literature on salience effects by disambiguating the role of inference making in how salience of stimuli causes biases in judgments. Broader implications for visual information processing, data visualization, financial decision making, and public policy are discussed.
1	Business-to-business (B2B) companies devote significant resources to measure customer satisfaction but lack guidance on critical aspects of implementing satisfaction programs. Accordingly, executives ask: (1) What are the key strategic attributes driving B2B customer satisfaction? (2) Are the strategic attributes satisfaction balancing, satisfaction maintaining, or satisfaction enhancing based on the pattern of asymmetry? (3) Do the sign and magnitude of asymmetry vary across industry and customer subgroups? and (4) Is there a generalizable link between satisfaction and financial performance for B2B firms? Study 1 uses qualitative and secondary research to identify and validate eight strategic attributes pertinent to B2B companies: quality of product/service, pricing, safety, sales process, project management, corporate social responsibility, communication, and ongoing service and support. Study 2 examines industry-subgroup heterogeneity in the nature of asymmetry across industries, then links satisfaction with performance (i.e., sales). Study 3 finds customer-subgroup heterogeneity in the nature of asymmetry within the customer base of a B2B service provider, then links satisfaction with performance (i.e., dollar value of purchase).
1	Crowdfunding has emerged as an alternative means of financing new ventures by utilizing the financial support of a large group of individual investors. This research asks a novel question: Does being crowdfunded carry any signal value for the broader market of observing consumers? Seven studies reveal a consumer preference for crowdfunded products, even after controlling for a product’s objective product characteristics. The authors identify two inferences that help explain this effect: (1) consumers perceive crowdfunded products to be of higher quality, and (2) they believe that supporting crowdfunding reduces inequality in the marketplace. The authors further document an important boundary condition of the first inference: the identified effect reverses in high-risk domains (e.g., products that involve high physical risk) due to consumer perceptions that the crowdfunding model lacks sufficient professionalism to mitigate risk. With regard to the second inference, the authors find that the positive crowdfunding effect is particularly strong among consumers who value social equality. Taken together, this work sheds new light on consumer perceptions of crowdfunding, elucidates why and when consumers prefer crowdfunded products, and offers actionable implications for managers.
1	When using group-based commission plans to motivate their sales force, should firms always compensate salespeople based on the average of team members’ sales outcomes? The theory suggests that when team members are heterogeneous in sales abilities, the proposed maximum contract (where the team output is set by the largest individual sales output) dominates the average contract (where the team output is determined by the average output of team members) in terms of overall team effort. This is because the stronger team member will exert higher effort under the maximum contract compared with the average contract, and this increase exceeds the decrease in the weaker team member’s effort. The authors validate the theoretical predictions by employing two laboratory experiments to provide a causal test of the theory and two randomized field experiments to deliver additional corroborating evidence. Overall, the experimental results are consistent and broadly confirm the theoretical predictions, pointing to the substantial gains from implementing the maximum contract when team members are heterogeneous in abilities. Interestingly, the weaker team members exert similar effort across the maximum and average contracts, although the theory predicts higher effort under the latter.
1	Many firms incorporate activity-based incentive (ABI) compensation into their pay plans. These ABIs are based on salespeople’s activity measures derived from their call reports. Despite their prevalence and theory-based expectations, there is a distinct lack of empirical work studying the sales productivity effects of ABI pay. With the cooperation of a large pharmaceutical firm, the authors conducted a three-year-long intervention based on a “treatment-removal” design. Their first intervention added modest ABI pay for frontline salespeople and their supervisors across 305 sales territories; the second intervention removed ABI pay from the salespeople; and the third intervention removed ABI pay from the supervisors as well, returning to the status quo. Using detailed territory-level data from the intervention in conjunction with syndicated market-level data and employing synthetic control procedures, the authors find sales gains of around 6%–9% from each of the two ABI interventions relative to the no-ABI baseline. These effects are moderated by the number of salespeople in a territory, with territories with more salespeople showing larger effects. Analyses of activity effects show that when supervisors are paid ABIs, they exert behavior control downward on salespeople. Managerially, both ABI schemes improve performance over an output-only pay plan. Between the two, a rudimentary gross profit impact calculation shows that ABIs targeted at supervisors alone are more efficient than ABIs targeted at both salespeople and their supervisors. The results support tying compensation to call reports despite the potential for self-serving biases in these measures because supervisors are able to exercise more behavior control with ABIs.
1	Consumers frequently engage in experiences (e.g., listening to music) in the presence of delicious food. Ten studies show that the presence (vs. absence) of such food decreases the enjoyment of a concurrent (target) experience across a wide array of consumption activities, such as listening to music, evaluating pictures, and coloring. The presence (vs. absence) of food prompts mental imagery of consuming that food, which decreases engagement with the target experience, resulting in lower enjoyment. Consistent with prior work on mental imagery, the effect occurs only for food that is considered tasty; when a food’s functional benefits are highlighted, the effect disappears. In addition, the effect can be triggered in the absence of food when participants are explicitly instructed to engage in mental imagery. The authors demonstrate the role of engagement by showing that the valence of the target experience moderates this effect, such that the presence of food decreases enjoyment of positive experiences but increases enjoyment of negative experiences. This work contributes to previous research on mental imagery and delayed consumption by highlighting the need to focus on how the presence of food affects concurrent experiences and provides important managerial insights given the proliferation of tasty food to enhance customer experience.
1	The authors revisit the question of alcohol consumption and public health over business cycles by decomposing overall alcohol consumption into drinking frequency and intensity in relation to consumer heterogeneity. To study this question, they use consumer-level panel data on the reported consumption (not purchases) of beer, which is the most heavily consumed alcoholic beverage and accounts for the majority of binge drinking in the United States. Leveraging the panel nature of the data, the authors find a negative (positive) relationship between unemployment and drinking frequency (intensity). Total consumption, which is the product of drinking frequency and intensity, is procyclical. To uncover differences in behavior across consumers and to provide policy recommendations at a segment level, the authors present a structural model where consumers simultaneously choose the frequency and intensity of their alcohol consumption. They find differences across consumers in their behaviors, notably with respect to income and age. They conduct policy simulations to compare the effectiveness of alcohol-related policies to counter the adverse effects of recessions on the health of vulnerable groups such as low-income and elderly populations.
1	This article studies the question and answer (Q&A) technology of electronic commerce platforms, an increasingly common form of user-generated content that allows consumers to publicly ask product-specific questions and receive responses, either from the platform or from other customers. Using data from a major online retailer, the authors show that Q&As complement consumer reviews: unlike reviews, questions are primarily asked prepurchase and focus on clarification of product attributes rather than discussion of quality; answers convey fit-specific information in a predominantly sentiment-free way. Drawing on these observations, the authors hypothesize that Q&As mitigate product fit uncertainty, leading to better matches between products and consumers and, therefore, improved product ratings. Indeed, when products suffering from fit mismatch start receiving Q&As, their subsequent ratings improve by approximately .1 to .5 stars, and the fraction of negative reviews that discuss fit-related issues declines. The extent of the rating increase due to Q&As is proportional to the probability that purchasers will experience fit mismatch without Q&A. These findings suggest that, by resolving product fit uncertainty in an e-commerce setting, the addition of Q&As can be a viable way for retailers to improve ratings of products that have incurred low ratings due to customer–product fit mismatch.
1	This research examines the effects of religious belief and religious priming on negative word-of-mouth (NWOM) behavior. Drawing on social exchange and norm paradigms, the authors theorize and find evidence of the unique effects of religious belief and religious priming on NWOM in everyday service failure encounters. Specifically, they find that religious belief is associated with higher NWOM, driven by a greater sensitivity to violations of fairness norms, which in turn reduces forgiveness. However, exposure to religious priming attenuates NWOM among more religious consumers by reducing sensitivity to violations of fairness norms, which in turn enhances forgiveness. A field study involving over 1.2 million online reviews of actual restaurant experiences, in addition to four lab studies, provides support for the theorized effects. This study sheds light on the religion–forgiveness discrepancy by establishing the mediating role of sensitivity to fairness violations on the relationship between religion and forgiveness in the NWOM context. Further, the results demonstrate the importance of religion as a strategic variable in the management of service failure experiences, providing theoretical implications for the literature on the effects of religion on consumer behavior.
1	The authors study the phenomenon of strategic group polarization, in which members take more extreme actions than their preferences. The analysis is relevant for a broad range of formal and informal group settings, including social media, online platforms, sales teams, corporate and academic committees, and political action committees. In the model, agents with private preferences choose a public action (voice opinions), and the mean of their actions represents the group’s realized outcome. The agents face a trade-off between influencing the group decision and truth-telling. In a simultaneous-move game, agents strategically shade their actions toward the extreme. The strategic group influence motive can create substantial polarization in actions and group decisions even when the preferences are relatively moderate. Compared with a simultaneous game, a randomized-sequential-actions game lowers polarization when agents’ preferences are relatively similar. Sequential actions can even lead to moderation if the later agents have moderate preferences. Endogenizing the order of moves (through a first-price sealed-bid auction) always increases polarization, but it is also welfare enhancing. These findings can help group leaders, firms, and platforms design mechanisms that moderate polarization, such as the choice of speaking order, the group size, and the knowledge members have of others’ preferences and actions.
1	Firms in technology markets often outsource the manufacture of core components—components that are central to product performance and comprise a substantial portion of product costs. Despite the strategic importance of core-component outsourcing, there is little empirical evidence (and many conflicting opinions) about its impact on consumer demand. The authors address this gap with an examination of panel data from the flat panel TV industry, across key regions globally. Results from their estimation indicate that core-component outsourcing reduces the firm’s ability to be on the technological frontier; this hurts demand, because the authors’ estimates suggest that consumers care about firms being on the frontier. However, such outsourcing also reduces costs. Finally, the authors find that outsourcing increases the intensity of competition in the marketplace. They assess these (often opposing) effects and conduct thought experiments to quantify the performance impact of core-component outsourcing.
1	The exponential growth in digital media has recently challenged the value of print media in the overall marketing mix. Across three studies, the authors evaluate the relative effectiveness of print ads versus digital ads. In Study 1, using eye tracking and biometric measures during exposure, the authors find stronger encoding and engagement for print ads over digital ads. A week later, participants showed no significant difference in recognizing ads across format, though they better remembered the encoding context of print ads. Notably, using functional magnetic resonance imaging, the authors find greater activation in hippocampus and parahippocampal regions for print ads relative to digital ads. Extending these insights, Study 2 demonstrates that participants better remember print ads across contents, context, and brand associations when using snippets as retrieval cues. In addition to establishing the robustness of previous findings, Study 3 provides further evidence that the observed memory advantage for print ads is primarily due to superior encoding during initial exposure. From a practical perspective, these findings suggest that marketers should not discount the value of print media in advertising, despite the rapid growth of digital media and communications.
1	The U.S. Department of Agriculture estimates that one in nine U.S. households is “food insecure”: unable to purchase sufficient, or healthy food. Public policy advocates and politicians have pointed to the prevailing federal minimum wage as a culprit, labeling it a “starvation wage.” This study examines whether and to what extent increases to the minimum wage have improved the quantity and nutritional quality of food purchased by minimum wage earners, and what implications these potential changes in consumer behavior have for marketers. The authors show that households likely to be earning the minimum wage increase their calories purchased in response to minimum wage increases, and that these gains are predominantly found among households purchasing the least amount of food prior to the minimum wage rising. Although the authors do not find evidence that the average household improves the nutritional content of calories purchased, they do find evidence that the least healthy households (as measured by past purchases) buy more healthy foods in response to rising minimum wages. Overall, the findings suggest that higher minimum wages may not only help households afford more calories but also encourage some households to purchase more healthy calories. In addition, the authors find an increased openness among minimum wage households to purchasing new grocery items. This openness to trying previously unpurchased products offers promotion and product line planning opportunities to manufacturers. It also offers retailers with a nutrition-friendly brand image an opportunity to nudge consumers toward purchasing more healthy foods.
1	In an effort to combat food waste, many firms have introduced rescue-based foods (RBFs), which are made from ingredients that are safe to eat but would otherwise be wasted, often due to aesthetic issues or oversupply. Although the benefits of RBF are varied, some firms adopt strategies that highlight RBF’s waste-reduction benefits, such as reduced landfill use or lower environmental impact. This research posits that when firms adopt strategies that highlight associations between physical waste and RBF, those associations can generate negative mental imagery, which can trigger disgust and mitigate positive consumer attitudes toward RBF. When such associations are not present, demand is consistent with demand for conventional foods. The authors find support for the role of mental imagery in this demand mitigation process, with some promotional appeals stimulating thoughts of physical waste. Counterintuitively, this research reveals that when marketers adopt the common practice of using environmental benefit appeals that can trigger physical waste associations, such as the color green, consumer demand for RBF diminishes. Conversely, focusing on the societal benefits or limiting the number of cues available to create physical waste associations generates consumer demand for these foods on a level equivalent to that of conventional food.
1	The authors link the rapid and dramatic move from second-price to first-price auction format in the display advertising market, on the one hand, to the move from the waterfalling mechanism employed by publishers for soliciting bids in a preordered cascade over exchanges to an alternate header bidding strategy that broadcasts the request for bid to all exchanges simultaneously, on the other. First, they argue that the move from waterfalling to header bidding was a revenue-improving move for publishers in the old regime when exchanges employed second-price auctions. Given the publisher move to header bidding, the authors show that exchanges move from second-price to first-price auctions to increase their expected clearing prices. Interestingly, when all exchanges move to first-price auctions, each exchange faces stronger competition from other exchanges, and some exchanges may end up with lower revenue than when all exchanges use second-price auctions; yet all exchanges move to first-price auctions in the unique equilibrium of the game. The authors show that the new regime hinders the exchanges’ ability to differentiate in equilibrium. Furthermore, it allows the publishers to achieve the revenue of the optimal mechanism despite not having direct access to the advertisers.
1	Despite marketers’ efforts to make consumers feel attractive in many sales and advertising contexts, little is known about how consumers’ self-perceived physical attractiveness influences their decision making. The authors examine whether a boost in consumers’ self-perceived attractiveness influences subsequent choices in domains unrelated to beauty. Across six studies, the authors find converging evidence that a boost in consumers’ self-perceived attractiveness enhances their general self-confidence and reduces preference uncertainty, resulting in less reliance on the choice context and thus fewer choices of compromise, all-average, and default options. The findings further show that consumers use self-confidence as metacognitive information for inferring preference uncertainty in subsequent decisions. This process is a misattribution that can be attenuated when consumers attribute their self-confidence to the self-perceived attractiveness. The article concludes with a discussion of theoretical and managerial implications.
1	The U.S. pay television service market was dominated by cable operators until the nationwide entry of satellite operators in the early 1990s. The latter have been consistently growing their footprints since. This study documents the role of television advertising to explain satellite operators’ success. Using data on U.S. households’ subscription choices and operators’ advertising decisions, the authors document both demand- and supply-side conditions conducive to the growth of the satellite operators. First, the authors find that consumers in this market were sensitive to advertising, and especially so to that of the satellite operators (ad elasticities of about .05–.06 for satellite operators vs. .02 for cable operators). The authors employ a border strategy to demonstrate advertising-elastic demand and discuss its robustness to potential threats to identification. Second, the authors provide suggestive evidence that a form of asymmetric cost efficiencies in television advertising benefited the entrants more than the incumbents. Specifically, the unit costs of local advertising tend to be higher than those of national advertising, which likely allowed the satellite operators to better leverage their national presence with (cheaper) national advertising. Overall, this study highlights the interaction between advertising efficiencies and the scale of entry in explaining the competition between market incumbents and entrants.
1	Numeric labeling of calories on restaurant menus has been implemented widely, but scientific studies have generally not found substantial effects on calories ordered. The present research tests the impact of a feedback format that is more targeted at how consumers select and revise their meals: real-time aggregation of calorie content to provide dynamic feedback about meal calories via a traffic light label. Because these labels intuitively signal when a meal shifts from healthy to unhealthy (via the change from green to a yellow or red light), they prompt decision makers to course-correct in real time, before they finalize their choice. Results from five preregistered experiments (N = 11,900) show that providing real-time traffic light feedback about the total caloric content of a meal reduces calories in orders, even compared with similar aggregated feedback in numeric format. Patterns of ordering reveal this effect to be driven by people revising high-calorie orders more frequently, leading them to choose fewer and lower-calorie items. Consumers also like traffic light aggregation, indicating greater satisfaction with their order and greater intentions to return to restaurants that use them. The authors discuss how dynamic feedback using intuitive signals could yield benefits in contexts beyond food choice.
1	Consumption of used products has the potential to symbolically connect present and previous users of these products, something that may appeal to lonely consumers. Accordingly, across seven studies, feeling lonely increased consumers’ preference for previously owned products. Specifically, the authors found that the proportion of lone shoppers was higher in a used versus a regular bookstore, lone individuals (vs. those sitting in pairs) were more likely to select a used over a new product, people without (vs. with) a date on Valentine’s Day expressed stronger preference for used products, and individual differences in loneliness during the COVID-19 pandemic predicted interest in used products. Other studies documented that the desire to symbolically connect underlies the effect of loneliness on consumption. At a time when loneliness is on the rise, the authors discuss implications for the marketing of used products and how loneliness might motivate consumers to reduce waste.
1	The success of customer relationship management programs ultimately depends on the firm's ability to identify and leverage differences across customers—a difficult task when firms attempt to manage new customers, for whom only the first purchase has been observed. The lack of repeated observations for these customers poses a structural challenge for firms to infer unobserved differences across them. This is what the authors call the “cold start” problem of customer relationship management, whereby companies have difficulties leveraging existing data when they attempt to make inferences about customers at the beginning of their relationship. The authors propose a solution to the cold start problem by developing a probabilistic machine learning modeling framework that leverages the information collected at the moment of acquisition. The main aspect of the model is that it flexibly captures latent dimensions that govern the behaviors observed at acquisition as well as future propensities to buy and to respond to marketing actions using deep exponential families. The model can be integrated with a variety of demand specifications and is flexible enough to capture a wide range of heterogeneity structures. The authors validate their approach in a retail context and empirically demonstrate the model's ability to identify high-value customers as well as those most sensitive to marketing actions right after their first purchase.
1	Video advertisements often show actors and influence agents consuming and enjoying products in slow motion. By prolonging depictions of influence agents’ consumption utility, slow motion cinematographic effects ostensibly enhance social proof and signal product qualities that are otherwise difficult to infer visually (e.g., pleasant tastes, smells, haptic sensations). In this research, seven studies, including an eye tracking study, a Facebook Ads field experiment, and lab and online experiments—all using real ads across diverse contexts—demonstrate that slow motion (vs. natural speed) can backfire and undercut product appeal by making the influence agent’s behavior seem more intentional and extrinsically motivated. The authors rule out several alternative explanations by showing that the effect attenuates for individuals with lower intentionality bias, is mitigated under cognitive load, and reverses when ads use nonhuman influence agents. The authors conclude by highlighting the potential for cross-pollination between visual information processing and social cognition research, particularly in contexts such as persuasion and trust, and they discuss managerial implications for visual marketing, especially on digital and social platforms.
1	The success of creative products depends on the felt experience of consumers. Capturing such consumer reactions requires the fusing of different types of experiential covariates and perceptual data in an integrated modeling framework. In this article, the authors develop a novel multimodal machine learning framework that combines multimedia data (e.g., metadata, acoustic features, user-generated textual data) in creative product settings and apply it to predict the success of musical albums and playlists. The authors estimate the proposed model on a unique data set collected using different online sources. The model integrates different types of nonparametrics to flexibly accommodate diverse types of effects. It uses penalized splines to capture the nonlinear impact of acoustic features and a supervised hierarchical Dirichlet process to represent crowd sourced textual tags, and it captures dynamics via a state-space specification. The authors show the predictive superiority of the model with respect to several benchmarks. The results illuminate the dynamics of musical success over the past five decades. The authors then use the components of the model for marketing decisions such as forecasting the success of new albums, conducting album tuning and diagnostics, constructing playlists for different generations of music listeners, and providing contextual recommendations.
1	This work advances and tests a theory of how news information evolves as it is successively retold by consumers. Drawing on data from over 11,000 participants across ten experiments, the authors offer evidence that when news is repeatedly retold, it undergoes a stylistic transformation termed “disagreeable personalization,” wherein original facts are increasingly supplanted by opinions and interpretations with a slant toward negativity. The central thesis is that when retellers believe they are more (vs. less) knowledgeable than their recipient about the information they are relaying, they feel more compelled to provide guidance on its meaning and to do so in a persuasive manner. This enhanced motivation to guide persuasively, in turn, leads retellers to not only select the subset of facts they deem most essential but, critically, to provide their interpretations and opinions on those facts, with negativity being used as a means of grabbing their audience’s attention. Implications of this work for research on retelling and consumer information diffusion are explored.
1	Video is one of the fastest growing online services offered to consumers. The rapid growth of online video consumption brings new opportunities for marketing executives and researchers to analyze consumer behavior. However, video also introduces new challenges. Specifically, analyzing unstructured video data presents formidable methodological challenges that limit the use of multimedia data to generate marketing insights. To address this challenge, the authors propose a novel video feature framework based on machine learning and computer vision techniques, which helps marketers predict and understand the consumption of online video from a content-based perspective. The authors apply this framework to two unique data sets: one provided by MasterClass, consisting of 771 online videos and more than 2.6 million viewing records from 225,580 consumers, and another from Crash Course, consisting of 1,127 videos focusing on more traditional education disciplines. The analyses show that the framework proposed in this article can be used to accurately predict both individual-level consumer behavior and aggregate video popularity in these two very different contexts. The authors discuss how their findings and methods can be used to advance management and marketing research with unstructured video data in other contexts such as video marketing and entertainment analytics.
1	As consumers move through their decision journey, they adopt different goals (e.g., transactional vs. informational). In this research, the authors propose that consumer goals can be detected through textual analysis of online search queries and that both marketers and consumers can benefit when paid search results and advertisements match consumer search–related goals. In bridging construal level theory and textual analysis, the authors show that consumers at different stages of the decision journey tend to assume different levels of mental construal, or mindsets (i.e., abstract vs. concrete). They find evidence of a fluency-driven matching effect in online search such that when consumer mindsets are more abstract (more concrete), consumers generate textual search queries that use more abstract (more concrete) language. Furthermore, they are more likely to click on search engine results and ad content that matches their mindset, thereby experiencing more search satisfaction and perceiving greater goal progress. Six empirical studies, including a pilot study, a survey, three lab experiments, and a field experiment involving over 128,000 ad impressions provide support for this construal matching effect in online search.
1	Using in-store ambulatory eye-tracking, the authors investigate the extent to which lateral and vertical biases drive consumers’ attention in a grocery store environment. The data set offers a complete picture of both where the shopper is located and the shopper’s field of view and visual fixations during the trip. The authors address two research questions: First, do shoppers have a higher propensity to pay attention to products on their left or right side as they traverse an aisle (i.e., is the right side the “right” side)? Second, do shoppers tend to pay more attention to products at their eye level (i.e., is eye level “buy level”)? The authors utilize the exogenous variations in the direction by which shoppers traverse an aisle to identify lateral bias. The exogenous variation of shoppers’ eye-level positions is used to identify vertical bias. The authors find that shoppers pay more attention to products on their right side when traversing an aisle. Contrary to many practitioners’ belief, eye level is not “buy level”; rather, the product level that has the greatest propensity to capture shoppers’ attention is approximately 14.7 inches below eye level (which is around chest level).
1	The author proposes a topic model tailored to the study of creative documents (e.g., academic papers, movie scripts), which extends Poisson factorization in two ways. First, the creativity literature emphasizes the importance of novelty in creative industries. Accordingly, this article introduces a set of residual topics that represent the portion of each document that is not explained by a combination of common topics. Second, creative documents are typically accompanied by summaries (e.g., abstracts, synopses). Accordingly, the author jointly models the content of creative documents and their summaries, and captures systematic variations in topic intensities between the documents and their summaries. This article validates and illustrates the model in three domains: marketing academic papers, movie scripts, and TV show closed captions. It illustrates how the joint modeling of documents and summaries provides some insight into how people summarize creative documents and enhances understanding of the significance of each topic. It shows that the model described produces new measures of distinctiveness that can inform the perennial debate on the relation between novelty and success in creative industries. Finally, the author shows how the proposed model may form the basis for decision support tools that assist people in writing summaries of creative documents.
1	Smartphones have made it nearly effortless to share images of branded experiences. This research classifies social media brand imagery and studies user response. Aside from packshots (standalone product images), two types of brand-related selfie images appear online: consumer selfies (featuring brands and consumers’ faces) and an emerging phenomenon the authors term “brand selfies” (invisible consumers holding a branded product). The authors use convolutional neural networks to identify these archetypes and train language models to infer social media response to more than a quarter-million brand-image posts (185 brands on Twitter and Instagram). They find that consumer-selfie images receive more sender engagement (i.e., likes and comments), whereas brand selfies result in more brand engagement, expressed by purchase intentions. These results cast doubt on whether conventional social media metrics are appropriate indicators of brand engagement. Results for display ads are consistent with this observation, with higher click-through rates for brand selfies than for consumer selfies. A controlled lab experiment suggests that self-reference is driving the differential response to selfie images. Collectively, these results demonstrate how (interpretable) machine learning helps extract marketing-relevant information from unstructured multimedia content and that selfie images are a matter of perspective in terms of actual brand engagement.
1	This project investigates emotionality by brands on social media. First, a field data set of over 200,000 text and images posts by brands across two major platforms is analyzed. Using recent automated text analysis (Study 1a) and computer vision methods (Studies 1b and 1c), the author provides initial documentation of a negative relationship between brand emotionality and status. Exploring this relationship further, in Studies 2, 3, and 4, the author finds that brands can leverage this association, reducing emotionality in brand communications to increase perceived brand status. This strategy is effective because reduced emotionality is associated with high-status communication norms, which evoke high-status reference groups. This finding is moderated by the status context of the brand (Study 2) and the product type (Study 4).
1	Massive open online courses (MOOCs) have the potential to democratize education by improving access. Although retention and completion rates for nonpaying users have not been promising, these statistics are much brighter for users who pay to receive a certificate upon completing the course. We investigate whether paying for the certificate option can increase engagement with course content. In particular, we consider two effects: (1) the certificate effect, which is the boost in motivation to stay engaged to receive the certificate; and (2) the sunk-cost effect, which arises solely because the user paid for the course. We use data from over 70 courses offered on the Coursera platform and study the engagement of individual participants at different milestones within each course. The panel nature of the data enables us to include controls for intrinsic differences between nonpaying and paying users in terms of their desire to stay engaged. We find evidence that the certificate and sunk-cost effects increase user engagement by approximately 8%–9% and 17%–20%, respectively. Whereas the sunk-cost effect is transient and lasts for only a few weeks after payment, the certificate effect lasts until the participant reaches the grade required to be eligible to receive the certificate. We discuss the implications of our findings for how platforms and content creators may design course milestones and schedule payment of course fees. Given that greater engagement tends to improve learning outcomes, our study serves as an important first step in understanding the role of prices and payment in enabling MOOCs to realize their full potential.
1	Online educational platforms increasingly allow learners to consume content at their own pace with on-demand formats, in contrast to the synchronous content of traditional education. Thus, it is important to understand and model learner engagement within these environments. Using data from four business courses hosted on Coursera, the authors model learner behavior as a two-stage decision process, with the first stage determining across-day continuation (vs. quitting) and the second stage determining within-day choices among lectures, quizzes, and breaks. By modeling the heterogeneity across learners pursuing lecture and quiz completion goals, the authors capture different patterns of consumption that correspond to extant theories of goal progress within an empirical field setting. They find that most individuals exhibit a learning style whereby lecture utility changes as an inverted U-shaped function of current progress. This model may also be used as an early detection system to anticipate changes in engagement, and it enables the authors to relate learning styles to final performance outcomes and enrollment in additional courses. Finally, the authors examine how quiz-taking varies across learners in different courses and between those who have paid versus not paid for the option to earn a course certificate.
1	In recent years, online learning platforms (e.g., Coursera, edX) have experienced massive growth and have reached nearly 200 million learners. Although their reach is quite large, the impact of these platforms is constrained by a low level of learner engagement. In traditional face-to-face classrooms, educators aim to engage learners by asking them to participate in class discussions and share information about their identity and ideas. However, the effectiveness of these strategies in online learning platforms is uncertain. The authors examine this issue by assessing the impact of two different types of content sharing on learner engagement. The authors conduct a textual analysis of over 12,000 text postings during an 18-month period (Study 1) and a field experiment among over 2,000 learners (Study 2) in a popular Coursera offering by a large U.S. university. The results indicate that asking learners to share ideas (vs. their identity) has a stronger effect on their video consumption and assessment completion. The authors explain this “idea advantage” by suggesting that learners who share ideas (vs. identity) exhibit a greater degree of elaboration. This idea advantage is strongest for learners from English-speaking countries and those new to online learning.
1	Despite a rising interest in artificial intelligence (AI) technology, research in services marketing has not evaluated its role in helping firms learn about customers’ needs and increasing the adaptability of service employees. Therefore, the authors develop a conceptual framework and investigate whether and to what extent providing AI assistance to service employees improves service outcomes. The randomized controlled trial in the context of tutoring services shows that helping service employees (tutors) adapt to students’ learning needs by providing AI-generated diagnoses significantly improves service outcomes measured by academic performance. However, the authors find that some tutors may not utilize AI assistance (i.e., AI aversion), and factors associated with unforeseen barriers to usage (i.e., technology overload) can moderate its impact on outcomes. Interestingly, tutors who significantly contribute to the firm's revenue relied heavily on AI assistance but unexpectedly benefited little from AI in improving service outcomes. Given the wide applicability of AI assistance in a variety of services marketing contexts, the authors suggest that firms should consider the potential difficulties employees face in using the technology rather than encourage them to use it as it is.
1	Crowdfunding has emerged as a market-based solution to give frontline complex public service employees the opportunity to acquire resources by advertising project proposals for donor patrons on crowdfunding platforms. However, whether crowdfunded resources can improve offline service outcomes, and if so, how and when, remains murky. Focusing on the context of public education crowdfunding and applying theories from crowdfunding and services marketing literature, the authors conceptualize that the combination of two factors—namely, teachers’ request for resources meant to satisfy unmet heterogeneous (i.e., diverse and evolving) intellectual needs of students and donors’ screening and approval (i.e., crowd screening) of promising projects—helps improve student academic achievement. Collating novel panel data from DonorsChoose and California Department of Education, the authors show that (1) crowdfunded resources positively affect student academic achievement, (2) student academic achievement improves with the increase in the heterogeneity of intellectual needs that crowdfunded resources likely satisfy, (3) crowd screening of project proposals plays a critical role in the offline effectiveness of crowdfunded resources, and (4) crowd screening effectiveness depends on the type of project.
1	Marketization—the entry of the market logic into a field originally insulated from it—is a transformative force that has reshaped many fields, including education, health care, the arts, and religion. Marketization brings a unique set of challenges for established organizations: it opens a field to market-style mechanisms of consumer choice and competition, which undermines the legitimacy of established organizations, and it creates contradictory demands for organizational actions. How can established organizations adapt to marketization? To answer this question, the authors study the adaptation of five established religious schools to the marketization of education in Brazil. They develop the novel hybridization strategy of nested coupling and explain that established organizations respond to marketization by balancing competing demands for differentiation and conformity. The authors show how religious schools nest the market logic within the religious logic by reconfiguring their resources to conform to market demands while differentiating themselves through their religious orientation. Nested coupling provides a novel strategic approach for established organizations in marketized or marketizing fields, such as hospitals, museums, and schools, to capitalize on a logic that preexists marketization and to create a unique competitive positioning in the market.
1	Student loans defer the cost of college until after graduation, allowing many students access to higher lifetime earnings and colleges and universities they otherwise could not afford. Even with student loans, however, the authors find that students psychologically realize the financial costs of a college education long before their loan repayments begin. This early cost realization frames financial decisions between most pairs of colleges as an intertemporal trade-off. Students choose between investments with (1) smaller short-term costs but smaller long-term returns (a lower-cost, lower-return [LC-LR] college) and (2) larger short-term costs but larger long-term returns (a higher-cost, higher-return [HC-HR] college). The authors find that early cost realization increases preferences for LC-LR colleges—preferences that could reduce lifetime earnings—in both simulations and experiments. Preferences for LC-LR colleges are pronounced among financially impatient students and in choice pairs of LC-LR and HC-HR colleges where the equilibrium is set at a low-discount-rate threshold. A return-on-investment strategy, future uncertainty, and debt aversion cannot explain these results. A decision aid synchronizing the psychological realization of costs and benefits reduced preferences for LC-LR colleges, illustrating that the preference is constructed and receptive to interventions.
1	Front-of-package and on-shelf nutrition labeling systems in supermarkets have been shown to lead to only modest increases in the purchase of more nutritious foods. Educational campaigns may increase the use of these types of product labels if (1) there is a lack of consumer awareness and/or understanding of the labels, and (2) the information provided leads consumers to prefer different products. The authors study a large-scale national campaign for the Guiding Stars nutrition labels conducted by a grocery retailer in Canada that implemented the labels. Using detailed household transaction data, the authors find only a small increase in the purchase of higher star–rated foods during the campaign, driven by produce purchases, and 60% of the effect disappears after the campaign’s conclusion. Exit surveys were conducted outside of stores before and after the campaign to explain the limited response. Awareness and understanding of the nutrition labeling system increased marginally after the campaign, but there was no increase in self-reported use.
1	One of the greatest challenges in education marketing is designing effective marketing messages, especially when targeting consumers with different cultural backgrounds. This research examines the impact of power distance belief (PDB) on the persuasiveness of affective appeal versus cognitive appeal in education marketing messages. The authors theorize that low-PDB consumers tend to prefer education products presented with affective appeal because of their process learning mindset, which focuses on self-discovery and self-development. By contrast, high-PDB consumers tend to prefer education products presented with cognitive appeal because of their outcome learning mindset, which focuses on acquiring skills and social/economic gains relevant to such skills. These effects were supported by converging results from four experiments, a field study, and a content analysis across 37 countries using a wide range of education products and services. This research contributes to the literature on PDB, education, and cross-cultural consumer behavior and provides guidelines for global education marketers.
1	This research documents systematic gender performance differences (GPD) at a top business school using a unique administrative data set and survey of students. The findings show that women’s grades are 11% of a standard deviation lower in quantitative courses than those of men with similar academic aptitude and demographics, and men’s grades are 23% of a standard deviation lower in nonquantitative courses than those of comparable women. The authors discuss and test for different reasons to explain this finding. They show that a female instructor significantly cuts down GPD for quantitative courses by raising the relative grades of female students. In addition, female instructors increase women’s interest and performance expectations in these courses and are perceived as role models by their female students. These results provide support for a gender stereotype process for GPD and show that faculty can serve as powerful exemplars to challenge gender stereotypes and increase student achievement. The authors discuss several important implications of these findings for business schools and for society.
1	Curriculum is at the core of school quality. Curriculum changes often attempt to cater to local preferences while adhering to national standards. This tension often drives a school’s decision to invest in curriculum changes even though little is known about how such changes affect student performance. To examine these interrelated issues of product quality and performance in the education sector, the authors analyze the effect of the 2008 Louisiana Science Education Act on students’ science test performance in nationally administered tests. The law allowed the teaching of creationism as an alternative “theory” to evolution in Louisiana schools. Using detailed data on Louisiana schools, the authors employ a difference-in-differences strategy to document that science test achievement declined after the law was passed, relative to schools in neighboring Texas. The effect of the law was driven by regions with high internet penetration and low parental education levels. After the change in policy, Louisiana students were more likely to seek out information on the internet using search terms that led them to web pages that reinforced a creationist message.
1	The authors use data from a field study concerning an online salesperson training program to investigate (1) the overall impact of program participation on sales performance for two kinds of products, “Focus” and “Other” (the direct impact); (2) heterogeneity in the impact of program participation across salespeople; and (3) spillover effect of program participation by others in the vicinity on salesperson performance (the indirect impact). The program contains short-duration training modules accessed via an online platform. Salespeople choose whether to take any module, how many modules to take, and when to take them. Results show that although training improved sales performance, the average impact of training on Other product sales was immediate, significant, and positive, and that on Focus product sales was delayed. Further, the impact of training diminishes over time. The authors find significant heterogeneity in the impact of training across salespeople and regions. Finally, the results show a mixed spillover effect of training by peers. There is a positive spillover effect on sales of the focal salesperson with an increase in the total number of trainings taken by peer salespeople, and a negative or no spillover effect with an increase in the number of peer salespeople taking training.
1	The field's knowledge of marketing-mix elasticities is largely restricted to developed countries in the North-Atlantic region, even though other parts of the world—especially the Indo-Pacific Rim region—have become economic powerhouses. To better allocate marketing budgets, firms need to have information about marketing-mix elasticities for countries outside the North-Atlantic region. The authors use data covering over 1,600 brands from 14 product categories collected in 7 developed and 7 emerging Indo-Pacific Rim countries across more than 10 years to estimate marketing elasticities for line length, price, and distribution and examine which brand, category, and country factors influence these elasticities. Averaged across brands, categories, and countries, line-length elasticity is .459, price elasticity is −.422, and distribution elasticity is .368, but with substantial variation across brands, categories, and countries. Contrary to what has been suggested in previous research, the authors find no systematic differences in marketing responsiveness between emerging and developed economies. Instead, the key country-level factor driving elasticities is societal stratification, with Hofstede's measure of power inequality (power distance) as its cultural manifestation and income inequality as its economic manifestation. As the effects of virtually all brand, category, and country factors differ across the three marketing-mix instruments, the field needs new theorizing that is contingent on the marketing-mix instrument studied.
1	Research has demonstrated that after making high goal progress, consumers feel liberated to engage in goal-inconsistent behaviors. But what happens after consumers make high progress in the context of joint goal pursuit? The authors examine how jointly made progress toward a joint goal pursued by couples affects subsequent individually made goal-relevant decisions. Across five experiments with both lab-created couples and married participants and financial data from a couples’ money management mobile app, the authors show that after making high progress on a joint goal (vs. low or no progress), partners with higher relationship power are more likely to disengage from the joint goal to pursue personal concerns (e.g., indulge themselves, pursue individual goals), whereas partners with lower relationship power do not disengage from the joint goal and continue engaging in goal-consistent actions that maintain its pursuit. The authors elucidate the underlying mechanism, providing evidence that the joint goal progress boosts the relational self-concept of high- (but not low-) relationship-power partners, and this drives the effects. Importantly, they demonstrate the effectiveness of two theory-grounded and easily implementable interventions that promote goal-consistent behaviors among high-relationship-power consumers in the context of joint savings goals.
1	Fear of escalating input prices in response to retail success is a commonly discussed phenomenon affecting supply chains. Such a ratchet effect arises when a retailer feels compelled to modify its investments to better serve the end customers in order to hide positive prospects and restrain future wholesale price hikes. In a two-period model of supply chain interactions, the authors demonstrate that such an endogenous ratchet effect can have multifaceted reverberations. A retailer fearing price hikes may be tempted to curtail near-term profits to ensure favorable long-term pricing. In response, the supplier can use deep discounts in its initial wholesale prices to convince the retailer to focus on its short-term profits rather than long-term pricing concerns. These deep discounts not only encourage mutually beneficial investments but also alleviate double marginalization inefficiencies along the supply chain. In light of these results, the authors show that a mandatory information disclosure policy to reduce the ratchet effect decreases total channel efficiency compared with no information disclosure, precisely because mandatory disclosure interrupts the healthy tension among supply chain partners. Thus, the model presents a scenario in which ratcheting concerns can create a degree of self-enforcing cooperation that results in socially beneficial responses in supply chains.
1	Retail is rapidly evolving to construct virtual environments for consumers. Online product images, videos, and virtual reality (VR) interfaces enliven consumer experiences and are a source of product information. Because consumers are unable to physically touch products in these digital environments, this research examines vicarious touch, or the observation of a hand in physical contact with a product in a digital environment. Across eight studies, the authors use images, GIFs, and VR to show that vicarious touch affects consumers’ psychological ownership and product valuation due to the active nature of product touch, which results in a felt sense of body ownership of the virtual hand. This is termed the “vicarious haptic effect.” Results demonstrate that it is not enough to show a hand in an advertisement; the hand must be touching a product. The vicarious haptic effect is strongest for people who become highly stimulated by an immersive VR experience (i.e., measured via the elevation in heart rate). The vicarious haptic effect is attenuated if the viewed interaction does not represent a diagnostic hand movement. The authors discuss theoretical and managerial implications for digital product presentation to encourage feelings of product ownership and valuation.
1	Consumers’ choices about health products are heavily influenced by public information, such as news articles, research articles, online customer reviews, online product discussion, and TV shows. Dr. Oz, a celebrity physician, often makes medical recommendations with limited or marginal scientific evidence. Although reputable news agencies have traditionally acted as gatekeepers of reliable information, they face the intense pressure of “the eyeball game.” Customer reviews, despite their authenticity, may come from deceived consumers. Therefore, it remains unclear whether public information sources can correct the misleading health information. In the context of over-the-counter weight loss products, the authors carefully analyze the cascading of information post endorsement. The analysis of extensive textual content with deep-learning methods reveals that legitimate news outlets respond to Dr. Oz's endorsement by generating more news articles about the ingredient; on average, articles after the endorsement contain higher sentiment, so news agencies seem to amplify rather than rectify the misleading endorsement. The finding highlights a serious concern: the risk of hype news diffusion. Research articles react too slowly to mitigate the problem, and online customer reviews and product discussions provide only marginal corrections. The findings underscore the importance of oversight to mitigate the risk of cascading hype news.
1	The proliferation of digital goods has led to an increased interest in how the digitization of products and services affects consumer behavior. In this article, the authors show that although consumers are willing to pay more for physical than digital goods, this difference attenuates—and even reverses—when consumers are asked to make a choice between the two product formats. This effect is explained by a contingent weighting principle: In willingness to pay, a quantitative task, consumers anchor on quantitative information (e.g., market beliefs). However, in choice, a qualitative task, consumers anchor on qualitative information (e.g., which good dominates on the most important attribute). These differences in contingent weighting result in physical goods being preferred in willingness to pay, but their digital equivalent being preferred relatively more in choice. The authors draw conclusions from ten preregistered experiments and six supplemental studies using a variety of goods in hypothetical and incentive-compatible contexts, as well as within- and between-subjects designs. The article concludes with a discussion of implications for the marketing of digital goods.
1	The authors examine whether and how ride-sharing services influence the demand for home-sharing services. Their identification strategy hinges on a natural experiment in which Uber/Lyft exited Austin, Texas, in May 2016 due to local regulation. Using a 12-month longitudinal data set of 11,536 Airbnb properties, they find that Uber/Lyft's exit led to a 14% decrease in Airbnb occupancy in Austin. In response, hosts decreased the nightly rate by $9.30 and the supply by 4.5%. The authors argue that when Uber/Lyft exited Austin, the transportation costs for most Airbnb guests increased significantly because most Airbnb properties (unlike hotels) have poor access to public transportation. The authors report three key findings: First, demand became less geographically dispersed, falling (increasing) for Airbnb properties with poor (excellent) access to public transportation. Second, demand decreased significantly for low-end properties, whose guests may be more price sensitive, but not for high-end properties. Third, the occupancy of Austin hotels increased after Uber/Lyft's exit; the increase occurred primarily among low-end hotels, which can substitute for low-end Airbnb properties. The results indicate that access to affordable, convenient transportation is critical for the success of home-sharing services in residential areas. Regulations that negatively affect ride-sharing services may also negatively affect the demand for home-sharing services.
1	The current research examines the interactive effect of consumers’ moral identity and risk factor stigma on health message effectiveness. The authors theorize that engaging in advocated health behaviors has moral associations; however, a stigmatized risk factor in a message “taints” the morality of the advocated health behavior. Thus, consumers with high (vs. low) moral identity are more likely to comply with health messages when risk factor stigma is low, and this positive moral identity effect is undermined when risk factor stigma is high. The authors test stigma's threat to moral identity by measuring defensive processing (Studies 1 and 2) and the attenuating effect of self-affirmation on the negative effect of stigma (Studies 3 and 4). They apply the stigma-by-association principle to develop and test a messaging intervention (Study 5). The studies suggest that, depending on whether a health message contains stigmatized risk factors, marketers could employ a combination of tactics such as activating moral identity, offering self-affirming message frames, and/or highlighting low-stigma risk factors to bolster message effectiveness.
1	The authors empirically examine how firms learn to set prices in a new market. The 2012 privatization of off-premises liquor sales in Washington State created a unique opportunity to observe retailers learning to set prices from the beginning of the learning process. Tracking this market as it evolved through time, the authors find that firms indeed learn to set more profitable prices, that these prices increasingly reflect demand fundamentals, and that prices ultimately converge to levels consistent with (static) profit maximization. The authors further demonstrate that initial pricing mistakes are largest for products whose demand conditions differ the most from those of previously privatized markets, that retailers with previous experience in the category are initially better informed, and that learning is faster for products with more precise sales information. These findings indicate that firm behavior converges to rational models of firm conduct, but such convergence takes time to unfold and plays out differently for different firms. These patterns suggest the important roles of firms’ learning and heterogeneous firm capabilities.
1	Consumers display an expense prediction bias in which they underpredict their future spending. The authors propose this bias occurs in large part because (1) consumers base their predictions on typical expenses that come to mind easily during prediction, (2) taken together, typical expenses lead to a prediction near the mode of a consumer's expense distribution rather than the mean, and (3) expenses display positive skew (with mode < mean). Accordingly, the authors also propose that prompting consumers to consider reasons why their expenses might be different than usual increases predictions—and therefore prediction accuracy—by bringing atypical expenses to mind. Ten studies (N = 6,044) provide support for this account of the bias and the “atypical intervention” developed to neutralize it.
1	Consumers often become “stuck in a rabbit hole” when consuming media. They may watch several YouTube videos in the same category or view several thematically similar artistic images on Instagram in a row, finding it difficult to stop. What causes individuals to choose to consume additional media on a topic that is similar to (vs. different from) what they just experienced? The authors examine a novel antecedent: the consecutive consumption of multiple similar media. After viewing multiple similar media consecutively, more consumers choose to (1) view additional similar media over dissimilar media or (2) complete a dissimilar activity entirely, even when the prior consumption pattern is externally induced. The rabbit hole effect occurs because of increased accessibility of the shared category: when a category is more accessible, people feel immersed in it and anticipate that future options within that category will be more enjoyable. The authors identify three characteristics of media consumption that contribute to the rabbit hole effect by increasing category accessibility: similarity, repetition, and consecutiveness of prior media consumption. This research contributes to literature on technology, choice, and variety seeking, and it offers implications for increasing (vs. slowing) similar consumption.
1	This article studies the impact of business modernization on the sales performance of traditional retailers in an emerging market. The authors define modernization as the adoption of physical structures and tangible practices of organized retail chains (e.g., exterior signage with store name and logo, a database to record product-level information). To address this research question, they implement a randomized field experiment in Mexico City with 1,148 retailers. The sample is randomized as follows: 385 firms are externally modernized in ways that are visible to customers, 383 firms are internally modernized in ways that are not visible to customers, and 380 firms form a control group. The authors find a significant and persistent main effect of modernization on sales. Firms in both treatment groups increase monthly sales by 15%–19% 24 months after study recruitment. In terms of novel mechanism evidence, externally modernized firms improve their store-level branding, while internally modernized firms strengthen their product management. This article also provides an exploratory analysis to guide policy makers, managerial stakeholders, and retailers on how to modernize for the highest returns. This analysis suggests that it is most useful to modernize a firm’s exterior appearance, customer engagement methods, demand analysis, and stock-ordering processes. Finally, the largest improvements come from the initial few modernizing changes, after which returns are diminishing.
1	Many consumers engage in frequent consumption indulgences. Because such indulgences accumulate resource costs (e.g., money, calories), consumers are often prompted to cut back, posing questions for how to design cutback programs with consumer appeal. This research distinguishes between frequent indulgences that consumers think of as social (vs. solitary), demonstrating that thinking of an indulgence as social (vs. solitary) decreases preferences to cut “frequency” (how often the indulgence occasion occurs) and increases preferences to cut “intensity” (choosing a within-category substitute that involves lower resource expenditure). The author explains these effects by differentiating between enjoyment from the product itself and enjoyment from aspects outside the product. Thinking of an indulgence as social (vs. solitary) heightens people’s anticipated enjoyment, particularly for aspects outside of the product, decreasing interest in cutting the number of occasions (cutting frequency) and increasing interest in cutting back on the product itself via a within-category substitute (cutting intensity). This divergence in cutback preferences for social (vs. solitary) experiences is thus eliminated (1) when consumers think of social experiences with distant (vs. close) others, which involve lower enjoyment outside of the product, or (2) when solitary experiences primarily involve heightened enjoyment for aspects outside of the product.
1	Online retail search traffic is often concentrated at a “prominent” retailer for a product. The authors unpack the ramifications of this pattern on pricing, profit, and consumer welfare in an intrabrand setting. Prominence denotes a larger number of heterogenous-search-cost consumers starting their search at the prominent retailer than at any other retailer. These analyses show that search traffic concentration can intensify intrabrand competition, lower average prices of all retailers, and improve consumer welfare. Interestingly, the prominent retailer's incremental traffic advantage can increase or reduce its own profit; the authors denote these as the “blessing” and “curse” of prominence, respectively. The authors extend their analysis to a setting where consumers consider searching only among those retailers they are individually aware of; the prominent retailer is included in all these individual awareness sets. The effects on market average prices and welfare carry over, but only below a critical threshold level of the prominent retailer's first-search traffic advantage. Above this threshold, market average prices rise and welfare decreases, making this the region where search concentration warrants scrutiny from policy makers. The authors close with policy remedies and managerial implications of search concentration.
1	Fake news advertising—advertising that mimics legitimate news articles—can be harmful if it misleads consumers to take actions they otherwise would not have taken (e.g., purchase an inferior product). However, little is known about whether fake news ads bring in new customers or are merely viewed by people already in the market for the advertised products. The author exploits a Federal Trade Commission–enabled shutdown of fake news advertisements for various products such as acai cleanses and teeth whiteners (but where the product sites continued to remain operational) to identify the extent of consumer interest in the presence and absence of fake news advertising. The findings indicate that interest wanes after the shutdown of fake news advertising, with the probability of a product site not receiving any new visits increasing by 22%. The overall decline in visits caused by the absence of fake news ads occurs despite some substitution by consumers to regular advertisements.
1	Given the increasing importance of the global sharing economy, consumers face the decision of whether to choose an access-based option versus an owned option. However, understanding of how consumers’ global–local identity may influence their preference for access-based consumption is rather limited. The authors fill this knowledge gap by proposing that consumers high in global identity (“globals”) have a greater preference for access-based consumption than those high in local identity (“locals”). Such effects are mediated by consumers’ consumption openness. Consistent with the “consumption openness” account, the authors find that when the desire for openness is enhanced by a contextual cue, locals’ preference for access-based consumption is elevated, whereas globals’ preference for access-based consumption is unaffected. However, when the desire for openness is suppressed by a contextual cue, globals’ preference for access-based consumption is reduced, whereas locals’ preference for access-based consumption is unaffected. Similarly, consumers’ traveler–settler orientation sets a boundary for the relationship between global–local identity and preference for access-based consumption, given its close association with consumption openness. Theoretical and managerial implications are discussed.
1	Retailers routinely present a posted or sale price together with a higher advertised reference price, in an effort to evoke a perception of the discount the consumer is receiving. However, if prices can be negotiated, what impact does this initial perceived discount (IPD) have on the ultimate discount, demand, and revenue? With data from consumers of a large durable goods retailer, in a natural decision-making environment, this study provides evidence that a greater IPD is associated with smaller negotiated discounts. Then, a lab experiment involving negotiation and purchase decisions for multiple products, with randomly assigned values of the IPD, establishes that a $1 increase in IPD lowers the negotiated discount by 5.7 cents. Furthermore, 55% of this decrease can be attributed to reduction in the participants’ likelihood to initiate a negotiation. Under bargaining, almost one-third of the increase in revenue from a higher IPD stems from an increase in the negotiated price, which is unlike fixed pricing, in which setting an increase in IPD affects revenue only through changes in demand. Finally, the optimal advertised reference prices a seller would post under bargaining and fixed pricing are similar, but the benefit from posting this price is significantly higher under bargaining. These findings in turn have implications for researchers, retailers, consumers and policy makers.
1	The authors address two significant challenges in using online text reviews to obtain fine-grained, attribute-level sentiment ratings. First, in contrast to methods that rely on word frequency, they develop a deep learning convolutional–long short-term memory hybrid model to account for language structure. The convolutional layer accounts for spatial structure (adjacent word groups or phrases), and long short-term memory accounts for the sequential structure of language (sentiment distributed and modified across nonadjacent phrases). Second, they address the problem of missing attributes in text when constructing attribute sentiment scores, as reviewers write about only a subset of attributes and remain silent on others. They develop a model-based imputation strategy using a structural model of heterogeneous rating behavior. Using Yelp restaurant review data, they show superior attribute sentiment scoring accuracy with their model. They identify three reviewer segments with different motivations: status seeking, altruism/want voice, and need to vent/praise. Surprisingly, attribute mentions in reviews are driven by the need to inform and vent/praise rather than by attribute importance. The heterogeneous model-based imputation performs better than other common imputations and, importantly, leads to managerially significant corrections in restaurant attribute ratings. More broadly, the results suggest that social science research should pay more attention to reducing measurement error in variables constructed from text.
1	In this article, the authors examine how historical price information influences consumers’ decision to defer a purchase. They focus on two aspects of historical price information: the direction and the frequency of past price changes. The authors advance a theoretical framework postulating that the interaction between these two factors shapes consumers’ decisions to buy now versus later. Controlling for the total magnitude of price changes, the authors propose that consumers are more likely to defer purchase when the price of the product has previously increased compared with when the price has decreased. Importantly, the authors hypothesize that this effect is more pronounced when consumers observe a single large change in price (e.g., a decrease of $100 vs. an increase of $100) compared with when they observe multiple smaller changes that establish a trend (e.g., four decreases of $25 vs. four increases of $25). The authors argue that these effects are driven by differences in consumers’ expectations about future prices. They test their predictions, as well as two moderators of the proposed effects—the monotonicity and the timing of price changes—in six well-powered preregistered experimental studies (N = 5,713) using both hypothetical and actual purchases.
1	Durable goods often come bundled with limited-time and sometimes generous factory/base warranties. Yet, for many durable goods, customers purchase extended warranties against which they rarely make claims. This study offers a reference-dependent-preferences-based theoretical explanation for why consumers purchase extended warranties even if their purchased good is already covered by a base warranty. The authors show that consumers treat base warranties as a reference point, thereby creating a qualitative difference in the valuation of an extended warranty on the purchased product. The proposed theory model is validated using observational data from the automobile industry and shows how the reference-dependent-preferences-based effect varies with vehicle quality and macroeconomic conditions. The analyses reveal auto buyers’ elevated loss-aversion motivations and higher price sensitivity during weaker macroeconomic conditions than during more robust macroeconomic conditions. Finally, the authors use the empirical model to identify opportunities for auto dealers to engage in targeted price promotions as a function of prevailing macroeconomic conditions. These findings have important implications for marketing managers, as they provide valuable guidance on when an extended warranty should be promoted, to whom it should be promoted, and which extended warranty should be marketed.
1	Marketing practices like money-back guarantees (MBGs) are prevalent in many expert-service markets but are often decried as marketing gimmicks that take advantage of vulnerable and poorly informed consumers. In this research, conducted in the market for in vitro fertilization services, the authors empirically assess differences in quality of care between clinics that offer MBG programs and those that don't, to investigate whether MBG programs can serve a purpose consistent with signaling and insurance theories. The analysis is conducted on a unique longitudinal data set that includes information on clinic-level treatment, outcome statistics, clinic characteristics, and marketing practices for fertility clinics in the United States, state-level insurance mandates, competition environments, and demographic and geographic characteristics. Using an instrumental variable approach to account for the endogeneity of MBG decisions made by fertility clinics, the authors find that MBG clinics, on average, offer better treatment outcomes in terms of success rates while undertaking lower risks. The results are consistent with signaling theory predictions that market-based programs like MBGs can serve as signals of unobservable clinic quality despite the incentives for clinics to engage in opportunistic behaviors.
1	In a range of studies across platforms, researchers have shown that online ratings are characterized by distributions with disproportionately heavy tails. The authors of this study focus on understanding the underlying process that yields such “J-shaped” or “extreme” distributions. They propose a novel theoretical mechanism behind the emergence of J-shaped distributions: differential attrition, or the idea that potential reviewers with moderate experiences are more likely to leave the pool of active reviewers than potential reviewers with extreme experiences. The authors present an analytical model that integrates this mechanism with two extant mechanisms: differential utility and base rates. They show that although all three mechanisms can give rise to extreme distributions, only the utility-based and attrition-based mechanisms can explain the authors’ empirical observation from a large-scale field experiment that an unincentivized solicitation email from an online travel platform reduces review extremity. Subsequent analyses provide clear empirical evidence for the existence of both differential attrition and differential utility.
1	Although companies increasingly are adopting algorithms for consumer-facing tasks (e.g., application evaluations), little research has compared consumers’ reactions to favorable decisions (e.g., acceptances) versus unfavorable decisions (e.g., rejections) about themselves that are made by an algorithm versus a human. Ten studies reveal that, in contrast to managers’ predictions, consumers react less positively when a favorable decision is made by an algorithmic (vs. a human) decision maker, whereas this difference is mitigated for an unfavorable decision. The effect is driven by distinct attribution processes: it is easier for consumers to internalize a favorable decision outcome that is rendered by a human than by an algorithm, but it is easy to externalize an unfavorable decision outcome regardless of the decision maker type. The authors conclude by advising managers on how to limit the likelihood of less positive reactions toward algorithmic (vs. human) acceptances.
1	Mobile app users are often exposed to a sequence of short-lived marketing interventions (e.g., ads) within each usage session. This study examines how an increase in the variety of ads shown in a session affects a user's response to the next ad. The authors leverage the quasi-experimental variation in ad assignment in their data and propose an empirical framework that accounts for different types of confounding to isolate the effects of a unit increase in variety. Across a series of models, the authors consistently show that an increase in ad variety in a session results in a higher response rate to the next ad: holding all else fixed, a unit increase in variety of the prior sequence of ads can increase the click-through rate on the next ad by approximately 13%. The authors then explore the underlying mechanism and document empirical evidence for an attention-based account. The article offers important managerial implications by identifying a source of interdependence across ad exposures that is often ignored in the design of advertising auctions. Furthermore, the attention-based mechanism suggests that platforms can incorporate real-time attention measures to help advertisers with targeting dynamics.
1	Drawing from a content analysis of publicly traded companies’ privacy notices, a survey of managers, a field study, and five online experiments, this research investigates how consumers respond to privacy notices. A privacy notice, by placing legally enforceable limits on a firm's data practices, communicating safeguards, and signaling transparency, might be expected to promote confidence that personal data will not be misused. Indeed, most managers expected a privacy notice to make customers feel more secure (Study 1). Yet, consistent with the analogy that bulletproof glass can increase feelings of vulnerability despite the protection offered, formal privacy notices undermined consumer trust and decreased purchase interest even when they emphasized objective protection (Studies 2, 3, and 5) or omitted any mention of potentially concerning data practices (Study 6). These unintended consequences did not occur, however, when consumers had an a priori reason to be distrustful (Study 4) or when benevolence cues were added to privacy notices (Studies 5 and 6). Finally, Study 7 showed that both the presence and conspicuous absence of privacy information are sufficient to trigger decreased purchase intent. Together, these results provide actionable guidance to managers on how to effectively convey privacy information (without hurting purchase interest).
1	Firms use aggregate data from data brokers (e.g., Acxiom, Experian) and external data sources (e.g., Census) to infer the likely characteristics of consumers in a target list and thus better predict consumers’ profiles and needs unobtrusively. The authors demonstrate that the simple count method most commonly used in this effort relies implicitly on an assumption of conditional independence that fails to hold in many settings of managerial interest. They develop a Bayesian profiling introducing different conditional independence assumptions. They also show how to introduce additional observed covariates into this model. They use simulations to demonstrate that in managerially relevant settings, the Bayesian method will outperform the simple count method, often by an order of magnitude. The authors then compare different conditional independence assumptions in two case studies. The first example estimates customers’ age on the basis of their first names; prediction errors decrease substantially. In the second example, the authors infer the income, occupation, and education of online visitors of a marketing analytic software company based exclusively on their IP addresses. The face validity of the predictions improves dramatically and reveals an interesting (and more complex) endogenous list-selection mechanism than the one suggested by the simple count method.
1	Credit card minimum payments are designed to ensure that individuals pay down their debt over time, and scheduling minimum automatic repayments helps people avoid forgetting to repay. Yet minimum payments have additional, unintended psychological default effects by drawing attention away from the card balance due. First, once individuals set the minimum automatic repayment as the default, they then neglect to make the occasional larger repayments they made previously. As a result, individuals incur considerably more credit card interest than late payment fees avoided. Using detailed transaction data, the authors show that approximately 8% of all of the interest ever paid is due to this effect. Second, manual credit card payments are lower when individuals are prompted with minimum payment information. In an experiment, the authors test two new interventions to mitigate this effect—a prompt for full repayment and a prompt asking those repaying little to pay more—yielding large counter effects. Thus, shrouding the minimum payment option for automatic and manual payments and directing attention to the full balance may remedy the unintended effects of default minimum payments.
1	In many sectors of the entertainment industry, a few employees attract the public spotlight when performing the core service. For example, in professional team sports, a team of players competes in games, and in TV shows, a cast of artists acts in different episodes. These employees, coined “spotlight personnel,” are an essential but expensive element of ongoing service delivery. Despite their importance and cost, very little is known about how changes in spotlight personnel affect service performance and demand. To address this gap, this article uses unique data on professional German soccer teams, tracking the quantity (number of players) and quality (average transfer price) of spotlight personnel hiring (incoming transfers) and turnover (outgoing transfers), objective service performance (winning percentage), and demand (ticket sales) across four decades, using both traditional and novel time series methods. The results show that service performance and demand are primarily affected by spotlight personnel hiring rather than by turnover. Hiring quantity decreases service performance yet increases demand, whereas hiring quality benefits both service performance and demand. The analysis further uncovers that these effects are subject to dynamic interactions and nonlinearities. Investment scenarios showcase how understanding these effects can substantially improve managerial decision making.
1	Seven experiments (total N = 3,509) and a large field data set (N = 1,820,671) demonstrate that time periods of equal duration are not always perceived as equivalent. The authors find that periods feel longer when they span more time categories (e.g., hour, month). For example, periods like 1:45 p.m.–3:15 p.m. and March 31–April 6 (boundary-expanded) feel longer than, say, 1:15 p.m.–2:45 p.m. and April 2–April 8 (boundary-compressed). Reflecting this, participants anticipated completing more work during boundary-expanded periods than during equivalent boundary-compressed periods. This effect appears to result from the salience and placement of time boundaries. Consequently, participants preferred scheduling pleasant activities for boundary-expanded periods and unpleasant activities for boundary-compressed periods. Moreover, participants were willing to pay more to avoid—and required more money to endure—a long wait when that wait was presented as boundary-expanded. Finally, data from more than 1.8 million rideshare trips suggest that consumers are more likely to choose independent rides (e.g., UberX) when they are boundary-compressed when the alternative shared option (e.g., UberPool) is boundary-expanded. Together, our studies reveal that time periods feel longer when they span more boundaries and that this phenomenon shapes consumers’ scheduling and purchasing decisions.
1	A trend reported by both academics and practitioners is that advertising on TV has become increasingly energetic. This study investigates the association between the energy level in ad content and consumers’ ad-tuning tendency. Using a data set of over 27,000 TV commercials delivered to U.S. homes between 2015 and 2018, the authors first present a framework to algorithmically measure the energy level in ad content from the video of ads. This algorithm-based measure is then compared with human-perceived energy levels showing that the measure is related to the level of arousal stimulated by ad content. By relating the energy levels in ad content with the ad-tuning tendency using two empirical procedures, the authors document the following: overall, more energetic commercials are more likely to be tuned in or less likely to be avoided by viewers. The positive association between energy levels in ad content and ad tuning is statistically significant after controlling for placement and other aspects of commercials. However, the association varies across product categories and program genres. The main implication of this study is that advertisers should pay attention to components of ad content other than loudness, which has been regulated by law.
1	This article proposes a panel data generalization for a recently suggested instrumental variable‐free estimation method that builds on joint estimation. The author shows how the method can be extended to linear panel models by combining fixed-effects transformations with the common generalized least squares transformation to allow for heterogeneous intercepts. To account for between-regressor dependence, the author proposes determining the joint distribution of the error term and all explanatory variables using a Gaussian copula function, with the distinction that some variables are endogenous and the others are exogenous. The identification does not require any instrumental variables if the regressor–error relation is nonlinear. With a normally distributed error, nonnormally distributed endogenous regressors are therefore required. Monte Carlo simulations assess the finite sample performance of the proposed estimator and demonstrate its superiority to conventional instrumental variable estimation. A specific advantage of the proposed method is that the estimator is unbiased in dynamic panel models with small time dimensions and serially correlated errors; therefore, it is a useful alternative to generalized-method-of-moments-style instrumentation. The practical applicability of the proposed method is demonstrated via an empirical example.
1	This article develops the first structural model of organizational buying to study innovation diffusion in business-to-business markets. The model is particularly applicable for routinized exchange relationships, whereby centralized buyers periodically evaluate and choose contracts and then downstream users order items on contracted terms. The model captures different utility trade-offs for users and buyers while accounting for how buyer and user choices interact to influence user adoption/usage and buyer contracting. Further, the authors consider the dynamics induced by share-of-wallet (SOW) pricing contracts, commonly used in business-to-business markets to reward customer loyalty with discounts for buying more than a threshold share from a supplier. The authors assemble novel panel data on surgeons’ product usage, SOW contracts, contract switching, and hospital characteristics. They find two segments of hospitals in terms of the relative power of surgeons and buyers: a buyer-centric segment and a surgeon-centric segment. Further, they find that innovations diffuse faster in teaching hospitals and when surgical procedures are concentrated among a few surgeons. Finally, the authors answer questions such as these: Should the marketer focus on push (buyer-focused) or pull (user-focused) strategies? Do SOW contracts hurt the innovations of smaller firms? Surprisingly, the authors find that SOW contracts can help speed the diffusion of major innovations from smaller players.
1	Swearing can violate norms and thereby offend consumers. Yet the prevalence of swear word use suggests that an offensiveness perspective may not fully capture their impact in marketing. This article adopts a linguistic perspective to develop and test a model of how, why, and when swear word use affects consumers in online word of mouth. In two field data sets and four experiments, the authors show that relative to reviews with no swear words, or with non-swear-word synonyms (e.g., super), reviews with swear words (e.g., damn) impact review readers. First, reviews with swear words are rated as more helpful. Second, when a swear word qualifies a desirable [undesirable] product attribute, readers’ attitudes toward the product increase [decrease] (e.g., “This dishwasher is damn quiet [loud]!”). Swear words impact readers because they convey meaning about (1) the reviewer and (2) the topic (product) under discussion. These two meanings function as independent, parallel mediators that drive the observed effects. Further, these effects are moderated by swear word number and style: they do not emerge when a review contains many swear words and are stronger for uncensored and euphemistic swear words (e.g., darn) than censored swear words (e.g., d*mn). Overall, swear words in reviews provide value to readers—and review platforms—because they efficiently and effectively convey two meanings.
1	Consumers may need help using an inherently complex product after purchase. This article studies a manufacturer's and a retailer's incentives to provide presales service and after-sales support in a distribution channel. The authors consider a model in which a manufacturer makes wholesale price and channel service decisions. Subsequently, a retailer makes retail price and channel service decisions. They find that, in equilibrium, both channel members provide presales service. If the fixed-cost investment needed to enhance the effectiveness of after-sales support is small, the manufacturer lets the retailer provide after-sales support. Yet when it is above a threshold and the retailer becomes unwilling to invest in providing after-sales support, the manufacturer steps in and does so. As expected, when the fixed cost is too large, the manufacturer also opts out of providing after-sales support. Interestingly, when the retailer provides after-sales support, the level of presales service and the demand for after-sales support can simultaneously be the highest among all configurations. Finally, the authors demonstrate the robustness of their main results by studying alternative channel service configurations.
1	It is common in moderation analysis that at least one of the target moderation variables is latent and measured with measurement error. This article compares six methods for latent moderation analysis: multigroup, means, corrected means, factor scores, product indicators, and latent product. It reviews their use in marketing research, describes their assumptions, and compares their performance with Monte Carlo simulations. Several recommendations follow from the results. First, although the means method is the most frequently used method in the review (95% of articles), it should only be used when reliabilities of the moderation variables are close to 1, which is rare. Then, all methods except the multigroup method perform similarly well. Second, the results support using the factor scores method and latent product method when reliabilities are smaller than 1. These methods perform best with parameter and standard error bias less than or equal to 5% under most investigated conditions. Third, specific settings can warrant using the multigroup method (if the moderator is discrete), the corrected means method (if moderation variables are single indicators), and the product indicators method (if indicators are nonnormally distributed). Practical guidelines and sample code for four statistical platforms (SPSS, Stata, R, and Mplus) are provided.
1	Researchers and practitioners in marketing, economics, and public policy often use preference elicitation tasks to forecast real-world behaviors. These tasks typically ask a series of similarly structured questions. The authors posit that every time a respondent answers an additional elicitation question, two things happen: (1) they provide information about some parameter(s) of interest, such as their time preference or the partworth for a product attribute, and (2) the respondent increasingly “adapts” to the task—that is, using task-specific decision processes specialized for this task that may or may not apply to other tasks. Importantly, adaptation comes at the cost of potential mismatch between the task-specific decision process and real-world processes that generate the target behaviors, such that asking more questions can reduce external validity. The authors used mouse and eye tracking to trace decision processes in time preference measurement and conjoint choice tasks. Respondents increasingly relied on task-specific decision processes as more questions were asked, leading to reduced external validity for both related tasks and real-world behaviors. Importantly, the external validity of measured preferences peaked after as few as seven questions in both types of tasks. When measuring preferences, less can be more.
1	Environmental sustainability is a common practice of global brands, with 90% of the top 100 Interbrand global brands making statements about environmental efforts on their websites. In this research, the authors explore how a consumer’s global–local identity can affect consumer engagement with a global brand’s environmental sustainability initiative. Specifically, they examine consumer engagement in response to environmental messaging based on regulatory focus, spatial construal, and temporal construal. They theorize and find, across six experimental studies, that consumers with a strong global identity are more engaged with environmental sustainability initiatives when messaging includes frames congruent with their global identity, specifically promotion frames coupled with distant spatial frames and with proximal temporal frames. For consumers with a local identity, these regulatory and construal messaging frames do not impact consumer engagement with environmental sustainability initiatives. Consumer environmental mindset mediates the effect of global–local identity on consumer engagement with environmental sustainability initiatives when such congruent frames are used, and consumer eagerness to act provides additional process explanation for the asymmetric spatial (distant) and temporal (proximal) construal effects. The findings have significant implications for the design of global brand and environmental policy messaging, particularly for consumers with a strong global identity.
1	Ad blockers allow users to browse websites without viewing ads. Online news publishers that rely on advertising income tend to perceive users’ adoption of ad blockers purely as a threat to revenue. Yet, this perception ignores the possibility that avoiding ads—which users presumably dislike—may affect users’ online news consumption behavior in positive ways. Using 3.1 million visits from 79,856 registered users on a news website, this research finds that ad blocker adoption has robust positive effects on the quantity and variety of articles users consume. Specifically, ad blocker adoption increases the number of articles that users read by 21.0%–43.2%, and it increases the number of content categories that users consume by 13.4%–29.1%. These effects are stronger for less-experienced users of the website. The increase in news consumption stems from increases in repeat visits to the news website, rather than in the number of page impressions per visit. These postadoption visits tend to start from direct navigation to the news website, rather than from referral sources. The authors discuss how news publishers could benefit from these findings, including exploring revenue models that consider users’ desire to avoid ads.
1	In today’s advanced economies, consumers are constantly exposed to an increasing number of upgraded products. This research examines consumer response to a brand's launching of an upgraded product and identifies the consumer's ownership status of a previous version of the product as a key dimension that can influence their reaction. Contrary to common intuition, the research demonstrates that while the release of an upgraded product is received positively by nonowners of a previous version, this is not always the case for owners. The authors propose that owners respond unfavorably because the new upgrade increases perceived distance between the owners and the brand as the brand progresses forward with the enhanced products. That is, when the new product replaces an existing product the consumers own, consumers perceive that the brand is moving away from them. This negative effect of an upgrade is attenuated if the owners are provided with an extra source of connection to the brand. The authors investigate this phenomenon in five studies and discuss the implications of their findings.
1	Ad exchanges, where real-time auctions for display ad impressions take place, have historically emphasized user targeting, and advertisers sometimes did not know on which sites their ads would appear; in other words, they had no ad context. More recently, some ad exchanges have been encouraging publishers to provide context information to ad buyers that would allow them to adjust their bids for ads at specific sites. This article explores the empirical effect of a change in context information provided by a private European ad exchange. Analyzing this as a quasi-experiment using difference in differences, the authors find that average revenue per impression rose when the exchange provided subdomain information to ad buyers. Thus, ad context information is important to ad buyers, and they will act on it. Revenue per impression rises for nearly all sites, which is what auction theory predicts will happen when rational buyers with heterogeneous preferences are given more information. Exceptions are sites with thin markets prior to the policy change; consistent with theory, these sites do not show a rise in prices. This study adds evidence that ad exchanges with reputable publishers, particularly smaller volume, high-quality sites, should provide ad buyers with context information, which can be done at almost no cost.
1	Trademarks play an important role in protecting intangible brand assets. However, the impact of trademark rights on brand assets has received little attention in the literature. The authors examine the impact of trademark rights on brand assets from the perspective of appropriability—that is, the firm’s ability to benefit from innovation and creativity. To ensure causal identification, they use a natural experimental context in which U.S. Supreme Court decisions on trademark rights provide an exogenous variation in strength of trademark rights. Using a database of trademarks registered in the United States and a difference-in-difference estimation approach, the authors show that, overall, trademark applications and applications to register in other categories increase when trademark rights are strengthened and decrease when trademark rights are weakened. However, trademarking responses of brands to a change in property rights are muted for design trademarks and amplified when a firm has multiple brands. The authors discuss the theoretical, substantive, and managerial implications of the findings and provide guidance for further research.
1	Low-socioeconomic-status (SES) consumers tend to be more price sensitive than their high-SES counterparts. Nonetheless, various economic-related burdens, such as mobility costs and lack of information, often hinder their ability to attend to scarcity—a phenomenon called “ghetto tax.” The current research moves a step further to show that even when very poor consumers can exert price sensitivity and are fully informed, a “psychological ghetto tax” often discourages them from doing so. Across five studies, the authors demonstrate that, relative to (1) high-SES consumers or (2) contexts of intragroup interaction, low-SES consumers are willing to pay higher prices and to accept lower-value rewards to avoid commercial settings that require intergroup interaction (e.g., poor consumers in a high-end shopping mall). This effect is driven by the poor consumers’ heightened expectations of discrimination in upscale commercial settings, a concern virtually nonexistent among wealthy consumers. Companies’ inclusion statements emphasizing customer equality and/or customer diversity can serve as safety cues against stigmatized identities and increase low-SES consumers’ price sensitivity.
1	Subscription programs have become increasingly popular among a wide variety of retailers and marketplace platforms. Subscription programs give members access to a set of exclusive benefits for a fixed fee up front. In this article, the authors examine the causal effect of a subscription program on customer behavior. To account for self-selection and identify the individual-level treatment effects, they combine a difference-in-differences approach with a generalized random forests procedure that matches each member of the subscription program with comparable nonmembers. The authors find that subscription leads to a large increase in customer purchases. The effect of subscription is economically significant, persistent over time, and heterogeneous across customers. Interestingly, only one-third of the effect on customer purchases is due to the economic benefits of the subscription program, and the remaining two-thirds is attributed to the noneconomic effect. Evidence supports that members experience a sunk cost fallacy due to the up-front payment that subscription programs entail. Finally, the authors illustrate how firms can calculate the profitability of a subscription program and discuss the implications for customer retention and subscription programs.
1	Existing research treats sales performance as a series of discrete, independent events rather than a series of sales attempts with intertemporal spillover across these attempts. This research examines whether there are systematic short-term trends (“momentum”) in sales performance. To do so, the authors use the clumpiness approach to examine the existence of sales momentum in a high-frequency call-level data set obtained from two call centers of a large European firm. They further investigate the effect of positive (negative) momentum, or the positive (negative) deviation from the long-term expected performance on subsequent sales performance. Exploiting the differences in the social environment of the call centers, the authors find that the social working environment mitigates the harmful effect of negative momentum and sustains positive momentum. Further, they demonstrate that calls made midday, early-week, and late-week boost performance by mitigating the adverse effects of negative momentum. The findings suggest that monitoring sales performance can help managers detect momentum and use timely interventions to enhance sales productivity. Managers can also leverage momentum by creating a more social working environment to optimize overall salesperson performance.
1	This research investigates when and why consumers purchase products with social costs (e.g., environmental harm). Six studies demonstrate that upper-class consumers are more likely to purchase a product with social costs when it has a higher price because they experience greater entitlement, which the authors term the “price entitlement effect,” allowing for purchase justification. In contrast, lower-class consumers do not feel entitled to purchase a product with social costs when it is higher-priced. This effect occurs because upper-class consumers tend to have a greater self-focus, with a higher price entitling them to more resources than others. Consistent with the entitlement mechanism, when egalitarian values are made salient, the price entitlement effect is mitigated, reducing upper-class consumers’ purchase of socially costly products. Notably, the price entitlement effect occurs only when products have social costs rather than for all higher-priced products. However, when the social costs of a product are severe, price entitlement does not sufficiently justify product purchase. This research provides theoretical and practical insights regarding when and why higher price entitles purchase of socially costly products, contributing to research on social class and socially responsible (vs. costly) consumption as well as choice justification.
1	This research examines the interaction effect of two dimensions of preference on social contagion: preference similarity between a consumer (i.e., who seeks recommendation) and a peer (i.e., who potentially provides recommendation) and the fit of an experience good with the consumer's preference. For empirical analyses, the authors collected rich information from Last.fm, a music social networking website, including individual users’ music play histories, friendship information, social tags (i.e., user-generated keywords associated with artists and songs), and new song profiles. The results show that consumers’ trial of a song that fits less with their preference is influenced more by peers with similar preferences. By contrast, consumers’ trial of a song that fits more with their preference is influenced more by peers with dissimilar preferences. This research enriches the understanding of the nuanced role of preference in social contagion and offers managerial implications to better leverage social dynamics.
1	People who engage in indulgent consumption often are viewed as having poor self-control. In this research, however, eight studies provide converging support that indulgent consumption can have a positive effect: signaling interpersonal warmth. Specifically, consumers who post indulgent (vs. healthy) consumption content on social media are perceived as warmer (Study 1). The effect occurs because consumers believe that indulgent consumption is what people genuinely prefer, so indulgent (vs. healthy) consumption seems more authentic, and authenticity mediates the effect of indulgent consumption on perceived warmth (Studies 2a and 2b). Providing additional support for the authenticity mechanism, the authors show that the positive effect of indulgent (vs. healthy) consumption on perceived warmth is attenuated when the indulgent content is sponsored, which casts doubt on its authenticity (Study 3). Further, sharing sharing indulgent consumption can increase the appeal of a service provider among consumers who are seeking a warm service provider, but this occurs only when the content is not sponsored (Studies 4a and 4b). Finally, the effect of sharing indulgent (vs. healthy) consumption has downstream consequences for audience engagement on Instagram (Studies 5a and 5b). This research sheds light on how to cultivate interpersonal warmth in marketing communication and personal branding.
1	Pandemics, natural disasters (e.g., hurricanes, droughts, fires), strikes, piracy, and other events can unexpectedly disrupt supply or spike demand, creating shortages. Unlike ordinary stockouts caused by store-specific inventory policies, shortages involve the entire supply chain. One tool for managing shortages is imposing purchase limits. Purchase limits restrict the quantity each shopper can purchase of the scarce product (e.g., gasoline, toilet paper, sanitizers, meat, batteries), possibly increasing availability to other shoppers. Although altruistic stores might use purchase limits for egalitarian goals (e.g., reducing hoarding, waste, panic buying, arbitrage, unfair distribution), the authors find that profit-maximizing stores can use purchase limits to increase profits during shortages. These findings suggest that stores’ price-and-limit strategies depend on shortage severity, store size, competition, and seasonality. For moderate shortages, large multiproduct stores, where average shopping basket sizes are large, should maintain low prices and impose limits, whereas small stores should increase prices and not impose limits. For severe shortages, by contrast, large stores should keep low prices but not impose limits, whereas small stores should increase prices and impose limits. Generally, large stores benefit from increased future store traffic when they impose limits. Interestingly, purchase limits can improve both store profits and, with lower prices, consumer surplus.
1	This article explores how consumers’ relative spatial location influences their preferences and choices. Drawing on the conceptual metaphor literature, the author proposes that people interpret the abstract concept of risk using a more tangible concept: their location relative to the center or edge of a space. Five main studies (and a pilot) reveal the existence of a metaphorical association between risk and spatial location and show how this association systematically affects consumer risk-taking behavior. Specifically, people positioned closer to the edges (vs. center) of space are disproportionately more likely to seek (vs. avoid) risky choices. This phenomenon is demonstrated across various decision-making scenarios in the laboratory and field, using both physical and virtual manipulations of space. This effect occurs because being located closer to the edges (vs. center) evokes concepts related to risk (vs. safety), making risky (vs. safe) products easier to process and, as a result, more desirable. Moreover, this research sheds light on the effect characteristics and boundary conditions. The author concludes with a discussion of the implications of these findings for consumers and businesses.
1	Consumers often try to achieve multiple goals when purchasing products and services, where choices are sought that maximize utility and other objectives such as to minimize regret. If the performance of choice outcomes are associated with high level of uncertainty at the time of purchase, consumers worry that the alternative of interest may turn out to be not optimal, an outcome they want to avoid when making a purchase. The authors propose a new regret function and explore its properties. Then, they propose a generalized framework of multiple goal pursuit and apply it to a utility maximization and regret minimization problem. Pareto-optimal sets are an outcome of multiple goal optimization problems where there are multiple alternatives that are nondominated. The proposed framework enables the authors to generalize a dual-goal problem as a constrained optimization problem where either utility is maximized subject to the constraint on regret or regret is minimized subject to the utility constraint. The proposed model fits the data better, provides improved predictions, and offers a tractable solution to a problem of utility maximization and regret minimization.
1	A prominent hallmark of competitive interaction is the desire to differentiate from rivals. In this article, the authors examine under what conditions firms will differentiate through product quality versus advertising intensity. Firms select quality in a first stage, advertising in a second stage, and price in the last stage. The probability that a consumer is informed of a firm’s offering depends on its advertising expenditure. The authors find that when advertising is not cost effective, both firms choose a light ad spending. This allows them to minimally differentiate in quality without concern of intense price competition, as each firm has a segment of “captive” consumers. When advertising is moderately cost effective, one firm spends heavily on advertising. The rival advertises lightly, while choosing the same maximal quality level. This strategy softens price competition by inducing the heavy advertiser to price high more often to capitalize on its large captive segment. When advertising is very cost effective, both firms advertise heavily and differentiate in quality. Three extensions are examined: upfront fixed quality costs, continuous advertising levels, and simultaneous advertising and pricing decisions. This research reveals that letting market awareness be determined endogenously suggests less product differentiation than previously suspected and regions of advertising differentiation.
1	In a combinatorial auction m heterogenous indivisible items are sold to n bidders. This paper considers settings in which the valuation functions of the bidders are known to be complement free (a.k.a. subadditive). We provide several approximation algorithms for the social-welfare maximization problem in such settings. First, we present a logarithmic upper bound for the case that the access to the valuation functions is via demand queries. For the weaker value queries model we provide a tight O(√m) approximation. Unlike the other algorithms we present, this algorithm is also incentive compatible. Finally, we present two approximation algorithms for the more restricted class of XOS valuations: A simple deterministic algorithm that provides an approximation ratio of two and an optimal e/(e −1) approximation achieved via randomized rounding. We also present optimal lower bounds for both the demand oracles model and the value oracles model.
1	This paper provides sufficient conditions for the optimal value function of a given linear semi-infinite programming (LSIP) problem to depend linearly on the size of the perturbations, when these perturbations involve either the cost coefficients or the right-hand side function or both, and they are sufficiently small. Two kinds of partitions are considered. The first concerns the effective domain of the optimal value as a function of the cost coefficients and consists of maximal regions on which this value function is linear. The second class of partitions considered in this paper concerns the index set of the constraints through a suitable extension of the concept of optimal partition from ordinary to LSIP. These partitions provide convex sets, in particular, segments, on which the optimal value is a linear function of the size of the perturbations, for the three types of perturbations considered in this paper.
1	In this paper, we study cost functions over a finite collection of random variables. For these types of models, a calculus of differentiation is developed that allows us to obtain a closed-form expression for derivatives where “differentiation” has to be understood in the weak sense. The technique for proving the results is new and establishes an interesting link between functional analysis and gradient estimation. The key contribution of this paper is a product rule of weak differentiation. In addition, a product rule of weak analyticity is presented that allows for Taylor series approximations of finite products measures. In particular, from characteristics of the individual probability measures, a lower bound (i.e., domain of convergence) can be established for the set of parameter values for which the Taylor series converges to the true value. Applications of our theory to the ruin problem from insurance mathematics and to stochastic activity networks arising in project evaluation review techniques are provided.
1	The goal of this paper is to introduce the notion of certificates, which verify the accuracy of solutions of computational problems with convex structure. Such problems include minimizing convex functions, variational inequalities with monotone operators, computing saddle points of convex-concave functions, and solving convex Nash equilibrium problems. We demonstrate how the implementation of the ellipsoid method and other cutting plane algorithms can be augmented with the computation of such certificates without significant increase of the computational effort. Further, we show that (computable) certificates exist for any algorithm that is capable of producing solutions of guaranteed accuracy.
1	This paper studies an extension of the k-median problem under uncertain demand. We are given an n-vertex metric space (V, d) and m client sets . The goal is to open a set of k facilities F such that the worst-case connection cost over all the client sets is minimized, i.e.,  where for any , . This is a “min-max” or “robust” version of the k-median problem. Note that in contrast to the recent papers on robust and stochastic problems, we have only one stage of decision-making where we select a set of k facilities to open. Once a set of open facilities is fixed, each client in the uncertain client-set connects to the closest open facility. We present a simple, combinatorial O(log n + log m)-approximation algorithm for the robust k-median problem that is based on reweighting/Lagrangean-relaxation ideas. In fact, we give a general framework for (minimization) k-facility location problems where there is a bound on the number of open facilities. We show that if the location problem satisfies a certain “projection” property, then both the robust and stochastic versions of the location problem admit approximation algorithms with logarithmic ratios. We use our framework to give the first approximation algorithms for robust and stochastic versions of several location problems such as k-tree, capacitated k-median, and fault-tolerant k-median.
1	The asymptotic tail behaviour of sums of independent subexponential random variables is well understood, one of the main characteristics being the principle of the single big jump. We study the case of dependent subexponential random variables, for both deterministic and random sums, using a fresh approach, by considering conditional independence structures on the random variables. We seek sufficient conditions for the results of the theory with independent random variables to still hold. For a subexponential distribution, we introduce the concept of a boundary class of functions, which we hope will be a useful tool in studying many aspects of subexponential random variables. The examples we give demonstrate a variety of effects owing to the dependence, and are also interesting in their own right.
1	This paper studies solution properties of a parametric variational condition under the constant rank constraint qualification (CRCQ), and properties of its underlying set. We start by showing that if the CRCQ holds at a point in a fixed set, then there exists a one-to-one correspondence between the collection of nonempty faces of the normal cone to the set at that point and the collection of active index sets at points nearby. We then study the behavior of the Euclidean projector, and prove under the CRCQ that the set of multipliers associated with the Euclidean projection is locally a continuous multifunction. Following that, we apply the degree theory to a localized normal map, to show that the combination of the CRCQ and the so-called strong coherent orientation condition suffices for the parametric variational condition to have a locally unique, continuous solution, which is selected from finitely many C1 functions. Applications of this result under additional assumptions extend or recover some earlier results on parametric variational inequalities.
1	We explicitly solve the optimal switching problem for one-dimensional diffusions by directly using the dynamic programming principle and the excessive characterization of the value function. The shape of the value function and the smooth fit principle then can be proved using the properties of concave functions.
1	Products of random matrices in the max-plus algebra are used as models of a wide range of discrete event systems, including train or queueing networks, job shops, timed digital circuits, or parallel processing systems. Several mathematical models such as timed event graph or task-resources models also lead to max-plus products of matrices.Some stability and computability results, such as convergence of waiting times to a unique stationary regime or limit theorems for the throughput, have been proved under the so-called memory loss property (MLP).When the random matrices are i.i.d., we prove that this property is generic in the following sense: if it is not fulfilled, the support of the common law of the random matrices is included in a union of finitely many affine hyperplanes.
1	We consider a communication network where each pair of users requests a connection guaranteeing a certain capacity. The cost of building capacity is identical across pairs. Efficiency is achieved by any maximal cost spanning tree. We construct cost sharing methods ensuring standalone core stability, monotonicity of one's cost share in one's capacity requests, and continuity in everyone's requests. We define a solution for simple problems where each pairwise request is zero or one, and extend it piecewise linearly to all problems. The uniform solution obtains if we require one's cost share to be weakly increasing in everyone's capacity request. In the solution, we propose, on the contrary, one's cost share is weakly decreasing in other agents' requests. The computational complexity of both solutions is polynomial in the number of users. The uniform solution admits a closed form expression, and is the analog of a popular solution for the minimal cost spanning tree problem.
1	We consider a class of n-player stochastic games with the following properties: (1) in every state, the transitions are controlled by one player; (2) the payoffs are equal to zero in every nonabsorbing state; (3) the payoffs are nonnegative in every absorbing state. We propose a new iterative method to analyze these games. With respect to the expected average reward, we prove the existence of a subgame-perfect ε-equilibrium in pure strategies for every ε > 0. Moreover, if all transitions are deterministic, we obtain a subgame-perfect 0-equilibrium in pure strategies.
1	In this paper we study the problem of upper bounding the probability that a random variable is above its expected value by a small amount (relative to the expected value), by means of the second and the fourth (central) moments of the random variable. In this particular context, many classical inequalities yield only trivial bounds. We obtain tight upper bounds by studying the primal-dual moments-generating conic optimization problems. As an application, we demonstrate that given the new probability bounds, a substantial sharpening and simplification of a recent result and its analysis by Feige (see Feige, U. 2006. On sums of independent random variables with unbounded variances, and estimating the average degree in a graph. SIAM J. Comput.35 964–984) can be obtained; also, these bounds lead to new properties of the distribution of the cut values for the max-cut problem. We expect the new probability bounds to be useful in many other applications.
1	A maximal lattice free polyhedron L has max-facet-width equal to w if  for all facets  of L, and  for some facet  of L. The set obtained by adding all cuts whose validity follows from a maximal lattice free polyhedron with max-facet-width at most w is called the wth split closure. We show the wth split closure is a polyhedron. This generalizes a previous result showing this to be true when w = 1. We also consider the design of finite cutting plane proofs for the validity of an inequality. Given a measure of “size” of a maximal lattice free polyhedron, a natural question is how large a size s* of a maximal lattice free polyhedron is required to design a finite cutting plane proof for the validity of an inequality. We characterize s* based on the faces of the linear relaxation of the mixed integer linear set.
1	This paper studies the queue-length process in series Jackson networks with external input to the first station only. We show that its Markov transition probabilities can be written as a finite sum of noncrossing probabilities, so that questions on time-dependent queueing behavior are translated to questions on noncrossing probabilities. This makes previous work on noncrossing probabilities relevant to queueing systems and allows new queueing results to be established. To illustrate the latter, we prove that the relaxation time (i.e., the reciprocal of the “spectral gap”) of a positive recurrent system equals the relaxation time of an M/M/1 queue with the same arrival and service rates as the network's bottleneck station. This resolves a conjecture of Blanc that he proved for two queues in series.
1	For p ≥ 2 we consider the problem of, given an n × n matrix A = (aij) whose diagonal entries vanish, approximating in polynomial time the number When p = 2 this is simply the problem of computing the maximum eigenvalue of A, whereas for p = ∞ (actually it suffices to take p ≈ log n) it is the Grothendieck problem on the complete graph, which was shown to have a O(log n) approximation algorithm in Nemirovski et al. [Nemirovski, A., C. Roos, T. Terlaky. 1999. On maximization of quadratic form over intersection of ellipsoids with common center. Math. Program. Ser. A86(3) 463–473], Megretski [Megretski, A. 2001. Relaxations of quadratic programs in operator theory and system analysis. Systems, Approximation, Singular Integral Operators, and Related Topics (Bordeaux, 2000), Vol. 129. Operator Theory Advances and Applications. Birkhäuser, Basel, 365–392], Charikar and Wirth [Charikar, M., A. Wirth. 2004. Maximizing quadratic programs: Extending Grothendieck's inequality. Proc. 45th Annual Sympos. Foundations Comput. Sci., IEEE Computer Society, 54–60] and was used in the work of Charikar and Wirth noted above, to design the best known algorithm for the problem of computing the maximum correlation in correlation clustering. Thus the problem of approximating Optp(A) interpolates between the spectral (p = 2) case and the correlation clustering (p = ∞) case. From a physics point of view this problem corresponds to computing the ground states of spin glasses in a hard-wall potential well.We design a polynomial time algorithm which, given p ≥ 2 and an n × n matrix A = (aij) with zeros on the diagonal, computes Optp(A) up to a factor p/e + 30 log p. On the other hand, assuming the unique games conjecture (UGC) we show that it is NP-hard to approximate Optp(A) up to a factor smaller than p/e + ¼. Hence as p → ∞ the UGC-hardness threshold for computing Optp(A) is exactly (p/e)(1 + o(1)).
1	We consider a two-stage mixed integer stochastic optimization problem and show that a static robust solution is a good approximation to the fully adaptable two-stage solution for the stochastic problem under fairly general assumptions on the uncertainty set and the probability distribution. In particular, we show that if the right-hand side of the constraints is uncertain and belongs to a symmetric uncertainty set (such as hypercube, ellipsoid or norm ball) and the probability measure is also symmetric, then the cost of the optimal fixed solution to the corresponding robust problem is at most twice the optimal expected cost of the two-stage stochastic problem. Furthermore, we show that the bound is tight for symmetric uncertainty sets and can be arbitrarily large if the uncertainty set is not symmetric. We refer to the ratio of the optimal cost of the robust problem and the optimal cost of the two-stage stochastic problem as the stochasticity gap. We also extend the bound on the stochasticity gap for another class of uncertainty sets referred to as positive.If both the objective coefficients and right-hand side are uncertain, we show that the stochasticity gap can be arbitrarily large even if the uncertainty set and the probability measure are both symmetric. However, we prove that the adaptability gap (ratio of optimal cost of the robust problem and the optimal cost of a two-stage fully adaptable problem) is at most four even if both the objective coefficients and the right-hand side of the constraints are uncertain and belong to a symmetric uncertainty set. The bound holds for the class of positive uncertainty sets as well. Moreover, if the uncertainty set is a hypercube (special case of a symmetric set), the adaptability gap is one under an even more general model of uncertainty where the constraint coefficients are also uncertain.
1	We consider linear fixed point equations and their approximations by projection on a low dimensional subspace. We derive new bounds on the approximation error of the solution, which are expressed in terms of low dimensional matrices and can be computed by simulation. When the fixed point mapping is a contraction, as is typically the case in Markov decision processes (MDP), one of our bounds is always sharper than the standard contraction-based bounds, and another one is often sharper. The former bound is also tight in a worst-case sense. Our bounds also apply to the noncontraction case, including policy evaluation in MDP with nonstandard projections that enhance exploration. There are no error bounds currently available for this case to our knowledge.
1	We investigate the impact of Stackelberg routing to reduce the price of anarchy in network routing games. In this setting, an α fraction of the entire demand is first routed centrally according to a predefined Stackelberg strategy and the remaining demand is then routed selfishly by (nonatomic) players. Although several advances have been made recently in proving that Stackelberg routing can, in fact, significantly reduce the price of anarchy for certain network topologies, the central question of whether this holds true in general is still open. We answer this question negatively by constructing a family of single-commodity instances such that every Stackelberg strategy induces a price of anarchy that grows linearly with the size of the network. Moreover, we prove upper bounds on the price of anarchy of the largest-latency-first (LLF) strategy that only depend on the size of the network. Besides other implications, this rules out the possibility to construct constant-size networks to prove an unbounded price of anarchy. In light of this negative result, we consider bicriteria bounds. We develop an efficiently computable Stackelberg strategy that induces a flow whose cost is at most the cost of an optimal flow with respect to demands scaled by a factor of 1 + √1−α. Finally, we analyze the effectiveness of an easy-to-implement Stackelberg strategy, called SCALE. We prove bounds for a general class of latency functions that includes polynomial latency functions as a special case. Our analysis is based on an approach that is simple yet powerful enough to obtain (almost) tight bounds for SCALE in general networks.
1	We study G/G/n + GI queues in which customer patience times are independent, identically distributed following a general distribution. When a customer's waiting time in queue exceeds his patience time, the customer abandons the system without service. For the performance of such a system, we focus on the abandonment process and the queue length process. We prove that under some conditions, a deterministic relationship between the two stochastic processes holds asymptotically under the diffusion scaling when the number of servers n goes to infinity. These conditions include a minor assumption on the arrival processes that can be time-nonhomogeneous and a key assumption that the sequence of diffusion-scaled queue length processes, indexed by n, is stochastically bounded. We also establish a comparison result that allows one to verify the stochastic boundedness by studying a corresponding sequence of systems without customer abandonment.
1	In this paper, we prove the optimality of disturbance-affine control policies in the context of one-dimensional, constrained, multistage robust optimization. Our results cover the finite-horizon case, with minimax (worst-case) objective, and convex state costs plus linear control costs. We develop a new proof methodology, which explores the relationship between the geometrical properties of the feasible set of solutions and the structure of the objective function. Apart from providing an elegant and conceptually simple proof technique, the approach also entails very fast algorithms for the case of piecewise-affine state costs, which we explore in connection with a classical inventory management application.
1	We consider bandit problems involving a large (possibly infinite) collection of arms, in which the expected reward of each arm is a linear function of an r-dimensional random vector Z ∈ ℝr, where r ≥ 2. The objective is to minimize the cumulative regret and Bayes risk. When the set of arms corresponds to the unit sphere, we prove that the regret and Bayes risk is of order Θ(r √T), by establishing a lower bound for an arbitrary policy, and showing that a matching upper bound is obtained through a policy that alternates between exploration and exploitation phases. The phase-based policy is also shown to be effective if the set of arms satisfies a strong convexity condition. For the case of a general set of arms, we describe a near-optimal policy whose regret and Bayes risk admit upper bounds of the form O(r √T log3/2T).
1	In this paper, we provide Laplace transform-based analytical solutions to pricing problems of various occupation-time-related derivatives such as step options, corridor options, and quantile options under Kou's double exponential jump diffusion model. These transforms can be inverted numerically via the Euler Laplace inversion algorithm, and the numerical results illustrate that our pricing methods are accurate and efficient. The analytical solutions can be obtained primarily because we derive the closed-form Laplace transform of the joint distribution of the occupation time and the terminal value of the double exponential jump diffusion process. Beyond financial applications, the mathematical results about occupation times of a jump diffusion process are of more general interest in applied probability.
1	We study the convergence properties of an alternating proximal minimization algorithm for nonconvex structured functions of the type: L(x,y)=f(x)+Q(x,y)+g(y), where f and g are proper lower semicontinuous functions, defined on Euclidean spaces, and Q is a smooth function that couples the variables x and y. The algorithm can be viewed as a proximal regularization of the usual Gauss-Seidel method to minimize L.We work in a nonconvex setting, just assuming that the function L satisfies the Kurdyka-Łojasiewicz inequality. An entire section illustrates the relevancy of such an assumption by giving examples ranging from semialgebraic geometry to “metrically regular” problems.Our main result can be stated as follows: If L has the Kurdyka-Łojasiewicz property, then each bounded sequence generated by the algorithm converges to a critical point of L. This result is completed by the study of the convergence rate of the algorithm, which depends on the geometrical properties of the function L around its critical points. When specialized to  and to f,  g indicator functions, the algorithm is an alternating projection mehod (a variant of von Neumann's) that converges for a wide class of sets including semialgebraic and tame sets, transverse smooth manifolds or sets with “regular” intersection. To illustrate our results with concrete problems, we provide a convergent proximal reweighted ℓ1 algorithm for compressive sensing and an application to rank reduction problems.
1	The notion of a “market” has undergone a paradigm shift with the Internet. Totally new and highly successful markets have been defined and launched by Internet companies, which already form an important part of today's economy and are projected to grow considerably in the future. Another major change is the availability of massive computational power for running these markets in a centralized or distributed manner.In view of these new realities, the study of market equilibria, an important, though essentially nonalgorithmic, theory within mathematical economics, needs to be revived and rejuvenated via an inherently algorithmic approach. Such a theory should not only address traditional market models, but also define new models for some of the new markets.We present a new, natural class of utility functions that allow buyers to explicitly provide information on their relative preferences as a function of the amount of money spent on each good. These utility functions offer considerable expressivity, especially in Google's Adwords market. In addition, they lend themselves to efficient computation, while retaining some of the nice properties of traditional models. The latter include weak gross substitutability, and the uniqueness and continuity of equilibrium prices and utilities.
1	We extend the work of Letchford [Letchford, A. N. 2000. Separating a superclass of comb inequalities in planar graphs. Math. Oper. Res.25 443–454] by introducing a new class of valid inequalities for the traveling salesman problem, called the generalized domino-parity (GDP) constraints. Just as Letchford's domino-parity constraints generalize comb inequalities, GDP constraints generalize the most well-known multiple-handle constraints, including clique-tree, bipartition, path, and star inequalities. Furthermore, we show that a subset of GDP constraints containing all of the clique-tree inequalities can be separated in polynomial time, provided that the support graph G* is planar, and provided that we bound the number of handles by a fixed constant h.
1	We develop first-order smoothing techniques for saddle-point problems that arise in finding a Nash equilibrium of sequential games. The crux of our work is a construction of suitable prox-functions for a certain class of polytopes that encode the sequential nature of the game. We also introduce heuristics that significantly speed up the algorithm, and decomposed game representations that reduce the memory requirements, enabling the application of the techniques to drastically larger games. An implementation based on our smoothing techniques computes approximate Nash equilibria for games that are more than four orders of magnitude larger than what prior approaches can handle. Finally, we show near-linear further speedups from parallelization.
1	We consider the problem of locating a facility on a network represented by a graph. A set of strategic agents have different ideal locations for the facility; the cost of an agent is the distance between its ideal location and the facility. A mechanism maps the locations reported by the agents to the location of the facility. We wish to design mechanisms that are strategyproof (SP) in the sense that agents can never benefit by lying and, at the same time, provide a small approximation ratio with respect to the minimax measure. We design a novel “hybrid” strategyproof randomized mechanism that provides a tight approximation ratio of 3/2 when the network is a circle (known as a ring in the case of computer networks). Furthermore, we show that no randomized SP mechanism can provide an approximation ratio better than 2 - o(1), even when the network is a tree, thereby matching a trivial upper bound of two.
1	Given an oblique reflection map Γ and functions  (the space of ℝK-valued functions that have finite left and right limits at every point), the directional derivative  of Γ along χ, evaluated at ψ, is defined to be the pointwise limit, as , of the family of functions . Directional derivatives are shown to exist and lie in  for oblique reflection maps associated with reflection matrices of the so-called Harrison-Reiman class. When ψ and χ are continuous, the convergence of  to  is shown to be uniform on compact subsets of continuity points of the limit , and the derivative  is shown to have an autonomous characterization as the unique fixed point of an associated map. Directional derivatives arise as functional central limit approximations to time-inhomogeneous queueing networks. In this case ψ and χ correspond, respectively, to the functional strong law of large numbers and functional central limits of the so-called netput process. In this work it is also shown how the various types of discontinuities of the derivative  are related to the reflection matrix and properties of the function Γ(ψ). In the queueing network context, this describes the influence of the topology of the network and the states (of underloading, overloading, or criticality) of the various queues in the network on the discontinuities of the directional derivative. Directional derivatives have also been found useful for identifying optimal controls for fluid approximations of time-inhomogoeneous queueing networks and are also of relevance to the study of differentiability of stochastic flows of obliquely reflected Brownian motions.
1	We investigate the optimal portfolio problem under the threat of a financial market crash in a multidimensional jump-diffusion framework. We set up a nonprobabilistic crash model and consider an investor that seeks to maximize CRRA utility in the worst possible crash scenario. We recast the problem as a stochastic differential game; with the help of the fundamental notion of indifference strategies, we completely solve the portfolio problem using martingale arguments.
1	We propose a semidefinite optimization (SDP) model for the class of minimax two-stage stochastic linear optimization problems with risk aversion. The distribution of second-stage random variables belongs to a set of multivariate distributions with known first and second moments. For the minimax stochastic problem with random objective, we provide a tight SDP formulation. The problem with random right-hand side is NP-hard in general. In a special case, the problem can be solved in polynomial time. Explicit constructions of the worst-case distributions are provided. Applications in a production-transportation problem and a single facility minimax distance problem are provided to demonstrate our approach. In our experiments, the performance of minimax solutions is close to that of data-driven solutions under the multivariate normal distribution and better under extremal distributions. The minimax solutions thus guarantee to hedge against these worst possible distributions and provide a natural distribution to stress test stochastic optimization problems under distributional ambiguity.
1	We consider the mixed-integer version of bipartite vertex cover. This is equivalent to a mixed-integer network dual model, introduced recently, that generalizes several mixed-integer sets arising in production planning. We derive properties of inequalities that are valid for the convex hull of the mixed-integer bipartite covers by projecting an extended formulation onto the space of the original variables. This permits us to give a complete description of the facet-inducing inequalities of the double mixing set and of the continuous mixing set with flows, two mixed-integer sets that generalize several models studied in the literature.
1	A successful method to describe the asymptotic behavior of a discrete time stochastic process governed by some recursive formula is to relate it to the limit sets of a well-chosen mean differential equation. Under an attainability condition, Benaïm proved that convergence to a given attractor of the flow induced by this dynamical system occurs with positive probability for a class of Robbins Monro algorithms. Benaïm, Hofbauer, and Sorin generalised this approach for stochastic approximation algorithms whose average behavior is related to a differential inclusion instead. We pursue the analogy by extending to this setting the result of convergence with positive probability to an attractor.
1	We study randomized variants of two classical algorithms: coordinate descent for systems of linear equations and iterated projections for systems of linear inequalities. Expanding on a recent randomized iterated projection algorithm of Strohmer and Vershynin (Strohmer, T., R. Vershynin. 2009. A randomized Kaczmarz algorithm with exponential convergence. J. Fourier Anal. Appl.15 262–278) for systems of linear equations, we show that, under appropriate probability distributions, the linear rates of convergence (in expectation) can be bounded in terms of natural linear-algebraic condition numbers for the problems. We relate these condition measures to distances to ill-posedness and discuss generalizations to convex systems under metric regularity assumptions.
1	A hypergraph is called box-Mengerian if the linear system Ax ≥ 1, x ≥ 0 is box-totally dual integral (box-TDI), where A is the edge-vertex incidence matrix of the hypergraph. Because it is NP-hard in general to recognize box-Mengerian hypergraphs, a basic theme in combinatorial optimization is to identify such objects associated with various problems. In this paper, we show that the so-called equitably subpartitionable (ESP) property, first introduced by Ding and Zang (Ding, G., W. Zang. 2002. Packing cycles in graphs. J. Combin. Theory Ser. B86 381–407) in their characterization of all graphs with the min-max relation on packing and covering cycles, turns out to be even sufficient for box-Mengerian hypergraphs. We also establish several new classes of box-Mengerian hypergraphs based on ESP property. This approach is of transparent combinatorial nature and is hence fairly easy to work with.
1	This paper provides heavy traffic limit theorems for a polling station serving jobs from K exogenous renewal arrival streams. It is a standard result that the limiting diffusion-scaled total workload process is semimartingale reflected Brownian motion. For polling stations, however, no such limit exists in general for the diffusion-scaled, K-dimensional queue length or workload vector processes. Instead, we prove that these processes admit averaging principles, the natures of which depend on the service discipline employed at the polling station. Parameterized families of exhaustive and gated service disciplines are investigated.Each policy under consideration has K stochastic matrices associated with it—one for each job class—that describe the transition of the workload vector while a given job class is being polled. These matrices give rise to K probability vectors that are vertices of a K - 1-simplex. Loosely speaking, each point of this simplex acts as an instantaneous lifting operator, converting the one-dimensional limiting total workload process to the K-dimensional workload vector. The averaging principles—needed because the workload vector traverses a Hamiltonian cycle over K edges of the simplex infinitely fast under diffusion scaling—are limits of cumulative cost functions of the diffusion-scaled workload vector, evaluated over a fixed time interval. The “averaging” takes place over the edges of the simplex.
1	We consider a model that arises in integer programming and show that all irredundant inequalities are obtained from maximal lattice-free convex sets in an affine subspace. We also show that these sets are polyhedra. The latter result extends a theorem of Lovász characterizing maximal lattice-free convex sets in ℝn.
1	We provide yet another proof of the existence of calibrated forecasters; it has two merits. First, it is valid for an arbitrary finite number of outcomes. Second, it is short and simple and it follows from a direct application of Blackwell's approachability theorem to a carefully chosen vector-valued payoff function and convex target set. Our proof captures the essence of existing proofs based on approachability (e.g., the proof by Foster [Foster, D. 1999. A proof of calibration via Blackwell's approachability theorem. Games Econom. Behav.29 73–78] in the case of binary outcomes) and highlights the intrinsic connection between approachability and calibration.
1	We study profit maximization in inventory control problems where demands are unknown. Neither probabilistic distributions nor sets are available to characterize the unknown demand parameters. Therefore, we adopt an online optimization perspective for our analysis. The usual competitive ratio is not well defined for the problems we analyze; consequently, we introduce a new worst-case metric that is suitable. We consider two inventory management frameworks: (1) perishable products with lost sales and (2) durable products with backlogged demand. We consider both finite and infinite planning horizons. We design best-possible online procurement strategies for all cases.
1	We prove that every multiplayer perfect-information game with bounded and lower-semicontinuous payoffs admits a subgame-perfect ε-equilibrium in pure strategies. This result complements Example 3 in Solan and Vieille [Solan, E., N. Vieille. 2003. Deterministic multi-player Dynkin games. J. Math. Econom.39 911–929], which shows that a subgame-perfect ε-equilibrium in pure strategies need not exist when the payoffs are not lower-semicontinuous. In addition, if the range of payoffs is finite, we characterize in the form of a Folk Theorem the set of all plays and payoffs that are induced by subgame-perfect 0-equilibria in pure strategies.
1	Suppose that a Wiener process gains a known drift rate at some unobservable disorder time with some zero-modified exponential distribution. The process is observed only at known fixed discrete time epochs, which may not always be spaced in equal distances. The problem is to detect the disorder time as quickly as possible by means of an alarm that depends only on the observations of Wiener process at those discrete time epochs. We show that Bayes optimal alarm times, which minimize expected total cost of frequent false alarms and detection delay time, always exist. Optimal alarms may in general sound between observation times and when the space-time process of the odds that disorder happened in the past hits a set with a nontrivial boundary. The optimal stopping boundary is piecewise-continuous and explodes as time approaches from left to each observation time. On each observation interval, if the boundary is not strictly increasing everywhere, then it first decreases and then increases. It is strictly monotone wherever it does not vanish. Its decreasing portion always coincides with some explicit function. We develop numerical algorithms to calculate nearly optimal detection algorithms and their Bayes risks, and we illustrate their use on numerical examples. The solution of Wiener disorder problem with discretely spaced observation times will help reduce risks and costs associated with disease outbreak and production quality control, where the observations are often collected and/or inspected periodically.
1	We investigate the diameter of a natural abstraction of the 1-skeleton of polyhedra. Even if this abstraction is more general than other abstractions previously studied in the literature, known upper bounds on the diameter of polyhedra continue to hold here. On the other hand, we show that this abstraction has its limits by providing an almost quadratic lower bound.
1	Submodular function maximization is a central problem in combinatorial optimization, generalizing many important NP-hard problems including max cut in digraphs, graphs, and hypergraphs; certain constraint satisfaction problems; maximum entropy sampling; and maximum facility location problems. Our main result is that for any k ≥ 2 and any ε > 0, there is a natural local search algorithm that has approximation guarantee of 1/(k + ε) for the problem of maximizing a monotone submodular function subject to k matroid constraints. This improves upon the 1/(k + 1)-approximation of Fisher, Nemhauser, and Wolsey obtained in 1978 [Fisher, M., G. Nemhauser, L. Wolsey. 1978. An analysis of approximations for maximizing submodular set functions—II. Math. Programming Stud.8 73–87]. Also, our analysis can be applied to the problem of maximizing a linear objective function and even a general nonmonotone submodular function subject to k matroid constraints. We show that, in these cases, the approximation guarantees of our algorithms are 1/(k − 1 + ε) and 1/(k + 1 + 1/(k − 1) + ε), respectively.Our analyses are based on two new exchange properties for matroids. One is a generalization of the classical Rota exchange property for matroid bases, and another is an exchange property for two matroids based on the structure of matroid intersection.
1	Power law distributions have been repeatedly observed in a wide variety of socioeconomic, biological, and technological areas. In many of the observations, e.g., city populations and sizes of living organisms, the objects of interest evolve because of the replication of their many independent components, e.g., births and deaths of individuals and replications of cells. Furthermore, the rates of replications are often controlled by exogenous parameters causing periods of expansion and contraction, e.g., baby booms and busts, economic booms and recessions, etc. In addition, the sizes of these objects often have reflective lower boundaries, e.g., cities do not fall below a certain size, low-income individuals are subsidized by the government, companies are protected by bankruptcy laws, etc.Hence, it is natural to propose reflected modulated branching processes as generic models for many of the preceding observations. Indeed, our main results show that the proposed mathematical models result in power law distributions under quite general polynomial Gärtner-Ellis conditions, the generality of which could explain the ubiquitous nature of power law distributions. In addition, on a logarithmic scale, we establish an asymptotic equivalence between the reflected branching processes and the corresponding multiplicative ones. The latter, as recognized by Goldie [Goldie, C. M. 1991. Implicit renewal theory and tails of solutions of random equations. Ann. Appl. Probab.1(1) 126–166], is known to be dual to queueing/additive processes. We emphasize this duality further in the generality of stationary and ergodic processes.
1	We consider the instantaneous control of a diffusion process on the real line. Two types of costs are incurred. The holding cost rate, incurred at all times, is modeled by a convex function. Transactions costs have both fixed and proportional components, making it an impulse control problem. The objective is to minimize the expected infinite horizon discounted cost. The solution to a quasi-variational inequality, which takes the form of a free-boundary problem, can be shown to be the optimal solution. We develop a methodology that converts the free-boundary problem into a sequence of fixed boundary problems. We show that the arising sequence is monotonic and converges. Provided the converged solution is C1, we show its optimality. We also provide an epsilon-optimality result. Finally, we illustrate a couple of popular applications of this model.
1	We study zero-sum games with incomplete information and analyze the impact that the information players receive has on the payoffs. It turns out that the functions that measure the value of information share two properties. The first is Blackwell monotonicity, which means that each player gains from knowing more. The second is concavity on the space of conditional probabilities. We prove that any function satisfying these two properties is the value function of a zero-sum game.
1	We investigate in this paper the Lagrangian duality properties of linear equality constrained binary quadratic programming. We derive an underestimation of the duality gap between the primal problem and its Lagrangian dual or SDP relaxation, using the distance from the set of binary integer points to certain affine subspace, while the computation of this distance can be achieved by the cell enumeration of hyperplane arrangement. Alternative Lagrangian dual schemes via the exact penalty and the squared norm constraint reformulations are also discussed.
1	We consider a single server discrete-time system with a fixed number of users where the server picks operating points from a compact, convex, and coordinate convex set. For this system, we analyse the performance of a stabilising policy that at any given time picks operating points from the allowed rate region that maximise a weighted sum of rates, where the weights depend on the work loads of the users. Assuming a large deviations principle (LDP) for the arrival processes in the Skorohod space of functions that are right continuous with left-hand limits, we establish an LDP for the work load process using a generalised version of the contraction principle to derive the corresponding rate function. With the LDP result available, we then analyse the tail probabilities of the work loads under different buffering scenarios.
1	We consider a model with two simultaneous VCG ad auctions A and B where each advertiser chooses to participate in a single ad auction. We prove the existence and uniqueness of a symmetric equilibrium in that model. Moreover, when the click rates in A are pointwise higher than those in B, we prove that the expected revenue in A is greater than the expected revenue in B in this equilibrium. In contrast, we show that this revenue ranking does not hold when advertisers can participate in both auctions.
1	We consider the problem of minimizing the weighted sum of completion times on a single machine subject to bipartite precedence constraints in which all minimal jobs have unit processing time and zero weight, and all maximal jobs have zero processing time and unit weight. For various probability distributions over these instances—including the uniform distribution—we show several “almost all”-type results. First, we show that almost all instances are prime with respect to a well-studied decomposition for this scheduling problem. Second, we show that for almost all instances, every feasible schedule is arbitrarily close to optimal. Finally, for almost all instances, we give a lower bound on the integrality gap of various linear programming relaxations of this problem.
1	In this paper, we show a significant role that geometric properties of uncertainty sets, such as symmetry, play in determining the power of robust and finitely adaptable solutions in multistage stochastic and adaptive optimization problems. We consider a fairly general class of multistage mixed integer stochastic and adaptive optimization problems and propose a good approximate solution policy with performance guarantees that depend on the geometric properties of the uncertainty sets. In particular, we show that a class of finitely adaptable solutions is a good approximation for both the multistage stochastic and the adaptive optimization problem. A finitely adaptable solution generalizes the notion of a static robust solution and specifies a small set of solutions for each stage; the solution policy implements the best solution from the given set, depending on the realization of the uncertain parameters in past stages. Therefore, it is a tractable approximation to a fully adaptable solution for the multistage problems. To the best of our knowledge, these are the first approximation results for the multistage problem in such generality. Moreover, the results and the proof techniques are quite general and also extend to include important constraints such as integrality and linear conic constraints.
1	We consider linear optimization over a nonempty convex semialgebraic feasible region F. Semidefinite programming is an example. If F is compact, then for almost every linear objective there is a unique optimal solution, lying on a unique “active” manifold, around which F is “partly smooth,” and the second-order sufficient conditions hold. Perturbing the objective results in smooth variation of the optimal solution. The active manifold consists, locally, of these perturbed optimal solutions; it is independent of the representation of F and is eventually identified by a variety of iterative algorithms such as proximal and projected gradient schemes. These results extend to unbounded sets F.
1	We consider approximation algorithms for buy-at-bulk network design, with the additional constraint that demand pairs be protected against a single edge or node failure in the network. In practice, the most popular model used in high-speed telecommunication networks for protection against failures is the so-called 1 + 1 model. In this model, two-edge or node-disjoint paths are provisioned for each demand pair. We obtain the first nontrivial approximation algorithms for buy-at-bulk network design in the 1 + 1 model for both edge and node-disjoint protection requirements. Our results are for the single-cable cost model, which is prevalent in optical networks. More specifically, we present a constant-factor approximation for the single-sink case and a polylogarithmic factor approximation for the multicommodity case. These results are of interest for practical applications and also suggest several new challenging theoretical problems.
1	We analyze two popular semidefinite programming relaxations for quadratically constrained quadratic programs with matrix variables. These relaxations are based on vector lifting and on matrix lifting; they are of different size and expense. We prove, under mild assumptions, that these two relaxations provide equivalent bounds. Thus, our results provide a theoretical guideline for how to choose a less expensive semidefinite programming relaxation and still obtain a strong bound. The main technique used to show the equivalence and that allows for the simplified constraints is the recognition of a class of nonchordal sparse patterns that admit a smaller representation of the positive semidefinite constraint.
1	This paper deals with denumerable continuous-time Markov decision processes (MDP) with constraints. The optimality criterion to be minimized is expected discounted loss, while several constraints of the same type are imposed. The transition rates may be unbounded, the loss rates are allowed to be unbounded as well (from above and from below), and the policies may be history-dependent and randomized. Based on Kolmogorov's forward equation and Dynkin's formula, we remind the reader about the Bellman equation, introduce and study occupation measures, reformulate the optimization problem as a (primary) linear program, provide the form of optimal policies for a constrained optimization problem here, and establish the duality between the convex analytic approach and dynamic programming. Finally, a series of examples is given to illustrate all of our results.
1	This work concerns Markov decision processes with finite state space and compact action set. The performance of a control policy is measured by a risk-sensitive average cost criterion and, under standard continuity-compactness conditions, it is shown that the discounted approximations converge to the optimal value function, and that the superior and inferior limit average criteria have the same optimal value function. These conclusions are obtained for every nonnull risk-sensitivity coefficient, and regardless of the communication structure induced by the transition law.
1	We consider a nonhomogeneous stochastic infinite horizon optimization problem whose objective is to minimize the overall average cost per period of an infinite sequence of actions (average optimality). Optimal solutions to such problems will in general be nonstationary. Moreover, a solution that initially makes poor decisions, and then selects wisely thereafter, can be average optimal. However, we seek average optimal solutions with optimal short-term, as well as long-term, behavior. Our approach is to first transform our stochastic problem into one that is deterministic, using the standard device of formulating the problem as one of choosing a sequence of policies, as opposed to actions. Within this deterministic framework, states become probability distributions over the original stochastic states. Then, by weakening the notion of state reachability, and strengthening the notion of efficiency traditionally used in the deterministic framework, we prove that such efficient solutions exist and are average optimal, thus simultaneously exhibiting both optimal long- and short-run behavior. This deterministic view of the property of stochastic ergodicity offers the potential to relax the traditional conditions for average optimality that use coefficients of ergodicity, as well as the opportunity to strengthen the criterion of average optimality through the property of efficiency.
1	The multiobjective bilevel program is a sequence of two optimization problems, with the upper-level problem being multiobjective and the constraint region of the upper level problem being determined implicitly by the solution set to the lower-level problem. In the case where the Karush-Kuhn-Tucker (KKT) condition is necessary and sufficient for global optimality of all lower-level problems near the optimal solution, we present various optimality conditions by replacing the lower-level problem with its KKT conditions. For the general multiobjective bilevel problem, we derive necessary optimality conditions by considering a combined problem, with both the value function and the KKT condition of the lower-level problem involved in the constraints. Most results of this paper are new, even for the case of a single-objective bilevel program, the case of a mathematical program with complementarity constraints, and the case of a multiobjective optimization problem.
1	The virtual private network problem (VPN) models scenarios in which traffic is uncertain or rapidly changing. The goal is supporting at minimum cost a given family of traffic matrices, which are implicitly given by upper bounds on the ingoing and outgoing traffic at each node. Costs are classically defined by a linear function (linear VPN), but we consider here also the more general case of concave increasing costs (concave VPN).In this paper we give the first constant factor approximation for concave VPN, and we improve the best-known approximation factor for linear VPN. Our approximation results build on a novel reduction, based on König's theorem, which allows us to turn uncertainty of traffic into nonlinearity of the objective function. This way, we are able to reduce linear VPN and concave VPN to the single-sink rent-or-buy problem (SROB) and the single-sink buy-at-bulk problem (SSBB), respectively. Using the machinery developed for the latter two problems plus additional ideas, we are able to improve the approximation ratio for VPN.Along the way we also obtain, among other results, an improved approximation algorithm for SSBB and a tighter bound on the gap between the costs of arbitrary solutions and tree solutions for VPN. Furthermore, solving an open problem, we show that VPN remains NP-hard even in the balanced case, where the sum of ingoing and outgoing traffic bounds is equal.
1	One of the most important variants of the standard linear assignment problem is the bottleneck assignment problem. In this paper we give a method by which one can find all of the asymptotic moments of a random bottleneck assignment problem in which costs (independent and identically distributed) are chosen from a wide variety of continuous distributions. Our method is obtained by determining the asymptotic moments of the time to first complete matching in a random bipartite graph process and then transforming those, via a Maclaurin series expansion for the inverse cumulative distribution function, into the desired moments for the bottleneck assignment problem. Our results improve on the previous best-known expression for the expected value of a random bottleneck assignment problem, yield the first results on moments other than the expected value, and produce the first results on the moments for the time to first complete matching in a random bipartite graph process.
1	In this paper, we prove that the Chvátal-Gomory closure of a set obtained as an intersection of a strictly convex body and a rational polyhedron is a polyhedron. Thus, we generalize a result of Schrijver [Schrijver, A. 1980. On cutting planes. Ann. Discrete Math.9 291–296], which shows that the Chvátal-Gomory closure of a rational polyhedron is a polyhedron.
1	We consider the problem of determining an optimal appointment schedule for a given sequence of jobs (e.g., medical procedures) on a single processor (e.g., operating room, examination facility, physician), to minimize the expected total underage and overage costs when each job has a random processing duration given by a joint discrete probability distribution. Simple conditions on the cost rates imply that the objective function is submodular and L-convex. Then there exists an optimal appointment schedule that is integer and can be found in polynomial time. Our model can handle a given due date for the total processing (e.g., end of day for an operating room) after which overtime is incurred, as well as no-shows and some emergencies.
1	We develop a new, random walk-based, algorithm for the Hamiltonian cycle problem. The random walk is on pairs of extreme points of two suitably constructed polytopes. The latter are derived from geometric properties of the space of discounted occupational measures corresponding to a given graph. The algorithm searches for a measure that corresponds to a common extreme point in these two specially constructed polyhedral subsets of that space. We prove that if a given undirected graph is Hamiltonian, then with probability one this random walk algorithm detects its Hamiltonicity in a finite number of iterations. We support these theoretical results by numerical experiments that demonstrate a surprisingly slow growth in the required number of iterations with the size of the given graph.
1	We consider a class of queueing systems that consist of server pools in parallel and multiple customer classes. Customer service times are assumed to be exponentially distributed. We study the asymptotic behavior of these queueing systems in a heavy traffic regime that is known as the Halfin-Whitt many-server asymptotic regime. Our main contribution is a general framework for establishing state space collapse results in this regime for parallel server systems. In our work, state space collapse refers to a decrease in the dimension of the processes tracking the number of customers in each class waiting for service and the number of customers in each class being served by various server pools. We define and introduce a “state space collapse” function, which governs the exact details of the state space collapse. We show that a state space collapse result holds in many-server heavy traffic if a corresponding deterministic hydrodynamic model satisfies a similar state space collapse condition. Unlike the single-server heavy traffic setting for multiclass queueing network, our hydrodynamic model is different from the standard fluid model for many-server queues. Our methodology is similar in spirit to that in Bramson [Bramson, M. 1998. State space collapse with application to heavy traffic limits for multiclass queueing networks. Queueing Systems30 89–148.], which focuses on the single-server heavy traffic regime. We illustrate the applications of our results by establishing state space collapse results in many-server diffusion limits for V-model systems under static-buffer-priority policy and the threshold policy proposed in the literature.
1	We introduce a notion of complete monotone quasiconcave duality, motivated by some economic applications. We show that this duality holds for important classes of quasiconcave functions.
1	This paper proposes a general duality framework for the problem of minimizing a convex integral functional over a space of stochastic processes adapted to a given filtration. The framework unifies many well-known duality frameworks from operations research and mathematical finance. The unification allows the extension of some useful techniques from these two fields to a much wider class of problems. In particular, combining certain finite-dimensional techniques from convex analysis with measure theoretic techniques from mathematical finance, we are able to close the duality gap in some situations where traditional topological arguments fail.
1	We formulate and solve a problem that combines the features of the so-called monotone follower of singular stochastic control theory with optimal stopping. In particular, we consider a stochastic system whose uncontrolled state dynamics are modelled by a general one-dimensional Itô diffusion. The aim of the problem that we solve is to maximise the utility derived from the system's state at the discretionary time when the system's control is terminated. This objective is reflected by the performance criterion that we maximise, which also penalises control expenditure as well as waiting. The model that we study is motivated by the so-called goodwill problem, a variant of which is concerned with how to optimally raise a new product's image, e.g., through advertising, and with determining the best time to launch the product into the market. In the presence of the rather general assumptions that we make, we fully characterise the optimal strategy, which can take one of three qualitatively different forms, depending on the problem data.
1	We propose a new concept of generalized differentiation of set-valued maps that captures first-order information. This concept encompasses the standard notions of Fréchet differentiability, strict differentiability, calmness and Lipschitz continuity in single-valued maps, and the Aubin property and Lipschitz continuity in set-valued maps. We present calculus rules, sharpen the relationship between the Aubin property and coderivatives, and study how metric regularity and open covering can be refined to have a directional property similar to our concept of generalized differentiation. Finally, we discuss the relationship between the robust form of generalized differentiation and its one-sided counterpart.
1	Consider a convex set , where G(x) is a symmetric matrix whose every entry is a polynomial or rational function, 𝒟 ⫅ ℝn is a domain on which G(x) is defined, and  means G(x) is positive semidefinite. The set S is called semidefinite representable if it equals the projection of a higher dimensional set that is defined by a linear matrix inequality (LMI). This paper studies sufficient conditions guaranteeing semidefinite representability of S. We prove that S is semidefinite representable in the following cases: (i) 𝒟 = ℝn, G(x) is a matrix polynomial and matrix sos-concave; (ii) 𝒟 is compact convex, G(x) is a matrix polynomial and strictly matrix concave on 𝒟 (iii) G(x) is a matrix rational function and q-module matrix concave on 𝒟. Explicit constructions of semidefinite representations are given. Some examples are illustrated.
1	A separable assignment problem (SAP) is defined by a set of bins and a set of items to pack in each bin; a value, fij, for assigning item j to bin i; and a separate packing constraint for each bin—i.e., for each bin, a family of subsets of items that fit in to that bin. The goal is to pack items into bins to maximize the aggregate value. This class of problems includes the maximum generalized assignment problem (GAP)1 and a distributed caching problem (DCP) described in this paper.Given a β-approximation algorithm for finding the highest value packing of a single bin, we giveA polynomial-time LP-rounding based ((1 − 1/e)β)-approximation algorithm.A simple polynomial-time local search (β/(β + 1) − ε)-approximation algorithm, for any ε > 0.Therefore, for all examples of SAP that admit an approximation scheme for the single-bin problem, we obtain an LP-based algorithm with (1 − 1/e − ε)-approximation and a local search algorithm with (½ - ε)-approximation guarantee. Furthermore, for cases in which the subproblem admits a fully polynomial approximation scheme (such as for GAP), the LP-based algorithm analysis can be strengthened to give a guarantee of 1 − 1/e. The best previously known approximation algorithm for GAP is a ½-approximation by Shmoys and Tardos and Chekuri and Khanna. Our LP algorithm is based on rounding a new linear programming relaxation, with a provably better integrality gap.To complement these results, we show that SAP and DCP cannot be approximated within a factor better than 1 − 1/e unless NP ⫅ DTIME(nO(log log n)), even if there exists a polynomial-time exact algorithm for the single-bin problem.We extend the (1 − 1/e)-approximation algorithm to a constant-factor approximation algorithms for a nonseparable assignment problem with applications in maximizing revenue for budget-constrained combinatorial auctions and the AdWords assignment problem. We generalize the local search algorithm to yield a ½ - ε approximation algorithm for the maximum k-median problem with hard capacities.
1	A simple relaxation consisting of two rows of a simplex tableau is a mixed-integer set with two equations, two free integer variables, and nonnegative continuous variables. Recently, Andersen et al. and Cornuéjols and Margot showed that the facet-defining inequalities of this set are either split cuts or intersection cuts obtained from lattice-free triangles and quadrilaterals. From an example given by Cook et al. it is known that one particular class of facet-defining triangle inequality does not have finite split rank. In this paper we show that all other facet-defining triangle and quadrilateral inequalities have finite split rank.
1	We show that up to unimodular equivalence in each dimension there are only finitely many lattice polytopes without interior lattice points that do not admit a lattice projection onto a lower-dimensional lattice polytope without interior lattice points. This was conjectured by Treutlein [Treutlein, J. 2008. 3-Dimensional lattice polytopes without interior lattice points. September 10, http://arXiv.org/abs/0809.1787.] As an immediate corollary, we get a short proof of a recent result of Averkov, Wagner, and Weismantel [Averkov, G., C. Wagner, R. Weismantel. 2010. Maximal lattice-free polyhedra: Finiteness and an explicit description in dimension three. Math. Oper. Res. Forthcoming.], namely, the finiteness of the number of maximal lattice polytopes without interior lattice points. Moreover, we show that, in dimension four and higher, some of these finitely many polytopes are not maximal as convex bodies without interior lattice points.
1	It has been shown that every n-person, perfect information game with no chance moves and bounded, lower semicontinuous payoffs has a subgame perfect ϵ-equilibrium in pure strategies. Here the same is proved when the payoffs are bounded and upper semicontinuous.
1	In this paper we introduce a novel flow representation for finite games in strategic form. This representation allows us to develop a canonical direct sum decomposition of an arbitrary game into three components, which we refer to as the potential, harmonic, and nonstrategic components. We analyze natural classes of games that are induced by this decomposition, and in particular, focus on games with no harmonic component and games with no potential component. We show that the first class corresponds to the well-known potential games. We refer to the second class of games as harmonic games, and demonstrate that this new class has interesting properties which contrast with properties of potential games. Exploiting the decomposition framework, we obtain explicit expressions for the projections of games onto the subspaces of potential and harmonic games. This enables an extension of the equilibrium properties of potential and harmonic games to “nearby” games.
1	We introduce nondegeneracy and the C-index for C-stationary points of a QPCC, that is, for a mathematical program with a quadratic objective function and linear complementarity constraints. The C-index characterizes the qualitative local behavior of a QPCC around a nondegenerate C-stationary point. The article focuses on the structure of the C-stationary set of QPCCs depending on a real parameter. We show that, for generic QPCC data, the C-index changes exactly at turning points of the C-stationary set, and that it changes by exactly one.To illustrate this concept, we introduce and analyze two homotopy methods for finding C-stationary points. Numerical results illustrate that, for randomly generated test problems, the two homotopy methods very often identify B-stationary points.
1	We consider a stochastic approximation (SA) method for finding the minimizer of a function f, which is convex but nondifferentiable at the minimizer. Due to the nondifferentiability at the minimizer, f is allowed to increase at a positive rate in a neighborhood of the minimizer. From this property, we show that the nth estimate for the minimizer generated by the SA procedure converges at a rate of 1/n in the mean, which is significantly faster than the classical convergence rates for differentiable functions f. We also discuss an example from an inventory control system that exhibits a convex but nondifferentiable cost function.
1	We provide an example of a feedforward first-in-system, first-out (FISFO) queueing network with unconventional, i.e., non-Brownian, heavy traffic diffusion approximation. We also prove that fluid models of subcritical feedforward earliest-deadline-first (EDF) queueing networks are asymptotically stable.
1	We study information transmission in large interim quasilinear economies using the theory of the core. We concentrate on the core with respect to equilibrium blocking, a core notion in which information is transmitted endogenously within coalitions, because blocking can be understood as an equilibrium of a communication mechanism used by players in coalitions. We focus on independent replicas of the basic economy. We offer both negative and positive convergence results as a function of the complexity of the mechanisms used by coalitions. Furthermore, all of the results are robust to the relaxation of the incentive constraints. Results for ex post and signal-based replica processes are also obtained.
1	This paper presents an asymptotic analysis of a Monte Carlo method, variously known as sample average approximation (SAA) or sample path optimization (SPO), for a general two-stage stochastic minimization problem. We study the case when the second-stage problem may have multiple local optima or stationary points that are not global solutions and SAA is implemented using a general nonlinear programming solver that is only guaranteed to find stationary points. New optimality conditions are developed for both the true problem and its SAA problem to accommodate Karush-Kuhn-Tucker points. Because the optimality conditions are essentially stochastic generalized equations, the asymptotic analysis is carried out for the generalized equations first and then applied to optimality conditions. For this purpose, we analyze piecewise continuous (PC0) stochastic mappings to understand when their expectations are piecewise continuous and thereby derive exponential convergence of SAA. It is shown under moderate conditions that, with probability one, an accumulation point of the SAA stationary points satisfies a relaxed stationary condition for the true problem and further that, with probability approaching one exponentially fast with increasing sample size, a stationary point of SAA converges to the set of relaxed stationary points. These results strengthen or complement existing results where the second-stage problem is often assumed to have a unique solution and the exponential convergence is focused on how fast a solution of the true problem becomes an approximate solution of an SAA problem rather than the other way around.
1	We prove that the classic policy-iteration method [Howard, R. A. 1960. Dynamic Programming and Markov Processes. MIT, Cambridge] and the original simplex method with the most-negative-reduced-cost pivoting rule of Dantzig are strongly polynomial-time algorithms for solving the Markov decision problem (MDP) with a fixed discount rate. Furthermore, the computational complexity of the policy-iteration and simplex methods is superior to that of the only known strongly polynomial-time interior-point algorithm [Ye, Y. 2005. A new complexity result on solving the Markov decision problem. Math. Oper. Res.30(3) 733–749] for solving this problem. The result is surprising because the simplex method with the same pivoting rule was shown to be exponential for solving a general linear programming problem [Klee, V., G. J. Minty. 1972. How good is the simplex method? Technical report. O. Shisha, ed. Inequalities III. Academic Press, New York], the simplex method with the smallest index pivoting rule was shown to be exponential for solving an MDP regardless of discount rates [Melekopoglou, M., A. Condon. 1994. On the complexity of the policy improvement algorithm for Markov decision processes. INFORMS J. Comput.6(2) 188–192], and the policy-iteration method was recently shown to be exponential for solving undiscounted MDPs under the average cost criterion. We also extend the result to solving MDPs with transient substochastic transition matrices whose spectral radii are uniformly below one.
1	This paper considers a portfolio management problem of Merton's type in which the risky asset return is related to the return history. The problem is modeled by a stochastic system with delay. The investor's goal is to choose the investment control as well as the consumption control to maximize his total expected, discounted utility. Under certain situations, we derive the explicit solutions in a finite dimensional space.
1	Proving verification theorems can be tricky for models with both optimal stopping and state constraints. We pose and solve two alternative models of optimal consumption and investment with an optimal retirement date (optimal stopping) and various wealth constraints (state constraints). The solutions are parametric in closed form up to at most a constant. We prove the verification theorem for the main case with a nonnegative wealth constraint by combining the dynamic programming and Slater condition approaches. One unique feature of the proof is the application of the comparison principle to the differential equation solved by the proposed value function. In addition, we also obtain analytical comparative statics.
1	We study three discrete fixed point concept (SPERNER, DPZP, BROUWER) under two different models: the polynomial-time function model and the oracle function model. We fully characterize the computational complexities of these three problems. The computational complexity unification of the above problems gives us more choices in the study of different applications. As an example, by a reduction from DPZP, we derive asymptotically equal lower and upper bound for TUCKER in the oracle model. The same reduction also allows us to derive a single proof for the PPAD-completeness of TUCKER in any constant dimension, which is significantly simpler than the recent proofs.
1	We consider the single-machine scheduling problem to minimize the weighted sum of completion times under precedence constraints. In a series of recent papers, it was established that this scheduling problem is a special case of minimum weighted vertex cover.In this paper, we show that the vertex cover graph associated with the scheduling problem is exactly the graph of incomparable pairs defined in the dimension theory of partial orders. Exploiting this relationship allows us to present a framework for obtaining (2-2/f)-approximation algorithms, provided that the set of precedence constraints has fractional dimension of at most f. Our approach yields the best-known approximation ratios for all previously considered special classes of precedence constraints, and it provides the first results for bounded degree and orders of interval dimension 2.On the negative side, we show that the addressed problem remains NP-hard even when restricted to the special case of interval orders. Furthermore, we prove that the general problem, if a fixed cost present in all feasible schedules is ignored, becomes as hard to approximate as vertex cover. We conclude by giving the first inapproximability result for this problem, showing under a widely believed assumption that it does not admit a polynomial-time approximation scheme.
1	This paper considers a one-stage stochastic mathematical program with a complementarity constraint (SMPCC), where uncertainties appear in both the objective function and the complementarity constraint, and an optimal decision on both upper- and lower-level decision variables must be made before the realization of the uncertainties. A partially exactly penalized sample average approximation (SAA) scheme is proposed to solve the problem. Asymptotic convergence of optimal solutions and stationary points of the penalized SAA problem is carried out. It is shown under some moderate conditions that the statistical estimators obtained from solving the penalized SAA problems converge almost surely to its true counterpart as the sample size increases. Exponential rate of convergence of estimators is also established under some additional conditions.
1	In this paper, we investigate the optimal selling scheme for a capacity-constrained seller who faces both aggregate demand and individual valuation uncertainties. As each consumer privately observes an initial estimate prior to the transactions and an updated information postpurchase, the problem exhibits the sequential screening feature and return policy emerges as part of the optimal selling scheme. We show that the optimal selling scheme nicely resembles the classical multiunit auction design, and the product is returned too often compared to the efficient scenario. Furthermore, even though the postpurchase information update is privately observed by the individual consumers, the seller does not give away additional surplus to the consumers. Our general framework can accommodate various heterogeneities and uncertainties regarding hassle costs, shipping costs, salvage values, and reservation values. Our results go through even if the seller cannot commit to the refund amounts, the seller has flexible capacity, or consumers have multiunit demands.
1	A convex set with nonempty interior is maximal lattice-free if it is inclusion maximal with respect to the property of not containing integer points in its interior. Maximal lattice-free convex sets are known to be polyhedra. The precision of a rational polyhedron P in ℝd is the smallest natural number s such that sP is an integral polyhedron. In this paper we show that, up to affine mappings preserving ℤd, the number of maximal lattice-free rational polyhedra of a given precision s is finite. Furthermore, we present the complete list of all maximal lattice-free integral polyhedra in dimension three. Our results are motivated by recent research on cutting plane theory in mixed-integer linear optimization.
1	In the cutting stock problem, we are given a set of objects of different types, and the goal is to pack them all in the minimum possible number of identical bins. All objects have integer lengths, and objects of different types have different sizes. The total length of the objects packed in a bin cannot exceed the capacity of the bin. In this paper, we consider the version of the problem in which the number of different object types is constant, and we present a polynomial-time algorithm that computes a solution using at most one more bin than an optimum solution.
1	An instance of the quadratic assignment problem (QAP) with cost matrix Q is said to be linearizable if there exists an instance of the linear assignment problem (LAP) with cost matrix C such that for each assignment, the QAP and LAP objective function values are identical. Several sufficiency conditions are known that guarantee linearizability of a QAP. However, no polynomial time algorithm is known to test if a given instance of QAP is linearizable. In this paper, we give a necessary and sufficient condition for an instance of a QAP to be linearizable and develop an O(n4) algorithm to solve the corresponding linearization problem, where n is the size of the QAP.
1	Recent results showing PPAD-completeness of the problem of computing an equilibrium for Fisher's market model under additively separable, piecewise-linear, concave (PLC) utilities have dealt a serious blow to the program of obtaining efficient algorithms for computing equilibria in “traditional” market models, and has prompted a search for alternative models that are realistic as well as amenable to efficient computation. In this paper, we show that introducing perfect price discrimination into the Fisher model with PLC utilities renders its equilibrium polynomial time computable. Moreover, its set of equilibria are captured by a convex program that generalizes the classical Eisenberg-Gale program, and always admits a rational solution. We also give a combinatorial, polynomial time algorithm for computing an equilibrium.Next, we introduce production into our model, and again give a rational convex program that captures its equilibria. We use this program to obtain surprisingly simple proofs of both welfare theorems for this model. Finally, we also give an application of our price discrimination market model to online display advertising marketplaces.
1	Let K be a polytope in ℝn defined by m linear inequalities. We give a new Markov chain algorithm to draw a nearly uniform sample from K. The underlying Markov chain is the first to have a mixing time that is strongly polynomial when started from a “central” point. We use this result to design an affine interior point algorithm that does a single random walk to solve linear programs approximately.
1	We consider mixed-integer linear programs where free integer variables are expressed in terms of nonnegative continuous variables. When this model only has two integer variables, Dey and Louveaux characterized the intersection cuts that have infinite split rank. We show that, for any number of integer variables, the split rank of an intersection cut generated from a rational lattice-free polytope L is finite if and only if the integer points on the boundary of L satisfy a certain “2-hyperplane property.” The Dey–Louveaux characterization is a consequence of this more general result.
1	The asymptotic many-server queue with abandonments, G/GI/N + GI, is considered in the quality- and efficiency-driven (QED) regime. Here the number of servers and the offered load are related via the square-root rule, as the number of servers increases indefinitely. QED performance entails short waiting times and scarce abandonments (high quality) jointly with high servers' utilization (high efficiency), which is feasible when many servers cater to a single queue. For the G/GI/N + GI queue, we derive diffusion approximations for both its queue-length and virtual-waiting-time processes. Special cases, for which closed-form analysis is provided, are the G/M/N + GI and G/D/N + GI queues, thus expanding and generalizing existing results.
1	We consider the classical finite-state discounted Markovian decision problem, and we introduce a new policy iteration-like algorithm for finding the optimal state costs or Q-factors. The main difference is in the policy evaluation phase: instead of solving a linear system of equations, our algorithm requires solving an optimal stopping problem. The solution of this problem may be inexact, with a finite number of value iterations, in the spirit of modified policy iteration. The stopping problem structure is incorporated into the standard Q-learning algorithm to obtain a new method that is intermediate between policy iteration and Q-learning/value iteration. Thanks to its special contraction properties, our method overcomes some of the traditional convergence difficulties of modified policy iteration and admits asynchronous deterministic and stochastic iterative implementations, with lower overhead and/or more reliable convergence over existing Q-learning schemes. Furthermore, for large-scale problems, where linear basis function approximations and simulation-based temporal difference implementations are used, our algorithm addresses effectively the inherent difficulties of approximate policy iteration due to inadequate exploration of the state and control spaces.
1	Motivated by data-driven decision making and sampling problems, we investigate probabilistic interpretations of robust optimization (RO). We establish a connection between RO and distributionally robust stochastic programming (DRSP), showing that the solution to any RO problem is also a solution to a DRSP problem. Specifically, we consider the case where multiple uncertain parameters belong to the same fixed dimensional space and find the set of distributions of the equivalent DRSP problem. The equivalence we derive enables us to construct RO formulations for sampled problems (as in stochastic programming and machine learning) that are statistically consistent, even when the original sampled problem is not. In the process, this provides a systematic approach for tuning the uncertainty set. The equivalence further provides a probabilistic explanation for the common shrinkage heuristic, where the uncertainty set used in an RO problem is a shrunken version of the original uncertainty set.
1	In the symmetric rendezvous search game played on n locations two players are initially placed at two distinct locations. The game is played in discrete steps, at each of which each player can either stay where he is or move to a different location. The players share no common labelling of the locations. We wish to find a strategy such that, if both players follow it independently, then the expected number of steps until they are in the same location is minimized. Informal versions of the rendezvous problem have received considerable attention in the popular press. The problem was proposed by Alpern in 1976 [Alpern, S. 1976. Hide and seek games. Seminar at the Institut für Höhere Studien, Vienna], and it has proved notoriously difficult to analyse. In this paper we prove a 20-year-old conjecture that the following strategy is optimal for the game on three locations: in each block of two steps, stay where you are, or search the other two locations in random order, doing these with probabilities 1/3 and 2/3, respectively. This is now the first nontrivial symmetric rendezvous game of any type to be fully solved.
1	Howard [Howard, J. V. 2006. Unsolved symmetric rendezvous search problems: Some old and some new. Presentation, Sixth International Workshop in Search Games and Rendezvous, July 26, London School of Economics, London] has described a simply but nontrivial symmetric rendezvous search game in which two players are initially placed in two distinct locations. The game is played in discrete steps, at each of which each player can either stay where she is or move to the other location. When the players are in the same location for the first time they do not see one another, but when they are in the same location for a second time, then they meet. We wish to find a strategy such that, if both players follow it independently, then the expected number of steps at which this second meeting occurs is minimized. Howard conjectured that the optimal strategy is 3-Markovian, such that in each successive block of three steps the players should, with equal probability, do SSS, SMS, MSM, MMM, where “M” means move and “S” means stay. We prove that this strategy is optimal.
1	This paper studies a discrete-time total-reward Markov decision process (MDP) with a given initial state distribution. A (randomized) stationary policy can be split on a given set of states if the occupancy measure of this policy can be expressed as a convex combination of the occupancy measures of stationary policies, each selecting deterministic actions on the given set and coinciding with the original stationary policy outside of this set. For a stationary policy, necessary and sufficient conditions are provided for splitting it at a single state as well as sufficient conditions for splitting it on the whole state space. These results are applied to constrained MDPs. The results are refined for absorbing (including discounted) MDPs with finite state and actions spaces. In particular, this paper provides an efficient algorithm that presents the occupancy measure of a given policy as a convex combination of the occupancy measures of finitely many (stationary) deterministic policies. This algorithm generates the splitting policies in a way that each pair of consecutive policies differs at exactly one state. The results are applied to constrained problems to efficiently compute an optimal policy by computing and splitting a stationary optimal policy.
1	We consider the general model of zero-sum repeated games (or stochastic games with signals), and assume that one of the players is fully informed and controls the transitions of the state variable. We prove the existence of the uniform value, generalizing several results of the literature. A preliminary existence result is obtained for a particular class of stochastic games played with pure strategies.
1	This paper presents a question of topological dynamics and demonstrates that its affirmation would establish the existence of approximate equilibria in all quitting games with only normal players. A quitting game is an undiscounted stochastic game with finitely many players where every player has only two moves, to end the game with certainty or to allow the game to continue. If nobody ever acts to end the game, all players receive payoffs of 0. A player is normal if and only if by quitting alone she receives at least her min-max payoff. This proof is based on a version of the Kohlberg–Mertens [Kohlberg, E., J.-F. Mertens. 1986. On the strategic stability of equilibria. Econometrica54(5) 1003–1037] structure theorem designed specifically for quitting games.
1	We show that bounds like those of Al-Najjar and Smorodinsky [Al-Najjar, N. I., R. Smorodinsky. 2000. Pivotal players and the characterization of influence. J. Econom. Theory92(2) 318–342] as well as of Gradwohl et al. [Gradwohl, R., O. Reingold, A. Yadin, A. Yehudayoff. 2009. Players' effects under limited independence. Math. Oper. Res.34(4) 971–980] on the number of α-pivotal agents, derived by a combinatorial approach, can be obtained by decomposition of variance, i.e., an orthogonality argument as used in Appendix 1 of Mailath and Postlewaite [Mailath, G. J., A. Postlewaite. 1990. Asymmetric information bargaining problems with many agents. Rev. Econom. Stud.57(3) 351–367]. All these bounds have a similar asymptotic behavior, up to constant factors. Our bound is weaker than that of Al-Najjar and Smorodinsky, but we require only pairwise independent—rather than independent—types. Our result strengthens the bound of Gradwohl et al.
1	INFORMS is saddened to announce that Editor-in-Chief Uriel G. Rothblum passed away on March 26, 2012, after a sudden illness. He was an important scholar and dedicated leader of the INFORMS journal Mathematics of Operations Research. Our thoughts are with his family.  An obituary on Professor Rothblum can be found at http://mor.pubs.inform.org.J. G. “Jim” Dai, MOR Area Editor for Stochastic Models, will serve as Interim Editor-in-Chief through the end of 2012.
1	We present upper and lower bounds for the tail distribution of the stationary waiting time D in the stable GI/GI/s first-come first-served (FCFS) queue. These bounds depend on the value of the traffic load ρ which is the ratio of mean service and mean interarrival times. For service times with intermediate regularly varying tail distribution the bounds are exact up to a constant, and we are able to establish a “principle of s − k big jumps” in this case (here k is the integer part of ρ), which gives the most probable way for the stationary waiting time to be large. Another corollary of the bounds obtained is to provide a new proof of necessity and sufficiency of conditions for the existence of moments of the stationary waiting time.
1	We introduce a new approach for analysis and numerical simulations of asymmetric first-price auctions, which is based on dynamical systems. We apply this approach to asymmetric auctions in which players' valuations are power-law distributed. We utilize a dynamical-systems formulation to provide a proof of the existence and uniqueness of the equilibrium strategies in the cases of two coalitions and of two types of players. In the case of n different players, the singular point of the original system at b = 0 corresponds to a saddle point of the dynamical system with n − 1 admissible directions. This insight enables us to use forward solutions in the analysis and in the numerical simulations, in contrast with previous analytic and numerical studies that used backward solutions. The dynamical-systems approach provides an intuitive explanation for why the standard backward-shooting method for computing the equilibrium strategies is inherently unstable, and enables us to devise a stable forward-shooting method. In particular, in the case of two types of players, this method is extremely simple, as it does not require any shooting.
1	We consider the problem of designing truthful mechanisms on m unrelated machines, to minimize some optimization goal. Nisan and Ronen [Nisan, N., A. Ronen. 2001. Algorithmic mechanism design. Games Econom. Behav.35 166–196] consider the specific goal of makespan minimization, and show a lower bound of 2, and an upper bound of m. This large gap inspired many attempts that yielded positive results for several special cases, but very partial success for the general case: the lower bound was slightly increased to 2.61 by Christodoulou et al. [Christodoulou, G., E. Koutsoupias, A. Kovács. 2010. Mechanism design for fractional scheduling on unrelated machines. ACM Trans. Algorithms (TALG)6(2) 1–18] and Koutsoupias and Vidali [Koutsoupias, E., A. Vidali. 2007. A lower bound of 1+phi for truthful scheduling mechanisms. Proc. 32nd Internat. Sympos. Math. Foundations Comput. Sci. (MFCS)], while the best upper bound remains unchanged. In this paper we show the optimal lower bound on truthful anonymous mechanisms: no such mechanism can guarantee an approximation ratio better than m. Moreover, our proof yields similar optimal bounds for two other optimization goals: the sum of completion times and the lp norm of the schedule.
1	Solving Markov chains is, in general, difficult if the state space of the chain is very large (or infinite) and lacking a simple repeating structure. One alternative to solving such chains is to construct models that are simple to analyze and provide bounds for a reward function of interest. We present a new bounding method for Markov chains inspired by Markov reward theory: Our method constructs bounds by redirecting selected sets of transitions, facilitating an intuitive interpretation of the modifications of the original system. We show that our method is compatible with strong aggregation of Markov chains; thus we can obtain bounds for an initial chain by analyzing a much smaller chain. We illustrate our method by using it to prove monotonicity results and bounds for assemble-to-order systems.
1	We consider Markov decision processes where the values of the parameters are uncertain. This uncertainty is described by a sequence of nested sets (that is, each set contains the previous one), each of which corresponds to a probabilistic guarantee for a different confidence level. Consequently, a set of admissible probability distributions of the unknown parameters is specified. This formulation models the case where the decision maker is aware of and wants to exploit some (yet imprecise) a priori information of the distribution of parameters, and it arises naturally in practice where methods for estimating the confidence region of parameters abound. We propose a decision criterion based on distributional robustness: the optimal strategy maximizes the expected total reward under the most adversarial admissible probability distributions. We show that finding the optimal distributionally robust strategy can be reduced to the standard robust MDP where parameters are known to belong to a single uncertainty set; hence, it can be computed in polynomial time under mild technical conditions.
1	A reflection map, induced by the deterministic Skorohod problem on the nonnegative orthant, is applied to a vector-valued function X on the nonnegative real line and then to a + X, where a is a nonnegative constant vector. A question that was posed over 15 years ago is, under what conditions does the difference between the two resulting regulated functions converge to zero for any choice of a as time diverges. This, in turn, implies that if one imposes enough stochastic structure that ensures that the reflection map applied to a multidimensional process X converges in distribution, then it will also converge in distribution when it is applied to η + X, where η is any almost surely finite-valued random vector that may even depend on the process X. In this paper we obtain a useful equivalent characterization of this property. As a result, we are able to identify a natural sufficient condition in terms of the given data X and the constant routing matrix. A similar necessary condition is also indicated. A particular implication of our analysis is that under additional stochastic assumptions, asymptotic irrelevance of the initial condition does not require the existence of a stationary distribution. As immediate corollaries of our (and earlier) results we conclude that under the natural stability conditions, a reflected Lévy process as well as a Markov additive process has a unique stationary distribution and converges in distribution to this stationary distribution for every initial condition. Extensions of the sufficient condition are then developed for reflection maps with drift and routing coefficients that may be time- and state-dependent; some implications to multidimensional insurance models are briefly discussed.
1	We consider a network revenue management problem with customer choice and exogenous prices. We study the performance of a class of certainty-equivalent heuristic control policies. These heuristics periodically re-solve the deterministic linear program (DLP) that results when all future random variables are replaced by their average values and implement the solutions in a probabilistic manner. We provide an upper bound for the expected revenue loss under such policies when compared to the optimal policy. Using this bound, we construct a schedule of re-solving times such that the resulting expected revenue loss, obtained by re-solving the DLP at these times and implementing the solution as a probabilistic scheme, is bounded by a constant that is independent of the size of the problem.
1	For a minimal inequality derived from a maximal lattice-free simplicial polytope in ℝn, we investigate the region where minimal liftings are uniquely defined, and we characterize when this region covers ℝn. We then use this characterization to show that a minimal inequality derived from a maximal lattice-free simplex in ℝn with exactly one lattice point in the relative interior of each facet has a unique minimal lifting if and only if all the vertices of the simplex are lattice points.
1	This study is concerned with finding a level ideal (LI) of a partially ordered set (poset). Given a finite poset P, the level of each element p ∈ P is defined as the number of ideals that do not include p, then the problem is to find the ith LI–the ideal consisting of elements whose levels are less than a given integer i. The concept of a level ideal is naturally derived from the generalized median stable matchings, introduced by Teo and Sethuraman [Teo, C. P., J. Sethuraman. 1998. The geometry of fractional stable matchings and its applications. Math. Oper. Res.23(4) 874–891] in the context of “fairness” of matchings in a stable marriage problem. Cheng [Cheng, C. T. 2010. Understanding the generalized median stable matchings. Algorithmica58(1) 34–51] showed that finding the ith LI is #P-hard when i = Θ(N), where N is the total number of ideals of P. This paper shows that finding the ith LI is #P-hard even if i = Θ(N1/c), where c is an arbitrary constant at least one. Meanwhile, we present a polynomial time exact algorithm when i = O((log N)c′), where c′ is an arbitrary positive constant. We also devise two randomized approximation schemes for the ideals of a poset, by using an oracle of an almost-uniform sampler.
1	Let ℒ be a family of lattice-free polyhedra in ℝm containing the splits. Given a polyhedron P in ℝm + n, we characterize when a valid inequality for P ∩ (ℤm × ℝn) can be obtained with a finite number of disjunctive cuts corresponding to the polyhedra in ℒ. We also characterize the lattice-free polyhedra M such that all the disjunctive cuts corresponding to M can be obtained with a finite number of disjunctive cuts corresponding to the polyhedra in ℒ for every polyhedron P. Our results imply interesting consequences, related to split rank and to integral lattice-free polyhedra, that extend recent research findings.
1	Scheduling jobs on unrelated parallel machines so as to minimize makespan is one of the basic problems in the area of machine scheduling. In the first part of the paper, we prove that the power of preemption, i.e., the worst-case ratio between the makespan of an optimal nonpreemptive and that of an optimal preemptive schedule, is at least 4. This matches the upper bound proposed in Lin and Vitter [Lin, J.-H., J. S. Vitter. 1992. ε-approximations with minimum packing constraint violation. Proc. 24th Annual ACM Sympos. Theory of Comput. (STOC), ACM, New York, 771–782] two decades ago. In the second part of the paper, we consider the more general setting in which orders, consisting of several jobs, have to be processed on unrelated parallel machines so as to minimize the sum of weighted completion times of the orders. We obtain the first constant factor approximation algorithms for the preemptive and nonpreemptive cases, improving and extending a recent result by Leung et al. [Leung, J., H. Li, M. Pinedo, J. Zhang. 2007. Minimizing total weighted completion time when scheduling orders in a flexible environment with uniform machines. Inform. Processing Lett.103 119–129]. Finally, we study this problem in a parallel machine environment, obtaining a polynomial-time approximation scheme for several special cases.
1	We consider a hypothetical company that is assumed to have just manufactured and sold a number of copies of a product. It is known that, with a small probability, the company has committed a manufacturing fault that will require a recall. The company is able to observe the expiration times of the sold items whose distribution depends on whether the fault is present or absent. At the expiration of each item, a public inspection takes place that may reveal the fault, if it exists. Based on this information, the company can recall the product at any moment and pay back each customer the price of the product. If the company is not able to recall before an inspection reveals the fault, it pays a fine per item sold, which is assumed to be much larger than the price of the product. We compute the optimal recall time that minimizes the expected cost of recall of this company. We then derive and solve a stationary limit recall problem and show that the original problem converges to it as the number of items initially sold increases to ∞. Finally, we propose two extensions of the original model and compute the optimal recall times for these. In the first extension, the expired items are inspected only if they expire earlier than expected; in the second extension, the company is able to conduct internal/private inspections on the expired items. We provide numerical examples and simulation results for all three models.
1	We study the existence of pure Nash equilibria in weighted congestion games. Let 𝒞 denote a set of cost functions. We say that 𝒞 is consistent if every weighted congestion game with cost functions in 𝒞 possesses a pure Nash equilibrium. Our main contribution is a complete characterization of consistency of continuous cost functions. We prove that a set 𝒞 of continuous functions is consistent for two-player games if and only if 𝒞 contains only monotonic functions and for all nonconstant functions c1, c2 ∈ 𝒞, there are constants a, b ∈ ℝ such that c1(x) = a c2(x) + b for all x ∈ ℝ≥0. For games with at least three players, we prove that 𝒞 is consistent if and only if exactly one of the following cases holds: (a) 𝒞 contains only affine functions; (b) 𝒞 contains only exponential functions such that c(x) = ac eϕx + bc for some ac, bc, ϕ ∈ ℝ, where ac and bc may depend on c, while ϕ must be equal for every c ∈ 𝒞. The latter characterization is even valid for three-player games. Finally, we derive several characterizations of consistency of cost functions for games with restricted strategy spaces, such as weighted network congestion games or weighted congestion games with singleton strategies.
1	We study zero-sum risk-sensitive stochastic differential games on the infinite horizon with discounted and ergodic payoff criteria. Under certain assumptions, we establish the existence of values and saddle-point equilibria. We obtain our results by studying the corresponding Hamilton–Jacobi–Isaacs equations. Finally, we show that the value of the ergodic payoff criterion is a constant multiple of the maximal eigenvalue of the generators of the associated nonlinear semigroups.
1	This paper studies an infinite horizon adverse selection model with an underlying two-state Markov decision process. It introduces a novel approach that constructs the continuation payoff frontier exactly, as the fixed point of a functional operator. If the model supports an incentive-compatible first-best (ICFB) contract, the continuation payoff frontier can be efficiently constructed, and the principal's optimal contracts converge to ICFB contracts over time. The existence of an ICFB contract is implied by the common assumption of private values and is a fairly general scenario. The paper generalizes some key findings in the dynamic adverse selection literature to this scenario.
1	We provide an explicit gradient formula for linear chance constraints under a (possibly singular) multivariate Gaussian distribution. This formula allows one to reduce the calculus of gradients to the calculus of values of the same type of chance constraints (in smaller dimension and with different distribution parameters). This is an important aspect for the numerical solution of stochastic optimization problems because existing efficient codes for, e.g., calculating singular Gaussian distributions or regular Gaussian probabilities of polyhedra can be employed to calculate gradients at the same time. Moreover, the precision of gradients can be controlled by that of function values, which is a great advantage over using finite difference approximations. Finally, higher order derivatives are easily derived explicitly. The use of the obtained formula is illustrated for an example of a transportation network with stochastic demands.
1	The look-ahead estimator is used to compute densities associated with Markov processes via simulation. We study a framework that extends the look-ahead estimator to a broader range of applications. We provide a general asymptotic theory for the estimator, where both L1 consistency and L2 asymptotic normality are established. The L2 asymptotic normality implies √n convergence rates for L2 deviation.
1	The present paper develops a simple, easy to interpret algorithm for a large class of dynamic allocation problems with unknown, volatile demand. Potential applications include ad display problems and network revenue management problems. The algorithm operates in an online fashion and relies on reoptimization and forecast updates. The algorithm is robust (as witnessed by uniform worst-case guarantees for arbitrarily volatile demand) and in the event that demand volatility (or equivalently deviations in realized demand from forecasts) is not large, the method is simultaneously optimal. Computational experiments, including experiments with data from real-world problem instances, demonstrate the practicality and value of the approach. From a theoretical perspective, we introduce a new device—a balancing property—that allows us to understand the impact of changing bases in our scheme.
1	Within a Brownian diffusion Markovian framework, we provide a direct PDE characterization of the minimal initial endowment required so that the terminal wealth of a financial agent (possibly diminished by the payoff of a random claim) can match a set of constraints in probability. Such constraints should be interpreted as a rough description of a targeted profit and loss (P&L) distribution. This allows us to give a price to options under a P&L constraint, or to provide a description of the discrete P&L profiles that can be achieved given an initial capital. This approach provides an alternative to the standard utility indifference (or marginal) pricing rules that is better adapted to market practices. From the mathematical point of view, this is an extension of the stochastic target problem under controlled loss, studied in Bouchard, Touzi, and Elie [Bouchard B, Touzi N, Elie R (2009) Stochastic target problems with controlled loss. SIAM J. Control Optim. 48(5):3123–3150], to the case of multiple constraints. Although the associated Hamilton-Jacobi-Bellman operator is fully discontinuous, and the terminal condition is irregular, we are able to construct a numerical scheme that converges at any continuity points of the pricing function.
1	We consider the online stochastic matching problem proposed by Feldman et al. [Feldman J, Mehta A, Mirrokni VS, Muthukrishnan S (2009) Online stochastic matching: Beating 1 − 1/e. Annual IEEE Sympos. Foundations Comput. Sci. 117–126] as a model of display ad allocation. We are given a bipartite graph; one side of the graph corresponds to a fixed set of bins, and the other side represents the set of possible ball types. At each time step, a ball is sampled independently from the given distribution and it needs to be matched upon its arrival to an empty bin. The goal is to maximize the number of allocations.We present an online algorithm for this problem with a competitive ratio of 0.702. Before our result, algorithms with a competitive ratio better than 1 − 1/e were known under the assumption that the expected number of arriving balls of each type is integral. A key idea of the algorithm is to collect statistics about the decisions of the optimum offline solution using Monte Carlo sampling and use those statistics to guide the decisions of the online algorithm. We also show that our algorithm achieves a competitive ratio of 0.705 when the rates are integral.On the hardness side, we prove that no online algorithm can have a competitive ratio better than 0.823 under the known distribution model (and henceforth under the permutation model). This improves upon the 5/6 hardness result proved by Goel and Mehta [Goel G, Mehta A (2008) Online budgeted matching in random input models with applications to adwords. ACM-SIAM Symposium Discrete Algorithms 982–991] for the permutation model.
1	We consider a stochastic version of the well-known Blotto game, called the gladiator game. In this zero-sum allocation game two teams of gladiators engage in a sequence of one-on-one fights in which the probability of winning is a function of the gladiators' strengths. Each team's strategy is the allocation of its total strength among its gladiators. We find the Nash equilibria and the value of this class of games and show how they depend on the total strength of teams and the number of gladiators in each team. To do this, we study interesting majorization-type probability inequalities concerning linear combinations of gamma random variables. Similar inequalities have been used in models of telecommunications and research and development.
1	This paper presents sufficient conditions for the existence of stationary optimal policies for average cost Markov decision processes with Borel state and action sets and weakly continuous transition probabilities. The one-step cost functions may be unbounded, and the action sets may be noncompact. The main contributions of this paper are: (i) general sufficient conditions for the existence of stationary discount optimal and average cost optimal policies and descriptions of properties of value functions and sets of optimal actions, (ii) a sufficient condition for the average cost optimality of a stationary policy in the form of optimality inequalities, and (iii) approximations of average cost optimal actions by discount optimal actions.
1	This paper provides a systematic solvability analysis for (generalized) variational inequalities on separable Hilbert lattices. By contrast to a large part of the existing literature, our approach is lattice-theoretic, and is not based on topological fixed point theory. This allows us to establish the solvability of certain types of (generalized) variational inequalities without requiring the involved (set-valued) maps be hemicontinuous or monotonic. Some of our results generalize those obtained in the context of nonlinear complementarity problems in earlier work, and appear to have scope for applications. This is illustrated by means of several applications to fixed point theory, optimization, and game theory.
1	A family of constrained diffusions in a random environment is considered. Constraint set is a polyhedral cone and coefficients of the diffusion are governed by, in addition to the system state, a finite-state Markov process that is independent of the driving noise. Such models arise as limit objects in the heavy traffic analysis of generalized Jackson networks (GJN) with Markov-modulated arrival and processing rates. We give sufficient conditions (which, in particular, includes a requirement on the regularity of the underlying Skorohod map) for positive recurrence and geometric ergodicity. When the coefficients only depend on the modulating Markov process (i.e., they are independent of the system state), a complete characterization for stability and transience is provided. The case, where the pathwise Skorohod problem is not well posed but the underlying reflection matrix is completely-S, is treated as well. As consequences of geometric ergodicity various results, such as exponential integrability of invariant measures and central limit results (CLT) for fluctuations of long-time averages of process functionals about their stationary values, are obtained. Conditions for stability are formulated in terms of the averaged drift, where the average is taken with respect to the stationary distribution of the modulating Markov process. Finally, steady-state distributions of the underlying GJN are considered and it is shown that under suitable conditions, such distributions converge to the unique stationary distribution of the constrained random environment diffusion.
1	This work shows that the formation of a finite number of coalitions in a nonatomic network congestion game benefits everyone. At the equilibrium of the composite game played by coalitions and individuals, the average cost to each coalition and the individuals' common cost are all lower than in the corresponding nonatomic game (without coalitions). The individuals' cost is lower than the average cost to any coalition. Similarly, the average cost to a coalition is lower than that to any larger coalition. Whenever some members of a coalition become individuals, the individuals' payoff is increased. In the case of a unique coalition, both the average cost to the coalition and the individuals' cost are decreasing with respect to the size of the coalition. In a sequence of composite games, if a finite number of coalitions are fixed, and the size of the remaining coalitions goes to zero, the equilibria of these games converge to the equilibrium of a composite game played by the same fixed coalitions and the remaining individuals.
1	Provan and Billera defined the notion of weak k-decomposability for pure simplicial complexes in the hopes of bounding the diameter of convex polytopes. They showed the diameter of a weakly k-decomposable simplicial complex Δ is bounded above by a polynomial function of the number of k-faces in Δ and its dimension. For weakly 0-decomposable complexes, this bound is linear in the number of vertices and the dimension. In this paper we exhibit the first examples of non-weakly 0-decomposable simplicial polytopes. Our examples are in fact polar to certain transportation polytopes.
1	We study a tractable opinion dynamics model that generates long-run disagreements and persistent opinion fluctuations. Our model involves an inhomogeneous stochastic gossip process of continuous opinion dynamics in a society consisting of two types of agents: (1) regular agents who update their beliefs according to information that they receive from their social neighbors and (2) stubborn agents who never update their opinions and might represent leaders, political parties, or media sources attempting to influence the beliefs in the rest of the society. When the society contains stubborn agents with different opinions, the belief dynamics never lead to a consensus (among the regular agents). Instead, beliefs in the society fail to converge almost surely, the belief profile keeps on fluctuating in an ergodic fashion, and it converges in law to a nondegenerate random vector.The structure of the graph describing the social network and the location of the stubborn agents within it shape the opinion dynamics. The expected belief vector is proved to evolve according to an ordinary differential equation coinciding with the Kolmogorov backward equation of a continuous-time Markov chain on the graph with absorbing states corresponding to the stubborn agents, and hence to converge to a harmonic vector, with every regular agent's value being the weighted average of its neighbors' values, and boundary conditions corresponding to the stubborn agents' beliefs. Expected cross products of the agents' beliefs allow for a similar characterization in terms of coupled Markov chains on the graph describing the social network.We prove that, in large-scale societies, which are highly fluid, meaning that the product of the mixing time of the Markov chain on the graph describing the social network and the relative size of the linkages to stubborn agents vanishes as the population size grows large, a condition of homogeneous influence emerges, whereby the stationary beliefs' marginal distributions of most of the regular agents have approximately equal first and second moments.
1	To address the plurality of interpretations of the subjective notion of risk, we describe it by means of a risk order and concentrate on the context invariant features of diversification and monotonicity. 	Our main results are uniquely characterized robust representations of lower semicontinuous risk orders on vector spaces and convex sets. 	This representation covers most instruments related to risk and allows for a differentiated interpretation depending on the underlying context that is illustrated in different settings: 	for random variables, risk perception can be interpreted as model risk, and we compute among others the robust representation of the economic index of riskiness. 	For lotteries, risk perception can be viewed as distributional risk and we study the “value at risk.” 	For consumption patterns, which excerpt an intertemporality dimension in risk perception, we provide an interpretation in terms of discounting risk and discuss some examples.
1	The question as to whether the Gomory-Chvátal closure of a nonrational polytope is a polytope has been a longstanding open problem in integer programming. In this paper, we answer this question in the affirmative by combining ideas from polyhedral theory and the geometry of numbers.
1	Bandit problems model the trade-off between exploration and exploitation in various decision problems. We study two-armed bandit problems in continuous time, where the risky arm can have two types: High or Low; both types yield stochastic payoffs generated by a Lévy process. We show that the optimal strategy is a cut-off strategy and we provide an explicit expression for the cut-off and for the optimal payoff.
1	In this paper we study a Markov decision process with a nonlinear discount function. First, we define a utility on the space of trajectories of the process in the finite and infinite time horizon and then take their expected values. It turns out that the associated optimization problem leads to a nonstationary dynamic programming and an infinite system of Bellman equations, which result in obtaining persistently optimal policies. Our theory is enriched by examples.
1	In this paper we study various approaches for exploiting symmetries in polynomial optimization problems within the framework of semidefinite programming relaxations. Our special focus is on constrained problems especially when the symmetric group is acting on the variables. In particular, we investigate the concept of block decomposition within the framework of constrained polynomial optimization problems, show how the degree principle for the symmetric group can be computationally exploited, and also propose some methods to efficiently compute the geometric quotient.
1	In this paper we discuss representations of law invariant coherent risk measures in a form of integrals of the average value-at-risk measures. We show that such an integral representation exists iff the dual set of the considered risk measure is generated by one of its elements, and this representation is uniquely defined. On the other hand, representation of risk measures as a maximum of such integral forms is not unique. The suggested approach gives a constructive way for writing such representations.
1	Markov decision processes (MDPs) are powerful tools for decision making in uncertain dynamic environments. However, the solutions of MDPs are of limited practical use because of their sensitivity to distributional model parameters, which are typically unknown and have to be estimated by the decision maker. To counter the detrimental effects of estimation errors, we consider robust MDPs that offer probabilistic guarantees in view of the unknown parameters. To this end, we assume that an observation history of the MDP is available. Based on this history, we derive a confidence region that contains the unknown parameters with a prespecified probability 1-β. Afterward, we determine a policy that attains the highest worst-case performance over this confidence region. By construction, this policy achieves or exceeds its worst-case performance with a confidence of at least 1-β. Our method involves the solution of tractable conic programs of moderate size.
1	Joint use of resources with usage-dependent cost raises the question: who pays how much? We study cost sharing in resource selection games where the strategy spaces are either singletons or bases of a matroid defined on the ground set of resources. Our goal is to design cost sharing protocols so as to minimize the resulting price of anarchy and price of stability. We investigate three classes of protocols: basic protocols guarantee the existence of at least one pure Nash equilibrium; separable protocols additionally require that the resulting cost shares only depend on the set of players on a resource; uniform protocols are separable and require that the cost shares on a resource may not depend on the instance, that is, they remain the same even if new resources are added to or removed from the instance. We find optimal basic and separable protocols that guarantee the price of stability and price of anarchy to grow logarithmically in the number of players, except for the case of matroid games induced by separable protocols where the price of anarchy grows linearly with the number of players. For uniform protocols we show that the price of anarchy is unbounded even for singleton games.
1	We consider a totally asynchronous stochastic approximation algorithm, Q-learning, for solving finite space stochastic shortest path (SSP) problems, which are undiscounted, total cost Markov decision processes with an absorbing and cost-free state. For the most commonly used SSP models, existing convergence proofs assume that the sequence of Q-learning iterates is bounded with probability one, or some other condition that guarantees boundedness. We prove that the sequence of iterates is naturally bounded with probability one, thus furnishing the boundedness condition in the convergence proof by Tsitsiklis [Tsitsiklis JN (1994) Asynchronous stochastic approximation and Q-learning. Machine Learn. 16:185–202] and establishing completely the convergence of Q-learning for these SSP models.
1	We study a weaker formulation of the nullspace property which guarantees recovery of sparse signals from linear measurements by 𝓁1 minimization. We require this condition to hold only with high probability, given a distribution on the nullspace of the coding matrix A. Under some assumptions on the distribution of the reconstruction error, we show that testing these weak conditions means bounding the optimal value of two classical graph partitioning problems: the k-Dense-Subgraph and MaxCut problems. Both problems admit efficient, relatively tight relaxations, and we use a randomization argument to produce new approximation bounds for k-Dense-Subgraph. We test the performance of our results on several families of coding matrices.
1	In this paper, we address the basic geometric question of when a given convex set is the image under a linear map of an affine slice of a given closed convex cone. Such a representation or lift of the convex set is especially useful if the cone admits an efficient algorithm for linear optimization over its affine slices. We show that the existence of a lift of a convex set to a cone is equivalent to the existence of a factorization of an operator associated to the set and its polar via elements in the cone and its dual. This generalizes a theorem of Yannakakis that established a connection between polyhedral lifts of a polytope and nonnegative factorizations of its slack matrix. Symmetric lifts of convex sets can also be characterized similarly. When the cones live in a family, our results lead to the definition of the rank of a convex set with respect to this family. We present results about this rank in the context of cones of positive semidefinite matrices. Our methods provide new tools for understanding cone lifts of convex sets.
1	We introduce two subclasses of convex measures of risk, referred to as entropy coherent and entropy convex measures of risk. Entropy coherent and entropy convex measures of risk are special cases of φ-coherent and φ-convex measures of risk. Contrary to the classical use of coherent and convex measures of risk, which for a given probabilistic model entails evaluating a financial position by considering its expected loss, φ-coherent and φ-convex measures of risk evaluate a financial position under a given probabilistic model by considering its normalized expected φ-loss. We prove that (i) entropy coherent and entropy convex measures of risk are obtained by requiring φ-coherent and φ-convex measures of risk to be translation invariant; (ii) convex, entropy convex, and entropy coherent measures of risk emerge as certainty equivalents under variational, homothetic, and multiple priors preferences upon requiring the certainty equivalents to be translation invariant; and (iii) φ-convex measures of risk are certainty equivalents under variational and homothetic preferences if and only if they are convex and entropy convex measures of risk. In addition, we study the properties of entropy coherent and entropy convex measures of risk, derive their dual conjugate function, and characterize entropy coherent and entropy convex measures of risk in terms of properties of the corresponding acceptance sets.
1	We prove a many-server heavy-traffic fluid limit for an overloaded Markovian queueing system having two customer classes and two service pools, known in the call-center literature as the X model. The system uses the fixed-queue-ratio-with-thresholds (FQR-T) control, which we proposed in a recent paper as a way for one service system to help another in face of an unexpected overload. Under FQR-T, customers are served by their own service pool until a threshold is exceeded. Then, one-way sharing is activated with customers from one class allowed to be served in both pools. After the control is activated, it aims to keep the two queues at a prespecified fixed ratio. For large systems that fixed ratio is achieved approximately. For the fluid limit, or FWLLN (functional weak law of large numbers), we consider a sequence of properly scaled X models in overload operating under FQR-T. Our proof of the FWLLN follows the compactness approach, i.e., we show that the sequence of scaled processes is tight and then show that all converging subsequences have the specified limit. The characterization step is complicated because the queue-difference processes, which determine the customer-server assignments, need to be considered without spatial scaling. Asymptotically, these queue-difference processes operate on a faster time scale than the fluid-scaled processes. In the limit, because of a separation of time scales, the driving processes converge to a time-dependent steady state (or local average) of a time-varying fast-time-scale process (FTSP). This averaging principle allows us to replace the driving processes with the long-run average behavior of the FTSP.
1	The Lipschitz constant of a finite normal-form game is the maximal change in some player's payoff when a single opponent changes his strategy. We prove that games with small Lipschitz constant admit pure ϵ-equilibria, and pinpoint the maximal Lipschitz constant that is sufficient to imply existence of a pure ϵ-equilibrium as a function of the number of players in the game and the number of strategies of each player. Our proofs use the probabilistic method.
1	We consider the use of importance sampling to compute expectations of functionals of Markov processes. For a class of expectations that can be characterized as positive solutions to a linear system, we show there exists an importance measure that preserves the Markovian nature of the underlying process, and for which a zero-variance estimator can be constructed. The class of expectations considered includes expected infinite horizon discounted rewards as a particular case. In this setting, the zero-variance estimator and associated importance measure can exhibit behavior that is not observed when estimating simpler path functionals (like exit probabilities). The zero-variance estimators are not implementable in practice, but their characterization can guide the design of a good importance measure and associated estimator by trying to approximate the zero-variance ones. We present bounds on the mean-square error of such an approximate zero-variance estimator, based on Lyapunov inequalities.
1	Roth (Roth AE (1985) Conflict and coincidence of interest in job matching: Some new results and open questions. Math. Oper. Res. 10(3):379–389) claimed that (i) if each firm is allowed to select its most preferred subset of employees from those that assigned to it at two different stable matchings, then the choices result in a stable matching; and (ii) the set of stable matchings is a lattice under the partial order of the firms' common interests. Here, we provide counterexamples that show that these claims are incorrect, and we explain the flaws in Roth's reasoning.
1	Choosing a proper external risk measure is of great regulatory importance, as exemplified in the Basel II and Basel III Accords, which use value-at-risk with scenario analysis as the risk measures for setting capital requirements. We argue that a good external risk measure should be robust with respect to model misspecification and small changes in the data. A new class of data-based risk measures called natural risk statistics is proposed to incorporate robustness. Natural risk statistics are characterized by a new set of axioms. They include the Basel II and III risk measures and a subclass of robust risk measures as special cases; therefore, they provide a theoretical framework for understanding and, if necessary, extending the Basel Accords.
1	We consider the inverse optimization problem associated with the polynomial program  and a given current feasible solution . We provide a systematic numerical scheme to compute an inverse optimal solution. That is, we compute a polynomial  (which may be of the same degree as f, if desired) with the following properties: (a) y is a global minimizer of  on K with a Putinar's certificate with an a priori degree bound d fixed, and (b)  minimizes  (which can be the l1, l2 or l∞-norm of the coefficients) over all polynomials with such properties. Computing  reduces to solving a semidefinite program whose optimal value also provides a bound on how far f(y) is from the unknown optimal value f*. The size of the semidefinite program can be adapted to the available computational capabilities. Moreover, if one uses the l1-norm, then  takes a simple and explicit canonical form. Some variations are also discussed.
1	We discuss consistency of vanishingly smooth fictitious play, a strategy in the context of game theory, which can be regarded as a smooth fictitious play procedure, where the smoothing parameter is time dependent and asymptotically vanishes. This answers a question initially raised by Drew Fudenberg and Satoru Takahashi.
1	We consider the class of continuous functions that map an open set Ω ⫅ ℝn to ℝ with an epigraph having (locally) positive reach with an additional property. This class contains all finite convex and C1, 1 functions, but also ones that are not necessarily Lipschitz continuous. We provide a representation formula for the Clarke generalized gradient of such functions using convex combinations and limits of gradients at differentiability points, thus offering an alternative to the well-known proximal normal formula by replacing a pointedness assumption by one of positive reach. Our proof consists of a detailed analysis of singularities using methods taken from both nonsmooth analysis and geometric measure theory, and is based on an induction argument. As an application, we prove for a particular class of Hamilton-Jacobi equations that an a.e. solution whose hypograph has positive reach and satisfies an additional property is indeed the unique viscosity solution.
1	We analyze the behavior of closed multiclass product-form queueing networks when the number of customers grows to infinity and remains proportionate on each route (or class). First, we focus on the stationary behavior and prove the conjecture that the stationary distribution at nonbottleneck queues converges weakly to the stationary distribution of an ergodic, open product-form queueing network, which is geometric. This open network is obtained by replacing bottleneck queues with per-route Poissonian sources whose rates are uniquely determined by the solution of a strictly concave optimization problem. We strengthen such results by also proving convergence of the first moment of the queue lengths of nonbottleneck stations. Then we focus on the transient behavior of the network and use fluid limits to prove that the amount of fluid, or customers, on each route eventually concentrates on the bottleneck queues only and that the long-term proportions of fluid in each route and in each queue solve the dual of the concave optimization problem that determines the throughputs of the previous open network.
1	We construct a continuum of games on a countable set of players that does not possess a measurable equilibrium selection that satisfies a natural homogeneity property. The explicit nature of the construction yields counterexamples to the existence of equilibria in models with overlapping generations and in games with a continuum of players.
1	We study the steady-state behavior of multiserver queues with general job size distributions under size interval task assignment (SITA) policies. Assuming Poisson arrivals and the existence of the αth moment of the job size distribution for some α > 1, we show that if the job arrival rate and the number of servers increase to infinity with the traffic intensity held fixed, the SITA policy parameterized by α minimizes in a large deviation sense the steady-state probability that the total number of jobs in the system is greater than or equal to the number of servers. The optimal large deviation decay rate can be arbitrarily close to the one for the corresponding probability in an infinite-server queue, which only depends on the system traffic intensity but not on any higher moments of the job size distribution. This supports in a many-server asymptotic framework the common wisdom that separating large jobs from small jobs protects system performance against job size variability.
1	We study the space-and-time automaton-complexity of two related problems concerning the cycle length of a periodic stream of input bits. One problem is to find the exact cycle length of a periodic stream of input bits provided that the cycle length is bounded by a known parameter n. The other problem is to find a large number k that divides the cycle length. By “large” we mean that there is an unbounded increasing function f(n), such that either k is greater than f(n) or k is the exact cycle length.Our main results include that finding a large divisor of the cycle length can be solved in deterministic linear TIME and sub-linear SPACE, whereas finding the exact cycle length cannot be solved in deterministic TIME × SPACE smaller than a constant times n squared. Results involving probabilistic automata and applications to rate-distortion theory and repeated games are also discussed.
1	We consider the linear programming approach to approximate dynamic programming with an average cost objective and a finite state space. Using a Lagrangian form of the linear program (LP), the average cost error is shown to be a multiple of the best fit differential cost error. This result is analogous to previous error bounds for a discounted cost objective. Second, bounds are derived for average cost error and performance of the policy generated from the LP that involve the mixing time of the Markov decision process (MDP) under this policy or the optimal policy. These results improve on a previous performance bound involving mixing times.
1	The sample average approximation (SAA) method is a basic approach for solving stochastic variational inequalities (SVI). It is well known that under appropriate conditions the SAA solutions provide asymptotically consistent point estimators for the true solution to an SVI. It is of fundamental interest to use such point estimators along with suitable central limit results to develop confidence regions of prescribed level of significance for the true solution. However, standard procedures are not applicable because the central limit theorem that governs the asymptotic behavior of SAA solutions involves a discontinuous function evaluated at the true solution of the SVI. This paper overcomes such a difficulty by exploiting the precise geometric structure of the variational inequalities and by appealing to certain large deviations probability estimates, and proposes a method to build asymptotically exact confidence regions for the true solution that are computable from the SAA solutions. We justify this method theoretically by establishing a precise limit theorem, apply it to complementarity problems, and test it with a linear complementarity problem.
1	Farkas' lemma is a fundamental result from linear programming providing linear certificates for infeasibility of systems of linear inequalities. In semidefinite programming, such linear certificates only exist for strongly infeasible linear matrix inequalities. We provide nonlinear algebraic certificates for all infeasible linear matrix inequalities in the spirit of real algebraic geometry: A linear matrix inequality  is infeasible if and only if −1 lies in the quadratic module associated to A. We also present a new exact duality theory for semidefinite programming, motivated by the real radical and sums of squares certificates from real algebraic geometry.
1	Generating sample paths of stochastic differential equations (SDE) using the Monte Carlo method finds wide applications in financial engineering. Discretization is a popular approximate approach to generating those paths: it is easy to implement but prone to simulation bias. This paper presents a new simulation scheme to exactly generate samples for SDEs. The key observation is that the law of a general SDE can be decomposed into a product of the law of standard Brownian motion and the law of a doubly stochastic Poisson process. An acceptance-rejection algorithm is devised based on the combination of this decomposition and a localization technique. The numerical results corroborates that the mean-square error of the proposed method is in the order of O(t−1/2), which is superior to the conventional discretization schemes. Furthermore, the proposed method also can generate exact samples for SDE with boundaries which the discretization schemes usually find difficulty in dealing with.
1	This paper considers a general model of repeated games with incomplete information and imperfect monitoring. We study belief-free communication equilibria (BFCE) defined as follows. Players communicate with a mediator who receives types and signals and recommends actions. A BFCE is a communication device such that all players have an incentive to play faithfully, irrespectively of their belief about the state. We characterize BFCE payoffs for any repeated game with incomplete information in terms of one-shot payoff functions, information, and signaling structure.
1	We construct a generic, simple, and efficient scheduling policy for stochastic processing networks, and provide a general framework to establish its stability. Our policy is randomized and prioritized: with high probability it prioritizes jobs that have been least routed through the network. We show that the network is globally stable under this policy if there exists an appropriate quadratic local Lyapunov function that provides a negative drift with respect to nominal loads at servers. Applying this generic framework, we obtain stability results for our policy in many important examples of stochastic processing networks: open multiclass queueing networks, parallel server networks, networks of input-queued switches, and a variety of wireless network models with interference constraints. Our main novelty is the construction of an appropriate global Lyapunov function from quadratic local Lyapunov functions, which we believe to be of broader interest.
1	The normal cone to a constraint set plays a key role in optimization theory, algorithms, and applications. We consider the question of how to approximate the normal cone to a set under the assumption that the set is provided through an oracle function or collection of oracle functions, but contains some exploitable structure. We provide a new simplex gradient-based approximation technique that works for sets defined through a finite number of oracle-based functions. We further present novel results showing that, under a non-degeneracy condition, approximating normal cones to intersections of sets is possible by taking sums of approximations. Finally, we provide numerical results that exemplify the accuracy of the simplex gradient approximation when it is applicable, and the failure of this technique when a linear independence constraint qualification is not met.
1	A monopolist seller has multiple units of an indivisible good to sell over a discrete, finite time horizon. Buyers with unit demand arrive over time and each buyer privately knows her arrival time, her value for a unit, and her deadline. We study whether the seller's optimal allocation rule is a simple index rule. Each buyer is assigned an index and the allocation rule is calculated by a dynamic knapsack algorithm using those indices. “Simple” indicates that the index of a buyer depends only on “local” information, i.e., the distribution information for that time period. If buyer deadlines are public, such simple index rules are optimal if the standard increasing hazard rate condition on the distribution of valuations holds, and, given two buyers with the same deadline, the later-arriving one has a lower hazard rate (implying stochastically higher valuations). When buyer deadlines are private, this condition is neither sufficient nor necessary. If the rule we identify is not feasible, then the optimal allocation rule is not a simple index rule and cannot be calculated by backward induction.
1	We consider the problems of computing overflow probabilities at level N in any subset of stations in a Jackson network and of simulating sample paths conditional on overflow. We construct algorithms that take O(N) function evaluations to estimate such overflow probabilities within a prescribed relative accuracy and to simulate paths conditional on overflow at level N. The algorithms that we present are optimal in the sense that the best possible performance that can be expected for conditional sampling involves Ω(N) running time. As we explain in our development, our techniques have the potential to be applicable to more general classes of networks.
1	The bipartite traveling tournament problem (BTTP) is an NP-complete scheduling problem whose solution is a double round-robin inter-league tournament with minimum total travel distance. The 2n-team BTTP is a variant of the well-known traveling salesman problem (TSP), albeit much harder as it involves the simultaneous coordination of 2n teams playing a sequence of home and away games under fixed constraints, rather than a single entity passing through the locations corresponding to the teams' home venues. As the BTTP requires a distance-optimal schedule linking venues in close proximity, we provide an approximation algorithm for the BTTP based on an approximate solution to the corresponding TSP.We prove that our polynomial-time algorithm generates a 2n-team inter-league tournament schedule whose total distance is at most 1 + 2c/3 + (3 − c)/(3n) times the total distance of the optimal BTTP solution, where c is the approximation factor of the TSP. In practice, the actual approximation factor is far better; we provide a specific example by generating a nearly-optimal inter-league tournament for the 30-team National Basketball Association, with total travel distance just 1.06 times the trivial theoretical lower bound.
1	Submodular maximization generalizes many fundamental problems in discrete optimization, including Max-Cut in directed/undirected graphs, maximum coverage, maximum facility location, and marketing over social networks.In this paper we consider the problem of maximizing any submodular function subject to d knapsack constraints, where d is a fixed constant. We establish a strong relation between the discrete problem and its continuous relaxation, obtained through extension by expectation of the submodular function. Formally, we show that, for any nonnegative submodular function, an α-approximation algorithm for the continuous relaxation implies a randomized (α − ε)-approximation algorithm for the discrete problem. We use this relation to obtain an (e−1 − ε)-approximation for the problem, and a nearly optimal (1 − e−1 − ε)-approximation ratio for the monotone case, for any ε > 0. We further show that the probabilistic domain defined by a continuous solution can be reduced to yield a polynomial-size domain, given an oracle for the extension by expectation. This leads to a deterministic version of our technique.
1	In this paper, a unified framework of a nonlinear augmented Lagrangian dual problem is investigated for the primal problem of minimizing an extended real-valued function by virtue of a nonlinear augmenting penalty function. Our framework is more general than the ones in the literature in the sense that our nonlinear augmenting penalty function is defined on an open set and that our assumptions are presented in terms of a substitution of the dual variable, so our scheme includes barrier penalty functions and the weak peak at zero property as special cases. By assuming that the increment of the nonlinear augmenting penalty function with respect to the penalty parameter satisfies a generalized peak at zero property, necessary and sufficient conditions for the zero duality gap property are established and the existence of an exact penalty representation is obtained.
1	We consider a dynamic control problem for a parallel server system commonly known as the N-system. An N-system is a two-server parallel server system with two job classes, one server that can serve both classes, and one server that can only serve one class. We assume that jobs within each class arrive according to a renewal process. The random service time of a job has a general distribution that may depend on both the job's class and the server providing the service. Each job independently reneges, or abandons the queue without receiving service, if service does not begin within an exponentially distributed amount of time. The objective is to minimize the expected infinite horizon discounted cost of holding jobs in the system and having customers abandon, by dynamically scheduling waiting jobs to available servers.It is not possible to solve this control problem exactly, and so, we consider an asymptotic regime in which the system satisfies both a heavy traffic and a resource pooling condition. Then, we solve the limiting Brownian control problem, and interpret its solution as a policy in the original N-system. We label the servers and job classes so that server 1 can only serve class 1 and server 2 can serve both classes. The policy we propose has two thresholds. There is one threshold on the total number of jobs in the system, and one threshold on the number of class 1 jobs in the system. These thresholds are used to determine which job class server 2 should serve. We show that this proposed policy is asymptotically optimal within a specified class of admissible policies in the heavy traffic limit, and has the same limiting cost as the Brownian control problem solution.
1	We consider linear systems of equations, Ax = b, with an emphasis on the case where A is singular. Under certain conditions, necessary as well as sufficient, linear deterministic iterative methods generate sequences {xk} that converge to a solution as long as there exists at least one solution. This convergence property can be impaired when these methods are implemented with stochastic simulation, as is often done in important classes of large-scale problems. We introduce additional conditions and novel algorithmic stabilization schemes under which {xk} converges to a solution when A is singular and may also be used with substantial benefit when A is nearly singular.
1	We address online linear optimization problems when the possible actions of the decision maker are represented by binary vectors. The regret of the decision maker is the difference between her realized loss and the minimal loss she would have achieved by picking, in hindsight, the best possible action. Our goal is to understand the magnitude of the best possible (minimax) regret. We study the problem under three different assumptions for the feedback the decision maker receives: full information, and the partial information models of the so-called “semi-bandit” and “bandit” problems. In the full information case we show that the standard exponentially weighted average forecaster is a provably suboptimal strategy. For the semi-bandit model, by combining the Mirror Descent algorithm and the INF (Implicitely Normalized Forecaster) strategy, we are able to prove the first optimal bounds. Finally, in the bandit case we discuss existing results in light of a new lower bound, and suggest a conjecture on the optimal regret in that case.
1	We consider packing linear programs with m rows where all constraint coefficients are normalized to be in the unit interval. The n columns arrive in random order and the goal is to set the corresponding decision variables irrevocably when they arrive to obtain a feasible solution maximizing the expected reward. Previous (1 − ϵ)-competitive algorithms require the right-hand side of the linear program to be Ω((m/ϵ2)log(n/ϵ)), a bound that worsens with the number of columns and rows. However, the dependence on the number of columns is not required in the single-row case, and known lower bounds for the general case are also independent of n.Our goal is to understand whether the dependence on n is required in the multirow case, making it fundamentally harder than the single-row version. We refute this by exhibiting an algorithm that is (1 − ϵ)-competitive as long as the right-hand sides are Ω((m2/ϵ2)log(m/ϵ)). Our techniques refine previous probably approximately correct learning based approaches that interpret the online decisions as linear classifications of the columns based on sampled dual prices. The key ingredient of our improvement comes from a nonstandard covering argument together with the realization that only when the columns of the linear program belong to few one-dimensional subspaces we can obtain such small covers; bounding the size of the cover constructed also relies on the geometry of linear classifiers. General packing linear programs are handled by perturbing the input columns, which can be seen as making the learning problem more robust.
1	We consider the weighted maximum multiflow problem with respect to a terminal weight. We show that if the dimension of the tight span associated with the weight is at most 2, then this problem has a 1/12-integral optimal multiflow for every Eulerian supply graph. This result solves a weighted generalization of Karzanov's conjecture for classifying commodity graphs with finite fractionality. In addition, our proof technique proves the existence of an integral or half-integrality optimal multiflow for a large class of multiflow maximization problems, and it gives a polynomial time algorithm.
1	We investigate the problem of minimizing a certainty equivalent of the total or discounted cost over a finite and an infinite horizon that is generated by a Markov decision process (MDP). In contrast to a risk-neutral decision maker this optimization criterion takes the variability of the cost into account. It contains as a special case the classical risk-sensitive optimization criterion with an exponential utility. We show that this optimization problem can be solved by an ordinary MDP with extended state space and give conditions under which an optimal policy exists. In the case of an infinite time horizon we show that the minimal discounted cost can be obtained by value iteration and can be characterized as the unique solution of a fixed-point equation using a “sandwich” argument. Interestingly, it turns out that in the case of a power utility, the problem simplifies and is of similar complexity than the exponential utility case, however has not been treated in the literature so far. We also establish the validity (and convergence) of the policy improvement method. A simple numerical example, namely, the classical repeated casino game, is considered to illustrate the influence of the certainty equivalent and its parameters. Finally, the average cost problem is also investigated. Surprisingly, it turns out that under suitable recurrence conditions on the MDP for convex power utility, the minimal average cost does not depend on the parameter of the utility function and is equal to the risk-neutral average cost. This is in contrast to the classical risk-sensitive criterion with exponential utility.
1	A class of stochastic processes known as semi-martingale reflecting Brownian motions (SRBMs) is often used to approximate the dynamics of heavily loaded queueing networks. In two influential papers, Bramson [Bramson M (1998) State space collapse with applications to heavy-traffic limits for multiclass queueing networks. Queueing Systems 30:89–148] and Williams [Williams RJ (1998b) Diffusion approximations for open multiclass queueuing networks: Sufficient conditions involving state space collapse. Queueing Systems 30:27–88] laid out a general and structured approach for proving the validity of such heavy-traffic approximations, in which an SRBM is obtained as a diffusion limit from a sequence of suitably normalized workload processes. However, for multiclass networks it is still not known in general whether the steady-state distribution of the SRBM provides a valid approximation for the steady-state distribution of the original network. In this paper we study the case of queue-ratio disciplines and provide a set of sufficient conditions under which the above question can be answered in the affirmative. In addition to standard assumptions made in the literature towards the stability of the pre- and post-limit processes and the existence of diffusion limits, we add a requirement that solutions to the fluid model are attracted to the invariant manifold at a linear rate. For the special case of static-priority networks such linear attraction is known to hold under certain conditions on the network primitives. The analysis elucidates interesting connections between stability of the pre- and post-limit processes, their respective fluid models and state-space collapse, and identifies the respective roles played by all of the above in establishing validity of heavy-traffic steady-state approximations.
1	In this paper we study a stochastic production/inventory system with finite production capacity and random demand. The cumulative production and demand are modeled by a two-dimensional Brownian motion process. There is a setup cost for switching on the production and a convex holding and shortage cost, and our objective is to find the optimal production/inventory control that minimizes the average cost. Both lost-sales and backlog cases are studied. For the lost-sales model we show that, within a large class of policies, the optimal production strategy is either to produce according to an (s, S) policy, or never turn on the machine at all (thus it is optimal for the firm to not enter the business); whereas for the backlog model, we prove that the optimal production policy is always of the (s, S) type. Our approach first develops a lower bound for the average cost among a large class of nonanticipating policies and then shows that the value function of the desired policy reaches the lower bound. The results offer insights on the structure of the optimal control policies as well as the interplays between system parameters.
1	In the classical secretary problem an employer would like to choose the best candidate among n competing candidates that arrive in a random order. In each iteration, one candidate's rank vis-a-vis previously arrived candidates is revealed and the employer makes an irrevocable decision about her selection. This basic concept of n elements arriving in a random order and irrevocable decisions made by an algorithm have been explored extensively over the years, and used for modeling the behavior of many processes. Our main contribution is a new linear programming technique that we introduce as a tool for obtaining and analyzing algorithms for the secretary problem and its variants. The linear program is formulated using judiciously chosen variables and constraints and we show a one-to-one correspondence between algorithms for the secretary problem and feasible solutions to the linear program. Capturing the set of algorithms as a linear polytope holds the following immediate advantages:  Computing the optimal algorithm reduces to solving a linear program.Proving an upper bound on the performance of any algorithm reduces to finding a feasible solution to the dual program.Exploring variants of the problem is as simple as adding new constraints, or manipulating the objective function of the linear program.  We demonstrate these ideas by exploring some natural variants of the secretary problem. In particular, using our approach, we design optimal secretary algorithms in which the probability of selecting a candidate at any position is equal. We refer to such algorithms as position independent and these algorithms are motivated by the recent applications of secretary problems to online auctions. We also show a family of linear programs that characterize all algorithms that are allowed to choose J candidates and gain profit from the K best candidates. We believe that a linear programming based approach may be very helpful in the context of other variants of the secretary problem.
1	We consider a sequence of many-server queueing systems with impatient customers of the type G/M/n + GI in heavy traffic. This sequence is indexed by n, where the parameter n represents the number of servers in the nth system. The state process is considered to be the diffusion-scaled total customer count in the system and the service rate is a state-dependent perturbation of a given basic service rate μ0 > 0. When the system is critically loaded in the Halfin-Whitt heavy traffic regime, we obtain the limiting diffusion for the state processes. We also establish the asymptotic relationships among the diffusion-scaled processes representing the total customer count, virtual waiting time, and the number of customer abandonments. Motivated by the cost structures of telephone call centers, we formulate a cost functional and show that the expected value of this cost functional in the nth system converges to that of the limiting diffusion under mild assumptions.
1	We consider a decision network on an undirected graph in which each node corresponds to a decision variable, and each node and edge of the graph is associated with a reward function whose value depends only on the variables of the corresponding nodes. The goal is to construct a decision vector that maximizes the total reward. This decision problem encompasses a variety of models, including maximum-likelihood inference in graphical models (Markov Random Fields), combinatorial optimization on graphs, economic team theory, and statistical physics. The network is endowed with a probabilistic structure in which rewards are sampled from a distribution. Our aim is to identify sufficient conditions on the network structure and rewards distributions to guarantee average-case polynomiality of the underlying optimization problem. Additionally, we wish to characterize the efficiency of a decentralized solution generated on the basis of local information.We construct a new decentralized algorithm called Cavity Expansion and establish its theoretical performance for a variety of graph models and reward function distributions. Specifically, for certain classes of models we prove that our algorithm is able to find a near-optimal solution with high probability in a decentralized way. The success of the algorithm is based on the network exhibiting a certain correlation decay (long-range independence) property, and we prove that this property is indeed exhibited by the models of interest. Our results have the following surprising implications in the area of average-case complexity of algorithms. Finding the largest independent (stable) set of a graph is a well known NP-hard optimization problem for which no polynomial time approximation scheme is possible even for graphs with largest connectivity equal to three unless P = NP. Yet we show that the closely related Maximum Weight Independent Set problem for the same class of graphs admits a PTAS when the weights are independently and identically distributed with the exponential distribution. Namely, randomization of the reward function turns an NP-hard problem into a tractable one.
1	Reflected diffusions in polyhedral domains are commonly used as approximate models for stochastic processing networks in heavy traffic. Stationary distributions of such models give useful information on the steady-state performance of the corresponding stochastic networks, and thus it is important to develop reliable and efficient algorithms for numerical computation of such distributions. In this work we propose and analyze a Monte-Carlo scheme based on an Euler type discretization of the reflected stochastic differential equation using a single sequence of time discretization steps which decrease to zero as time approaches infinity. Appropriately weighted empirical measures constructed from the simulated discretized reflected diffusion are proposed as approximations for the invariant probability measure of the true diffusion model. Almost sure consistency results are established That, in particular, show that weighted averages of polynomially growing continuous functionals evaluated on the discretized simulated system converge a.s. to the corresponding integrals with respect to the invariant measure. Proofs rely on constructing suitable Lyapunov functions for tightness and uniform integrability and characterizing almost sure limit points through an extension of Echeverria's criteria for reflected diffusions. Regularity properties of the underlying Skorohod problems play a key role in the proofs. Rates of convergence for suitable families of test functions are also obtained. A key advantage of Monte-Carlo methods is the ease of implementation, particularly for high-dimensional problems. A numerical example of an eight-dimensional Skorohod problem is presented to illustrate the applicability of the approach.
1	An axiomatic model is presented in which a utility function over consequences, unique up to location and unit, is derived. The assumptions apply to a binary relation over purely subjective acts, namely, no exogenous probabilities are assumed. The main assumption used is a weak trade-off consistency condition. The model generalizes the biseparable model of Ghirardato and Marinacci [Ghirardato P, Marinacci M (2001) Risk, ambiguity, and the separation of utility and beliefs. Math. Oper. Res. 26(4):864–890].
1	This paper is devoted to the study of general nonsmooth problems of cone-constrained optimization (or conic programming) important for various aspects of optimization theory and applications. Based on advanced constructions and techniques of variational analysis and generalized differentiation, we derive new necessary optimality conditions (in both “exact” and “fuzzy” forms) for nonsmooth conic programs, establish characterizations of well-posedness for cone-constrained systems, and develop new applications to semi-infinite programming.
1	We revisit many-server approximations for the well-studied Erlang-A queue. This is a system with a single pool of i.i.d. servers that serve one class of impatient i.i.d. customers. Arrivals follow a Poisson process and service times are exponentially distributed as are the customers' patience times. We propose a diffusion approximation that applies simultaneously to all existing many-server heavy-traffic regimes: quality and efficiency driven, efficiency driven, quality driven, and nondegenerate slowdown. We prove that the approximation provides accurate estimates for a broad family of steady-state metrics. Our approach is “metric-free” in that we do not use the specific formulas for the steady-state distribution of the Erlang-A queue. Rather, we study excursions of the underlying birth-and-death process and couple these to properly defined excursions of the corresponding diffusion process. Regenerative process and martingale arguments, together with derivative bounds for solutions to certain ordinary differential equations, allow us to control the accuracy of the approximation. We demonstrate the appeal of universal approximation by studying two staffing optimization problems of practical interest.
1	We formulate the well-known economic lot scheduling problem (ELSP) with sequence-dependent setup times and costs as a semi-Markov decision process. Using an affine approximation of the bias function, we obtain a semi-infinite linear program determining a lower bound for the minimum average cost rate. Under a very mild condition, we can reduce this problem to a relatively small convex quadratically constrained linear problem by exploiting the structure of the objective function and the state space. This problem is equivalent to the lower bound problem derived by Dobson [Dobson G (1992) The cyclic lot scheduling problem with sequence-dependent setups. Oper. Res. 40:736–749] and reduces to the well-known lower bound problem introduced in Bomberger [Bomberger EE (1966) A dynamic programming approach to a lot size scheduling problem. Management Sci. 12:778–784] for sequence-dependent setups. We thus provide a framework that unifies previous work, and opens new paths for future research on tighter lower bounds and dynamic heuristics.
1	Determining the precise integrality gap for the subtour linear programming (LP) relaxation of the traveling salesman problem is a significant open question, with little progress made in thirty years in the general case of symmetric costs that obey triangle inequality. Boyd and Carr [Boyd S, Carr R (2011) Finding low cost TSP and 2-matching solutions using certain half-integer subtour vertices. Discrete Optim. 8:525–539. Prior version accessed June 27, 2011, http://www.site.uottawa.ca/~sylvia/recentpapers/halftri.pdf.] observe that we do not even know the worst-case upper bound on the ratio of the optimal 2-matching to the subtour LP; they conjecture the ratio is at most 10/9.In this paper, we prove the Boyd-Carr conjecture. In the case that the support of a fractional 2-matching has no cut edge, we can further prove that an optimal 2-matching has cost at most 10/9 times the cost of the fractional 2-matching.
1	We address a conjecture introduced by Massoulié [Massoulié L (2007) Structural properties of proportional fairness: Stability and insensitivity. Ann. Appl. Probab. 17(3):809–839], concerning the large deviations of the stationary measure of bandwidth-sharing networks functioning under the proportional fair allocation. For Markovian networks, we prove that proportional fair and an associated reversible allocation are geometrically ergodic and have the same large deviations characteristics using Lyapunov functions and martingale arguments. For monotone networks, we give a more direct proof of the same result, relying on stochastic comparisons, that holds for general service time distribution. These results support the intuition that proportional fairness is “close” to allocations of service being insensitive to the service time distribution.
1	The main contribution of this paper is to propose a new dynamic-programming approach that ε-approximates the joint replenishment problem, with stationary demands and holding costs, in its discrete-time finite-horizon setting. Our first and foremost objective is to show that the computation time of classical dynamic-programming algorithms can be improved on by orders of magnitude when one is willing to lose an ε-factor in optimality. Based on synthesizing ideas such as commodity aggregation, approximate dynamic programming, and a few guessing tricks, we show that one can attain any required degree of accuracy in near-polynomial time.
1	We consider the consumer problem under uncertainty when the consumer can choose the quantity of a risk-free good and the lottery, or distribution, of a risky good from a set of distributions. These goods are imperfect substitutes in the consumer preferences, with additive preferences a special case. We develop sufficient conditions for the choice of the risky good to be monotone with respect to income, exploring different notions of monotonicity. The sufficient conditions are ordinal, independent of concavity, and do not require differentiability or continuity. Cardinal conditions and conditions from the single good case are not necessary and are not always sufficient. The sufficient conditions are formulated in appropriate value lattices. The framework is flexible and adaptable to handle different uncertainty applications. Examples demonstrate the sufficient conditions and different applications where available lotteries may be finite in number, may have discrete support, or may form a chain or a lattice.
1	We consider general singular control problems for random fields given by a stochastic partial differential equation (SPDE). We show that under some conditions the optimal singular control can be identified with the solution of a coupled system of SPDE and a reflected backward SPDE (RBSPDE). As an illustration we apply the result to a singular optimal harvesting problem from a population whose density is modeled as a stochastic reaction-diffusion equation. Existence and uniqueness of solutions of RBSPDEs are established, as well as comparison theorems. We then establish a relation between RBSPDEs and optimal stopping of SPDEs, and apply the result to a risk-minimizing stopping problem.
1	Enlightened by the theory of Watanabe [Watanabe S (1987) Analysis of Wiener functionals (Malliavin calculus) and its applications to heat kernels. Ann. Probab. 15:1–39] for analyzing generalized random variables and its further development in Yoshida [Yoshida N (1992a) Asymptotic expansions for statistics related to small diffusions. J. Japan Statist. Soc. 22:139–159], Takahashi [Takahashi A (1995) Essays on the valuation problems of contingent claims. Ph.D. thesis, Haas School of Business, University of California, Berkeley, Takahashi A (1999) An asymptotic expansion approach to pricing contingent claims. Asia-Pacific Financial Markets 6:115–151] as well as Kunitomo and Takahashi [Kunitomo N, Takahashi A (2001) The asymptotic expansion approach to the valuation of interest rate contingent claims. Math. Finance 11(1):117–151, Kunitomo N, Takahashi A (2003) On validity of the asymptotic expansion approach in contingent claim analysis. Ann. Appl. Probab. 13(3):914–952] etc., we focus on a wide range of multivariate diffusion models and propose a general probabilistic method of small-time asymptotic expansions for approximating option price in simple closed-form up to an arbitrary order. To explicitly construct correction terms, we introduce an efficient algorithm and novel closed-form formulas for calculating conditional expectation of multiplication of iterated stochastic integrals, which are potentially useful in a wider range of topics in applied probability and stochastic modeling for operations research. The performance of our method is illustrated through various models nested in constant elasticity of variance type processes. With an application in pricing options on VIX under GARCH diffusion and its multifactor generalization to the Gatheral double lognormal stochastic volatility models, we demonstrate the versatility of our method in dealing with analytically intractable non-Lévy and non-affine models. The robustness of the method is theoretically supported by justifying uniform convergence of the expansion over the whole set of parameters.
1	In this paper, we consider a single-item, one-machine production-inventory system with compound Poisson demand. The production facility may be on or off. While on, the production rate is constant, and, while off, the production rate is zero. System costs consist of switching costs and inventory and backlogging costs. We provide conditions when (s, S)-policies are optimal under the long-run average expected cost criterion. These conditions are met in particular, but not necessarily, when the inventory costs are convex. The developed method in the proof is easy to apply to more general cases. Moreover, the method allows us to compute optimal policies very efficiently.
1	Bandwidth-sharing networks as considered by Roberts and Massoulié [28] (Roberts JW, Massoulié L (1998) Bandwidth sharing and admission control for elastic traffic. Proc. ITC Specialist Seminar, Yokohama, Japan) provide a natural modeling framework for describing the dynamic flow-level interaction among elastic data transfers. Under mild assumptions, it has been established that a wide family of so-called α-fair bandwidth-sharing strategies achieve stability in such networks provided that no individual link is overloaded.In the present paper we focus on bandwidth-sharing networks where the load on one or several of the links exceeds the capacity. To characterize the overload behavior, we examine the fluid limit, which emerges when the flow dynamics are scaled in both space and time. We derive a functional equation characterizing the fluid limit, and show that any strictly positive solution must be unique, which in particular implies the convergence of the scaled number of flows to the fluid limit for nonzero initial states when the load is sufficiently high. For the case of a zero initial state and a zero-degree homogeneous rate allocation function, we show that there exists a linear solution to the fluid-limit equation, and obtain a fixed-point equation for the corresponding asymptotic growth rates. It is proved that a fixed-point solution is also a solution to a related strictly concave optimization problem, and hence exists and is unique. In addition, we establish uniqueness of fluid-model solutions for monotone rate-preserving networks (in particular tree networks).
1	We consider robust (undirected) network design (RND) problems where the set of feasible demands may be given by an arbitrary convex body. This model, introduced by Ben-Ameur and Kerivin [Ben-Ameur W, Kerivin H (2003) New economical virtual private networks. Comm. ACM 46(6):69–73], generalizes the well-studied virtual private network (VPN) problem. Most research in this area has focused on constant factor approximations for specific polytope of demands, such as the class of hose matrices used in the definition of VPN. As pointed out in Chekuri [Chekuri C (2007) Routing and network design with robustness to changing or uncertain traffic demands. SIGACT News 38(3):106–128], however, the general problem was only known to be APX-hard (based on a reduction from the Steiner tree problem). We show that the general robust design is hard to approximate to within polylogarithmic factors. We establish this by showing a general reduction of buy-at-bulk network design to the robust network design problem. Gupta pointed out that metric embeddings imply an O(log n)-approximation for the general RND problem, and hence this is tight up to polylogarithmic factors.In the second part of the paper, we introduce a natural generalization of the VPN problem. In this model, the set of feasible demands is determined by a tree with edge capacities; a demand matrix is feasible if it can be routed on the tree. We give a constant factor approximation algorithm for this problem that achieves factor of 8 in general, and 2 for the case where the tree has unit capacities. As an application of this result, we consider so-called H-tope demand polytopes. These correspond to demands which are routable in some graph H. We show that the corresponding RND problem has an O(1)-approximation if H admits a stochastic constant-distortion embedding into tree metrics.
1	We consider a nonlinear extension of the generalized network flow model, with the flow leaving an arc being an increasing concave function of the flow entering it, as proposed by Truemper [Truemper K (1978) Optimal flows in nonlinear gain networks. Networks 8(1):17–36] and by Shigeno [Shigeno M (2006) Maximum network flows with concave gains. Math. Programming 107(3):439–459]. We give a polynomial time combinatorial algorithm for solving corresponding flow maximization problems, finding an ε-approximate solution in O(m(mσ+log n)log(MUm/ε)) arithmetic operations, where M and U are upper bounds on simple parameters, and σ is the complexity of a value oracle query for the gain functions. This also gives a new algorithm for linear generalized flows, an efficient, purely scaling variant of the Fat-Path algorithm by Goldberg et al. [Goldberg AV, Plotkin SA, Tardos É (1991) Combinatorial algorithms for the generalized circulation problem. Math. Oper. Res. 16(2):351–381], not using any cycle cancellations.We show that this general convex programming model serves as a common framework for several market equilibrium problems, including the linear Fisher market model and its various extensions. Our result immediately provides combinatorial algorithms for various extensions of these market models. This includes nonsymmetric Arrow-Debreu Nash bargaining, settling an open question by Vazirani [Vazirani VV (2012) The notion of a rational convex program, and an algorithm for the Arrow-Debreu Nash bargaining game. J. ACM 59(2), Article 7].
1	This paper considers infinite-horizon finite state-and-action Markov population decision chains (MPDCs) in which the goal is to find a stationary stopping policy with maximum stopping value, that is, with maximum value over all deterministic Markov stopping policies. A policy is stopping if the resulting expected population size in a period diminishes to zero as the period converges to infinity. The paper shows that the following are equivalent: (a) there is a stationary maximum-stopping value policy; (b) the maximum stopping value is finite; (c) there is a stopping policy and an excessive point of the optimal return operator; and (d) the maximum stopping value is the least excessive (resp., fixed) point of the optimal return operator. The paper shows how to use linear programming, policy improvement and successive approximations to solve the problem. The problem arises in stopping a Markov chain, as in optimally eradicating a pest or disease. The problem is one of two key subproblems used repeatedly to find Blackwell and Cesàro-overtaking optimal policies in finite Markov decision chains (MDCs), both of which, with their generalizations, have a vast number of applications. The problem for MDCs and/or MPDCs has been studied under various conditions, and over the years, by many investigators including Bellman, Bertsekas, Blackwell, Dantzig, Denardo, d'Epenoux, Derman, Dynkin, Eaton, Erickson, Ford, Hordijk, Howard, Kallenberg, Manne, O'Sullivan, Rothblum, Shoemaker, Strauch, Tsitsiklis, Veinott, Wagner, and Zadeh.†1934-2012.
1	A linear problem is completely solved by a randomizing linear algorithm if the set of data-solution pairs of the problem equals the set of input–output pairs of the algorithm over all randomizations. Linear problems are based on the predicate language over ordered fields. This class contains, for example, all linear programs, all bounded variable integer programs, all satisfiability problems in sentential logic, and models of conflict. Randomizing linear algorithms are based on a tree, arithmetic, comparisons, and random selections, over ordered fields, and with a restriction on certain operations. We show that the correspondence “completely solved by” is one-to-one and onto from equivalences of linear problems to equivalences of randomizing linear algorithms.†1947–2012.
1	We consider variants of the online stochastic bipartite matching problem motivated by Internet advertising display applications, as introduced in Feldman et al. [Feldman J, Mehta A, Mirrokni VS, Muthukrishnan S (2009) Online stochastic matching: Beating 1 − 1/e. FOCS '09: Proc. 50th Annual IEEE Sympos. Foundations Comput. Sci. (IEEE, Washington, DC), 117–126]. In this setting, advertisers express specific interests into requests for impressions of different types. Advertisers are fixed and known in advance, whereas requests for impressions come online. The task is to assign each request to an interested advertiser (or to discard it) immediately upon its arrival.In the adversarial online model, the ranking algorithm of Karp et al. [Karp RM, Vazirani UV, Varirani VV (1990) An optimal algorithm for online bipartite matching. STOC '90: Proc. 22nd Annual ACM Sympos. Theory Comput. (ACM, New York), 352–358] provides a best possible randomized algorithm with competitive ratio 1 − 1/e ≈ 0.632.In the stochastic i.i.d. model, when requests are drawn repeatedly and independently from a known probability distribution over the different impression types, Feldman et al. [Feldman J, Mehta A, Mirrokni VS, Muthukrishnan S (2009) Online stochastic matching: Beating 1 − 1/e. FOCS '09: Proc. 50th Annual IEEE Sympos. Foundations Comput. Sci. (IEEE, Washington, DC), 117–126] prove that one can do better than 1 − 1/e. Under the restriction that the expected number of request of each impression type is an integer, they provide a 0.670-competitive algorithm, later improved by Bahmani and Kapralov [Bahmani B, Kapralov M (2010) Improved bounds for online stochastic matching. ESA '10: Proc. 22nd Annual Eur. Sympos. Algorithms (Springer-Verlag, Berlin, Heidelberg), 170–181] to 0.699 and by Manshadi et al. [Manshadi V, Gharan SO, Saberi A (2012) Online stochastic matching: Online actions based on offline statistics. Math. Oper. Res. 37(4):559–573] to 0.705. Without this integrality restriction, Manshadi et al. are able to provide a 0.702-competitive algorithm.In this paper we consider a general class of online algorithms for the i.i.d. model that improve on all these bounds and that use computationally efficient offline procedures (based on the solution of simple linear programs of maximum flow types). Under the integrality restriction on the expected number of impression types, we get a 1 − 2e−2(≈0.729)-competitive algorithm. Without this restriction, we get a 0.706-competitive algorithm.Our techniques can also be applied to other related problems such as the online stochastic vertex-weighted bipartite matching problem as defined in Aggarwal et al. [Aggarwal G, Goel G, Karande C, Mehta A (2011) Online vertex-weighted bipartite matching and single-bid budgeted allocations. SODA '11: Proc. 22nd Annual ACM-SIAM Sympos. Discrete Algorithms (SIAM, Philadelphia), 1253–1264]. For this problem, we obtain a 0.725-competitive algorithm under the stochastic i.i.d. model with integral arrival rate.Finally, we show the validity of all our results under a Poisson arrival model, removing the need to assume that the total number of arrivals is fixed and known in advance, as is required for the analysis of the stochastic i.i.d. models described above.
1	The importance of “paired comparisons” has led to the development of several approaches. Missing is a common analytical way to compare techniques and explain properties. To do so, the approach developed here creates a “data space” coordinate system where data aspects that satisfy a strong transitivity condition are separated from those that represent “noise” as characterized by cyclic effects. With this system, paired comparison rules can be compared and paradoxical behavior explained.
1	In the setting of the stable matching (SM) problem, it has been observed that some of the man-woman pairs cannot be removed although they participate in no stable matching, since such a removal would alter the set of solutions. These pairs are yet to be identified. Likewise (and despite the sizeable literature), some of the fundamental characteristics of the SM polytope (e.g., its dimension, its facets, etc.) have not been established. In the current work, we show that these two seemingly distant open issues are closely related. More specifically, we identify the pairs with the above-mentioned property and present a polynomial algorithm for producing a set of minimal preference lists. We utilize this result in the context of two different representations of the SM structure (rotation-poset graph and algebraic formulation) and derive the dimension of the SM polytope to obtain all alternative minimal linear descriptions.
1	A multiclass many-server system is considered, in which customers are served according to a nonpreemptive priority policy and may renege while waiting to enter service. The service and reneging time distributions satisfy mild conditions. Building on an approach developed by Kaspi and Ramanan, the law-of-large-numbers many-server asymptotics are characterized as the unique solution to a set of differential equations in a measure space, regarded as fluid model equations. In stationarity, convergence to the explicitly solved invariant state of the fluid-model equations is established. An immediate consequence of the results in the case of exponential reneging is the asymptotic optimality of an index policy, called the cμ/θ rule, for the problem of minimizing linear queue-length and reneging costs. A certain Skorohod map plays an important role in obtaining both uniqueness of solutions to the fluid-model equations and convergence.
1	A point lies on a network according to some unknown probability distribution. Starting at a specified root of the network, a Searcher moves to find this point at speeds that depend on his location and direction. He seeks the randomized search algorithm that minimizes the expected search time. This is equivalent to modeling the problem as a zero-sum hide-and-seek game whose value is called the search value of the network.We make a new and direct derivation of an explicit formula for the search value of a tree, proving that it is equal to half the sum of the minimum tour time of the tree and a quantity called its incline. The incline of a tree is an average over the leaf nodes of the difference between the time taken to travel from the root to a leaf node and the time taken to travel from a leaf node to the root. This difference can be interpreted as height of a leaf node, assuming uphill is slower than downhill. We then apply this formula to obtain numerous results for general networks. We also introduce a new general method of comparing the search value of networks that differ in a single arc. Some simple networks have very complicated optimal strategies that require mixing of a continuum of pure strategies. Many of our results generalize analogous ones obtained for constant velocity (in both directions) by S. Gal, but not all of those results can be extended.
1	Bewley and Kohlberg [Bewley T, Kohlberg E (1976) The asymptotic theory of stochastic games. Math. Oper. Res. 1:197–208] proved that the discounted values of finite zero-sum stochastic games have a limit, as the discount factor tends to zero, using the Tarski-Seidenberg elimination theorem from real algebraic geometry. This was a fundamental step in the development of the theory of stochastic games. The current paper provides a new and direct proof for this result, relying on the explicit description of asymptotically optimal strategies. Moreover, we prove that our approach can also be used to obtain the existence of the uniform value, as in Mertens and Neyman [Mertens J-F, Neyman A (1981) Stochastic games. Internat. J. Game Theory 10:53–66].
1	In the generalized assignment problem (gap), a set of jobs seek to be assigned to a set of machines. For every job-machine pair, there are a specific value and an accommodated capacity for the assignment. The objective is to find an assignment that maximizes the total sum of values given that the capacity constraint of every machine is satisfied.The gap is a classic optimization problem and has been studied extensively from the algorithmic perspective. Dughmi and Ghosh [Dughmi S, Ghosh A (2010) Truthful assignment without money. ACM Conf. Electronic Commerce (ACM, New York), 325–334.] proposed the game theoretical framework in which every job is owned by a selfish agent who aims to maximize the value of his own assignment. They gave a logarithmic approximation truthful in expectation mechanism and left open the problem whether there exists a truthful mechanism with a constant approximation factor. In this paper, we give an affirmative answer to this question and provide a constant approximation mechanism that enjoys a stronger incentive property of universal truthfulness than that of truthfulness in expectation.Our mechanism is inspired by stable matching, which is a fundamental solution concept in the context of matching marketplaces. The mechanism uses a stable matching algorithm as a critical component and adopts other approaches like random sampling.
1	Our primary query is to find conditions under which the closure of a preorder on a topological space remains transitive. We study this problem for translation invariant preorders on topological groups. The results are fairly positive; we find that the closure of preorders and normal orders remain as such in this context. The same is true for factor orders as well under quite general conditions. In turn, in the context of topological linear spaces, these results allow us to obtain a simple condition under which the order-duals with respect to a vector order and its closure coincide. Various order-theoretic applications of these results are also provided in the paper.
1	Bandwidth-sharing networks as introduced by Roberts and Massoulié [Roberts JW, Massoulié L (1998) Bandwidth sharing and admission control for elastic traffic. Proc. ITC Specialist Seminar, Yokohama, Japan], Massoulié and Roberts [Massoulié L, Roberts JW (1999) Bandwidth sharing: Objectives and algorithms. Proc. IEEE Infocom. (Books in Statistics, New York), 1395–1403] model the dynamic interaction among an evolving population of elastic flows competing for several links. With policies based on optimization procedures, such models are of interest both from a queueing theory and operations research perspective. In the present paper, we focus on bandwidth-sharing networks with capacities and arrival rates of a large order of magnitude compared to transfer rates of individual flows. This regime is standard in practice. In particular, we extend previous work by Reed and Zwart [Reed J, Zwart B (2010) Limit theorems for bandwidth-sharing networks with rate constraints. Revised, preprint http://people.stern.nyu.edu/jreed/Papers/BARevised.pdf] on fluid approximations for such networks: we allow interarrival times, flow sizes, and patient times (i.e., abandonment times measured from the arrival epochs) to be generally distributed, rather than exponentially distributed. We also develop polynomial-time computable fixed-point approximations for stationary distributions of bandwidth-sharing networks, and suggest new techniques for deriving these types of results.
1	In this paper, we introduce a notion to be called k-wise uncorrelated random variables, which is similar but not identical to the so-called k-wise independent random variables in the literature. We show how to construct k-wise uncorrelated random variables by a simple procedure. The constructed random variables can be applied, e.g., to express the quartic polynomial (xTQx)2, where Q is an n × n positive semidefinite matrix, by a sum of fourth powered polynomial terms, known as Hilbert's identity. By virtue of the proposed construction, the number of required terms is no more than 2n4 + n. This implies that it is possible to find a (2n4 + n)-point distribution whose fourth moments tensor is exactly the symmetrization of Q ⊗ Q. Moreover, we prove that the number of required fourth powered polynomial terms to express (xTQx)2 is at least n(n + 1)/2. The result is applied to prove that computing the matrix 2 ↦ 4 norm is NP-hard. Extensions of the results to complex random variables are discussed as well.
1	In this paper we propose a closed-form asymptotic expansion approach to pricing discretely monitored Asian options in general one-dimensional diffusion models. Our expansion is a small-time expansion because the expansion parameter is selected to be the square root of the length of monitoring interval. This expansion method is distinguished from many other pricing-oriented expansion algorithms in the literature because of two appealing features. First, we illustrate that it is possible to explicitly calculate not only the first several expansion terms but also any general expansion term in a systematic way. Second, the convergence of the expansion is proved rigorously under some regularity conditions. Numerical experiments suggest that the closed-form expansion formula with only a few terms (e.g., four terms up to the third order) is accurate, fast, and easy to implement for a broad range of diffusion models, even including those violating the regularity conditions.
1	Lattice-based reformulation techniques have been used successfully both theoretically and computationally. One such reformulation is obtained from the kernel lattice associated with an input matrix. Some of the hard instances in the literature that have been successfully tackled by lattice-based techniques have randomly generated input. Since the considered instances are very hard even in low dimension, less experience is available for larger instances. Recently, we have studied larger instances and observed that the LLL-reduced basis of the kernel lattice has a specific sparse structure. In particular, this translates into a map in which some of the original variables get a “rich”' translation into a new variable space, whereas some variables are only substituted in the new space. If an original variable is important in the sense of branching or cutting planes, this variable should be translated in a nontrivial way. In this paper we partially explain, through a probabilistic analysis, the obtained structure of the LLL-reduced basis in the case that the input matrix consists of one row. The key ingredient is a bound on the probability that the LLL-algorithm will interchange two subsequent basis vectors.
1	On the interior of a regular convex cone K in n-dimensional real space there exist two canonical Hessian metrics, the one generated by the logarithm of the characteristic function, and the Cheng-Yau metric. The former is associated with a self-concordant logarithmically homogeneous barrier on K, the universal barrier. It is invariant with respect to the unimodular automorphism subgroup of K and is compatible with the operation of taking product cones, but in general it does not behave well under duality. Here we introduce a barrier associated with the Cheng-Yau metric, the canonical barrier. It shares with the universal barrier the invariance, existence, and uniqueness properties and is compatible with the operation of taking product cones, but in addition is well behaved under duality. The canonical barrier can be characterized as the convex solution of the partial differential equation log det F” = 2F that tends to infinity as the argument tends to the boundary of K. Its barrier parameter does not exceed the dimension n of the cone. On homogeneous cones both barriers essentially coincide.
1	In two-sided matching markets, the concept of stability proposed by Gale and Shapley is one of the most important solution concepts. In this paper, we consider a problem related to stability of a matching in a two-sided matching market with indifferences. It is known that stability does not guarantee Pareto efficiency in a two-sided matching market with indifferences. However, Erdil and Ergin proved that there always exists a stable and Pareto efficient matching in a many-to-one matching market with indifferences and gave a polynomial-time algorithm for finding it. Later on, Chen proved that there always exists a stable and Pareto efficient matching in a many-to-many matching market with indifferences and gave a polynomial-time algorithm for finding it. In this paper, we propose a new approach to the problem of finding a stable and Pareto efficient matching in a many-to-many matching market with indifferences. Our algorithm is an alternative proof of the existence of a stable and Pareto efficient matching in a many-to-many matching market with indifferences.
1	We study a dynamic pricing problem with multiple products and infinite inventories. The demand for these products depends on the selling prices and on parameters unknown to the seller. Their value can be learned from accumulating sales data using statistical estimation techniques. The quality of the parameter estimates is influenced by the amount of price dispersion; however, a large amount of variation in the selling prices can be costly since it means that suboptimal prices are used. The seller thus needs to balance optimizing the quality of the parameter estimates and optimizing instant revenue, i.e., exploitation and exploration.In this study we propose a pricing policy for this dynamic pricing problem. The key idea is to use at each time period the price that is optimal with respect to current parameter estimates, with an additional constraint that ensures sufficient price dispersion. We measure the price dispersion by the smallest eigenvalue of the design matrix and show how a desired growth rate of this eigenvalue can be achieved by a simple quadratic constraint in the price-optimization problem. We study the performance of our pricing policy by providing bounds on the regret, which measures the expected revenue loss caused by using suboptimal prices.
1	Random sampling is a simple but powerful method in statistics and in the design of randomized algorithms. In a typical application, random sampling can be applied to estimate an extreme value, say maximum, of a function f over a set S ⊆ ℝn. To do so, one may select a simpler (even finite) subset S0 ⊆ S, randomly take some samples over S0 for a number of times, and pick the best sample. The hope is to find a good approximate solution with reasonable chance. This paper sets out to present a number of scenarios for f, S and S0 where certain probability bounds can be established, leading to a quality assurance of the procedure. In our setting, f is a multivariate polynomial function. We prove that if f is a d-th order homogeneous polynomial in n variables and F is its corresponding super-symmetric tensor, and ξi (i = 1, 2, …, n) are i.i.d. Bernoulli random variables taking 1 or −1 with equal probability, then Prob{f(ξ1, ξ2, …, ξn) ≥ τn−d/2 ‖F‖1} ≥ θ, where τ, θ > 0 are two universal constants and ‖·‖1 denotes the summation of the absolute values of all its entries. Several new inequalities concerning probabilities of the above nature are presented in this paper. Moreover, we show that the bounds are tight in most cases. Applications of our results in optimization are discussed as well.
1	A G/M/N queue is considered in the moderate deviation heavy traffic regime. The rate function for the customers-in-system process is obtained for the single class model. A risk-sensitive type control problem is considered for multiclass G/M/N model under the moderate deviation scaling and shown that the optimal control problem is related to a differential game problem.
1	Given a simple and undirected graph, nonnegative node weights, a nonnegative integer j, and a positive integer k, a k-matching in the graph is a subgraph with no isolated nodes and with maximum degree no more than k, a j-restricted k-matching is a k-matching with each connected component having at least j + 1 edges, and the total node weight of a j-restricted k-matching is the total weight of the nodes covered by the edges in the j-restricted k-matching. When j = 1 and k = 2, Kaneko [Kaneko A (2003) A necessary and sufficient condition for the existence of a path factor every component of which is a path of length at least two. J. Combin. Theory, B 88:195–218] and Kano et al. [Kano M, Katona G, Király Z (2005) Packing paths of length at least two. Discrete Math. 283:129–135] studied the problem of maximizing the number of nodes covered by the edges in a 1-restricted 2-matching. In this paper, we consider the problem of finding a j-restricted k-matching with the maximum total node weight. We present a polynomial-time algorithm for the problem as well as a min-max theorem in the case of j < k. We also prove that, when j ≥ k ≥ 2, the problem of maximizing the number of nodes covered by the edges in a j-restricted k-matching is NP-hard.
1	In college admissions and student placements at public schools, the admission decision can be thought of as assigning indivisible objects with capacity constraints to a set of students such that each student receives at most one object and monetary compensations are not allowed. In these important market design problems, the agent-proposing deferred-acceptance (DA) mechanism with responsive strict priorities performs well, and economists have successfully implemented DA-mechanisms or slight variants thereof. We show that almost all real-life mechanisms used in such environments—including the large classes of priority mechanisms and linear programming mechanisms—satisfy a set of simple and intuitive properties. Once we add strategy-proofness to these properties, DA-mechanisms are the only ones surviving. In market design problems that are based on weak priorities (like school choice), generally multiple tie-breaking (MTB) procedures are used and then a mechanism is implemented with the obtained strict priorities. By adding stability with respect to the weak priorities, we establish the first normative foundation for MTB-DA-mechanisms that are used in New York City.
1	In a partial monitoring game, the learner repeatedly chooses an action, the environment responds with an outcome, and then the learner suffers a loss and receives a feedback signal, both of which are fixed functions of the action and the outcome. The goal of the learner is to minimize his regret, which is the difference between his total cumulative loss and the total loss of the best fixed action in hindsight. In this paper we characterize the minimax regret of any partial monitoring game with finitely many actions and outcomes. It turns out that the minimax regret of any such game is either zero or scales as T1/2, T2/3, or T up to constants and logarithmic factors. We provide computationally efficient learning algorithms that achieve the minimax regret within a logarithmic factor for any game. In addition to the bounds on the minimax regret, if we assume that the outcomes are generated in an i.i.d. fashion, we prove individual upper bounds on the expected regret.
1	A covering integer program (CIP) is a mathematical program of the form min{c⊤x ∣ Ax ≥ 1, 0 ≤ x ≤ u, x  ∈ ℤn}, where all entries in A, c, u are nonnegative. In the online setting, the constraints (i.e., the rows of the constraint matrix A) arrive over time, and the algorithm can only increase the coordinates of x to maintain feasibility. As an intermediate step, we consider solving the covering linear program (CLP) online, where the integrality constraints are dropped.Our main results are (a) an O(log k)-competitive online algorithm for solving the CLP, and (b) an O(log k · log l)-competitive randomized online algorithm for solving the CIP. Here k ≤ n and l ≤ m respectively denote the maximum number of nonzero entries in any row and column of the constraint matrix A. Our algorithm is based on the online primal-dual paradigm, where a novel ingredient is to allow dual variables to increase and decrease throughout the course of the algorithm. It is known that this result is the best possible for polynomial-time online algorithms, even in the special case of set cover (where all entries in A, c, and u are 0 or 1.
1	In this paper, we revisit the auction design problem for multi-item auctions with budget constrained buyers by introducing a robust optimization approach to model (a) concepts such as incentive compatibility and individual rationality that are naturally expressed in the language of robust optimization and (b) the auctioneer’s beliefs on the buyers' valuations of the items. Rather than using probability distributions (the classical probabilistic approach) or an adversarial model to model valuations, we introduce an uncertainty set based model for these valuations. We construct these uncertainty sets to incorporate historical information available to the auctioneer in a way that is consistent with limit theorems of probability theory or knowledge of the probability distribution. In this setting, we formulate the auction design problem as a robust optimization problem and provide a characterization of the optimal solution as an auction with reservation prices, thus extending the work of Myerson [Myerson RB (1981) Optimal auction design. Math. Oper. Res. 6(1):58–73] from single item without budget constraints to multiple items with budgets, potentially correlated valuations, and uncertain budgets. Unlike the Myerson auction where the reservation prices do not depend on the item, the reservation prices in our approach are a function of both the bidder and the item. We propose an algorithm for calculating the reservation prices by solving a bilinear optimization problem that, although theoretically difficult in general, is numerically tractable for the polyhedral uncertainty sets we consider. Moreover, this bilinear optimization problem reduces to a linear optimization problem for auctions without budget constraints and the auction becomes the classical second price auction. We report computational evidence that suggests the proposed approach (a) is numerically tractable for large scale auction design problems with the polyhedral uncertainty sets we consider, (b) leads to improved revenue compared to the classical probabilistic approach when the true distributions are different from the assumed ones, and (c) leads to higher revenue when correlations in the buyers' valuations are explicitly modeled.
1	We consider the single-item, multiperiod stochastic inventory problem with nonstationary and correlated demands. We provide the first proof that a well-known myopic policy, which we call look-ahead optimization (LA), is in fact an approximation algorithm for solving the large dynamic program that arises from this problem. We prove that LA provides the tightest known approximation bound for this problem. The expected cost of LA is at most twice the expected holding cost plus the expected backorder cost of an optimal policy. We introduce a new cost-accounting technique and a new class of invariances on the ongoing performance of LA relative to that of an optimal policy. We use these invariances to extend the myopic optimality of LA to a global bound on its performance. We allow convex and nonlinear holding and backorder cost functions, integral order quantities, and positive lead times under a technical condition on the growth of demand. We show in computational experiments that LA has excellent average-case performance.
1	Blackwell’s theory of approachability, introduced in 1956, has since proved a useful tool in the study of a range of repeated multiagent decision problems. Given a repeated matrix game with vector payoffs, a target set S is approachable by a certain player if he can ensure that the average payoff vector converges to that set, for any strategy of the opponent. In this paper we consider the case where a set need not be approachable in general, but may be approached if the opponent played favorably in some sense. In particular, we consider nonconvex sets that satisfy Blackwell’s dual condition, namely, can be approached when the opponent plays a stationary strategy. Whereas the convex hull of such a set is approachable, this is not generally the case for the original nonconvex set itself. We start by defining a sense of restricted play of the opponent (with stationary strategies being a special case), and then formulate appropriate goals for an opportunistic approachability algorithm that can take advantage of such restricted play as it unfolds during the game. We then consider a calibration-based approachability strategy that is opportunistic in that sense. A major motivation for this study comes from no-regret problems that lack a convex structure such as the problem of online learning with sample-path constraints, as formulated in Mannor et al. [Mannor S, Tsitsiklis JN, Yu JY (2009) Online learning with sample path constraints. J. Machine Learn. Res. 10:569–590]. Here the best-response-in-hindsight is not generally attainable, but only a convex relaxation thereof. Our proposed algorithm, while ensuring that relaxed goal, also comes closer to the nonrelaxed one when the opponent’s play is restricted in a well-defined sense.
1	In this paper, we establish hardness and approximation results for various Lp-ball constrained homogeneous polynomial optimization problems, where p ∈ [2, ∞]. Specifically, we prove that for any given d ≥ 3 and p ∈ [2, ∞], both the problem of optimizing a degree-d homogeneous polynomial over the Lp-ball and the problem of optimizing a degree-d multilinear form (regardless of its super-symmetry) over Lp-balls are NP-hard. On the other hand, we show that these problems can be approximated to within a factor of Ω((log n)(d−2)/p / nd/2−1) in deterministic polynomial time, where n is the number of variables. We further show that with the help of randomization, the approximation guarantee can be improved to Ω((log n/n)d/2−1), which is independent of p and is currently the best for the aforementioned problems. Our results unify and generalize those in the literature, which focus either on the quadratic case or the case where p ∈ {2, ∞}. We believe that the wide array of tools used in this paper will have further applications in the study of polynomial optimization problems.
1	We solve, theoretically and numerically, the problems of optimal portfolio choice and indifference valuation in a general continuous-time setting. The setting features (i) ambiguity and time-consistent ambiguity-averse preferences, (ii) discontinuities in the asset price processes, with a general and possibly infinite activity jump part next to a continuous diffusion part, and (iii) general and possibly nonconvex trading constraints. We characterize our solutions as solutions to backward stochastic differential equations (BSDEs). Generalizing Kobylanski's result for quadratic BSDEs to an infinite activity jump setting, we prove existence and uniqueness of the solution to a general class of BSDEs, encompassing the solutions to our portfolio choice and valuation problems as special cases. We provide an explicit decomposition of the excess return on an asset into a risk premium and an ambiguity premium, and a further decomposition into a piece stemming from the diffusion part and a piece stemming from the jump part. We further compute our solutions in a few examples by numerically solving the corresponding BSDEs using regression techniques.
1	We develop rare-event simulation methodology for the analysis of loss events in a many-server loss system under the quality-driven regime, focusing on the steady-state loss probability (i.e., fraction of lost customers over arrivals) and the behavior of the whole system leading to loss events. The analysis of these events requires working with the full measure-valued process describing the system. This is the first algorithm that is shown to be asymptotically optimal, in the rare-event simulation context, under the setting of many-server queues involving a full measure-valued representation.
1	We consider a single-product, periodic-review inventory system with remanufacturable returned products, in which the serviceable product used to fulfill stochastic customer demand can be either manufactured from new parts or remanufactured from returned products. Demand and returns follow general stochastic processes and may be correlated, nonstationary, and evolving across different periods. The system costs include remanufacturing and manufacturing costs as well as inventory holding and demand backlogging costs. The objective is to minimize the expected total discounted cost over a finite planning horizon. The optimal policy for this model has a simple structure but its computation can be practically intractable because of the large and high-dimensional state space. Henceforth, we propose an efficient approximation algorithm based on cost balancing techniques to compute manufacturing and remanufacturing quantities in each period and show that the expected cost under this remanufacturing balancing policy is at most twice of the optimal one. Moreover, a numerical study demonstrates that the balancing policy generally performs much better than its worst-case bound. The analysis and results are also extended to cases with capacitated manufacturing and remanufacturing and multiple types of returned products.
1	We study a constrained stochastic control problem with jumps; the jump times of the controlled process are given by a Poisson process. The cost functional comprises quadratic components for an absolutely continuous control and the controlled process and an absolute value component for the control of the jump size of the process. We characterize the value function by a “polynomial” of degree two whose coefficients depend on the state of the system; these coefficients are given by a coupled system of ODEs. The problem hence reduces from solving the Hamilton Jacobi Bellman (HJB) equation (i.e., a PDE) to solving an ODE whose solution is available in closed form. The state space is separated by a time dependent boundary into a continuation region where the optimal jump size of the controlled process is positive and a stopping region where it is zero. We apply the optimization problem to a problem faced by investors in the financial market who have to liquidate a position in a risky asset and have access to a dark pool with adverse selection.
1	This paper considers the use of a simple posterior sampling algorithm to balance between exploration and exploitation when learning to optimize actions such as in multiarmed bandit problems. The algorithm, also known as Thompson Sampling and as probability matching, offers significant advantages over the popular upper confidence bound (UCB) approach, and can be applied to problems with finite or infinite action spaces and complicated relationships among action rewards. We make two theoretical contributions. The first establishes a connection between posterior sampling and UCB algorithms. This result lets us convert regret bounds developed for UCB algorithms into Bayesian regret bounds for posterior sampling. Our second theoretical contribution is a Bayesian regret bound for posterior sampling that applies broadly and can be specialized to many model classes. This bound depends on a new notion we refer to as the eluder dimension, which measures the degree of dependence among action rewards. Compared to UCB algorithm Bayesian regret bounds for specific model classes, our general bound matches the best available for linear models and is stronger than the best available for generalized linear models. Further, our analysis provides insight into performance advantages of posterior sampling, which are highlighted through simulation results that demonstrate performance surpassing recently proposed UCB algorithms.
1	We consider the problem of determining the optimal schedules for a given sequence of jobs on a single processor. The objective is to minimize the expected total cost incurred by job waiting and processor idling, where the job processing times are random variables. It is known in the prior literature that if the processing times are integers and the costs are linear functions satisfying a mild condition, then the problem can be solved in a polynomial number of expected cost evaluations. In this work, we extend the result to piecewise linear cost functions, which include many useful objective functions in practice. Our analysis explores the (hidden) dual network flow structure of the appointment scheduling problem and thus greatly simplifies that of prior work. We also find the number of samples needed to compute a near optimal solution when only some independent samples of processing times are known.
1	We consider the problem of designing distribution rules to share “welfare” (cost or revenue) among individually strategic agents. There are many known distribution rules that guarantee the existence of a (pure) Nash equilibrium in this setting, e.g., the Shapley value and its weighted variants; however, a characterization of the space of distribution rules that guarantees the existence of a Nash equilibrium is unknown. Our work provides an exact characterization of this space for a specific class of scalable and separable games that includes a variety of applications such as facility location, routing, network formation, and coverage games. Given arbitrary local welfare functions 𝕨, we prove that a distribution rule guarantees equilibrium existence for all games (i.e., all possible sets of resources, agent action sets, etc.) if and only if it is equivalent to a generalized weighted Shapley value on some “ground” welfare functions 𝕨′, which can be distinct from 𝕨. However, if budget-balance is required in addition to the existence of a Nash equilibrium, then 𝕨′ must be the same as 𝕨. We also provide an alternate characterization of this space in terms of “generalized” marginal contributions, which is more appealing from the point of view of computational tractability. A possibly surprising consequence of our result is that, in order to guarantee equilibrium existence in all games with any fixed local welfare functions, it is necessary to work within the class of potential games.
1	We consider dynamic stochastic scheduling of preemptive jobs with processing times that follow independent discrete probability distributions. We derive a policy with a guaranteed performance ratio of 2 for the problem of minimizing the sum of weighted completion times on identical parallel machines subject to release dates. The analysis is tight. Our policy as well as their analysis applies also to the more general model of stochastic online scheduling.In contrast to previous results for nonpreemptive stochastic scheduling, our preemptive policy yields an approximation guarantee that is independent of the processing time distributions. However, our policy extensively utilizes information on the distributions other than the first (and second) moments. We also introduce a new nontrivial lower bound on the expected value of an unknown optimal policy. It relies on a relaxation to the basic problem on a single machine without release dates, which is known to be solved optimally by the Gittins index priority rule. This dynamic priority index is crucial to the analysis and also inspires the design of our policy.
1	We consider the problem of finding upper and lower bounds for the probability of the union of events when the probabilities of the single events and the probabilities of the intersections of up to m events are given.It is known that the best possible bounds can be obtained by solving linear programming problems with a number of variables that is exponential in the number of events. Because of their size and structure, these large linear programs are known to be very hard to solve. In the literature simpler, polynomially sized aggregations are considered and numerous closed form or polynomially computable bounds are derived from those.We present here a new approach that introduces additional constraints to the dual linear programming problems in such a way that those become polynomially solvable. By using different sets of additional constraints, we introduce three new classes of polynomially computable upper and lower bounds. We show that they dominate almost all efficiently computable bounds known in the literature. Furthermore, by characterizing the vertices of two new classes of polyhedra, we can show that in two cases our bounds coincide with classical bounds, proving new extremal properties for those well-known bounds. Finally, we provide extensive numerical results comparing the average tightness of the various bounds on a large number of instances.
1	It is well known that the permutahedron Πn has 2n − 2 facets. The Birkhoff polytope provides a symmetric extended formulation of Πn of size Θ(n2). Recently, Goemans described a non-symmetric extended formulation of Πn of size Θ(n log n). In this paper, we prove that Ω(n2) is a lower bound for the size of symmetric extended formulations of Πn. Moreover, we prove that the cardinality indicating polytope has the same tight lower bounds for the sizes of symmetric and nonsymmetric extended formulations as the permutahedron.
1	Using an intuition from metric geometry, we prove that any flag normal simplicial complex satisfies the nonrevisiting path conjecture. As a consequence, the diameter of its facet-ridge graph is smaller than the number of vertices minus the dimension, as in the Hirsch conjecture. This proves the Hirsch conjecture for all flag polytopes and, more generally, for all (connected) flag homology manifolds.
1	One of the fundamental problems in a cognitive radio network, known as the multichannel rendezvous problem, is for two secondary users to find a common channel that is not blocked by primary users. The basic idea for solving such a problem in most works in the literature is for the two users to select their own channel hopping sequences and then rendezvous when they both hop to a common unblocked channel at the same time. In this paper, we focus on the fundamental limits of the multichannel rendezvous problem and formulate such a problem as a constrained optimization problem, where the selection of the random hopping sequences of the two secondary users must satisfy certain constraints. We derive various lower bounds for the expected (respectively, maximum) time-to-rendezvous under certain constraints. For some of these lower bounds, we are also able to construct optimal channel hopping sequences that achieve the lower bounds. Inspired by the constructions of quorum systems and relative difference sets, our constructions of the channel hopping sequences are based on the mathematical theories of finite projective planes, orthogonal Latin squares, and sawtooth sequences. The use of such theories in the constructions of channel hopping sequences appear to be new and better than other existing schemes in terms of minimizing the expected (respectively, maximum) time-to-rendezvous.
1	We study the structure of the solution set of a class of infinite-horizon dynamic programming problems with one-dimensional state spaces, as well as their bifurcations, as problem parameters are varied. The solutions are represented as the integral curves of a multivalued optimal vector field on state space. Generically, there are three types of integral curves: stable points, open intervals that are forward asymptotic to a stable point and backward asymptotic to an unstable point, and half-open intervals that are forward asymptotic to a stable point and backward asymptotic to an indifference point; the latter are initial states to multiple optimal trajectories. We characterize all bifurcations that occur generically in one- and two-parameter families. Most of these are related to global dynamical bifurcations of the state-costate system of the problem.
1	In the stochastic orienteering problem, we are given a finite metric space, where each node contains a job with some deterministic reward and a random processing time. The processing time  distributions are known and independent across nodes. However the actual processing time of a job is not known until it is completely processed. The objective is to compute a nonanticipatory policy to visit nodes (and run the corresponding jobs) so as to maximize the total expected reward, subject to the total distance traveled plus the total processing time being at most a given budget of B. This problem combines aspects of the stochastic knapsack problem with uncertain item sizes as well as the deterministic orienteering problem.In this paper, we consider both nonadaptive and adaptive policies for Stochastic Orienteering. We present a constant-factor approximation algorithm for the nonadaptive version and an O(log log B)-approximation algorithm for the adaptive version. We extend both these results to directed metrics and a more general sequence orienteering problem.Finally, we address the stochastic orienteering problem when the node rewards are also random and possibly correlated with the processing time and obtain  an O(log n log B)-approximation algorithm; here n is the number of nodes in the metric. All our results for adaptive policies also bound  the corresponding “adaptivity gaps”.
1	This work is devoted to extend several asymptotic results concerning repeated games with incomplete information on one side. The model we consider is a generalization of the classical model of Aumann and Maschler (Aumann et al. [Aumann RJ, Maschler M, Stearns RE (1995) Repeated Games with Incomplete Information (MIT Press, Cambridge, MA)]) to infinite action spaces and partial information. We prove an extension of the classical “Cav(u)” Theorem in this model for both the lower and upper value functions using two different methods: respectively a probabilistic method based on martingales and a functional one based on approximation schemes for viscosity solutions of Hamilton Jacobi equations similar to the dual differential approach of Laraki [Laraki R (2002) Repeated games with lack of information on one side: The dual differential approach. Math. Oper. Res. 27(2):419–440]. Moreover, we show that solutions of these two asymptotic problems provide asymptotically optimal strategies for both players in any game of length n. All these results are based on a compact approach, which consists in identifying a continuous-time problem defined on the time interval [0,1] representing the “limit” of a sequence of finitely repeated games, as the number of repetitions is going to infinity. Finally, our results imply the existence of the uniform value of the infinitely repeated game whenever the value of the non-revealing game exists.
1	We give an algorithm for testing the extremality of minimal valid functions for Gomory and Johnson's infinite group problem that are piecewise linear (possibly discontinuous) with rational breakpoints. This is the first set of necessary and sufficient conditions that can be tested algorithmically for deciding extremality in this important class of minimal valid functions. We also present an extreme function that is a piecewise linear function with some irrational breakpoints, whose extremality follows from a new principle.
1	We prove the almost-sure convergence of a class of sampling-based nested decomposition algorithms for multistage stochastic convex programs in which the stage costs are general convex functions of the decisions and uncertainty is modelled by a scenario tree. As special cases, our results imply the almost-sure convergence of stochastic dual dynamic programming, cutting-plane and partial-sampling (CUPPS) algorithm, and dynamic outer-approximation sampling algorithms when applied to problems with general convex cost functions.
1	Fourier-Motzkin elimination is a projection algorithm for solving finite linear programs. We extend Fourier-Motzkin elimination to semi-infinite linear programs, which are linear programs with finitely many variables and infinitely many constraints. Applying projection leads to new characterizations of important properties for primal-dual pairs of semi-infinite programs such as zero duality gap, feasibility, boundedness, and solvability. Extending the Fourier-Motzkin elimination procedure to semi-infinite linear programs yields a new classification of variables that is used to determine the existence of duality gaps. In particular, the existence of what the authors term dirty variables can lead to duality gaps. Our approach has interesting applications in finite-dimensional convex optimization. For example, sufficient conditions for a zero duality gap, such as the Slater constraint qualification, are reduced to guaranteeing that there are no dirty variables. This leads to completely new proofs of such sufficient conditions for zero duality.
1	Definable zero-sum stochastic games involve a finite number of states and action sets, and reward and transition functions, that are definable in an o-minimal structure. Prominent examples of such  games are finite, semi-algebraic, or globally subanalytic stochastic games. We prove that the Shapley operator of any definable stochastic game with separable transition and reward functions is definable in the same  structure. Definability in the same structure does not hold systematically: we provide a counterexample of a stochastic game with semi-algebraic data yielding a non-semi-algebraic but globally subanalytic Shapley  operator. Our definability results on Shapley operators are used to prove that any separable definable game has a uniform value; in the case of polynomially bounded structures, we also provide convergence rates. Using an  approximation procedure, we actually establish that general zero-sum games with separable definable transition functions have a uniform value. These results highlight the key role played by the tame structure of transition  functions. As particular cases of our main results, we obtain that stochastic games with polynomial transitions, definable games with finite actions on one side, and definable games with perfect information or  switching controls have a uniform value. Applications to nonlinear maps arising in risk sensitive control and Perron-Frobenius theory are also given.
1	We consider the maximization of a gross substitutes utility function under budget constraints. This problem naturally arises in applications such as exchange economies in mathematical economics and combinatorial auctions in (algorithmic) game theory. We show that this problem admits a polynomial-time approximation scheme (PTAS). More generally, we present a PTAS for maximizing a discrete concave function called an M♮-concave function under budget constraints. Our PTAS is based on rounding an optimal solution of a continuous relaxation problem, which is shown to be solvable in polynomial time by the ellipsoid method. We also consider the maximization of the sum of two M♮-concave functions under a single budget constraint. This problem is a generalization of the budgeted max-weight matroid intersection problem to the one with certain nonlinear objective functions. We show that this problem also admits a PTAS.
1	The paper is devoted to full stability of optimal solutions in general settings of finite-dimensional optimization with applications to particular models of constrained optimization problems, including those of conic and specifically semidefinite programming. Developing a new technique of variational analysis and generalized differentiation, we derive second-order characterizations of full stability, in both Lipschitzian and Hölderian settings, and establish their relationships with the conventional notions of strong regularity and strong stability for a large class of problems of constrained optimization with twice continuously differentiable data.
1	Mathematical programs with equilibrium (or complementarity) constraints, MPECs for short, form a difficult class of optimization problems. The feasible set has a very special structure and violates most of the standard constraint qualifications. Therefore, one typically applies specialized algorithms in order to solve MPECs. One prominent class of specialized algorithms is the relaxation (or regularization) methods. The first relaxation method for MPECs is due to Scholtes [35] [Scholtes S (2001) Convergence properties of a regularization scheme for mathematical programs with complementarity constraints. SIAM Journal on Optimization 11:918–936.], but in the meantime, there exists a number of different regularization schemes that try to relax the difficult constraints in different ways. Some of these more recent schemes have better theoretical properties than does the original method by Scholtes. Nevertheless, numerical experience shows that the Scholtes relaxation method is still among the fastest and most reliable ones. To give a possible explanation for this, we consider that, numerically, the regularized subproblems are not solved exactly. In this light, we analyze the convergence properties of a number of relaxation schemes and study the impact of inexactly solved subproblems on the kind of stationarity we can expect in a limit point. Surprisingly, it turns out that the inexact version of Scholtes' method has the same convergence properties as its exact counterpart, whereas most of the other relaxation schemes lose a lot of their original properties.
1	We consider the separation problem for sets X that are pre-images of a given set S by a linear mapping. Classical examples occur in integer programming, as well as in other optimization problems such as complementarity. One would like to generate valid inequalities that cut off some point not lying in X, without reference to the linear mapping. To this aim, we introduce a concept: cut-generating functions (CGF) and we develop a formal theory for them, largely based on convex analysis. They are intimately related to S-free sets and we study this relation, disclosing several definitions for minimal CGF's and maximal S-free sets. Our work unifies and puts into perspective a number of existing works on S-free sets; in particular, we show how CGF's recover the celebrated Gomory cuts.
1	This article analyzes a continuous time back-ordered inventory system with stochastic demand and stochastic delivery lags for placed orders. This problem in general has an infinite dimensional state space and is hence intractable. We first obtain the set of minimal conditions for reducing such a system's state space to one dimension and show how this reduction is done. Next, by modeling demand as a diffusion process, we reformulate the inventory control problem as an impulse control problem. We simplify the impulse control problem to a Quasi-Variation Inequality (QVI). Based on the QVI formulation, we obtain the optimality of the (s, S) policy and the limiting distribution of the inventory level. We also obtain the long run average cost of such an inventory system. Finally, we provide a method to solve the QVI formulation. Using a set of computational experiments, we show that significant losses are incurred in approximating a stochastic lead-time system with a fixed lead-time system, thereby highlighting the need for such stochastic lead-time models. We also provide insights into the dependence of this value loss on various problem parameters.
1	Using a geometric argument, we show that under a reasonable continuity condition, the Clarke subdifferential of a semi-algebraic (or more generally stratifiable) directionally Lipschitzian function admits a simple form: The normal cone to the domain and limits of gradients generate the entire Clarke subdifferential. The characterization formula we obtain unifies various apparently disparate results that have appeared in the literature. Our techniques also yield a simplified proof that closed semialgebraic functions on Rn have a limiting subdifferential graph of uniform local dimension n.
1	In this work, we introduce and study the forbidden-vertices problem. Given a polytope P and a subset X of its vertices, we study the complexity of linear optimization over the subset of vertices of P that are not contained in X. This problem is closely related to finding the k-best basic solutions to a linear problem. We show that the complexity of the problem changes significantly depending on the encoding of both P and X. We provide additional tractability results and extended formulations when P has binary vertices only. Some applications and extensions to integral polytopes are discussed.
1	This paper studies a problem of Bayesian parameter estimation for a sequence of scaled counting processes whose weak limit is a Brownian motion with an unknown drift. The main result of the paper is that the limit of the posterior distribution processes is, in general, not equal to the posterior distribution process of the mentioned Brownian motion with the unknown drift. Instead, it is equal to the posterior distribution process associated with a Brownian motion with the same unknown drift and a different standard deviation coefficient. The difference between the two standard deviation coefficients can be arbitrarily large. The characterization of the limit of the posterior distribution processes is then applied to a family of stopping time problems. We show that the proper way to find asymptotically optimal solutions to stopping time problems, with respect to the scaled counting processes, is by looking at the limit of the posterior distribution processes rather than by the naive approach of looking at the limit of the scaled counting processes themselves. The difference between the performances can be arbitrarily large.
1	We consider a stochastic fluid EOQ-type model with demand rates operating in a two-state random environment. This environment alternates between exponentially distributed periods of high demand and generally distributed periods of low demand. The inventory level starts at some level q, and decreases linearly at rate βH during the periods of high demand, and at rate βL < βH at periods of low demand. Refilling of the inventory level to level q is required when the first of two events takes place: Either the buffer level reaches zero, or the buffer content becomes outdated. If such an event occurs during a high demand period, an order is instantaneously placed; otherwise, ordering is postponed until the beginning of the next high demand period.We determine the steady-state distribution of the inventory level, as well as other quantities of interest such as the distribution of the time until a refill is required. Finally, for a given cost/revenue structure, we determine the long-run average profit, and we consider the problem of choosing q such that the profit is optimized.
1	We are interested in the convergence of the value of n-stage games as n goes to infinity and the existence of the uniform value in stochastic games with a general set of states and finite sets of actions where the transition is commutative. This means that playing an action profile a1 followed by an action profile a2, leads to the same distribution on states as playing first the action profile a2 and then a1. For example, absorbing games can be reformulated as commutative stochastic games.When there is only one player and the transition function is deterministic, we show that the existence of a uniform value in pure strategies implies the existence of 0-optimal strategies. In the framework of two-player stochastic games, we study a class of games where the set of states is ℝm and the transition is deterministic and 1-Lipschitz for the L1-norm, and prove that these games have a uniform value. A similar proof shows the existence of an equilibrium in the nonzero-sum case.These results remain true if one considers a general model of finite repeated games, where the transition is commutative and the players observe the past actions but not the state.
1	In the classical theory of monotone equimeasurable rearrangements of functions, “equimeasurability” (i.e., that two functions have the same distribution) is defined relative to a given additive probability measure. These rearrangement tools have been successfully used in many problems in economic theory dealing with uncertainty where the monotonicity of a solution is desired. However, in all of these problems, uncertainty refers to the classical Bayesian understanding of the term, where the idea of ambiguity is absent. Arguably, Knightian uncertainty, or ambiguity, is one of the cornerstones of modern decision theory. It is hence natural to seek an extension of these classical tools of equimeasurable rearrangements to the non-Bayesian or neo-Bayesian context. This paper introduces the idea of a monotone equimeasurable rearrangement in the context of nonadditive probabilities, or capacities that satisfy a property that I call strong diffuseness. The latter is a strengthening of the usual notion of diffuseness, and these two properties coincide for additive measures and for submodular (i.e., concave) capacities. To illustrate the usefulness of these tools in economic theory, I consider an application to a problem arising in the theory of production under uncertainty.
1	In the classical k-median problem, we are given a metric space and want to open k centers so as to minimize the sum (over all the vertices) of the distance of each vertex to its nearest open center. In this paper we present the first constant-factor approximation algorithms for two natural generalizations of this problem that handle matroid or knapsack constraints.In the matroid median problem, there is an underlying matroid on the vertices and the set of open centers is constrained to be independent in this matroid. When the matroid is uniform, we recover the k-median problem. Another previously studied special case is the red-blue median problem where we have a partition matroid with two parts. Our algorithm for matroid median is based on rounding a natural linear programming relaxation in two stages, and it relies on a connection to matroid intersection.In the knapsack median problem, centers have weights and the total weight of open centers is constrained to be at most a given capacity. When all weights are uniform, this reduces to the k-median problem. The algorithm for knapsack median is based on a novel LP relaxation that constrains the set of centers that each vertex can get connected to. The rounding procedure uses a two-stage approach similar to that for matroid median.
1	The stable allocation model is a many-to-many matching model in which each pair’s partnership is represented by a nonnegative integer. This paper establishes a link between two different formulations of this model: the choice function model studied thoroughly by Alkan and Gale and the discrete-concave (M♮-concave) value function model introduced by Eguchi, Fujishige, and Tamura. We show that the choice functions induced from M♮-concave value functions are endowed with consistency, persistence, and size monotonicity. This implies, by the result of Alkan and Gale, that the stable allocations for M♮-concave value functions form a distributive lattice with several significant properties such as polarity, complementarity, and uni-size property. Furthermore, we point out that these results can be extended for quasi M♮-concave value functions.
1	One of the most attractive recent approaches to processing well-structured large-scale convex optimization problems is based on smooth convex-concave saddle point reformulation of the problem of interest and solving the resulting problem by a fast first order saddle point method utilizing smoothness of the saddle point cost function. In this paper, we demonstrate that when the saddle point cost function is polynomial, the precise gradients of the cost function required by deterministic first order saddle point algorithms and becoming prohibitively computationally expensive in the extremely large-scale case, can be replaced with incomparably cheaper computationally unbiased random estimates of the gradients. We show that for large-scale problems with favorable geometry, this randomization accelerates, progressively as the sizes of the problem grow, the solution process. This extends significantly previous results on acceleration by randomization, which, to the best of our knowledge, dealt solely with bilinear saddle point problems. We illustrate our theoretical findings by instructive and encouraging numerical experiments.
1	In this paper, we consider the problem of optimal design of experiments. A two-step inference strategy is proposed. The first step consists in minimizing the condition number of the so-called information matrix. This step can be turned into a semidefinite programming problem. The second step is more classical, and it entails the minimization of a convex integral functional under linear constraints. This step is formulated in some infinite-dimensional space and is solved by means of a dual approach. Numerical simulations will show the relevance of our approach.
1	This paper proposes a unified method for precise estimates of the error bounds in asymptotic expansions of an option price and its Greeks (sensitivities) under a stochastic volatility model. More generally, we also derive an error estimate for an asymptotic expansion around a general partially elliptic diffusion and a more general Wiener functional, which is applicable to various important valuation and risk management tasks in the financial business such as the ones for multidimensional diffusion and nondiffusion models. In particular, we take the Malliavin calculus approach, and estimate the error bounds for the Malliavin weights of both the coefficient and the residual terms in the expansions by effectively applying the properties of Kusuoka-Stroock functions introduced by Kusuoka [Kusuoka S (2003) Malliavin calculus revisited. J. Math. Sci. Univ. Tokyo 10:261–277.] functions. Moreover, a numerical experiment under the Heston-type model confirms the effectiveness of our method.
1	We consider a general class of online optimization problems, called online selection problems, where customers arrive sequentially, and one has to decide upon arrival whether to accept or reject each customer. If a customer is rejected, then a rejection cost is incurred. The accepted customers are served with minimum possible cost, either online or after all customers have arrived. The goal is to minimize the total production costs for the accepted customers plus the rejection costs for the rejected customers. These selection problems are related to online variants of offline prize collecting combinatorial optimization problems that have been widely studied in the computer science literature. In this paper, we provide a general framework to develop online algorithms for this class of selection problems. In essence, the algorithmic framework leverages any cost sharing mechanism with certain properties into a poly-logarithmic competitive online algorithm for the respective problem; the competitive ratios are shown to be near-optimal. We believe that the general and transparent connection we establish between cost sharing mechanisms and online algorithms could lead to additional online algorithms for problems beyond the ones studied in this paper.
1	We consider the G/GI/N queue with multiple server pools, each possessing a pool-specific service time distribution. The class of nonidling routing policies that we consider are referred to as u-greedy policies. These policies route incoming customers to the server pool with the longest weighted cumulative idle time to equitably spread incoming work amongst the server pools in the system. Our first set of results demonstrates that asymptotically in the Halfin-Whitt regime and under any u-greedy policy, the diffusion scaled cumulative idle time processes of each of the server pools are held in fixed proportion to one another. We next provide a heavy traffic limit theorem for the process keeping track of the total number of customers in the system. Our limit may be characterized as the solution to a stochastic convolution equation that is driven by a Gaussian process. To prove our main results, we introduce a new methodology for studying the G/GI/N queue in the Halfin-Whitt regime that has as its starting point a simple conservation of flow identity.
1	We represent any repeated game with partial monitoring as an abstract repeated game with full monitoring where outcomes are probability measures, to be interpreted as the “maximal information” the players can obtain in the original game. One of our objectives is to define and generalize Blackwell’s approachability theory in this space of probability measures. We characterize approachable sets with, as usual, a simple and complete formulation for convex sets. Translated back into the original games with partial monitoring, these results provide the first necessary and sufficient approachability condition. As there is not a unique way to define averages of probability measures, we also investigate the case of displacement interpolation. We obtain similar results along with rates of convergence.
1	Starting from a heuristic learning scheme for strategic N-person games, we derive a new class of continuous-time learning dynamics consisting of a replicator-like drift adjusted by a penalty term that renders the boundary of the game’s strategy space repelling. These penalty-regulated dynamics are equivalent to players keeping an exponentially discounted aggregate of their ongoing payoffs and then using a smooth best response to pick an action based on these performance scores. Owing to this inherent duality, the proposed dynamics satisfy a variant of the folk theorem of evolutionary game theory and they converge to (arbitrarily precise) approximations of Nash equilibria in potential games. Motivated by applications to traffic engineering, we exploit this duality further to design a discrete-time, payoff-based learning algorithm that retains these convergence properties and only requires players to observe their in-game payoffs. Moreover, the algorithm remains robust in the presence of stochastic perturbations and observation errors, and it does not require any synchronization between players.
1	In routing games with infinitesimal players, it follows from well-known convexity arguments that equilibria exist and are unique. In routing games with atomic players with splittable flow, equilibria exist, but uniqueness of equilibria has been demonstrated only in limited cases: in two-terminal nearly parallel graphs, when all players control the same amount of flow, and when latency functions are polynomials of degree at most three. There are no known examples of multiple equilibria in these games. In this work, we show that in contrast to routing games with infinitesimal players, atomic splittable routing games admit multiple equilibria. We demonstrate this multiplicity via two specific examples. In addition, we show that our examples are topologically minimal by giving a complete characterization of the class of network topologies for which multiple equilibria exist. Our proofs and examples are based on a novel characterization of these topologies in terms of sets of circulations.
1	This paper compares two frameworks for measuring risk in a multiperiod setting. The first corresponds to applying a single coherent risk measure to the cumulative future costs, and the second involves applying a composition of one-step coherent risk mappings. We characterize several necessary and sufficient conditions under which one measurement always dominates the other and introduce a metric to quantify how close the two measures are. Using this notion, we address the question of how tightly a given coherent measure can be approximated by lower or upper bounding compositional measures. We exhibit an interesting asymmetry between the two cases: the tightest upper bound can be exactly characterized and corresponds to a popular construction in the literature, whereas the tightest lower bound is not readily available. We show that testing domination and computing the approximation factors are generally NP-hard, even when the risk measures are comonotonic and law-invariant. However, we characterize conditions and discuss examples where polynomial-time algorithms are possible. One such case is the well-known conditional value-at-risk measure, which we explore in more detail. Our theoretical and algorithmic constructions exploit interesting connections between the study of risk measures and the theory of submodularity and combinatorial optimization, which may be of independent interest.
1	A single-server queueing model is considered with customers that have deadlines. If a customer’s deadline elapses before service is offered, the customer abandons the system (customers do not abandon while being served). When the server becomes available, it offers service to the customer having the earliest deadline among those that are in the queue. We obtain a fluid limit of the queue length and abandonment processes and for the occupation measure of deadlines, in the form of measure-valued processes. We characterize the limit by means of a Skorohod problem in a time-varying domain that has an explicit solution. The fluid limits also describe a certain process called the frontier that is well known to play a key role in systems operating under this scheduling policy.
1	Metric subregularity and regularity of multifunctions are fundamental notions in variational analysis and optimization. Using the concept of strong slope, in this paper we first establish a criterion for metric subregularity of multifunctions between metric spaces. Next, we use a combination of abstract coderivatives and contingent derivatives to derive verifiable first order conditions ensuring metric subregularity of multifunctions between Banach spaces. Then using second order approximations of convex multifunctions, we establish a second order condition for metric subregularity of mixed smooth-convex constraint systems, which generalizes a result established recently by Gfrerer [Gfrerer H (2011) First order and second order characterizations of metric subregularity and calmness of constraint set mapping. SIAM J. Optim. 21(4):1439–1474].
1	The standard quadratic optimization problem (StQP) refers to the problem of minimizing a quadratic form over the standard simplex. Such a problem arises from numerous applications and is known to be NP-hard. In a recent paper [Chen X, Peng J, Zhang S (2013) Sparse solutions to random standard quadratic optimization problems. Math. Programming 141(1–2):273–293], Chen, et al. showed that with a high probability close to 1, StQPs with random data have sparse optimal solutions when the associated data matrix is randomly generated from a certain distribution such as uniform and exponential distributions. In this paper, we present a new analysis for random StQPs combining probability inequalities derived from the first-order and second-order optimality conditions. The new analysis allows us to significantly improve the probability bounds. More important, it allows us to handle normal distributions, which is left open in Chen et al. (2013). The existence of sparse approximate solutions to convex StQPs and extensions to other classes of QPs are discussed as well.
1	We introduce the concept of attainable sets of payoffs in two-player repeated games with vector payoffs. A set of payoff vectors is called attainable by a player if there is a positive integer such that the player can guarantee that in all finite game longer than that integer, the distance between the set and the cumulative payoff is arbitrarily small, regardless of the strategy Player 2 is using. We provide a necessary and sufficient condition for the attainability of a convex set, using the concept of B-sets. We then particularize the condition to the case in which the set is a singleton, and provide some equivalent conditions. We finally characterize when all vectors are attainable.
1	We develop a framework for proving approximation limits of polynomial size linear programs (LPs) from lower bounds on the nonnegative ranks of suitably defined matrices. This framework yields unconditional impossibility results that are applicable to any LP as opposed to only programs generated by hierarchies. Using our framework, we prove that O(n1/2-ϵ)-approximations for CLIQUE require LPs of size 2nΩ(ϵ)2𝑛Ω(𝜖). This lower bound applies to LPs using a certain encoding of CLIQUE as a linear optimization problem. Moreover, we establish a similar result for approximations of semidefinite programs by LPs.Our main technical ingredient is a quantitative improvement of Razborov’s [38] rectangle corruption lemma for the high error regime, which gives strong lower bounds on the nonnegative rank of shifts of the unique disjointness matrix.
1	We seek to characterize the trading behavior of an agent, in the context of a continuous-time portfolio choice model, if she measures the risk by a so called weighted value-at-risk (VaR), which is a generalization of both VaR and conditional VaR. We show that when bankruptcy is allowed the agent displays extreme risk-taking behaviors, unless the downside risk is significantly penalized, in which case an asymptotically optimal strategy is to invest a very small amount of money in an extremely risky but highly rewarding lottery, and save the rest in the risk-free asset. When bankruptcy is prohibited, extreme risk-taking behaviors are prevented in most cases in which the asymptotically optimal strategy is to spend a very small amount of money in an extremely risky but highly rewarding lottery and put the rest in an asset with moderate risk. Finally, we show that the trading behaviors remain qualitatively the same if the weighted VaR is replaced by a law-invariant coherent risk measure.
1	We establish a central limit theorem and a large deviations principle for affine point processes, which are stochastic models of correlated event timing widely used in finance and economics. These limit results generate closed-form approximations to the distribution of an affine point process. They also facilitate the construction of an asymptotically optimal importance sampling estimator of tail probabilities. Numerical tests illustrate our results.
1	We consider zero-sum repeated games with incomplete information on both sides, where the states privately observed by each player follow independent Markov chains. It generalizes the model, introduced by Aumann and Maschler in the sixties and solved by Mertens and Zamir in the seventies, where the private states of the players were fixed. It also includes the model introduced in Renault [19] [Renault J (2006) The value of Markov chain games with lack of information on one side. Math. Oper. Res. 31(3):490–512.], of Markov chain repeated games with lack of information on one side, where only one player privately observes the sequence of states. We prove here that the limit value exists, and we obtain a characterization via the Mertens-Zamir system, where the “nonrevealing value function” plugged in the system is now defined as the limit value of an auxiliary “nonrevealing” dynamic game. This nonrevealing game is defined by restricting the players not to reveal any information on the limit behavior of their own Markov chain, as in Renault [19]. There are two key technical difficulties in the proof: (1) proving regularity, in the sense of equicontinuity, of the T-stage nonrevealing value functions and (2) constructing strategies by blocks in order to link the values of the nonrevealing games with the original values.
1	Recently, a strictly contractive Peaceman-Rachford splitting method (PRSM) was proposed for a separable convex minimization model whose variables are subject to some linear constraints and two additional generic constraints. In general, the strictly contractive PRSM requires to solve two constrained minimization subproblems at each iteration. In this paper, we consider the case where the additional constraints on variables are positive orthants and apply the well-developed logarithmic-quadratic proximal (LQP) regularization to regularize the subproblems of the strictly contractive PRSM. A new algorithm in combination with the strictly contractive PRSM and the LQP regularization is thus proposed. The new algorithm only needs to solve two unconstrained subproblems at each iteration. An inexact version allowing the unconstrained subproblems to be solved approximately subject to certain inexactness criterion is also studied. We prove the global convergence and establish a worst-case convergence rate measured by the iteration complexity for both the exact and inexact versions of the new algorithm.
1	We prove that the simplex method with the highest gain/most-negative-reduced cost pivoting rule converges in strongly polynomial time for deterministic Markov decision processes (MDPs) regardless of the discount factor. For a deterministic MDP with n states and m actions, we prove the simplex method runs in O(n3m2 log2n) iterations if the discount factor is uniform and O(n5m3 log2n) iterations if each action has a distinct discount factor. Previously the simplex method was known to run in polynomial time only for discounted MDPs where the discount was bounded away from 1.
1	We investigate a game of singular control and strategic exit in a model of competitive market share control. In the model, each player can make irreversible investments to increase his market share, which is modeled as a diffusion process. In addition, each player has an option to exit the market at any point in time. We formulate a verification theorem for best responses of the game and characterize Markov perfect equilibria (MPE) under a set of verifiable assumptions. We find a class of MPEs with a rich structure. In particular, each player maintains up to two disconnected intervals of singular control regions, one of which plays a defensive role, and the other plays an offensive role. We also identify a set of conditions under which the outcome of the game may be unique despite the multiplicity of the equilibria.
1	Stochastic billiards can be used for approximate sampling from the boundary of a bounded convex set through the Markov Chain Monte Carlo paradigm. This paper studies how many steps of the underlying Markov chain are required to get samples (approximately) from the uniform distribution on the boundary of the set, for sets with an upper bound on the curvature of the boundary. Our main theorem implies a polynomial-time algorithm for sampling from the boundary of such sets.
1	We propose and solve a general entrepreneurial/managerial decision-making problem. Instead of employing concave objective functions, we use a broad class of nonconcave objective functions. We approach the problem by a martingale method. We show that the optimization problem with a nonconcave objective function has the same solution as the optimization problem when the objective function is replaced by its concave hull, and thus the problems are equivalent to each other. The value function is shown to be strictly concave and to satisfy the Hamilton-Jacobi-Bellman equation of dynamic programming. We also show that the final wealth cannot take values in the region where the objective function is not concave: the entrepreneur would like to avoid her or his wealth ending up in the nonconcave region. Because of this, the entrepreneur’s risk taking explodes as time nears maturity if her his wealth is equal to the right end point of the nonconcave region.
1	A key fact in the theory of Boolean functions f: {0, 1}n → {0, 1} is that they often undergo sharp thresholds. For example, if the function f: {0, 1}n → {0, 1} is monotone and symmetric under a transitive action with Ep[f] = ε and Eq[f] = 1−ε, then q − p → 0 as n → ∞. Here Ep denotes the product probability measure on {0, 1}n where each coordinate takes the value 1 independently with probability p.The fact that symmetric functions undergo sharp thresholds is important in the study of random graphs and constraint satisfaction problems as well as in social choice.In this paper we prove sharp thresholds for monotone functions taking values in an arbitrary finite set. We also provide examples of applications of the results to social choice and to random graph problems.Among the applications is an analog for Condorcet’s Jury Theorem and an indeterminacy result for a large class of social choice functions.
1	We consider stochastic optimal control models with Borel spaces and universally measurable policies. For such models the standard policy iteration is known to have difficult measurability issues and cannot be carried out in general. We present a mixed value and policy iteration method that circumvents this difficulty. The method allows the use of stationary policies in computing the optimal cost function in a manner that resembles policy iteration. It can also be used to address similar difficulties of policy iteration in the context of upper and lower semicontinuous models. We analyze the convergence of the method in infinite horizon total cost problems for the discounted case where the one-stage costs are bounded and for the undiscounted case where the one-stage costs are nonpositive or nonnegative.For undiscounted total cost problems with nonnegative one-stage costs, we also give a new convergence theorem for value iteration that shows that value iteration converges whenever it is initialized with a function that is above the optimal cost function and yet bounded by a multiple of the optimal cost function. This condition resembles Whittle’s bridging condition and is partly motivated by it. The theorem is also partly motivated by a result of Maitra and Sudderth that showed that value iteration, when initialized with the constant function zero, could require a transfinite number of iterations to converge. We use the new convergence theorem for value iteration to establish the convergence of our mixed value and policy iteration method for the nonnegative cost case.
1	In this paper, we study relative metric regularity of set-valued mappings with emphasis on directional metric regularity. We establish characterizations of relative metric regularity without assuming the completeness of the image spaces, by using the relative lower semicontinuous envelopes of the distance functions to set-valued mappings. We then apply these characterizations to establish a coderivative type criterion for directional metric regularity as well as for the robustness of metric regularity.
1	A caterpillar network (or graph) G is a tree with the property that removal of the leaf edges of G leaves one with a path. Here we focus on minimum weight spanning caterpillars where the vertices are points in the Euclidean plane and the costs of the path edges and the leaf edges are multiples of their corresponding Euclidean lengths. The flexibility in choosing the weight for path edges versus the weight for leaf edges gives some useful flexibility in modeling. In particular, one can accommodate problems motivated by communications theory such as the “last mile problem.” Geometric and probabilistic inequalities are developed that lead to a limit theorem that is analogous to the well-known Beardwood, Halton, and Hammersley theorem for the length of the shortest tour through a random sample, but the minimal spanning caterpillars fall outside the scope of the theory of subadditive Euclidean functionals.
1	The random priority (RP) mechanism is a popular way to allocate n objects to n agents with strict ordinal preferences over the objects. In the RP mechanism, an ordering over the agents is selected uniformly at random; the first agent is then allocated his most-preferred object, the second agent is allocated his most-preferred object among the remaining ones, and so on. The outcome of the mechanism is a bi-stochastic matrix in which entry (i, a) represents the probability that agent i is given object a. It is shown that the problem of computing the RP allocation matrix is #P-complete. Furthermore, it is NP-complete to decide if a given agent i receives a given object a with positive probability under the RP mechanism, whereas it is possible to decide in polynomial time whether or not agent i receives object a with probability 1. The implications of these results for approximating the RP allocation matrix as well as on finding constrained Pareto optimal matchings are discussed.
1	In this paper, we consider the sparse linear complementarity problem, denoted by k-LCP: the coefficient matrices are restricted to have at most k nonzero entries per row. It is known that the 1-LCP is solvable in linear time, and the 3-LCP is strongly NP-hard. We show that the 2-LCP is strongly NP-hard, and it can be solved in polynomial time if it is sign-balanced, i.e., each row of the matrix has at most one positive and one negative entry. Our second result matches the currently best-known complexity bound for the corresponding sparse linear feasibility problem. In addition, we show that an integer variant of the sign-balanced 2-LCP is weakly NP-hard and pseudo-polynomially solvable, and the generalized 1-LCP is strongly NP-hard.
1	In the context of decision making under uncertainty, I formalize the concept of analogy: an analogy between two decision problems is a mapping that transforms one problem into the other while preserving the problem’s structure. After identifying the basic structure of a decision problem, I introduce the concepts of analogical reasoning operator and of analogical reasoning preference. The former maps the decision problem at hand into a family of decision problems, which are analogous to the problem under consideration. The latter is the result of aggregating the various analogies. I provide several representations (in decreasing order of generality) of the analogical reasoning operators. After introducing two mild assumptions on the aggregators of analogies, I characterize analogical reasoning (AR) preferences. I give several examples of AR preferences and of the associated aggregators. These include Gilboa-Schmeidler similarities, Choquet integrals, and quantiles. Finally, I show that the class of monotone continuous invariant biseparable (MCIB) preferences (which includes many popular models of decision making under uncertainty) has an important stability property: any MCIB preference is an AR preference; conversely, every AR preference that results from aggregating MCIB preferences is an MCIB preference.
1	In this paper, we focus on the portfolio optimization problem associated with a quasiconvex risk measure (satisfying some additional assumptions). For coherent/convex risk measures, the portfolio optimization problem has been already studied in the literature.Following the approach of Ruszczyński and Shapiro [Ruszczyński A, Shapiro A (2006) Optimization of convex risk functions. Math. Oper. Res. 31(3):433–452.], but by means of quasiconvex analysis and notions of subdifferentiability, we characterize optimal solutions of the portfolio problem associated with quasiconvex risk measures. The shape of the efficient frontier in the mean-risk space and some particular cases are also investigated.
1	A sequential information collection problem, where a risk-averse decision maker updates a Bayesian belief about the unknown objective function of a linear program, is used to investigate the informational value of measurements performed to refine a robust optimization model. The information is collected in the form of a linear combination of the objective coefficients, subject to random noise. We have the ability to choose the weights in the linear combination, creating a new, nonconvex continuous-optimization problem, which we refer to as information blending. We develop two optimal blending strategies: (1) an active learning method that maximizes uncertainty reduction and (2) an economic approach that maximizes an expected improvement criterion. Semidefinite programming relaxations are used to create efficient convex approximations to the nonconvex blending problem.
1	We propose a two-stage risk-averse stochastic optimization problem with a stochastic-order constraint on a vector-valued function of the second-stage decisions. This model is motivated by a multiobjective second-stage problem. We formulate optimality conditions for the problem and analyse the Lagrangian relaxation of the order constraint. We propose two decomposition methods to solve the problems and prove their convergence. The methods are based on Lagrangian relaxation of the order constraints and on a construction of successive risk-neutral two-stage problems. Additionally, we propose a new combinatorial method for verification of the multivariate order relation, which is a key part of both methods. We analyse a supply chain problem using our model and we apply our methods to solve the optimization problem. Numerical results confirm the efficiency of the proposed methods.
1	The cutting plane approach to finding minimum-cost perfect matchings has been discussed by several authors over past decades. Its convergence has been an open question. We develop a cutting plane algorithm that converges in polynomial-time using only Edmonds’ blossom inequalities, and which maintains half-integral intermediate LP solutions supported by a disjoint union of odd cycles and edges. Our main insight is a method to retain only a subset of the previously added cutting planes based on their dual values. This allows us to quickly find violated blossom inequalities and argue convergence by tracking the number of odd cycles in the support of intermediate solutions.
1	We study the asymptotics of a class of two-player, zero-sum stochastic game with incomplete information on one side when the time span between two consecutive stages vanishes. The informed player observes the realization of a Markov chain on which the payoffs depend, whereas the noninformed player only observes his opponent’s actions. We show the existence of a limit value; this value is characterized through an auxiliary optimization problem and as the solution of a Hamilton-Jacobi equation.
1	The network revenue management (RM) problem arises in airline, hotel, media, and other industries where the sale products use multiple resources. It can be formulated as a stochastic dynamic program, but the dynamic program is computationally intractable because of an exponentially large state space, and a number of heuristics have been proposed to approximate its value function. In this paper we show that the piecewise-linear approximation to the network RM dynamic program is tractable; specifically we show that the separation problem of the approximation can be solved as a relatively compact linear program. Moreover, the resulting compact formulation of the approximate dynamic program turns out to be exactly equivalent to the Lagrangian relaxation of the dynamic program, an earlier heuristic method proposed for the same problem. We perform a numerical comparison of solving the problem by generating separating cuts or as our compact linear program. We discuss extensions to versions of the network RM problem with overbooking as well as the difficulties of extending it to the choice model of network revenue RM.
1	The theory of dynamic programming is formulated using finitely additive probability measures defined on sets of arbitrary cardinality. Many results from the conventional countably additive theory generalize, and the proofs are simpler.
1	We study a class of stochastic target games where one player tries to find a strategy such that the state process almost surely reaches a given target, no matter which action is chosen by the opponent. Our main result is a geometric dynamic programming principle, which allows us to characterize the value function as the viscosity solution of a nonlinear partial differential equation. Because abstract measurable selection arguments cannot be used in this context, the main obstacle is the construction of measurable almost optimal strategies. We propose a novel approach where smooth supersolutions are used to define almost-optimal strategies of Markovian type, similarly as in verification arguments for classical solutions of Hamilton-Jacobi-Bellman equations. The smooth supersolutions are constructed by an extension of Krylov’s method of shaken coefficients. We apply our results to a problem of option pricing under model uncertainty with different interest rates for borrowing and lending.
1	We study a class of games with a continuum of players for which a Cournot-Nash equilibria can be obtained by the minimisation of some cost related to optimal transport. This cost is not convex in the usual sense, in general, but it turns out to have hidden strict convexity properties in many relevant cases. This enables us to obtain new uniqueness results and a characterisation of equilibria in terms of some partial differential equations, a simple numerical scheme in dimension one as well as an analysis of the inefficiency of equilibria.
1	This paper investigates the problem of maximizing expected terminal utility in a (generically incomplete) discrete-time financial market model with finite time horizon. By contrast to the standard setting, a possibly nonconcave utility function U is considered, with domain of definition equal to the whole real line. Simple conditions are presented that guarantee the existence of an optimal strategy for the problem. In particular, the asymptotic elasticity of U plays a decisive role: Existence can be shown when it is strictly greater at −∞ than at +∞.
1	We propose a general discrete-time framework for deriving equilibrium prices of financial securities. It allows for heterogeneous agents, unspanned random endowments, and convex trading constraints. We give a dual characterization of equilibria and provide general results on their existence and uniqueness. In the special case where all agents have preferences of the same type and in equilibrium, all random endowments are replicable by trading in the financial market, we show that a one-fund theorem holds and give an explicit expression for the equilibrium pricing kernel.
1	We consider the problem of minimizing a general continuously differentiable function over symmetric sets under sparsity constraints. These type of problems are generally hard to solve because the sparsity constraint induces a combinatorial constraint into the problem, rendering the feasible set to be nonconvex. We begin with a study of the properties of the orthogonal projection operator onto sparse symmetric sets. Based on this study, we derive efficient methods for computing sparse projections under various symmetry assumptions. We then introduce and study three types of optimality conditions: basic feasibility, L-stationarity, and coordinatewise optimality. A hierarchy between the optimality conditions is established by using the results derived on the orthogonal projection operator. Methods for generating points satisfying the various optimality conditions are presented, analyzed, and finally tested on specific applications.
1	This work is concerned with finite-state irreducible Markov decision chains satisfying continuity-compactness requirements. It is supposed that the system is driven by a decision maker with utility function U, which, aside mild conditions, is arbitrary, and the performance of a control policy is measured by the long-run average cost criterion induced by U. The main conclusions about this performance index are as follows: (i) the optimal U-average value function coincides with the optimal V-average index for a certain exponential utility V, and (ii) the average criteria associated with U and V have the same class of optimal stationary policies.
1	We develop asymptotic approximations for the tail probabilities of integrals of lognormal random fields. We consider the asymptotic regime that the variance of the random field converges to zero. Under this setting, the integral converges to its limiting value. This analysis is of interest in considering short-term portfolio risk analysis (such as daily performance), for which the variances of log-returns could be as small as a few percent.
1	In this paper, we study the classical no-wait flowshop scheduling problem with makespan objective (F|no-wait|Cmax in the standard three-field notation). This problem is well known to be a special case of the asymmetric traveling salesman problem (ATSP) and as such has an approximation algorithm with logarithmic performance guarantee. In this work, we show a reverse connection, we show that any polynomial time α-approximation algorithm for the no-wait flowshop scheduling problem with makespan objective implies the existence of a polynomial time α(1 + ɛ)-approximation algorithm for the ATSP for any ɛ > 0. This, in turn, implies that all nonapproximability results for the ATSP (current or future) will carry over to its special case. In particular, it follows that the no-wait flowshop problem is APX-hard, which is the first nonapproximability result for this problem.
1	We initiate the study of congestion games with variable demands in which the players strategically choose both a nonnegative demand and a subset of resources. The players’ incentives to use higher demands are stimulated by nondecreasing and concave utility functions. The payoff for a player is defined as the difference between the utility of the demand and the associated cost on the used resources. Although this class of noncooperative games captures many elements of real-world applications, it has not been studied in this generality in the past. Specifically, we study the fundamental problem of the existence of pure Nash equilibria, PNE for short. We call a set of cost functions consistent if every congestion game with variable demands and cost functions from the set possesses a PNE. We show that only affine and homogeneous exponential functions are consistent. En route, we obtain novel characterizations of consistency for congestion games with fixed but resource-dependent demands.
1	A minimal diversity game is an n player strategic form game in which each player has m pure strategies at his disposal. The payoff to each player is always 1, unless all players select the same pure strategy, in which case, all players receive zero payoff. Such a game has a unique isolated completely mixed Nash equilibrium in which each player plays each strategy with equal probability, and a connected component of Nash equilibria consisting of those strategy profiles in which each player receives payoff 1. The Pareto superior component is shown to be asymptotically stable under a wide class of evolutionary dynamics, while the isolated equilibrium is not. In contrast, the isolated equilibrium is strategically stable, while the strategic stability of the Pareto-efficient component depends on the dimension of the component, and hence on the number of players, and the number of pure strategies.
1	Quantiles play an important role in modelling quality of service in the service industry and in modelling risk in the financial industry. The recent discovery that efficient simulation-based estimators can be obtained for quantile sensitivities has led to an intensive search for sample-path differentiation-based estimators for quantile sensitivities. In this paper, we present a novel approach to quantile sensitivity estimation. Our approach elaborates on the concept of measure-valued differentiation. Thereby, we overcome the main obstacle of the sample-path approach, which is the requirement that the sample cost have to be Lipschitz continuous with respect to the parameter of interest. Specifically, we perform a sensitivity analysis of the value at risk in financial models. In addition, we discuss an application of our sensitivity estimator to queueing networks.
1	The capacitated vehicle routing problem (CVRP) involves distributing identical items from a depot to a set of demand locations using a single capacitated vehicle. We introduce the heterogeneous capacitated vehicle routing problem, a generalization of CVRP to the setting of multiple vehicles having nonuniform speeds, and present for it a constant-factor approximation algorithm.Our main contribution is an approximation algorithm for the heterogeneous traveling salesman problem, which is the special case of heterogeneous CVRP with uncapacitated vehicles. Given a metric denoting distances between vertices, a depot r containing k vehicles having respective speeds {λi}ki=1{𝜆𝑖}𝑘𝑖=1, the objective in heterogeneous TSP is to find a tour for each vehicle (starting and ending at r) so that every vertex is covered in some tour and the maximum completion time is minimized; the completion time of a vehicle is the distance traveled divided by its speed.Our algorithm relies on a new approximate minimum spanning tree construction called Level-Prim, which is related to but different from Light Approximate Shortest-path Trees. We also extend the widely used tour-splitting technique to nonuniform speeds, using ideas from the 2-approximation algorithm for scheduling in unrelated machines.
1	We present a unified framework for designing deterministic monotone polynomial time approximation schemes (PTASs) for a wide class of scheduling problems on uniformly related machines. This class includes (among others) minimizing the makespan, maximizing the minimum load, and minimizing the p-norm of the machine loads vector. Previously, this kind of result was only known for the makespan objective. Monotone algorithms have the property that an increase in the speed of a machine cannot decrease the amount of work assigned to it. Our results imply the existence of a truthful mechanism that can be implemented in polynomial time, where the social goal is approximated within arbitrary precision.
1	We investigate computational and mechanism design aspects of allocating medical treatments at hospitals of different costs to patients who each value these hospitals differently. The payer wants to ensure that the total cost of all treatments is at most the budget, B. Access to overdemanded hospitals is rationed through waiting times.We first show that optimizing social welfare in equilibrium is NP-hard. But if the number of hospitals is small and the budget can be relaxed to (1 + ɛ)B for arbitrarily small ɛ, the optimum under budget B can be achieved efficiently. Next, we show waiting times emerge endogenously from the dynamics between hospitals and patients and the payer doesn’t have to explicitly enforce them; all it needs to do is enforce the amount of money paid to each hospital, and the dynamics will converge to the desired waiting times in finite time. Going beyond equilibrium solutions, we investigate the optimization problem over a much larger class of mechanisms. With two hospitals and concave preference profiles of the patients, optimal welfare is actually attained by the randomized assignment, which allocates patients at random and avoids waiting times. Finally, we discuss potential policy implications of our results, followup directions, and open problems.
1	In this paper, we study distributionally robust optimization approaches for a one-stage stochastic minimization problem, where the true distribution of the underlying random variables is unknown but it is possible to construct a set of probability distributions, which contains the true distribution and optimal decision is taken on the basis of the worst-possible distribution from that set. We consider the case when the distributional set (which is also known as the ambiguity set) varies and its impact on the optimal value and the optimal solutions. A typical example is when the ambiguity set is constructed through samples and we need to look into the impact of increasing the sample size. The analysis provides a unified framework for convergence of some problems where the ambiguity set is approximated in a process with increasing information on uncertainty and extends the classical convergence analysis in stochastic programming. The discussion is extended briefly to a stochastic Nash equilibrium problem where each player takes a robust action on the basis of the worst subjective expected objective values.
1	We propose empirical dynamic programming algorithms for Markov decision processes. In these algorithms, the exact expectation in the Bellman operator in classical value iteration is replaced by an empirical estimate to get “empirical value iteration” (EVI). Policy evaluation and policy improvement in classical policy iteration are also replaced by simulation to get “empirical policy iteration” (EPI). Thus, these empirical dynamic programming algorithms involve iteration of a random operator, the empirical Bellman operator. We introduce notions of probabilistic fixed points for such random monotone operators. We develop a stochastic dominance framework for convergence analysis of such operators. We then use this to give sample complexity bounds for both EVI and EPI. We then provide various variations and extensions to asynchronous empirical dynamic programming, the minimax empirical dynamic program, and show how this can also be used to solve the dynamic newsvendor problem. Preliminary experimental results suggest a faster rate of convergence than stochastic approximation algorithms.
1	In this paper, we study discounted stochastic games with Borel state and compact action spaces depending on the state variable. The primitives of our model satisfy standard continuity and measurability conditions. The transition probability is a convex combination of finitely many probability measures depending on states, and it is dominated by some finite measure on the state space. The coefficients of the combination depend on both states and action profiles. This class of models contains stochastic games with Borel state spaces and finite, state-dependent action sets. Our main result establishes the existence of subgame perfect equilibria, which are stationary in the sense that the equilibrium strategy for each player is determined by a single function of the current and previous states of the game. This dependence is called almost Markov. Our result enhances both the theorem of Mertens and Parthasarathy established in 1991 for games with finite, state-independent action sets, where the equilibrium strategies were also depended on the calendar time, and their result on stationary equilibria proved under an additional condition that the transition probabilities are atomless. A counterexample given very recently by Levy shows that stationary Markov perfect equilibria may not exist in the class of games considered in this paper. The presented results in this work are illustrated by the Cournot dynamic games, which were already considered in the literature under much stronger assumptions.
1	In view of solving nonsmooth and nonconvex problems involving complex constraints (like standard NLP problems), we study general maximization-minimization procedures produced by families of strongly convex subproblems. Using techniques from semi-algebraic geometry and variational analysis—in particular Łojasiewicz inequality—we establish the convergence of sequences generated by these types of schemes to critical points. The broad applicability of this process is illustrated in the context of NLP. In that case, critical points coincide with KKT points. When the data are semi-algebraic or real analytic our method applies (for instance) to the study of various sequential quadratic programming (SQP) schemes: the moving balls method, the penalized SQP method and the extended SQP method. Under standard qualification conditions, this provides—to the best of our knowledge—the first general convergence results for general nonlinear programming problems. We emphasize the fact that, unlike most works on this subject, no second-order conditions and/or convexity assumptions whatsoever are made. Rate of convergence are shown to be of the same form as those commonly encountered with first-order methods.
1	We revisit Machina’s local utility as a tool to analyze attitudes to multivariate risks. We show that for nonexpected utility maximizers choosing between multivariate prospects, aversion to multivariate mean preserving increases in risk is equivalent to the concavity of the local utility functions, thereby generalizing Machina’s result [Machina M (1982) “Expected utility” analysis without the independence axiom. Econometrica 50:277–323]. To analyze comparative risk attitudes within the multivariate extension of rank dependent expected utility of Galichon and Henry [Galichon A, Henry M (2012) Dual theory of choice with multivariate risks. J. Econom. Theory 147:1501–1516], we extend Quiggin’s monotone mean and utility preserving increases in risk and show that the useful characterization given in Landsberger and Meilijson [Landsberger M, Meilijson I (1994) Comonotone allocations, Bickel-Lehmann dispersion and the Arrow-Pratt measure of risk aversion. Ann. Oper. Res. 52:97–106] still holds in the multivariate case.
1	We study disjunctive conic sets involving a general regular (closed, convex, full dimensional, and pointed) cone 𝒦 such as the nonnegative orthant, the Lorentz cone, or the positive semidefinite cone. In a unified framework, we introduce 𝒦-minimal inequalities and show that, under mild assumptions, these inequalities together with the trivial cone-implied inequalities are sufficient to describe the convex hull. We focus on the properties of 𝒦-minimal inequalities by establishing algebraic necessary conditions for an inequality to be 𝒦-minimal. This characterization leads to a broader algebraically defined class of 𝒦-sublinear inequalities. We demonstrate a close connection between 𝒦-sublinear inequalities and the support functions of convex sets with a particular structure. This connection results in practical ways of verifying 𝒦-sublinearity and/or 𝒦-minimality of inequalities.Our study generalizes some of the results from the mixed integer linear case. It is well known that the minimal inequalities for mixed integer linear programs are generated by sublinear (positively homogeneous, subadditive, and convex) functions that are also piecewise linear. Our analysis easily recovers this result. However, in the case of general regular cones other than the nonnegative orthant, our study reveals that such a cut-generating function view, which treats the data associated with each individual variable independently, is far from sufficient.
1	We complete the complexity classification by degree of minimizing a polynomial over the integer points in a polyhedron in a real vector space of dimension two. Previous work shows that optimizing a quadratic polynomial over the integer points in a polyhedral region in a real vector space of dimension two can be done in polynomial time, whereas optimizing a quartic polynomial in the same type of region is NP-hard. We close the gap by showing that this problem can be solved in polynomial time for cubic polynomials.Furthermore, we show that the problem of minimizing a homogeneous polynomial of any fixed degree over the integer points in a bounded polyhedron in a real vector space of dimension two is solvable in polynomial time. We show that this holds for polynomials that can be translated into homogeneous polynomials, even when the translation vector is unknown. We demonstrate that such problems in the unbounded case can have smallest optimal solutions of exponential size in the size of the input, thus requiring a compact representation of solutions for a general polynomial time algorithm for the unbounded case.
1	We propose an accurate method for pricing arithmetic Asian options on the discrete or continuous average in a general model setting by means of a lower bound approximation. In particular, we derive analytical expressions for the lower bound in the Fourier domain. This is then recovered by a single univariate inversion and sharpened using an optimization technique. In addition, we derive an upper bound to the error from the lower bound price approximation. Our proposed method can be applied to computing the prices and price sensitivities of Asian options with fixed or floating strike price, discrete or continuous averaging, under a wide range of stochastic dynamic models, including exponential Lévy models, stochastic volatility models, and the constant elasticity of variance diffusion. Our extensive numerical experiments highlight the notable performance and robustness of our optimized lower bound for different test cases.
1	We study a fork-join network of stations with multiple servers and nonexchangeable synchronization in heavy traffic under the first-come-first- served (FCFS) discipline. Tasks are only synchronized if all the tasks associated with the same job are completed. Service times of parallel tasks of each job can be correlated. We jointly consider the number of tasks in each waiting buffer for synchronization with the number of tasks in each parallel service station and the number of synchronized jobs. We develop a new approach to show a functional central limit theorem for these processes in the quality-driven regime, under general assumptions on arrival and service processes. Specifically, we represent these processes as functionals of a sequential empirical process driven by the sequence of service vectors for each job’s parallel tasks. All of the limiting processes are functionals of two independent processes, i.e., the limiting arrival process and a generalized Kiefer process driven by the service vector of each job. We characterize the transient and stationary distributions of the limiting processes.
1	We introduce concepts of metric regularity and metric subregularity of a positive-order for an implicit multifunction and provide new sufficient conditions for the implicit multifunctions to achieve the addressed properties. The conditions provided are presented in terms of the Fréchet/Mordukhovich coderivative of the corresponding parametric multifunction formulated the implicit multifunction. We show that such sufficient conditions are also necessary for the metric regularity/subregularity of a positive-order of the implicit multifunction when the corresponding parametric multifunction is (locally) convex and closed. In this way, we establish criteria ensuring that an implicit multifunction is Hölder-like and calm of a positive-order at a given point. As applications, we derive sufficient conditions in terms of coderivatives for a multifunction (resp., its inverse multifunction) to have the open covering property and the metric regularity/subregularity of a positive-order (resp., the Hölder-like/calm property).
1	Online learning and competitive analysis are two widely studied frameworks for online decision-making settings. Despite the frequent similarity of the problems they study, there are significant differences in their assumptions, goals, and techniques, hindering a unified analysis and richer interplay between the two. In this paper, we provide several contributions in this direction. We provide a single unified algorithm, which, by parameter tuning, interpolates between optimal regret for learning from experts (in online learning) and optimal competitive ratio for the metrical task systems problem (MTS) (in competitive analysis), improving upon previous results. The algorithm also allows us to obtain new regret bounds against “drifting” experts, which might be of independent interest. Moreover, our approach allows us to go beyond experts/MTS, obtaining similar unifying results for structured action sets and “combinatorial experts,” whenever the setting has a certain matroid structure.
1	This paper is concerned with the optimality of a trend following trading rule. The underlying market is modeled like a bull-bear switching market in which the drift of the stock price switches between two states: the uptrend (bull market) and the down trend (bear market). We consider the case when the market mode is not directly observable and model the switching process as a hidden Markov chain. This is a continuation of our earlier study reported in Dai et al. [Dai M, Zhang Q, Zhu Q (2010) Trend following trading under a regime-switching model. SIAM J. Fin. Math. 1:780–810] where a trend following rule is obtained in terms of a sequence of stopping times. Nevertheless, a severe restriction imposed in Dai et al. [Dai M, Zhang Q, Zhu Q (2010) trend following trading under a regime-switching model. SIAM J. Fin. Math. 1:780–810] is that only a single share can be traded over time. As a result, the corresponding wealth process is not self-financing. In this paper, we relax this restriction. Our objective is to maximize the expected log-utility of the terminal wealth. We show, via a thorough theoretical analysis, that the optimal trading strategy is trend following. Numerical simulations and backtesting, in support of our theoretical findings, are also reported.
1	In the context of pairwise comparison ranking, we show that HodgeRank (row geometric mean), is the limit of Perron Rank (ranking with principal eigenvector) as a certain parameter k goes to 0. This result provides a novel mathematical link between two important pairwise ranking methods. It complements the known result that as k approaches infinity, Perron Rank converges to Tropical Rank. Thus, these three pairwise ranking methods belong to the same parametrized family. Our proof technique is useful for mathematical comparison of these methods. As a sample application, we show that for ranking models with i.i.d noise, HodgeRank is a linear approximation of Perron Rank. In this particular setup, for large numbers of items with sufficiently large score differences, the two methods yield identical ordinal rankings.
1	We show that in zero-sum  polymatrix games, a multiplayer generalization of two-person zero-sum games, Nash equilibria can be found efficiently with linear programming. We also show that the set of coarse correlated equilibria collapses to the set of Nash equilibria. In contrast, other important properties of two-person zero-sum games are not preserved: Nash equilibrium payoffs need not be unique, and Nash equilibrium strategies need not be exchangeable or max-min.
1	This paper describes sufficient conditions for the existence of optimal policies for partially observable Markov decision processes (POMDPs) with Borel state, observation, and action sets, when the goal is to minimize the expected total costs over finite or infinite horizons. For infinite-horizon problems, one-step costs are either discounted or assumed to be nonnegative. Action sets may be noncompact and one-step cost functions may be unbounded. The introduced conditions are also sufficient for the validity of optimality equations, semicontinuity of value functions, and convergence of value iterations to optimal values. Since POMDPs can be reduced to completely observable Markov decision processes (COMDPs), whose states are posterior state distributions, this paper focuses on the validity of the above-mentioned optimality properties for COMDPs. The central question is whether the transition probabilities for the COMDP are weakly continuous. We introduce sufficient conditions for this and show that the transition probabilities for a COMDP are weakly continuous, if transition probabilities of the underlying Markov decision process are weakly continuous and observation probabilities for the POMDP are continuous in total variation. Moreover, the continuity in total variation of the observation probabilities cannot be weakened to setwise continuity. The results are illustrated with counterexamples and examples.
1	In management and planning it is commonplace for additional information to become available gradually over time. It is well known that most risk measures (risk functionals) are time inconsistent in the following sense: it may happen that at a given time period, some loss distribution appears to be less risky than another one, but looking at the conditional distribution at a later time, the opposite relation holds almost surely.The extended conditional risk functionals introduced in this paper enable a temporal decomposition of the initial risk functional that can be used to ensure consistency between past and future preferences. The central result is a decomposition theorem, which allows recomposing the initial coherent risk functional by compounding the conditional risk functionals without losing information or preferences. It follows from our results that the revelation of partial information in time must change the decision maker’s preferences—for consistency reasons—among the remaining courses of action. Further, in many situations, the extended conditional risk functional allows ranking of different policies, even based on incomplete information.In addition, we use counterexamples to show that without change-of-measures, the only time-consistent risk functionals are the expectation and the essential supremum.
1	This paper addresses Bruss’ odds problem with multiple stopping chances. A decision maker sequentially observes a sequence of independent 0/1 (failure/success) random variables to correctly predict the last success with multiple stopping chances. First, we give a nontrivial lower bound of the probability of win (obtaining the last success) for the problem with m-stoppings. Next, we show that the asymptotic value for each classical secretary problem with multiple stoppings attains our lower bound. Finally, we prove a conjecture on the classical secretary problem, which gives a connection between the probability of win and the threshold values of the optimal stopping strategy.
1	In the multidimensional 0-1 knapsack problem, we are given a set of items, each with a value and multiple attributes, and we want to select a subset in such a way that the total value is maximized while the total quantity of each attribute satisfies a capacity constraint. In this paper, we assume that quantities of the item attributes are independent random variables such that those of the same attribute across different items follow the same type of probability distribution, not necessarily with the same parameters. A joint probabilistic constraint is imposed on the capacity constraints, and the objective function is the same as that of the underlying deterministic problem. We prove that the problem is convex, under some condition on the parameters, for special continuous and discrete distributions: gamma, normal, Poisson, and binomial, in which the latter two discrete distribution functions are extended to log-concave continuous distribution functions. We present computational experiments to demonstrate the tractability of our approach.
1	In Pennanen [2] [Pennanen T (2011) Convex duality in stochastic optimization and mathematical finance. Math. Oper. Res. 36(2):340–362], Theorem 2.2 is not valid as stated. It omits certain integrability conditions that are needed in general. The additional conditions are satisfied in most of the applications given in Pennanen [2]. For the remaining ones, sufficient conditions are given below. The topological results in Section 5 remain unaffected. A corrected version is given.
1	In 2010, Huang introduced the laminar classified stable matching problem (lcsm for short) that is motivated by academic hiring. This problem is an extension of the well-known hospitals/residents problem in which a hospital has laminar classes of residents and it sets lower and upper bounds on the number of residents that it can hire in each class. Against the intuition that variations of the stable matching problem with lower quotas are difficult in general, Huang proved that lcsm can be solved in polynomial time. In this paper, we present a matroid-based approach to lcsm and we obtain the following results. (i) We solve a generalization of lcsm in which both sides have quotas. (ii) Huang raised a question about a polyhedral description of the set of stable assignments in lcsm. We give a positive answer for this question by exhibiting a polyhedral description of the set of stable assignments in a generalization of lcsm. (iii) We prove that the set of stable assignments in a generalization of lcsm has a lattice structure that is similar to the (ordinary) stable matching problem.
1	The asymmetric traveling salesperson path problem (ATSPP) is one where, given an asymmetric metric space (V, d) with specified vertices s and t, the goal is to find an s-t path of minimum length that passes through all the vertices in V.This problem is closely related to the asymmetric TSP (ATSP), which seeks to find a tour (instead of an s-t path) visiting all the nodes: for ATSP, a ρ-approximation guarantee implies an O(ρ)-approximation for ATSPP. However, no such connection is known for the integrality gaps of the linear programming (LP) relaxations for these problems: the current-best approximation algorithm for ATSPP is O(ln n/ln ln n), whereas the best bound on the integrality gap of the natural LP relaxation (the subtour elimination LP) for ATSPP is O(ln n).In this paper, we close this gap, and improve the current best bound on the integrality gap from O(ln n) to O(ln n/ln ln n). The resulting algorithm uses the structure of narrow s-t cuts in the LP solution to construct a (random) tree spanning tree that can be cheaply augmented to contain an Eulerian s-t walk.We also build on a result of Oveis Gharan and Saberi and show a strong form of Goddyn’s conjecture about thin spanning trees implies the integrality gap of the subtour elimination LP relaxation for ATSPP is bounded by a constant. Finally, we give a simpler family of instances showing the integrality gap of this LP is at least 2.
1	Given a Markov decision process (MDP) with n states and a total number m of actions, we study the number of iterations needed by policy iteration (PI) algorithms to  converge to the optimal γ-discounted policy. We consider two variations of PI: Howard’s PI that changes the actions in all states with a positive advantage, and  Simplex-PI that only changes the action in the state with maximal advantage. We show that Howard’s PI terminates after at  most O((m/(1 − γ))log(1/(1 − γ))) iterations, improving by a factor  O(log n) a result by Hansen et al. [Hansen TD, Miltersen PB, Zwick U (2013) Strategy iteration is strongly polynomial for two-player turn-based stochastic games with a  constant discount factor. J. ACM 60(1):1:1–1:16.], whereas Simplex-PI terminates after at most O((nm/(1 −  γ))log(1/(1 − γ))) iterations, improving by a factor O(log n) a result by Ye [Ye Y (2011) The simplex and  policy-iteration methods are strongly polynomial for the Markov decision problem with a fixed discount rate. Math. Oper. Res. 36(4):593–603.]. Under some structural properties of  the MDP, we then consider bounds that are independent of the discount factor γ: quantities of interest are bounds τt and  τr—uniform on all states and policies—respectively, on the expected time spent in transient states and the inverse of the  frequency of visits in recurrent states given that the process starts from the uniform distribution. Indeed, we show that Simplex-PI terminates after at  most Õ(n3m2τtτr) iterations.  This extends a recent result for deterministic MDPs by Post and Ye [Post I, Ye Y (2013) The simplex method is strongly polynomial for deterministic Markov decision processes. Khanna S, ed. Proc. 24th  ACM-SIAM Sympos. Discrete Algorithms, SODA '13 (SIAM, Philadelphia), 1465–1473.] in which τt ≤ 1 and τr ≤ n; in particular it shows that Simplex-PI is strongly polynomial for a much larger class of MDPs. We explain why similar results seem hard to derive for Howard’s PI. Finally, under the additional (restrictive) assumption that the state space is partitioned in two sets, respectively, states that are transient and recurrent for all policies, we show that both Howard’s PI and Simplex-PI terminate after at most Õ(m(n2τt + nτr)) iterations.
1	We study a queueing model of customer service chat systems. A unique feature of these queueing systems is that a single agent can serve multiple customers simultaneously. We prove the convergence of the queueing process to different diffusion processes in a many-server heavy-traffic regime in three different cases. Using this result, we are able to offer approximations for the steady-state performance measures such as the number of customers in the system, the abandonment probability, and the sojourn time of a customer. Our numerical experiments show that proposed approximations are accurate in various cases.
1	Many optimization problems in probabilistic combinatorics and mass transportation impose fixed marginal constraints. A natural and open question in this field is to determine all possible distributions of the sum of random variables with given marginal distributions; the notion of joint mixability is introduced to address this question. A tuple of univariate distributions is said to be jointly mixable if there exist random variables, with respective distributions, such that their sum is a constant. We obtain necessary and sufficient conditions for the joint mixability of some classes of distributions, including uniform distributions, distributions with monotone densities, distributions with unimodal-symmetric densities, and elliptical distributions with the same characteristic generator. Joint mixability is directly connected to many open questions on the optimization of convex functions and probabilistic inequalities with marginal constraints. The results obtained in this paper can be applied to find extreme scenarios on risk aggregation under model uncertainty at the level of dependence.
1	We consider a centralized multisensor online quickest disorder detection problem where the observation from each sensor is a Wiener process gaining a constant drift at a common unobservable disorder time. The objective is to detect the disorder time as quickly as possible with small probability of false alarms. Unlike the earlier work on multisensor change detection problems, we assume that the observer can apply a sequential sensor installation policy. At any time before a disorder alarm is raised, the observer can install new sensors to collect additional signals. The sensors are statistically identical, and there is a fixed installation cost per sensor. We propose a Bayesian formulation of the problem. We identify an optimal policy consisting of a sequential sensor installation strategy and an alarm time, which minimize a linear Bayes risk of detection delay, false alarm, and new sensor installations. We also provide a numerical algorithm and illustrate it on examples. Our numerical examples show that significant reduction in the Bayes risk can be attained compared to the case where we apply a static sensor policy only. In some examples, the optimal sequential sensor installation policy starts with 30% less number of sensors than the optimal static sensor installation policy and the total percentage savings reach to 12%.
1	Two important characteristics encountered in many real-world scheduling problems are heterogeneous processors and a certain degree of uncertainty about the processing times of jobs. In this paper we address both, and study for the first time a scheduling problem that combines the classical unrelated machine scheduling model with stochastic processing times of jobs. By means of a novel time-indexed linear programming relaxation, we show how to compute in polynomial time a scheduling policy with provable performance guarantee for the stochastic version of the unrelated parallel machine scheduling problem with the weighted sum of completion times objective. Our performance guarantee depends on the squared coefficient of variation of the processing times and we show that this dependence is tight. Currently best-known bounds for deterministic scheduling problems are contained as special cases.
1	We introduce a problem that is a common generalization of the uncapacitated facility location (UFL) and minimum latency (ML) problems, where facilities not only need to be opened to serve clients, but also need to be sequentially activated before they can provide service. This abstracts a setting where inventory demanded by customers needs to be stocked or replenished at facilities from a depot or warehouse.Formally, we are given a set ℱ of n facilities with facility-opening costs, a set 𝒟 of m clients, and connection costs c(i, j) specifying the cost of assigning a client j to a facility i, a root node r denoting the depot, and a time metric d() on ℱ ∪ {r}. Our goal is to open a subset of facilities, find a path P starting at r and spanning the open facilities to activate them, and connecting each client j to an open facility so as to minimize the total facility opening cost, the total client connection cost, and the total time of arrivals at each facility along P. We call this the minimum latency uncapacitated facility location (MLUFL) problem.Our main result is an O(log n max(log n, log m))-approximation for MLUFL. Via a reduction to the group Steiner tree (GST) problem, we show this result is tight in the sense that any improvement in the approximation guarantee for MLUFL, implies an improvement in the (currently known) approximation factor for GST. We obtain significantly improved constant approximation guarantees for two natural special cases of the problem: (a) Related MLUFL, where the connection costs form a metric that is a scalar multiple of the time metric; (b) Metric uniform MLUFL, where we have metric connection costs and the time-metric is uniform.Our LP-based methods are fairly versatile and are easily adapted with minor changes to yield approximation guarantees for MLUFL (and ML) in various more general settings, such as (i) the setting where the latency-cost of a client is a function (of bounded growth) of the delay faced by the facility to which it is connected; and (ii) the k-route version, where we can dispatch k vehicles in parallel to activate the open facilities.Our LP-based understanding of MLUFL also offers some LP-based insights into ML. We obtain two natural LP-relaxations for ML with constant integrality gap, which we believe shed new light upon the problem and offer a promising direction for obtaining improvements for ML.
1	The problem of finding a minimizer of the sum of two convex functions—or, more generally, that of finding a zero of the sum of two maximally monotone operators—is of central importance in variational analysis. Perhaps the most popular method of solving this problem is the Douglas–Rachford splitting method. Surprisingly, little is known about the range of the Douglas–Rachford operator.In this paper, we set out to study this range systematically. We prove that for 3* monotone operators a very pleasing formula can be found that reveals the range to be nearly equal to a simple set involving the domains and ranges of the underlying operators. A similar formula holds for the range of the corresponding displacement mapping. We discuss applications to subdifferential operators, to the infimal displacement vector, and to firmly nonexpansive mappings. Various examples and counterexamples are presented, including some concerning the celebrated Brezis–Haraux theorem.
1	Lost sales inventory models with large lead times, which arise in many practical settings, are notoriously difficult to optimize due to the curse of dimensionality. In this paper, we show that when lead times are large, a very simple constant-order policy, first studied by Reiman, performs nearly optimally. The main insight of our work is that when the lead time is very large, such a significant amount of randomness is injected into the system between when an order for more inventory is placed and when the order is received, that “being smart” algorithmically provides almost no benefit. Our main proof technique combines a novel coupling for suprema of random walks with arguments from queueing theory.
1	We consider a model for linear transient price impact for multiple assets that takes cross-asset impact into account. Our main goal is to single out properties that need to be imposed on the decay kernel so that the model admits well-behaved optimal trade execution strategies. We first show that the existence of such strategies is guaranteed by assuming that the decay kernel corresponds to a matrix-valued positive definite function. An example illustrates, however, that positive definiteness alone does not guarantee that optimal strategies are well-behaved. Building on previous results from the one-dimensional case, we investigate a class of nonincreasing, non-negative, and convex decay kernels with values in a space of symmetric matrices. We show that these decay kernels are always positive definite and characterize when they are even strictly positive definite, a result that may be of independent interest. Optimal strategies for kernels from this class are particularly well-behaved if one requires that the decay kernel is also commuting. We show how such decay kernels can be constructed by means of matrix functions and provide a number of examples. In particular, we completely solve the case of matrix exponential decay.
1	We consider a sequential inspection game where an inspector uses a limited number of inspections over a larger number of time periods to detect a violation (an illegal act) of an inspectee. Compared with earlier models, we allow varying rewards to the inspectee for successful violations. As one possible example, the most valuable reward may be the completion of a sequence of thefts of nuclear material needed to build a nuclear bomb. The inspectee can observe the inspector, but the inspector can only determine if a violation happens during a stage where he inspects, which terminates the game; otherwise the game continues.Under reasonable assumptions for the payoffs, the inspector’s strategy is independent of the number of successful violations. This allows to apply a recursive description of the game, even though this normally assumes fully informed players after each stage. The resulting recursive equation in three variables for the equilibrium payoff of the game, which generalizes several other known equations of this kind, is solved explicitly in terms of sums of binomial coefficients.We also extend this approach to nonzero-sum games and “inspector leadership” where the inspector commits to (the same) randomized inspection schedule, but the inspectee acts legally (rather than mixes as in the simultaneous game) as long as inspections remain.
1	We give an algorithmic solution of the optimal consumption problem supc∫[0,τ]e−βtdCtsup𝑐∫[0,𝜏]𝑒−𝛽𝑡𝑑𝐶𝑡, where Ct denotes the accumulated consumption until time t, and τ denotes the time of ruin. Moreover, the endowment process Xt is modeled by Xt=x+∫t0μ(Xs)ds−Ct𝑋𝑡=𝑥+∫𝑡0𝜇(𝑋𝑠)𝑑𝑠−𝐶𝑡. We solve the problem by showing that the function provided by the algorithm solves the Hamilton-Jacobi (HJ) equation in a viscosity sense and that the same is true for the value function of the problem. The argument is finished by a uniqueness result. It turns out that one has to change the optimal strategy at a sequence of endowment values, described by a free boundary value problem.Finally we give an illustrative example.
1	Let G be a digraph and let π(G) be the linear system consisting of nonnegativity, stability, and domination inequalities. We call G kernel ideal if π(H) defines an integral polytope for each induced subgraph H of G, and we call G kernel Mengerian if π(H) is totally dual integral (TDI) for each induced subgraph H of G. In this paper we show that a digraph is kernel ideal iff it is kernel Mengerian iff it contains none of three forbidden structures; our characterization yields a polynomial-time algorithm for the minimum weighted kernel problem on kernel ideal digraphs. We also prove that it is NP-hard to find a kernel of minimum size even in a planar bipartite digraph with maximum degree at most three.
1	Scheduling a set of n jobs on m identical parallel machines so as to minimize the makespan or maximize the minimum machine load are two of the most important and fundamental scheduling problems studied in the literature. We consider the general online scenario where jobs are consecutively added to and/or deleted from an instance. The goal is to maintain a near-optimal assignment of the current set of jobs to the m machines. This goal is essentially doomed to failure unless, upon arrival or departure of a job, we allow reassigning some other jobs. Considering that the reassignment of a job induces a cost proportional to its size, the total cost for reassigning jobs must preferably be bounded by a constant r times the total size of added or deleted jobs. The value r is called the reassignment factor of the solution and it is a measure of our willingness to adapt the solution over time.Our main result is that, for any ε > 0, it is possible to achieve (1 + ε)-competitive solutions with constant reassignment factor r(ε). For the minimum makespan problem this is the first improvement on the (2 + ε)-competitive algorithm by Andrews et al. (1999) [Andrews M, Goemans M, Zhang L (1999) Improved bounds for on-line load balancing. Algorithmica 23(4):278–301]. Crucial to our algorithm is a new insight into the structure of robust, almost optimal schedules.
1	In a stochastic probing problem we are given a universe E, and a probability that each element e in E is active. We determine if an element is active by probing it, and whenever a probed element is active, we must permanently include it in our solution. Moreover, throughout the process we need to obey inner constraints on the set of elements taken into the solution, and outer constraints on the set of all probed elements. All previous algorithmic results in this framework have considered only the problem of maximizing a linear function of the active elements. Here, we consider submodular objectives.We provide new, constant-factor approximations for maximizing a monotone submodular function subject to multiple matroid constraints on both the elements that may be taken and the elements that may be probed. We also obtain an improved approximation for linear objective functions, and show how our approach may be generalized to handle k-matchoid constraints.
1	We prove the fundamental theorem of asset pricing for a discrete time financial market where trading is subject to proportional transaction costs and the asset price dynamic is modeled by a family of probability measures, possibly nondominated. Using a backward-forward scheme, we show that when the market consists of a money market account and a single stock, no-arbitrage in a quasi-sure sense is equivalent to the existence of a suitable family of consistent price systems. We also show that when the market consists of multiple dynamically traded assets and satisfies efficient friction, strict no-arbitrage in a quasi-sure sense is equivalent to the existence of a suitable family of strictly consistent price systems.
1	A single queueing station serving K input streams with renewal arrivals and generally distributed independent and identically distributed service times is considered. Customers are served by the Shortest Remaining Processing Time policy. In the case of a tie, the first-in, first-out policy is utilized. We analyze a fluid model for the evolution of a measure-valued state descriptor of this system, with particular emphasis on its limiting behavior in the critical case as time gets large. We also prove a fluid limit theorem justifying our fluid model as the first-order approximation of the queueing system under consideration. Along the way, we establish fluid limits for the corresponding state-dependent response times.
1	This paper introduces a class of contest models in which each player decides when to stop a privately observed Brownian motion with drift and incurs costs depending on his stopping time. The player who stops his process at the highest value wins a prize. We prove existence and uniqueness of a Nash equilibrium outcome and derive the equilibrium distribution in closed form. As the variance tends to zero, the equilibrium outcome converges to the symmetric equilibrium of an all-pay auction. For two players and constant costs, each player’s equilibrium profit decreases if the drift increases, the variance decreases, or the costs decrease.
1	The switching multiple disorder problem seeks to determine an ordered infinite sequence of times of alarms which are as close as possible to the unknown times of disorders, or change-points, at which the observable process changes its probability characteristics. We study a Bayesian formulation of this problem for an observable Brownian motion with switching constant drift rates. The method of proof is based on the reduction of the initial problem to an associated optimal switching problem for a three-dimensional diffusion posterior probability process and the analysis of the equivalent coupled parabolic-type free-boundary problem. We derive analytic-form estimates for the Bayesian risk function and the optimal switching boundaries for the components of the the posterior probability process.
1	We give an efficient algorithm for computing a Cournot equilibrium when the producers are confined to integers, the inverse demand function is linear, and costs are quadratic. The method also establishes existence constructively. We use our characterization to discuss the multiplicity of integer Cournot equilibria and their relationship to the real Cournot equilibrium.
1	We propose a unified approach to establishing diffusion approximations for queues with impatient customers within a general framework of scaling customer patience time. The approach consists of two steps. The first step is to show that the diffusion-scaled abandonment process is asymptotically close to a function of the diffusion-scaled queue length process under appropriate conditions. The second step is to construct a continuous mapping not only to characterize the system dynamics using the system primitives, but also to help verify the conditions needed in the first step. The diffusion approximations can then be obtained by applying the continuous mapping theorem. The approach has two advantages: (i) it provides a unified procedure to establish the diffusion approximations regardless of the structure of the queueing model or the type of patience-time scaling; and (ii) it makes the diffusion analysis of queues with customer abandonment essentially the same as the diffusion analysis of queues without customer abandonment. We demonstrate the application of this approach via the single-server system with Markov-modulated service speeds in the traditional heavy-traffic regime and the many-server system in the Halfin-Whitt regime and the nondegenerate slowdown regime.
1	We study a resource-sharing network where each job requires the concurrent occupancy of a subset of links (servers/resources), and each link’s capacity is shared among job classes that require its service. The real-time allocation of the service capacity among job classes is determined by the so-called “proportional fair” scheme, which allocates the capacity among job classes taking into account the queue lengths and the shadow prices of link capacity. We show that the usual traffic condition is necessary and sufficient for the diffusion limit to have a stationary distribution. We also establish the uniform stability of the prelimit networks, and hence the existence of their stationary distributions. To justify the interchange of two limits, the limit in time and limit in diffusion scaling, we identify a bounded workload condition, and show it is a sufficient condition to justify the interchange for the stationary distributions and their moments. This last result is essential for the validity of the diffusion limit as an approximation to the stationary performance of the original network. We present a set of examples to illustrate justifying the validity of diffusion approximation in resource-sharing networks, and also discuss extensions to other multiclass networks via the well-known Kumar-Seidman/Rybko-Stolyar model.
1	We prove the existence of a pure subgame–perfect epsilon–equilibrium, for every epsilon > 0, in multiplayer perfect information games, provided that the payoff functions are bounded and exhibit common preferences at the limit. If, in addition, the payoff functions have finite range, then there exists a pure subgame–perfect 0–equilibrium. These results extend and unify recent existence theorems for bounded and semicontinuous payoffs.
1	Quasi-open-loop policies consist of sequences of Markovian decision rules that are insensitive to one component of the state space. Given a semi-Markov decision process (SMDP), we distinguish between exogenous and endogenous state components as follows: (i) the decision-maker’s actions do not impact the evolution of an exogenous state component, and (ii) between consecutive decision epochs, the exogenous and endogenous state components are conditionally independent given the decision-maker’s latest action. For simplicity, we consider an SMDP with one exogenous and one endogenous state component. When transition times between epochs are conditionally independent of the exogenous state given the most recent action, and the exogenous component is a multiplicative compound Poisson process, we provide an almost-everywhere condition on the reward function sufficient for the optimality of a quasi-open-loop policy. After adjusting the discount factor to account for the statistical properties of the exogenous state process, obtaining this policy amounts to solving a reduced SMDP in which the exogenous state is static. Depending on the relationship between the structure of the exogenous state process and the shape of the reward function, we can replace the almost-everywhere condition with one that applies only in expectation. Quasi-open-loop optimality holds even if the times between decision epochs depend on the Poisson process underlying the exogenous state component, and/or the Poisson process is replaced with a generic counting process.
1	We study a worst-case approach to measure the sensitivity to model misspecification in the performance analysis of stochastic systems. The situation of interest is when only minimal parametric information is available on the form of the true model. Under this setting, we post optimization programs that compute the worst-case performance measures, subject to constraints on the amount of model misspecification measured by Kullback-Leibler divergence. Our main contribution is the development of infinitesimal approximations for these programs, resulting in asymptotic expansions of their optimal values as the divergence shrinks to zero. The coefficients of these expansions can be computed via simulation, and are mathematically derived from the representation of the worst-case models as changes of measure that satisfy a well-defined class of functional fixed-point equations.
1	This article concerns the average criteria for continuous-time Markov decision processes with N constraints. We show the following; (a) every extreme point of the space of performance vectors corresponding to the set of stable measures is generated by a deterministic stationary policy; and (b) there exists a mixed optimal policy, where the mixture is over no more than N + 1 deterministic stationary policies.
1	We investigate a class of reinforcement learning dynamics where players adjust their strategies based on their actions’ cumulative payoffs over time—specifically, by playing mixed strategies that maximize their expected cumulative payoff minus a regularization term. A widely studied example is exponential reinforcement learning, a process induced by an entropic regularization term which leads mixed strategies to evolve according to the replicator dynamics. However, in contrast to the class of regularization functions used to define smooth best responses in models of stochastic fictitious play, the functions used in this paper need not be infinitely steep at the boundary of the simplex; in fact, dropping this requirement gives rise to an important dichotomy between steep and nonsteep cases. In this general framework, we extend several properties of exponential learning, including the elimination of dominated strategies, the asymptotic stability of strict Nash equilibria, and the convergence of time-averaged trajectories in zero-sum games with an interior Nash equilibrium.
1	An important challenge in Markov decision processes (MDP) is to ensure robustness with respect to unexpected or adversarial system behavior. A standard paradigm to tackle this challenge is the robust MDP framework that models the parameters as arbitrary elements of pre-defined “uncertainty sets,” and seeks the minimax policy—the policy that performs the best under the worst realization of the parameters in the uncertainty set. A crucial issue of the robust MDP framework, largely unaddressed in literature, is how to find appropriate description of the uncertainty in a principled data-driven way. In this paper we address this problem using an online learning approach: we devise an algorithm that, without knowing the true uncertainty model, is able to adapt its level of protection to uncertainty, and in the long run performs as well as the minimax policy as if the true uncertainty model is known. Indeed, the algorithm achieves similar regret bounds as standard MDP where no parameter is adversarial, which shows that with virtually no extra cost we can adapt robust learning to handle uncertainty in MDPs. To the best of our knowledge, this is the first attempt to learn uncertainty in robust MDPs.
1	We study a differential game that governs the moderate-deviation heavy-traffic asymptotics of a multiclass single-server queueing control problem with a risk-sensitive cost. We consider a cost set on a finite but sufficiently large time horizon, and show that this formulation leads to stationary feedback policies for the game. Several aspects of the game are explored, including its characterization via a (one-dimensional) free boundary problem, the semi-explicit solution of an optimal strategy, and the specification of a saddle point. We emphasize the analogy to the well-known Harrison-Taksar free boundary problem which plays a similar role in the diffusion-scale heavy-traffic literature.
1	For an integer linear program, Gomory’s corner relaxation is obtained by ignoring the nonnegativity of the basic variables in a tableau formulation. In this paper, we do not relax these nonnegativity constraints. We generalize a classical result of Gomory and Johnson characterizing minimal cut-generating functions in terms of subadditivity, symmetry, and periodicity. Our result is based on the notion of generalized symmetry condition. We also prove a 2-slope theorem for extreme cut-generating functions in our setting, in the spirit of the 2-slope theorem of Gomory and Johnson.
1	We consider the problem of solving packing/covering LPs online, when the columns of the constraint matrix are presented in random order. This problem has received much attention, and the main focus is to figure out how large the right-hand sides of the LPs have to be (compared to the entries on the left-hand side of the constraints) to allow (1 + ɛ)-approximations online. It is known that the right-hand sides have to be Ω(ɛ−2 log m) times the left-hand sides, where m is the number of constraints.In this paper, we give a primal-dual algorithm that achieves this bound for mixed packing/covering LPs. Our algorithms construct dual solutions using a regret-minimizing online learning algorithm in a black-box fashion, and use them to construct primal solutions. The adversarial guarantee that holds for the constructed duals helps us to take care of most of the correlations that arise in the algorithm; the remaining correlations are handled via martingale concentration and maximal inequalities. These ideas lead to conceptually simple and modular algorithms, which we hope will be useful in other contexts.
1	This paper is concerned with optimal switching over multiple modes in continuous time and on a finite horizon. The performance index includes a running reward, terminal reward, and switching costs that can belong to a large class of stochastic processes. Particularly, the switching costs are modelled by right-continuous with left-limits processes that are quasi-left-continuous and can take positive and negative values. We provide sufficient conditions leading to a well known probabilistic representation of the value function for the switching problem in terms of interconnected Snell envelopes. We also prove the existence of an optimal strategy in a suitable class of admissible controls, defined iteratively in terms of the Snell envelope processes.
1	We prove a central limit theorem for a class of additive processes that arise naturally in the theory of finite horizon Markov decision problems. The main theorem generalizes a classic result of Dobrushin for temporally nonhomogeneous Markov chains, and the principal innovation is that here the summands are permitted to depend on both the current state and a bounded number of future states of the chain. We show through several examples that this added flexibility gives one a direct path to asymptotic normality of the optimal total reward of finite horizon Markov decision problems. The same examples also explain why such results are not easily obtained by alternative Markovian techniques such as enlargement of the state space.
1	We study a simple adaptive model in the framework of an N-player normal form game. The model consists of a repeated game where the players only know their own action space and their own payoff scored at each stage, not those of the other agents. Each player, in order to update her mixed action, computes the average vector payoff she has obtained by using the number of times she has played each pure action. The resulting stochastic process is analyzed via the ODE method from stochastic approximation theory. We are interested in the convergence of the process to rest points of the related continuous dynamics. Results concerning almost sure convergence and convergence with positive probability are obtained and applied to a traffic game. We also provide some examples where convergence occurs with probability zero.
1	Markov decision processes are a common tool for modeling sequential planning problems under uncertainty. In almost all realistic situations, the system model cannot be perfectly known and must be approximated or estimated. Thus, we consider Markov decision processes under parameter uncertainty, which effectively adds a second layer of uncertainty. Most previous studies restrict to the case that uncertainties among different states are uncoupled, which leads to conservative solutions. On the other hand, robust MDPs with general coupled uncertainty sets are known to be computationally intractable. In this paper we make a first attempt at identifying subclasses of coupled uncertainty that are flexible enough to overcome conservativeness yet still lead to tractable problems. We propose a new class of uncertainty sets termed “k-rectangular uncertainty sets”—a geometric concept defined by the cardinality of possible conditional projections of the uncertainty set. The proposed scheme can model several intuitive formulations of coupled uncertainty that naturally arise in practice and leads to tractable formulations via state space augmentation.
1	We investigate the use of risk measures and theories of choice to model risk-averse routing and traffic equilibrium on networks with random travel times. We interpret the postulates of these theories in the context of routing, identifying additive consistency as a plausible condition that allows to reduce risk-averse route choice to a standard shortest path problem. Within the classical theories of choice, we show that the only preferences that satisfy this condition are the ones induced by the entropic risk measures.
1	We prove a Tauberian theorem for nonexpansive operators and apply it to the model of zero-sum stochastic game. Under mild assumptions, we prove that the value of the λ-discounted game converges uniformly when λ goes to zero if and only if the value of the n-stage game converges uniformly when n goes to infinity. This generalizes the Tauberian theorem of Lehrer and Sorin [Lehrer E, Sorin S (1992) A uniform Tauberian theorem in dynamic programming. Math. Oper. Res. 17(2):303–307] to the two-player zero-sum case. We also provide the first example of a stochastic game with public signals on the state and perfect observation of actions, with finite state space, signal sets, and action sets, in which for some initial state known by both players, the value of the λ-discounted game and the value of the n-stage game starting at that initial state converge to distinct limits.
1	The paper concerns the computation of the graphical derivative and the regular (Fréchet) coderivative of the normal-cone mapping related to C2 inequality constraints under very weak qualification conditions. This enables us to provide the graphical derivative and the regular coderivative of the solution map to a class of parameterized generalized equations with the constraint set of the investigated type. On the basis of these results, we finally obtain a characterization of the isolated calmness property of the mentioned solution map and derive strong stationarity conditions for an MPEC with control constraints.
1	We consider the propagation of a contagion process (“epidemic”) on a network and study the problem of dynamically allocating a fixed curing budget to the nodes of the graph, at each time instant. For bounded degree graphs, we provide a lower bound on the expected time to extinction under any such dynamic allocation policy, in terms of a combinatorial quantity that we call the resistance of the set of initially infected nodes, the available budget, and the number of nodes n.Specifically, we consider the case of bounded degree graphs, with the resistance growing linearly in n. We show that if the curing budget is less than a certain multiple of the resistance, then the expected time to extinction grows exponentially with n. As a corollary, if all nodes are initially infected and the CutWidth of the graph grows linearly, while the curing budget is less than a certain multiple of the CutWidth, then the expected time to extinction grows exponentially in n. The combination of the latter with our prior work establishes a fairly sharp phase transition on the expected time to extinction (sublinear versus exponential) based on the relation between the CutWidth and the curing budget.
1	We show that in any n-player, m-action normal-form game, we can obtain an approximate equilibrium by sampling any mixed-action equilibrium a small number of times. We study three types of equilibria: Nash, correlated, and coarse-correlated. For each one we obtain upper and lower bounds on the number of samples required for the empirical distribution over the sampled action profiles to form an approximate equilibrium with probability close to 1.These bounds imply that using a small number of samples we can test whether or not players are playing according to an approximate equilibrium, even in games where n and m are large. In addition, our results substantially improve previously known upper bounds on the support size of approximate equilibria in games with many players. In particular, for the three types of equilibria we show the existence of approximate equilibrium with support-size polylogarithmic in n and m, whereas the previously best-known upper bounds were polynomial in n (Hémon et al. [Hémon S, de Rougemont M, Santha M (2008) Approximate nash equilibria for multi-player games. Algorithmic Game Theory (Springer), 267–278], Germano and Lugosi [Germano F, Lugosi G (2007) Existence of sparsely supported correlated equilibria. Econom. Theory 32(3):575–578], Hart et al. [Hart S, Mas-Colell A, Babichenko Y (2013) Simple Adaptive Strategies: From Regret-Matching to Uncoupled Dynamics, Vol. 4 (World Scientific Publishing Company)]).
1	We introduce a dynamic credit portfolio framework where optimal investment strategies are robust against misspecifications of the reference credit model. The risk-averse investor models his fear of credit risk misspecification by considering a set of plausible alternatives whose expected log likelihood ratios are penalized. We provide an explicit characterization of the optimal robust bond investment strategy, in terms of default state dependent value functions associated with the max-min robust optimization criterion. The value functions can be obtained as the solutions of a recursive system of Hamilton-Jacobi-Bellman (HJB) equations. We show that each HJB equation is equivalent to a suitably truncated equation admitting a unique bounded regular solution. The truncation technique relies on estimates for the solution of the master HJB equation that we establish.
1	The ℓ0-minimization problem that seeks the sparsest point of a polyhedral set is a long-standing, challenging problem in the fields of signal and image processing, numerical linear algebra, and mathematical optimization. The weighted ℓ1-method is one of the most plausible methods for solving this problem. In this paper we develop a new weighted ℓ1-method through the strict complementarity theory of linear programs. More specifically, we show that locating the sparsest point of a polyhedral set can be achieved by seeking the densest possible slack variable of the dual problem of weighted ℓ1-minimization. As a result, ℓ0-minimization can be transformed, in theory, to ℓ0-maximization in dual space through some weight. This theoretical result provides a basis and an incentive to develop a new weighted ℓ1-algorithm, which is remarkably distinct from existing sparsity-seeking methods. The weight used in our algorithm is computed via a certain convex optimization instead of being determined locally at an iterate. The guaranteed performance of this algorithm is shown under some conditions, and the numerical performance of the algorithm has been demonstrated by empirical simulations.
1	This paper is concerned with so-called generic properties of general linear conic programs. Many results have been obtained on this subject during the last two decades. For example, it is known that uniqueness, strict complementarity, and nondegeneracy of optimal solutions hold for almost all problem instances. Strong duality holds generically in a stronger sense, i.e., it holds for a generic subset of problem instances.In this paper, we survey known results and present new ones. In particular we give an easy proof of the fact that Slater’s condition holds generically in linear conic programming. We further discuss the problem of stability of uniqueness, nondegeneracy, and strict complementarity. We also comment on the fact that in general, a conic program cannot be treated as a smooth problem and that techniques from nonsmooth geometric measure theory are needed.
1	Motivated by a class of applied problems arising from physical layer based security in a digital communication system, in particular, by a secrecy sum-rate maximization problem, this paper studies a nonsmooth, difference-of-convex (dc) minimization problem. The contributions of this paper are (i) clarify several kinds of stationary solutions and their relations; (ii) develop and establish the convergence of a novel algorithm for computing a d-stationary solution of a problem with a convex feasible set that is arguably the sharpest kind among the various stationary solutions; (iii) extend the algorithm in several directions including a randomized choice of the subproblems that could help the practical convergence of the algorithm, a distributed penalty approach for problems whose objective functions are sums of dc functions, and problems with a specially structured (nonconvex) dc constraint. For the latter class of problems, a pointwise Slater constraint qualification is introduced that facilitates the verification and computation of a B(ouligand)-stationary point.
1	The mathematical equivalence between linear scalarizations in multiobjective programming and expected-value functions in stochastic optimization suggests to investigate and establish further conceptual analogies between these two areas. In this paper, we focus on the notion of proper efficiency that allows us to provide a first comprehensive analysis of solution and scenario tradeoffs in stochastic optimization. In generalization of two standard characterizations of properly efficient solutions using weighted sums and augmented weighted Tchebycheff norms for finitely many criteria, we show that these results are generally false for infinitely many criteria. In particular, these observations motivate a slightly modified definition to prove that expected-value optimization over continuous random variables still yields bounded tradeoffs almost everywhere in general. Further consequences and practical implications of these results for decision-making under uncertainty and its related theory and methodology of multiple criteria, stochastic and robust optimization are discussed.
1	The Lasserre/Sum-of-Squares (SoS) hierarchy is a systematic procedure for constructing a sequence of increasingly tight semidefinite relaxations. It is known that the hierarchy converges to the 0/1 polytope in n levels and captures the convex relaxations used in the best available approximation algorithms for a wide variety of optimization problems. In this paper we characterize the set of 0/1 integer linear problems and unconstrained 0/1 polynomial optimization problems that can still have an integrality gap at level n − 1. These problems are the hardest for the Lasserre hierarchy in this sense.
1	Interdiction problems ask about the worst-case impact of a limited change to an underlying optimization problem. They are a natural way to measure the robustness of a system or to identify its weakest spots. Interdiction problems have been studied for a wide variety of classical combinatorial optimization problems. Most interdiction problems are NP-hard, and furthermore, even designing efficient approximation algorithms that allow for estimating the order of magnitude of a worst-case impact has turned out to be very difficult. Not very surprisingly, the few known approximation algorithms are heavily tailored for specific problems.Inspired by previous approaches to network flow interdiction we suggest a general method to obtain pseudoapproximations for many interdiction problems. More precisely, for any α > 0, our algorithm will return either a (1 + α)-approximation or a solution that may overrun the interdiction budget by a factor of at most 1 + 1/α but is also at least as good as the optimal solution that respects the budget. Furthermore, our approach can handle submodular interdiction costs when the underlying problem is to find a maximum weight independent set in a matroid. Additionally, our approach can sometimes be refined by exploiting additional structural properties of the underlying optimization problem to obtain stronger results. We demonstrate this by presenting a polynomial-time approximation scheme for interdicting b-stable sets in bipartite graphs.
1	In the Anscombe-Aumann setup, we provide conditions for a collection of observations to be consistent with a well-known class of smooth ambiguity preferences (Klibanoff P, Marinacci M, Mukerji S (2005) A smooth model of decision making under ambiguity. Econometrica 73(6):1849–1892.). Each observation is assumed to take the form of an equivalence between an uncertain act and a certain outcome. We provide three results that describe these conditions for data sets of different cardinality. Our findings uncover surprising links between the smooth ambiguity model and classic mathematical results in complex and functional analysis.
1	A strongly polynomial algorithm is given for the generalized flow maximization problem. It uses a new variant of the scaling technique called continuous scaling. The main measure of progress is that within a strongly polynomial number of steps, an arc can be identified that must be tight in every dual optimal solution and thus can be contracted. As a consequence of the result, we also obtain a strongly polynomial algorithm for the linear feasibility problem with at most two nonzero entries per column in the constraint matrix.
1	A new approach to solve the continuous-time stochastic inventory problem using the fluctuation theory of Lévy processes is developed. This approach involves the recent developments of the scale function that is capable of expressing many fluctuation identities of spectrally one-sided Lévy processes. For the case with a fixed cost and a general spectrally positive Lévy demand process, we show the optimality of an (s, S)-policy. The optimal policy and the value function are concisely expressed via the scale function. Numerical examples under a Lévy process in the β-family with jumps of infinite activity are provided to confirm the analytical results. Furthermore, the case with no fixed ordering costs is studied.
1	Classified stable matching, proposed by Huang, describes a matching model between academic institutes and applicants, in which each institute has upper and lower quotas on classes, i.e., subsets of applicants. Huang showed that the problem to decide whether there exists a stable matching or not is NP-hard in general. On the other hand, he showed that the problem is solvable if classes form a laminar family. For this case, Fleiner and Kamiyama gave a concise interpretation in terms of matroids and showed the lattice structure of stable matchings.In this paper we introduce stable matchings on generalized matroids, extending the model of Fleiner and Kamiyama. We design a polynomial-time algorithm which finds a stable matching or reports the nonexistence. We also show that the set of stable matchings, if nonempty, forms a lattice with several significant properties. Furthermore, we extend this structural result to the polyhedral framework, which we call stable allocations on generalized polymatroids.
1	We develop the first algorithmic approach to compute provably good ordering policies for a multi-echelon, stochastic inventory system facing correlated, nonstationary and evolving demands over a finite horizon. Specifically, we study the serial system. Our approach is computationally efficient and provides worst-case guarantees. That is, the expected cost of the algorithms is guaranteed to be within a constant factor of the optimal expected cost; depending on the assumption the constant varies between two and three. Our algorithmic approach is based on an innovative scheme to account for costs in a multi-echelon, multi-period environment, as well as repeatedly balancing between opposing cost. The cost-accounting scheme, called a cause-effect cost-accounting scheme, is significantly different from traditional cost-accounting schemes in that it reallocates costs with the goal of assigning every unit of cost to the decision that caused the cost to be incurred. We believe it will have additional applications in other multi-echelon inventory models.
1	We consider a dynamic pricing problem in which a seller faces an unknown demand model that can change over time. The amount of change over a time horizon of T periods is measured using a variation metric that allows for a broad spectrum of temporal behavior. Given a finite variation “budget,” we first derive a lower bound on the expected performance gap between any pricing policy and a clairvoyant who knows a priori the temporal evolution of the underlying demand model, and then we design families of near-optimal pricing policies, the revenue performance of which asymptotically matches said lower bound. We also show that the seller can achieve a substantially better revenue performance in demand environments that change in “bursts” than in demand environments that change “smoothly,” among other things quantifying the net effect of the “volatility” in the demand environment on the seller’s revenue performance.
1	Fast algorithms for submodular maximization problems have a vast potential use in applicative settings, such as machine learning, social networks, and economics. Though fast algorithms were known for some special cases, only recently such algorithms were considered in the general case of maximizing a monotone submodular function subject to a matroid independence constraint. The known fast algorithm matches the best possible approximation guarantee, while trying to reduce the number of value oracle queries the algorithm performs.Our main result is a new algorithm for this general case that establishes a surprising trade-off between two seemingly unrelated quantities: the number of value oracle queries and the number of matroid independence queries performed by the algorithm. Specifically, one can decrease the former by increasing the latter, and vice versa, while maintaining the best possible approximation guarantee. Such a trade-off is very useful since various applications might incur significantly different costs in querying the value and matroid independence oracles. Furthermore, in case the rank of the matroid is O(nc), where n is the size of the ground set and c is an absolute constant smaller than 1, the total number of oracle queries our algorithm uses can be made to have a smaller magnitude compared to that needed by the current best known algorithm. We also provide even faster algorithms for the well-studied special cases of a cardinality constraint and a partition matroid independence constraint, both of which capture many real-world applications and have been widely studied both theoretically and in practice.
1	The proximal gradient and its variants is one of the most attractive first-order algorithm for minimizing the sum of two convex functions, with one being nonsmooth. However, it requires the differentiable part of the objective to have a Lipschitz continuous gradient, thus precluding its use in many applications. In this paper we introduce a framework which allows to circumvent the intricate question of Lipschitz continuity of gradients by using an elegant and easy to check convexity condition which captures the geometry of the constraints. This condition translates into a new descent lemma which in turn leads to a natural derivation of the proximal-gradient scheme with Bregman distances. We then identify a new notion of asymmetry measure for Bregman distances, which is central in determining the relevant step-size. These novelties allow to prove a global sublinear rate of convergence, and as a by-product, global pointwise convergence is obtained. This provides a new path to a broad spectrum of problems arising in key applications which were, until now, considered as out of reach via proximal gradient methods. We illustrate this potential by showing how our results can be applied to build new and simple schemes for Poisson inverse problems.
1	We study long-term Markov decision processes (MDPs) and gambling houses, with applications to any partial observation MDPs with finitely many states and zero-sum repeated games with an informed controller. We consider a decision maker who is maximizing the weighted sum ∑t ≥ 1 𝜃trt, where rt is the expected reward of the t-th stage. We prove the existence of a very strong notion of long-term value called general uniform value, representing the fact that the decision maker can play well independently of the evaluations (𝜃t)t ≥ 1 over stages, provided the total variation (or impatience) ∑t≥1|𝜃t+1−𝜃t|∑𝑡≥1∣∣𝜃𝑡+1−𝜃𝑡∣∣ is small enough. This result generalizes previous results of the literature that focus on arithmetic means and discounted evaluations. Moreover, we give a variational characterization of the general uniform value via the introduction of appropriate invariant measures for the decision problems, generalizing the fundamental theorem of gambling or the Aumann–Maschler cav(u) formula for repeated games with incomplete information. Apart the introduction of appropriate invariant measures, the main innovation in our proofs is the introduction of a new metric, d*, such that partial observation MDPs and repeated games with an informed controller may be associated with auxiliary problems that are nonexpansive with respect to d*. Given two Borel probabilities over a compact subset X of a normed vector space, we define d∗(u,v)=supf∈D1|u(f)−v(f)|𝑑∗(𝑢,𝑣)=sup𝑓∈𝐷1∣∣𝑢(𝑓)−𝑣(𝑓)∣∣, where D1 is the set of functions satisfying ∀ x, y ∈ X, ∀ a, b ≥ 0, af(x) − bf(y) ≤ ‖ax − by‖. The particular case where X is a simplex endowed with the L1-norm is particularly interesting: d* is the largest distance over the probabilities with finite support over X, which makes every disintegration nonexpansive. Moreover, we obtain a Kantorovich–Rubinstein-type duality formula for d*(u, v), involving couples of measures (α, β) over X × X such that the first marginal of α is u and the second marginal of β is v.
1	An instance of the tollbooth problem consists of an undirected network and a collection of single-minded customers, each of which is interested in purchasing a fixed path subject to an individual budget constraint. The objective is to assign a per-unit price to each edge in a way that maximizes the collective revenue obtained from all customers. The revenue generated by any customer is equal to the overall price of the edges in her desired path, when this cost falls within her budget; otherwise, that customer will not purchase any edge.Our main result is a deterministic algorithm for the tollbooth problem on trees whose approximation ratio is O(log m/log log m), where m denotes the number of edges in the underlying graph. This finding improves on the currently best performance guarantees for trees, and up until recently, also on the best ratio for paths (commonly known as the highway problem). An additional interesting consequence is a computational separation between tollbooth pricing on trees and the original prototype problem of single-minded unlimited supply pricing, under a plausible hardness hypothesis.
1	We study the polyhedral convex hull of a mixed-integer set 𝒮 defined by a collection of multilinear equations over the unit hypercube. Such sets appear frequently in the factorable reformulation of mixed-integer nonlinear optimization problems. In particular, the set 𝒮 represents the feasible region of a linearized unconstrained binary polynomial optimization problem. We define an equivalent hypergraph representation of the mixed-integer set 𝒮, which enables us to derive several families of facet-defining inequalities, structural properties, and lifting operations for its convex hull in the space of the original variables. Our theoretical developments extend several well-known results from the Boolean quadric polytope and the cut polytope literature, paving a way for devising novel optimization algorithms for nonconvex problems containing multilinear sub-expressions.
1	We revisit the classical maximum weight matching problem in general graphs with nonnegative integral edge weights. We present an algorithm that operates by decomposing the problem into W unweighted versions of the problem, where W is the largest edge weight. Our algorithm has running time as good as the current fastest algorithms for the maximum weight matching problem when W is small. One of the highlights of our algorithm is that it also produces an integral optimal dual solution; thus our algorithm also returns an integral certificate corresponding to the maximum weight matching that was computed.Our algorithm yields a new proof to the total dual integrality of Edmonds’ matching polytope and it also gives rise to a decomposition theorem for the maximum weight of a matching in terms of the maximum size of a matching in certain subgraphs. We also consider the maximum weight capacitated b-matching problem in bipartite graphs with nonnegative integral edge weights and show that it can also be decomposed into W unweighted versions of the problem, where W is the largest edge weight. Our second algorithm is competitive with known algorithms when W is small.
1	We consider n-player perfect information games with payoff functions having a finite image. We do not make any further assumptions, so in particular we refrain from making assumptions on the cardinality or the topology of the set of actions and assumptions like continuity or measurability of payoff functions. We show that there exists a best response cycle of length four, that is, a sequence of four pure strategy profiles where every successive element is a best response to the previous one. This result implies the existence of point-rationalizable strategy profiles. When payoffs are only required to be bounded, we show the existence of an ε-best response cycle of length four for every ε > 0.
1	This paper is concerned with the problem of locating a facility on a line in the presence of strategic agents, also located on that line. Each agent incurs a cost equal to her distance to the facility whereas the planner wishes to minimize the Lp norm of the vector of agent costs. The location of each agent is only privately known, and the goal is to design a strategyproof mechanism that approximates the optimal cost well. It is shown that the median mechanism provides a 21−1/p approximation ratio, and that this is the optimal approximation ratio among all deterministic strategyproof mechanisms. For randomized mechanisms, two results are shown: First, for any integer p larger than 2, no mechanism—from a rather large class of randomized mechanisms—has an approximation ratio better than that of the median mechanism. This is in contrast to the case of p = 2 and p = ∞ where a randomized mechanism provably helps improve the worst case approximation ratio. Second, for the case of 2 agents, the Left-Right-Middle (LRM) mechanism, first designed by Procaccia and Tennenholtz for the special case of infinity norm, provides the optimal approximation ratio among all randomized mechanisms.
1	Convex duality for two different super-replication problems in a continuous time financial market with proportional transaction cost is proved. In this market, static hedging in a finite number of options, in addition to usual dynamic hedging with the underlying stock, are allowed. The first one of the problems considered is the model-independent hedging that requires the super-replication to hold for every continuous path. In the second one the market model is given through a probability measure ℙ and the inequalities are understood the probability measure almost surely. The main result, using the convex duality, proves that the two super-replication problems have the same value provided that the probability measure satisfies the conditional full support property. Hence, the transaction costs prevents one from using the structure of a specific model to reduce the super-replication cost.
1	Given a polytope P ⊂ ℝn, we say that P has a positive semidefinite lift (psd lift) of size d if one can express P as the projection of an affine slice of the d × d positive semidefinite cone. Such a representation allows us to solve linear optimization problems over P using a semidefinite program of size d and can be useful in practice when d is much smaller than the number of facets of P. If a polytope P has symmetry, we can consider equivariant psd lifts, i.e., those psd lifts that respect the symmetries of P. One of the simplest families of polytopes with interesting symmetries is regular polygons in the plane. In this paper, we give tight lower and upper bounds on the size of equivariant psd lifts for regular polygons. We give an explicit construction of an equivariant psd lift of the regular 2n-gon of size 2n − 1, and we prove that our construction is essentially optimal by proving a lower bound on the size of any equivariant psd lift of the regular N-gon that is logarithmic in N. Our construction is exponentially smaller than the (equivariant) psd lift obtained from the Lasserre/sum-of-squares hierarchy, and it also gives the first example of a polytope with an exponential gap between equivariant psd lifts and equivariant linear programming lifts.
1	The Carathéodory, Helly, and Radon numbers are three main invariants in convexity theory. These invariants have been determined, exactly or approximately, for a number of different convexity structures. We consider convexity structures defined by the sublattices and by the convex sublattices of finite-dimensional Euclidian, integer, and Boolean spaces. Such sublattices arise in submodular optimization (lattice programming) and in monotone comparative statics of optimization and fixed point problems. We also consider integral L-natural convexities, induced by dual network flow constraint systems. We determine the exact Carathéodory, Helly, and Radon numbers of most of these convexities, and very close upper and lower bounds for the other Carathéodory numbers. Our results imply, for example, that if a set can be obtained with unions and intersections from a given family of subsets of a finite set then it can be obtained with unions and intersections from a small subfamily. We also show that finding the Carathéodory number of integral L-natural convexities reduces to an extremal problem in the theory of permutations, solved in a companion paper. We leave as open problems the determination of the Helly and Radon numbers of the integer convex sublattice convexity.
1	The present paper studies the optimal placement problem of a child order. In practice, short-term momentum indicators inferred from order book data play important roles in order placement decisions. In the present work, we first propose to explicitly model the short-term momentum indicator and then formulate the order placement problem as an optimal multiple stopping problem. In contrast to the common formulation in the existing literature, we allow zero refracting period between consecutive stopping times to take into account the common practice of submitting multiple orders at the same time. This optimal multiple stopping problem will be explored over both infinite and finite horizons. It is shown that the optimal placement of a child order and the optimal replacement of outstanding limit orders by market ones are determined by first passage times of the short-term momentum indicator across a sequence of time-dependent boundaries. The aggressiveness of the optimal order replacement strategy is also examined via several numerical examples. In particular, our work illustrates that the optimal order replacement strategy is more aggressive when the bid–ask spread is smaller, when the impact from the momentum indicator is larger, or when the remaining time becomes shorter. All of these decision-making behaviors predicted by our model are natural and agree with empirical studies in the existing literature.
1	We consider the portfolio decision problem of a risky investor. The investor borrows at a rate higher than his lending rate and invests in a risky bond whose market price is correlated with the credit quality of the investor. By viewing the concave drift of the wealth process as a continuous function of the admissible control, we characterize the optimal strategy in terms of a relation between a critical borrowing threshold and solutions of a system of first-order conditions. We analyze the nonlinear dynamic programming equation and prove the singular growth of its coefficients. Using a truncation technique relying on the locally Lipschitz continuity of the optimal strategy, we remove the singularity and show the existence and uniqueness of a global regular solution. Our explicit characterization of the strategy has direct financial implications: it indicates that the investor purchases a high number of bond shares when his borrowing costs are low and the bond sufficiently safe, and reduces the size of his long position or even sells short when his financing costs are high or the bond very risky.
1	This paper studies the problem of finding a stationary strong present-value optimal and, more generally, an n-present-value optimal, policy for an infinite-horizon stationary finite-state-action substochastic Markov decision chain. A policy is strong present-value optimal if it has maximum present value for all small positive interest rates ρ. A policy is n-present-value optimal if its present value falls below the maximum present value for all small positive ρ by O(ρn+1). The importance of stationary n-present-value optimal policies arises from the following known facts. The set of such policies diminishes with n and, for n ≥ S, is precisely the set of stationary strong present-value optimal policies. For 0 ≤ n < S, stationary n-present-value optimal policies are nearly strong present-value optimal and are of independent interest. The best algorithms for finding stationary strong present-value optimal policies find stationary n-present-value policies for n = −1,…,S in that order. This paper decomposes the problem of finding a stationary n-present-value optimal policy given a stationary (n − 1)-present-value optimal policy into a sequence of three subproblems, each entailing either maximizing transient value or reward rate. It is known that both subproblem types can be represented as linear programs and solved in polynomial time, e.g., by interior-point and ellipsoid methods. This paper also establishes the following results. The size of the input to each subproblem is polynomial in the size of the original problem data. A stationary strong present-value (respectively, n-present-value) optimal policy can be found in polynomial time. For the case of unique-transition systems, i.e., each action in a state sends the system to at most one state, a stationary strong present-value (respectively, n-present-value) optimal policy can be found in strongly polynomial time using combinatorial methods on the subproblems. The last case includes standard deterministic dynamic programs.
1	We analyze a continuous-time stochastic control problem that arises in the study of several important issues in financial economics. An agent controls the drift and volatility of a diffusion output process by dynamically selecting one of an arbitrary (but finite) number of projects and the termination time. The optimal policy depends on the projects’ risk-adjusted drifts that are determined by their drifts, volatilities, and the curvature (or relative risk aversion) of the agent’s payoff function. We prove that the optimal policy only selects projects in the spanning subset. Furthermore, if the projects’ risk-adjusted drifts are consistently ordered for all output values, then the optimal policy is characterized by at most K − 1 switching triggers, where K is the number of projects in the spanning subset. We also characterize the optimal policy when the consistent ordering condition does not hold, and we outline a general and tractable computational algorithm to derive the optimal policies.
1	A multiclass queue with many servers is considered, where customers make a join-or-leave decision upon arrival based on queue length information, without knowing the state of other queues. A game theoretic formulation is proposed and analyzed, that takes advantage of a phenomenon unique to heavy traffic regimes, namely, Reiman’s snaphshot principle, by which waiting times are predicted with high precision by the information available upon arrival. The payoff considered is given as a random variable, which depends on the customer’s decision, accounting for waiting time in the queue and penalty for leaving. The notion of an equilibrium is only meaningful in an asymptotic framework, which is taken here to be the Halfin-Whitt heavy traffic regime. The main result is the identification of an ɛ-Nash equilibrium with probability approaching 1. On the way to proving this result, new diffusion limit results for systems with finite buffers are obtained.
1	In this paper, the stability theorem of Borkar and Meyn is extended to include the case when the mean field is a set-valued map. Two different sets of sufficient conditions are presented that guarantee the “stability and convergence” of stochastic recursive inclusions. Our work builds on the works of Benaïm, Hofbauer and Sorin as well as Borkar and Meyn. As a corollary to one of the main theorems, a natural generalization of the Borkar and Meyn theorem follows. In addition, the original theorem of Borkar and Meyn is shown to hold under slightly relaxed assumptions. As an application to one of the main theorems, we discuss a solution to the “approximate drift problem.” Finally, we analyze the stochastic gradient algorithm with “constant-error gradient estimators” as yet another application of our main result.
1	Recently, in He et al. [He BS, Tao M, Yuan XM (2012) Alternating direction method with Gaussian back substitution for separable convex programming. SIAM J. Optim. 22(2):313–340], we have showed the first possibility of combining the Douglas-Rachford alternating direction method of multipliers (ADMM) with a Gaussian back substitution procedure for solving a convex minimization model with a general separable structure. This paper is a further study on this theme. We first derive a general algorithmic framework to combine ADMM with either a forward or backward substitution procedure. Then, we show that convergence of this framework can be easily proved from the contraction perspective, and its local linear convergence rate is provable if certain error bound condition is assumed. Without such an error bound assumption, we can estimate its worst-case convergence rate measured by the iteration complexity.
1	In many computing and networking applications, arriving tasks have to be routed to one of many servers, with the goal of minimizing queueing delays. When the number of processors is very large, a popular routing algorithm works as follows: select two servers at random and route an arriving task to the least loaded of the two. It is well known that this algorithm dramatically reduces queueing delays compared to an algorithm, which routes to a single randomly selected server. In recent cloud computing applications, it has been observed that even sampling two queues per arriving task can be expensive and can even increase delays due to messaging overhead. So there is an interest in reducing the number of sampled queues per arriving task. In this paper, we show that the number of sampled queues can be dramatically reduced by using the fact that tasks arrive in batches (called jobs). In particular, we sample a subset of the queues such that the size of the subset is slightly larger than the batch size (thus, on average, we only sample slightly more than one queue per task). Once a random subset of the queues is sampled, we propose a new load-balancing method called batch-filling to attempt to equalize the load among the sampled servers. We show that, asymptotically, our algorithm dramatically reduces the sample complexity compared to previously proposed algorithms.
1	Ever since Tassiulas and Ephremides in 1992 proposed the maximum weight scheduling algorithm of throughput optimality for constrained queueing networks that arise in the context of communication networks, extensive efforts have been devoted to resolving its most important drawback: high complexity. This paper proposes a generic framework for designing throughput-optimal and low-complexity scheduling algorithms for constrained queueing networks. Under our framework, a scheduling algorithm updates current schedules by interacting with a given oracle system that generates an approximate solution to a related optimization task. One can use our framework to design a variety of scheduling algorithms by choosing an oracle system such as random 7524 Markov chain, belief propagation, and primal-dual methods. The complexity of the resulting scheduling algorithm is determined by the number of operations required for an oracle to process a single query, which is typically small. We provide sufficient conditions for throughput optimality of the scheduling algorithm, in general, constrained queueing network models. The linear time algorithm of Tassiulas in 1998 and the random access algorithm of Shah and Shin in 2012 correspond to special cases of our framework using random search and Markov chain oracles, respectively. Our generic framework, however, provides a unified proof with milder assumptions.
1	The famous Braess paradox describes the counterintuitive phenomenon in which, in certain settings, an increase of resources, such as a new road built within a congested network, may in fact lead to larger costs for the players in an equilibrium. In this paper, we consider general nonatomic congestion games and give a characterization of the combinatorial property of strategy spaces for which the Braess paradox does not occur. In short, matroid bases are precisely the required structure. We prove this characterization by two novel sensitivity results for convex separable optimization problems over polymatroid base polyhedra, which may be of independent interest.
1	We consider the problem of reinforcement learning over episodes of a finite-horizon deterministic system and as a solution propose optimistic constraint propagation (OCP), an algorithm designed to synthesize efficient exploration and value function generalization. We establish that when the true value function lies within a given hypothesis class, OCP selects optimal actions over all but at most D episodes, where D is the eluder dimension of the given hypothesis class. We establish further efficiency and asymptotic performance guarantees that apply even if the true value function does not lie in the given hypothesis class, for the special case where the hypothesis class is the span of prespecified indicator functions over disjoint sets. We also discuss the computational complexity of OCP and present computational results involving two illustrative examples.
1	In this paper, we provide a comprehensive convergence rate analysis of the Douglas-Rachford splitting (DRS), Peaceman-Rachford splitting (PRS), and alternating direction method of multipliers (ADMM) algorithms under various regularity assumptions including strong convexity, Lipschitz differentiability, and bounded linear regularity. The main consequence of this work is that relaxed PRS and ADMM automatically adapt to the regularity of the problem and achieve convergence rates that improve upon the (tight) worst-case rates that hold in the absence of such regularity. All of the results are obtained using simple techniques.
1	In this paper we study optimal stopping problems with respect to distorted expectations with concave distortion functions. Our starting point is a seminal work of Xu and Zhou in 2013, who gave an explicit solution of such a stopping problem under a rather large class of distortion functionals. In this paper, we continue this line of research and prove a novel representation, which relates the solution of an optimal stopping problem under distorted expectation to the sequence of standard optimal stopping problems and hence makes the application of the standard dynamic programming-based approaches possible. Furthermore, by means of the well-known Kusuoka representation, we extend our results to optimal stopping under general law invariant coherent risk measures. Finally, based on our representations, we develop several Monte Carlo approximation algorithms and illustrate their power for optimal stopping under absolute semideviation risk measures.
1	We provide a monotone nonincreasing sequence of upper bounds 𝑓𝐻𝑘(𝑘≥1) converging to the global minimum of a polynomial f on simple sets like the unit hypercube in ℝn. The novelty with respect to the converging sequence of upper bounds in Lasserre [Lasserre JB (2010) A new look at nonnegativity on closed sets and polynomial optimization, SIAM J. Optim. 21:864–885] is that only elementary computations are required. For optimization over the hypercube [0, 1]n, we show that the new bounds 𝑓𝐻𝑘 have a rate of convergence in 𝑂(1/𝑘‾‾√). Moreover, we show a stronger convergence rate in O(1/k) for quadratic polynomials and more generally for polynomials having a rational minimizer in the hypercube. In comparison, evaluation of all rational grid points with denominator k produces bounds with a rate of convergence in O(1/k2), but at the cost of O(kn) function evaluations, while the new bound 𝑓𝐻𝑘 needs only O(nk) elementary calculations.
1	We consider a well-studied multi-echelon (deterministic) inventory control problem, known in the literature as the one-warehouse multi-retailer (OWMR) problem. We propose a simple and fast 2-approximation algorithm for this NP-hard problem, by recombining the solutions of single-echelon relaxations at the warehouse and at the retailers. We then show that our approach remains valid under quite general assumptions on the cost structures and under capacity constraints at some retailers. In particular, we present the first approximation algorithms for the OWMR problem with nonlinear holding costs, truckload discount on procurement costs, or with capacity constraints at some retailers. In all cases, the procedure is purely combinatorial and can be implemented to run in low polynomial time.
1	We consider the problem of constructing optimal decision trees: given a collection of tests that can disambiguate between a set of m possible diseases, each test having a cost, and the a priori likelihood of any particular disease, what is a good adaptive strategy to perform these tests to minimize the expected cost to identify the disease? This problem has been studied in several works, with O(log m)-approximations known in the special cases when either costs or probabilities are uniform. In this paper, we settle the approximability of the general problem by giving a tight O(log m)-approximation algorithm.We also consider a substantial generalization, the adaptive traveling salesman problem. Given an underlying metric space, a random subset S of vertices is drawn from a known distribution, but S is initially unknown—we get information about whether any vertex is in S only when it is visited. What is a good adaptive strategy to visit all vertices in the random subset S while minimizing the expected distance traveled? This problem has applications in routing message ferries in ad hoc networks and also models switching costs between tests in the optimal decision tree problem. We give a polylogarithmic approximation algorithm for adaptive TSP, which is nearly best possible due to a connection to the well-known group Steiner tree problem. Finally, we consider the related adaptive traveling repairman problem, where the goal is to compute an adaptive tour minimizing the expected sum of arrival times of vertices in the random subset S; we obtain a polylogarithmic approximation algorithm for this problem as well.
1	Deferred-acceptance auctions are mechanisms whose allocation rule can be implemented using an adaptive reverse greedy algorithm. Milgrom and Segal recently introduced these auctions and proved that they satisfy remarkable incentive guarantees: in addition to being dominant strategy and incentive compatible, they are weakly group-strategyproof and can be implemented by ascending-clock auctions. Neither forward greedy mechanisms nor the VCG mechanism generally possess any of these additional incentive properties. The goal of this paper is to initiate the study of deferred-acceptance auctions from an approximation standpoint. We study what fraction of the optimal social welfare can be guaranteed by these auctions in two canonical problems, knapsack auctions and combinatorial auctions with single-minded bidders. For knapsack auctions, we prove a separation between deferred-acceptance auctions and arbitrary dominant-strategy incentive-compatible mechanisms. For combinatorial auctions with single-minded bidders, we design novel polynomial-time mechanisms that achieve the best of both worlds: the incentive guarantees of a deferred-acceptance auction, and approximation guarantees close to the best possible.
1	In this paper we extend the framework of the evolutionary inspection game put forward recently by the author and coworkers to a large class of conflict interactions to address the pressure executed by the major player (or principal) on the large group of small players who can resist this pressure or collaborate with the major player. We prove rigorous results on the convergence of various Markov decision models of interacting small agents (including evolutionary growth), i.e., pairwise, in groups and by coalition formation, to a deterministic evolution on the distributions of the state spaces of small players paying main attention to situations with an infinite state-space of small players. We supply precise rates of convergence. The theoretical results of the paper are applied to the analysis of the processes of inspection, corruption, cyber-security, counter-terrorism, banks and firms merging, strategically enhanced preferential attachment, and many other.
1	Calculating optimal policies is known to be computationally difficult for Markov decision processes (MDPs) with Borel state and action spaces. This paper studies finite-state approximations of discrete time Markov decision processes with Borel state and action spaces, for both discounted and average costs criteria. The stationary policies thus obtained are shown to approximate the optimal stationary policy with arbitrary precision under quite general conditions for discounted cost and more restrictive conditions for average cost. For compact-state MDPs, we obtain explicit rate of convergence bounds quantifying how the approximation improves as the size of the approximating finite state space increases. Using information theoretic arguments, the order optimality of the obtained convergence rates is established for a large class of problems. We also show that as a pre-processing step, the action space can also be finitely approximated with sufficiently large number points; thereby, well known algorithms, such as value or policy iteration, Q-learning, etc., can be used to calculate near optimal policies.
1	We consider a continuous-review inventory system in which the setup cost of each order is a general function of the order quantity and the demand process is modeled as a Brownian motion with a positive drift. Assuming the holding and shortage cost to be a convex function of the inventory level, we obtain the optimal ordering policy that minimizes the long-run average cost by a lower bound approach. To tackle some technical issues in the lower bound approach under the quantity-dependent setup cost assumption, we establish a comparison theorem that enables one to prove the global optimality of a policy by examining a tractable subset of admissible policies. Since the smooth pasting technique does not apply to our Brownian inventory model, we also propose a selection procedure for computing optimal policy parameters when the setup cost is a step function.
1	Often in applications such as rare events estimation or optimal control it is required that one calculates the principal eigenfunction and eigenvalue of a nonnegative integral kernel. Except in the finite-dimensional case, usually neither the principal eigenfunction nor the eigenvalue can be computed exactly. In this paper, we develop numerical approximations for these quantities. We show how a generic interacting particle algorithm can be used to deliver numerical approximations of the eigenquantities and the associated so-called “twisted” Markov kernel as well as how these approximations are relevant to the aforementioned applications. In addition, we study a collection of random integral operators underlying the algorithm, address some of their mean and pathwise properties, and obtain error estimates. Finally, numerical examples are provided in the context of importance sampling for computing tail probabilities of Markov chains and computing value functions for a class of stochastic optimal control problems.
1	Lattice-free sets and their applications for cutting-plane methods in mixed-integer optimization have been studied in recent literature. The family of all integral lattice-free polyhedra that are not properly contained in another integral lattice-free polyhedron has been of particular interest. We call these polyhedra ℤd-maximal.For fixed d, the family of ℤd-maximal integral lattice-free polyhedra is finite up to unimodular equivalence. In view of possible applications in cutting-plane theory, one would like to have a classification of this family. This is a challenging task already for small dimensions.In contrast, the subfamily of all integral lattice-free polyhedra that are not properly contained in any other lattice-free set, which we call ℝd-maximal lattice-free polyhedra, allow a rather simple geometric characterization. Hence, the question was raised for which dimensions the notions of ℤd-maximality and ℝd-maximality are equivalent. This was known to be the case for dimensions one and two. On the other hand, for d ≥ 4 there exist integral lattice-free polyhedra that are ℤd-maximal but not ℝd-maximal. We consider the remaining case d = 3 and prove that for integral lattice-free polyhedra the notions of ℝ3-maximality and ℤ3-maximality are equivalent. This allows to complete the classification of all ℤ3-maximal integral lattice-free polyhedra.
1	In this paper, we consider a class of constrained optimization problems where the feasible set is a general closed convex set, and the objective function has a nonsmooth, nonconvex regularizer. Such a regularizer includes widely used SCAD, MCP, logistic, fraction, hard thresholding, and non-Lipschitz Lp penalties as special cases. Using the theory of the generalized directional derivative and the tangent cone, we derive a first order necessary optimality condition for local minimizers of the problem, and define the generalized stationary point of it. We show that the generalized stationary point is the Clarke stationary point when the objective function is Lipschitz continuous at this point, and satisfies the existing necessary optimality conditions when the objective function is not Lipschitz continuous at this point. Moreover, we prove the consistency between the generalized directional derivative and the limit of the classic directional derivatives associated with the smoothing function. Finally, we establish a lower bound property for every local minimizer and show that finding a global minimizer is strongly NP-hard when the objective function has a concave regularizer.
1	We study a two player repeated zero-sum game with asymmetric information introduced by Renault in which the underlying state of the game undergoes Markov evolution (parameterized by a transition probability, p, in the range 1212 to 1). Hörner, Rosenberg, Solan and Vieille identified an optimal strategy, σ* for the informed player for p in the range [12,23 ][12,23]. We extend the range on which σ* is proved to be optimal to about [12,0.719][12,0.719] and prove that it fails to be optimal at a value around 0.7328. Our techniques make use of tools from dynamical systems, specifically the notion of pressure, introduced by D. Ruelle.
1	In this paper we study width of semialgebraic proof systems and various cut-based procedures in integer programming. We focus on two important systems: Gomory-Chvátal cutting planes and Lovász-Schrijver lift-and-project procedures. We develop general methods for proving width lower bounds and apply them to random k-CNFs and several popular combinatorial principles, like the perfect matching principle and Tseitin tautologies. We also show how to apply our methods to various combinatorial optimization problems. We establish a “supercritical” trade-off between width and rank, that is we give an example in which small width proofs are possible but require exponentially many rounds to perform them.
1	This paper examines a Markovian model for the optimal irreversible investment problem of a firm aiming at minimizing total expected costs of production. We model market uncertainty and the cost of investment per unit of production capacity, as two independent one-dimensional regular diffusions, and we consider a general convex running cost function. The optimization problem is set as a three-dimensional degenerate singular stochastic control problem. We provide the optimal control as the solution of a reflected diffusion at a suitable boundary surface. Such boundary arises from the analysis of a family of two-dimensional parameter-dependent optimal stopping problems, and it is characterized in terms of the family of unique continuous solutions to parameter-dependent, nonlinear integral equations of Fredholm type.
1	We provide a characterization of subgame-perfect equilibrium plays in a class of perfect information games where each player’s payoff function is Borel measurable and has finite range. The set of subgame-perfect equilibrium plays is obtained through a process of iterative elimination of plays. Extensions to games with bounded Borel measurable payoff functions are discussed. As an application of our results, we show that if every player’s payoff function is bounded and upper semicontinuous, then, for every positive epsilon, the game admits a subgame-perfect epsilon-equilibrium. As we do not assume that the number of players is finite, this result generalizes the corresponding result of Purves and Sudderth [24] [Purves RA, Sudderth WD (2011) Perfect information games with upper semicontinuous payoffs. Math. Oper. Res. 36(3):468–473].
1	We consider the problem of minimizing a certainty equivalent of the total or discounted cost over a finite and an infinite time horizon that is generated by a partially observable Markov decision process (POMDP). In contrast to a risk-neutral decision maker, this optimization criterion takes the variability of the cost into account. It contains as a special case the classical risk-sensitive optimization criterion with an exponential utility. We show that this optimization problem can be solved by embedding the problem into a completely observable Markov decision process with extended state space and give conditions under which an optimal policy exists. The state space has to be extended by the joint conditional distribution of current unobserved state and accumulated cost. In case of an exponential utility, the problem simplifies considerably and we rediscover what in previous literature has been named information state. However, since we do not use any change of measure techniques here, our approach is simpler. A simple example, namely, a risk-sensitive Bayesian house selling problem, is considered to illustrate our results.
1	We design new approximation algorithms for the problems of optimizing submodular and supermodular functions subject to a single matroid constraint. Specifically, we consider the case in which we wish to  maximize a monotone increasing submodular function or minimize a monotone decreasing supermodular function with a bounded total curvature c. Intuitively, the parameter c represents how nonlinear a function f is: when c = 0, f is linear, while for c = 1, f may be an arbitrary monotone increasing submodular function. For the case of submodular maximization with total curvature c, we obtain a (1 − c/e)-approximation—the first improvement over the greedy algorithm of of Conforti and Cornuéjols from 1984, which holds for a cardinality constraint, as well as a recent analogous result for an arbitrary matroid constraint.Our approach is based on modifications of the continuous greedy algorithm and nonoblivious local search, and allows us to approximately maximize the sum of a nonnegative, monotone increasing submodular function and a (possibly negative) linear function. We show how to reduce both submodular maximization and supermodular minimization to this general problem when the objective function has bounded total curvature. We prove that the approximation results we obtain are the best possible in the value oracle model, even in the case of a cardinality constraint.We define an extension of the notion of curvature to general monotone set functions and show a (1 − c)-approximation for maximization and a 1/(1 − c)-approximation for minimization cases. Finally, we give two concrete applications of our results in the settings of maximum entropy sampling, and the column-subset selection problem.
1	Given an underlying undirected simple graph, we consider the set of its acyclic orientations. Each of these orientations induces a partial order on the vertices of our graph and, therefore, we can count the number of linear extensions of these posets. We want to know which choice of orientation maximizes the number of linear extensions of the corresponding poset, and this problem will be solved essentially for comparability graphs and odd cycles, presenting several proofs. The corresponding enumeration problem for arbitrary simple graphs will be studied, including the case of random graphs; this will culminate in (1) new bounds for the volume of the stable set polytope and (2) strong concentration results for our enumerative statistic and for the graph entropy, which hold true a.s. for random graphs. We will then argue that our problem springs up naturally in the theory of graphical arrangements and graphical zonotopes.
1	When using the standard McCormick inequalities twice to convexify trilinear monomials, as is often the practice in modeling and software, there is a choice of which variables to group first. For the important case in which the domain is a nonnegative box, we calculate the volume of the resulting relaxation, as a function of the bounds defining the box. In this manner, we precisely quantify the strength of the different possible relaxations defined by all three groupings, in addition to the trilinear hull itself. As a by-product, we characterize the best double-McCormick relaxation.We wish to emphasize that, in the context of spatial branch and bound for factorable formulations, our results do not only apply to variables in the input formulation. Our results apply to monomials that involve auxiliary variables as well. So, our results apply to the product of any three (possibly complicated) expressions in a formulation.
1	The concept of ambiguity designates those situations where the information available to the decision maker is insufficient to form a probabilistic view of the world. Thus, it has provided the motivation for departing from the subjective expected utility (SEU) paradigm. Yet, the formalization of the concept is missing. This is a grave omission as it leaves nonexpected utility models hanging on shaky ground. In particular, it leaves unanswered basic questions such as the following: (1) Does ambiguity exist? (2) If so, which situations should be labeled as “ambiguous”? (3) Why should one depart from SEU in the presence of ambiguity? (4) If so, what kind of behavior should emerge in the presence of ambiguity? The present paper fills these gaps. Specifically, it identifies those information structures that are incompatible with SEU theory, and shows that their mathematical properties are the formal counterpart of the intuitive idea of insufficient information. These are used to give a formal definition of ambiguity and, consequently, to distinguish between ambiguous and unambiguous situations. Finally, the paper shows that behavior not conforming to SEU theory must emerge in correspondence of insufficient information and identifies the class of non-EU models that emerge in the face of ambiguity. The paper also proposes a new comparative definition of ambiguity, and discusses its relation with some of the existing literature.
1	We define a stochastic model of a two-sided limit order book in terms of its key quantities best bid [ask] price and the standing buy [sell] volume density. For a simple scaling of the discreteness parameters, that keeps the expected volume rate over the considered price interval invariant, we prove a limit theorem. The limit theorem states that, given regularity conditions on the random order flow, the key quantities converge in probability to a tractable continuous limiting model. In the limit model the buy and sell volume densities are given as the unique solution to first-order linear hyperbolic PDEs, specified by the expected order flow parameters. We calibrate order flow dynamics to market data for selected stocks and show how our model can be used to derive endogenous shape functions for models of optimal portfolio liquidation under market impact.
1	We consider an optimal risk-sensitive portfolio allocation problem accounting for the possibility of cascading defaults. Default events have an impact on the distress state of the surviving stocks in the portfolio. We study the recursive system of non-Lipschitz quasilinear parabolic HJB-PDEs associated with the value function of the control problem in the different default states of the economy. We show the existence of a classical solution to this system via super-sub solution techniques and give an explicit characterization of the optimal feedback strategy in terms of the value function. We prove a verification theorem establishing the uniqueness of the solution. A numerical analysis indicates that the investor significantly accounts for contagion effects when making investment decisions, and that his strategy depends nonmonotonically on the aggregate risk level.
1	We consider a model of influence with a set of nonstrategic agents and two strategic agents. The nonstrategic agents have initial opinions and are linked through a simply connected network. They update their opinions as in the DeGroot model. The two strategic agents have fixed and opposed opinions. They each form a link with a nonstrategic agent in order to influence the average opinion that emerges due to interactions in the network. This procedure defines a zero-sum game whose players are the two strategic agents and whose strategy set is the set of nonstrategic agents. We focus on the existence and the characterization of pure strategy equilibria in this setting. Simple examples show that the existence of a pure strategy equilibrium does depend on the structure of the network. We characterize equilibrium with two notions: the influenceability of target agents, and their centrality, which in our context we call “intermediacy.” We also show that when the two strategic agents have the same impact, symmetric equilibria emerge as natural solutions. In the case where the impacts are uneven, the game has only equilibria in mixed strategies, the high impact agent focuses on his own centrality/intermediacy and the influenceability of his opponent’s target while the low influence agent focuses on the influenceability of his own target.
1	We consider two-person zero-sum games where the players control, at discrete times {tn} induced by a partition Π of ℝ+, a continuous time Markov process. We prove that the limit of the values υΠ exist as the mesh of Π goes to 0. The analysis covers the cases of (1) stochastic games (where both players know the state), and (2) games with unknown state and symmetric signals.The proof is by reduction to deterministic differential games.
1	Let G = (V, E) be a graph. The matching polytope of G, denoted by P(G), is the convex hull of the incidence vectors of all matchings in G. As proved by Edmonds [10] [Edmonds J (1965) Maximum matching and a polyhedron with 0, 1-vertices, J. Res. Nat. Bur. Standards Sect. B 69(1–2):125–130.], P(G) is determined by the following linear system π(G): x(e) ≥ 0 for each e ∈ E; x(δ(v)) ≤ 1 for each v ∈ V; and x(E[U]) ≤ ½|U|⌋ for each U ⊆ V with |U| odd. In 1978, Cunningham and Marsh [6] [Cunningham W, Marsh A (1978) A primal algorithm for optimum matching. Balinski ML, Hoffman AJ, eds. Polyhedral combinatorics. Mathematical Programming Studies, Vol. 8 (Springer, Berlin), 50–72.] strengthened this theorem by showing that π(G) is always totally dual integral. In 1984, Edmonds and Giles [11] [Edmonds J, Giles R (1984) Total dual integrality of linear inequality systems. Progress in Combinatorial Optimization (Academic Press, Toronto), 117–129.] initiated the study of graphs G for which π(G) is box-totally dual integral. In this paper, we present a structural characterization of all such graphs, and develop a general and powerful method for establishing box-total dual integrality.The online appendix is available at https://doi.org/10.1287/moor.2017.0852.
1	We furnish conditions on the primitives of a Bayesian game that guarantee the existence of a Bayes-Nash equilibrium. By allowing for payoff discontinuities in actions, we cover various applications that cannot be handled by extant results.
1	We present for the first time an asymptotic convergence analysis of two time-scale stochastic approximation driven by “controlled” Markov noise. In particular, the faster and slower recursions have nonadditive controlled Markov noise components in addition to martingale difference noise. We analyze the asymptotic behavior of our framework by relating it to limiting differential inclusions in both time scales that are defined in terms of the ergodic occupation measures associated with the controlled Markov processes. Finally, we present a solution to the off-policy convergence problem for temporal-difference learning with linear function approximation, using our results.
1	We consider a market model that consists of financial investors and producers of a commodity. Producers optionally store some production for future sale and go short on forward contracts to hedge the uncertainty of the future commodity price. Financial investors take positions in these contracts to diversify their portfolios. The spot and forward equilibrium commodity prices are endogenously derived as the outcome of the interaction between producers and investors. Assuming that both are utility maximizers, we first prove the existence of an equilibrium in an abstract setting. Then, in a framework where the consumers’ demand and the exogenously priced financial market are correlated, we provide semi-explicit expressions for the equilibrium prices and analyze their dependence on the model parameters. The model can explain why increased investors’ participation in forward commodity markets and higher correlation between the commodity and the stock market could result in higher spot prices and lower forward premia.
1	We analyze a tractable model of a limit order book on short time scales, where the dynamics are driven by stochastic fluctuations between supply and demand. We establish the existence of a limiting distribution for the highest bid, and for the lowest ask, where the limiting distributions are confined between two thresholds. We make extensive use of fluid limits to establish recurrence properties of the model. We use the model to analyze various high-frequency trading strategies, and comment on the Nash equilibria that emerge between high-frequency traders when a market in continuous time is replaced by frequent batch auctions.
1	In this paper, we provide a flexible framework allowing for a unified study of time consistency of risk measures and performance measures (also known as acceptability indices). The proposed framework not only integrates existing forms of time consistency, but also provides a comprehensive toolbox for analysis and synthesis of the concept of time consistency in decision making. In particular, it allows for in-depth comparative analysis of (most of) the existing types of time consistency—a feat that has not been possible before and, which is done in the companion paper by the authors. In our approach, the time consistency is studied for a large class of maps that are postulated to satisfy only two properties—monotonicity and locality. We call these maps LM-measures. The time consistency is defined in terms of an update rule. The form of the update rule introduced here is novel, and is perfectly suited for developing the unifying framework that is worked out in this paper. As an illustration of the applicability of our approach, we show how to recover almost all concepts of weak time consistency by means of constructing appropriate update rules.
1	We prove that combinatorial demand functions are characterized by two properties: continuity and the law of demand.
1	In this paper, we provide a framework to study the dependence structure of sampling schemes such as those produced by randomized quasi-Monte Carlo methods. The main goal of this new framework is to determine conditions under which the negative dependence structure of a sampling scheme enables the construction of estimators with reduced variance compared to Monte Carlo estimators. To do this, we establish a generalization of the well-known Hoeffding’s lemma—expressing the covariance of two random variables as an integral of the difference between their joint distribution function and the product of their marginal distribution functions—that is particularly well suited to study such sampling schemes. We also provide explicit formulas for the joint distribution of pairs of points randomly chosen from a scrambled (0, m, s)-net. In addition, we provide variance bounds establishing the superiority of dependent sampling schemes over Monte Carlo in a few different setups. In particular, we show that a scrambled (0, m, 2)-net yields an estimator with variance no larger than a Monte Carlo estimator for functions monotone in each variable.
1	We use probabilistic methods to characterise time-dependent optimal stopping boundaries in a problem of multiple optimal stopping on a finite time horizon. Motivated by financial applications, we consider a payoff of immediate stopping of “put” type, and the underlying dynamics follows a geometric Brownian motion. The optimal stopping region relative to each optimal stopping time is described in terms of two boundaries, which are continuous, monotonic functions of time and uniquely solve a system of coupled integral equations of Volterra-type. Finally, we provide a formula for the value function of the problem.
1	The conjugate gradient (CG) method is an efficient iterative method for solving large-scale strongly convex quadratic programming (QP). In this paper, we propose some generalized CG (GCG) methods for solving the ℓ1-regularized (possibly not strongly) convex QP that terminate at an optimal solution in a finite number of iterations. At each iteration, our methods first identify a face of an orthant and then either perform an exact line search along the direction of the negative projected minimum-norm subgradient of the objective function or execute a CG subroutine that conducts a sequence of CG iterations until a CG iterate crosses the boundary of this face or an approximate minimizer of over this face or a subface is found. We determine which type of step should be taken by comparing the magnitude of some components of the minimum-norm subgradient of the objective function to that of its rest components. Our analysis on finite convergence of these methods makes use of an error bound result and some key properties of the aforementioned exact line search and the CG subroutine. We also show that the proposed methods are capable of finding an approximate solution of the problem by allowing some inexactness on the execution of the CG subroutine. The overall arithmetic operation cost of our GCG methods for finding an ϵ-optimal solution depends on ϵ in O(log(1/ϵ)), which is superior to the accelerated proximal gradient method (Beck and Teboulle [Beck A, Teboulle M (2009) A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM J. Imaging Sci. 2(1):183–202], Nesterov [Nesterov Yu (2013) Gradient methods for minimizing composite functions. Math. Program. 140(1):125–161]) that depends on ϵ in 𝑂(1/𝜖√). In addition, our GCG methods can be extended straightforwardly to solve box-constrained convex QP with finite convergence. Numerical results demonstrate that our methods are very favorable for solving ill-conditioned problems.
1	In this paper, we present an analysis of the strength of sparse cutting planes for mixed integer linear programs (MILP) with sparse formulations. We examine three kinds of problems: packing problems, covering problems, and more general MILPs with the only assumption that the objective function is nonnegative. Given an MILP instance of one of these three types, assume that we decide on the support of cutting planes to be used and the strongest inequalities on these supports are added to the linear programming relaxation. We present bounds on the ratio of optimal value of the LP relaxation after adding cuts and the optimal value of the MILP that depends only on the sparsity structure of the constraint matrix and the support of sparse cuts selected, that is, these bounds are completely data independent. These results also shed light on the strength of single-scenario cuts for two-stage stochastic MILPs.
1	We consider small-influence aggregative games with a large number of players n. For this class of games we present a best-reply dynamic with the following two properties. First, the dynamic reaches Nash approximate equilibria in quasi-linear (in n) number of steps, and the quasi-linear bound is tight. Second, Nash approximate equilibria are played by the dynamic with a limit frequency that is exponentially (in n) close to 1.
1	This paper studies the dynamic portfolio choice problem with ambiguous jump risks in a multidimensional jump-diffusion framework. We formulate a continuous-time model of incomplete market with uncertain jumps. We develop an efficient pathwise optimization procedure based on the martingale methods and minimax results to obtain closed-form solutions for the indirect utility function and the probability of the worst scenario. We then introduce an orthogonal decomposition method for the multidimensional problem to derive the optimal portfolio strategy explicitly under ambiguity aversion to jump risks. Finally, we calibrate our model to real market data drawn from 10 international indices and illustrate our results by numerical examples. The certainty equivalent losses affirm the importance of jump uncertainty in optimal portfolio choice.
1	In this paper, we consider the optimal dividend payment strategy for an insurance company that has two collaborating business lines. The surpluses of the business lines are modeled by diffusion processes. The collaboration between the two business lines permits that money can be transferred from one line to another with or without proportional transaction costs, while money must be transferred from one line to another to help both business lines keep running before simultaneous ruin of the two lines eventually occurs.
1	We present a two-armed bandit model of decision making under uncertainty where the expected return to investing in the “risky arm” increases when choosing that arm and decreases when choosing the “safe” arm. These dynamics are natural in applications such as human capital development, job search, and occupational choice. Using new insights from stochastic control, along with a monotonicity condition on the payoff dynamics, we show that optimal strategies in our model are stopping rules that can be characterized by an index which formally coincides with Gittins’ index. Our result implies the indexability of a new class of restless bandit models.
1	Let E be a finite set of elements, and let L be a clutter over ground set E. We say distinct elements e, f are opposite if every member and every minimal cover of L contains at most one of e, f. In this paper, we investigate opposite elements and reveal a rich theory underlying such a seemingly simple restriction. The clutter C obtained from L after identifying some opposite elements is called an identification of L; inversely, L is called a split of C.We will show that splitting preserves three clutter properties, i.e., idealness, the max-flow min-cut property, and the packing property. We will also display several natural examples in which a clutter does not have these properties but a split of them does. We will develop tools for recognizing when splitting is not a useful operation, and as well, we will characterize when identification preserves the three mentioned properties. We will also make connections to spanning arborescences, Steiner trees, comparability graphs, degenerate projective planes, binary clutters, matroids, as well as the results of Menger, Ford and Fulkerson, the Replication Conjecture, and a conjecture on ideal, minimally nonpacking clutters.
1	We consider switched queueing networks with a mix of heavy-tailed (i.e., arrival processes with infinite variance) and exponential-type traffic and study the delay performance of the max-weight policy, known for its throughput optimality and asymptotic delay optimality properties. Our focus is on the impact of heavy-tailed traffic on exponential-type queues/flows, which may manifest itself in the form of subtle rate-dependent phenomena. We introduce a novel class of Lyapunov functions (piecewise linear and nonincreasing in the length of heavy-tailed queues), whose drift analysis provides exponentially decaying upper bounds to queue-length tail asymptotics despite the presence of heavy tails. To facilitate a drift analysis, we employ fluid approximations, proving that if a continuous and piecewise linear function is also a “Lyapunov function” for the fluid model, then the same function is a “Lyapunov function” for the original stochastic system. Furthermore, we use fluid approximations and renewal theory in order to prove delay instability results, i.e., infinite expected delays in steady state. We illustrate the benefits of the proposed approach in two ways: (i) analytically, by studying the delay stability regions of single-hop switched queueing networks with disjoint schedules, providing a precise characterization of these regions for certain queues and inner and outer bounds for the rest. As a side result, we prove monotonicity properties for the service rates of different schedules that, in turn, allow us to identify “critical configurations” toward which the state of the system is driven, and that determine to a large extent delay stability; (ii) computationally, through a bottleneck identification algorithm, which identifies (some) delay unstable queues/flows in complex switched queueing networks by solving the fluid model from certain initial conditions.
1	Scheduling control for a single-server queue with I customer classes and reneging is considered, with linear holding or reneging cost. An asymptotically optimal (AO) policy in heavy traffic is identified where classes are prioritized according to a workload-dependent dynamic index rule. Denote by ci, μi, and θi, i ∈ ℐ := {1, …, I} the queue length cost, service rate, and reneging rate, for class-i customers. Then, a relabeling of the classes and a partition 0 = w0 < w1 < ⋯ < wK = ∞, K ≤ I are identified such that the policy acts to always assign least priority to the class i when the rescaled workload is in the interval [wi−1, wi). The relabeling is such that when workload is withing the lowest [resp., highest] interval [wi−1, wi), the least priority class is the one with smallest cμ [resp., greatest θ] value. This result stands in sharp contrast to known fluid-scale results where it is AO to prioritize by the fixed cμ/θ index. One of the technical challenges is the discontinuity of the limiting queue length process under optimality. Discontinuities occur whenever the workload reaches one of the levels wi.
1	The infinite horizon risk-sensitive discounted-cost and ergodic-cost nonzero-sum stochastic games for controlled Markov chains with countably many states are analyzed. For the discounted-cost game, we prove the existence of Nash equilibrium strategies in the class of Markov strategies under fairly general conditions. Under an additional weak geometric ergodicity condition and a small cost criterion, the existence of Nash equilibrium strategies in the class of stationary Markov strategies is proved for the ergodic-cost game. The key nontrivial contributions in the ergodic part are to prove the existence of a particular form of a (relative) value function solution to a player’s Bellman equation and the continuity of this solution with respect to the opponent’s strategies.
1	For a clutter 𝒞 over ground set E, a pair of distinct elements e, f ∈ E are coexclusive if every minimal cover contains at most one of them. An identification of 𝒞 is another clutter obtained after identifying coexclusive elements of 𝒞. If a clutter is nonpacking, then so is any identification of it.Inspired by this observation, and impelled by the lack of a qualitative characterization for ideal minimally nonpacking (mnp) clutters, we reduce ideal mnp clutters even further by taking their identifications. In doing so, we reveal chains of ideal mnp clutters, demonstrate the centrality of mnp clutters with covering number two, as well as provide a qualitative characterization of irreducible ideal mnp clutters with covering number two. At the core of this characterization lies a class of objects, called marginal cuboids, that naturally give rise to ideal nonpacking clutters with covering number two. We present an explicit class of marginal cuboids, and show that the corresponding clutters have one of Q6, Q2, 1, Q10 as a minor, where Q6, Q2, 1 are known ideal mnp clutters, and Q10 is a new ideal mnp clutter.
1	In this paper, we consider a finite-horizon Markov decision process (MDP) for which the objective at each stage is to minimize a quantile-based risk measure (QBRM) of the sequence of future costs; we call the overall objective a dynamic quantile-based risk measure (DQBRM). In particular, we consider optimizing dynamic risk measures where the one-step risk measures are QBRMs, a class of risk measures that includes the popular value at risk (VaR) and the conditional value at risk (CVaR). Although there is considerable theoretical development of risk-averse MDPs in the literature, the computational challenges have not been explored as thoroughly. We propose data-driven and simulation-based approximate dynamic programming (ADP) algorithms to solve the risk-averse sequential decision problem. We address the issue of inefficient sampling for risk applications in simulated settings and present a procedure, based on importance sampling, to direct samples toward the “risky region” as the ADP algorithm progresses. Finally, we show numerical results of our algorithms in the context of an application involving risk-averse bidding for energy storage.The online appendix is available at https://doi.org/10.1287/moor.2017.0872.
1	We introduce an iterative method for solving linearly constrained optimization problems, whose nonsmooth nonconvex objective function is defined as the pointwise maximum of finitely many concave functions. Such problems often arise from reformulations of certain constraint structures (e.g., binary constraints, finite max-min constraints) in diverse areas of optimization. We state a local optimization strategy, which exploits piecewise-concavity of the objective function, giving rise to a linearized model corrected by a proximity term. In addition we introduce an approximate line-search strategy, based on a curvilinear model, which, similarly to bundle methods, can return either a satisfactory descent or a null-step declaration. Termination at a point satisfying an approximate stationarity condition is proved. We embed the local minimization algorithm into a Variable Neighborhood Search scheme and a Coordinate Direction Search heuristic, whose aim is to improve the current estimate of the global minimizer by exploring different parts of the feasible region. We finally provide computational results obtained on two sets of small- to large-size test problems.
1	The model of FCFS infinite bipartite matching was introduced in Caldentey, Kaplan and Weiss, 2009. In this model, there is a sequence of items that are chosen i.i.d. from a finite set 𝒞 and an independent sequence of items that are chosen i.i.d. from a finite set 𝒮, and a bipartite compatibility graph G between 𝒞 and 𝒮. Items of the two sequences are matched according to the compatibility graph, and the matching is FCFS, meaning that each item in the one sequence is matched to the earliest compatible unmatched item in the other sequence. In Adan and Weiss, 2012, a Markov chain associated with the matching was analyzed, a condition for stability was derived, and a product form stationary distribution was obtained. In the current paper, we present several new results that unveil the fundamental structure of the model. First, we provide a pathwise Loynes’ type construction which enables to prove the existence of a unique matching for the model defined over all the integers. Second, we prove that the model is dynamically reversible: we define an exchange transformation in which we interchange the positions of each matched pair, and show that the items in the resulting permuted sequences are again independent and i.i.d., and the matching between them is FCFS in reversed time. Third, we obtain product form stationary distributions of several new Markov chains associated with the model. As a by-product, we compute useful performance measures, for instance the link lengths between matched items.
1	In this paper, we aim to prove the linear rate convergence of the alternating direction method of multipliers (ADMM) for solving linearly constrained convex composite optimization problems. Under a mild calmness condition, which holds automatically for convex composite piecewise linear-quadratic programming, we establish the global Q-linear rate of convergence for a general semi-proximal ADMM with the dual step-length being taken in (0, (1+51/2)/2). This semi-proximal ADMM, which covers the classic one, has the advantage to resolve the potentially nonsolvability issue of the subproblems in the classic ADMM and possesses the abilities of handling the multi-block cases efficiently. We demonstrate the usefulness of the obtained results when applied to two- and multi-block convex quadratic (semidefinite) programming.
1	Only recently, progress has been made in obtaining o(log(rank))-competitive algorithms for the matroid secretary problem. More precisely, Chakraborty and Lachish (2012) presented a O((log(rank))1/2)-competitive procedure, and Lachish (2014) later presented a O(log log(rank))-competitive algorithm. Both these algorithms and their analyses are very involved, which is also reflected in the extremely high constants in their competitive ratios. Using different tools, we present a considerably simpler O(log log(rank))-competitive algorithm for the matroid secretary problem. Our algorithm can be interpreted as a distribution over a simple type of matroid secretary algorithms that are easy to analyze. Because of the simplicity of our procedure, we are also able to vastly improve on the hidden constant in the competitive ratio.
1	In a Standard Quadratic Optimization Problem (StQP), a possibly indefinite quadratic form (the simplest nonlinear function) is extremized over the standard simplex, the simplest polytope. Despite this simplicity, the nonconvex instances of this problem class allow for remarkably rich patterns of coexisting local solutions, which is closely related to practical difficulties in solving StQPs globally. In this study, we improve on existing lower bounds for the number of strict local solutions by a new technique to construct instances with a rich solution structure. Furthermore, we provide extensive case studies where the system of supports (the so-called pattern) of solutions are analyzed in detail. Note that by naive simulation, in accordance to theory, most of the interesting patterns would not be encountered, since random instances have, with a high probability, quite sparse solutions (with singleton or doubleton supports), and likewise their expected numbers are considerably lower than in the worst case. Hence, instances with a rich solution pattern are rather rare. On the other hand, by concentrating on (thin) subsets of promising instances, we are able to give an empirical answer on the size distribution of supports of strict local solutions to the StQP and their patterns, complementing average-case analysis of this NP-hard problem class.
1	The following game is played on a weighted graph: Alice selects a matching M and Bob selects a number k. Alice’s payoff is the ratio of the weight of the k heaviest edges of M to the maximum weight of a matching of size at most k. If M guarantees a payoff of at least α then it is called α-robust. In 2002, Hassin and Rubinstein gave an algorithm that returns a 1/2‾√1/2-robust matching, which is best possible.We show that Alice can improve her payoff to 1/ln(4) by playing a randomized strategy. This result extends to a very general class of independence systems that includes matroid intersection, b-matchings, and strong 2-exchange systems. It also implies an improved approximation factor for a stochastic optimization variant known as the maximum priority matching problem and translates to an asymptotic robustness guarantee for deterministic matchings, in which Bob can only select numbers larger than a given constant. Moreover, we give a new LP-based proof of Hassin and Rubinstein’s bound.
1	Sequential optimality conditions for constrained optimization are necessarily satisfied by local minimizers, independently of the fulfillment of constraint qualifications. These conditions support the employment of different stopping criteria for practical optimization algorithms. On the other hand, when an appropriate property on the constraints holds at a point that satisfies a sequential optimality condition, such a point also satisfies the Karush-Kuhn-Tucker conditions. Those properties will be called strict constraint qualifications in this paper. As a consequence, for each sequential optimality condition, it is natural to ask for its weakest strict associated constraint qualification. This problem has been solved in a recent paper for the Approximate Karush-Kuhn-Tucker sequential optimality condition. In the present paper, we characterize the weakest strict constraint qualifications associated with other sequential optimality conditions that are useful for defining stopping criteria of algorithms. In addition, we prove all the implications between the new strict constraint qualifications and other (classical or strict) constraint qualifications.
1	Let S ⊆ {0, 1}n and R be any polytope contained in [0, 1]n with R ∩ {0, 1}n = S. We prove that R has bounded Chvátal-Gomory rank (CG-rank) provided that S has bounded notch and bounded gap, where the notch is the minimum integer p such that all p-dimensional faces of the 0/1-cube have a nonempty intersection with S, and the gap is a measure of the size of the facet coefficients of conv(S).Let 𝐻[𝑆̄ ] denote the subgraph of the n-cube induced by the vertices not in S. We prove that if 𝐻[𝑆̄ ] does not contain a subdivision of a large complete graph, then the notch and the gap are bounded. By our main result, this implies that the CG-rank of R is bounded as a function of the treewidth of 𝐻[𝑆̄ ]. We also prove that if S has notch 3, then the CG-rank of R is always bounded. Both results generalize a recent theorem of Cornuéjols and Lee [Cornuéjols G, Lee D (2016) On some polytopes contained in the 0,1 hypercube that have a small Chvátal rank. Louveaux Q, Skutella M, eds. Proc. 18th Internat. Conf. Integer Programming Combinatorial Optim., IPCO ’16 (Springer International, Cham, Switzerland), 300–311], who proved that the CG-rank is bounded by a constant if the treewidth of 𝐻[𝑆̄ ] is at most 2.
1	The main result of this paper is motivated by the following two apparently unrelated graph optimization problems: (A) As an extension of Edmonds’ disjoint branchings theorem, characterize digraphs comprising k disjoint branchings Bi each having a specified number μi of arcs. (B) As an extension of Ryser’s maximum term rank formula, determine the largest possible matching number of simple bipartite graphs complying with degree-constraints. The solutions to these problems and to their generalizations will be obtained from a new min-max theorem on covering a supermodular function by a simple degree-constrained bipartite graph. A specific feature of the result is that its minimum cost extension is already NP-hard. Therefore classic polyhedral tools themselves definitely cannot be sufficient for solving the problem, even though they make some good service in our approach.
1	Ryser’s max term rank formula with graph theoretic terminology is equivalent to a characterization of degree sequences of simple bipartite graphs with a specific matching number. In a previous paper by the authors, a generalization was developed for the case when the degrees are constrained by upper and lower bounds. Here, two other extensions of Ryser’s theorem are discussed. The first one is a matroidal model, while the second one settles the augmentation version. In fact, the two directions shall be integrated into one single framework.
1	By generalizing a recent result of Hong, Liu and Lai on characterizing the degree-sequences of simple strongly connected directed graphs, a characterization is provided for degree-sequences of simple k-node-connected digraphs. More generally, we solve the directed node-connectivity augmentation problem when the augmented digraph is degree-specified and simple. As for edge-connectivity augmentation, we solve the special case when the edge-connectivity is to be increased by one and the augmenting digraph must be simple.
1	The multiple exchange property for matroid bases is generalized for valuated matroids and M-natural concave set functions. The proof is based on the Fenchel-type duality theorem in discrete convex analysis. The present result has an implication in economics: The strong no complementarities condition of Gul and Stacchetti is, in fact, equivalent to the gross substitutes condition of Kelso and Crawford.
1	We study the multi-armed bandit problem with arms which are Markov chains with rewards. In the finite-horizon setting, the celebrated Gittins indices do not apply, and the exact solution is intractable. We provide approximation algorithms for the general model of Markov decision processes with nonunit transition times. When preemption is allowed, we provide a (1/2 − ε)-approximation, along with an example showing this is tight. When preemption isn’t allowed, we provide a 1/12-approximation, which improves to a 4/27-approximation when transition times are unity. Our model captures the Markovian Bandits model of Gupta et al., the Stochastic Knapsack model of Dean et al., and the Budgeted Learning model of Guha and Munagala. Our algorithms improve existing results in all three areas. In our analysis, we encounter and overcome to our knowledge a new obstacle: an algorithm that provably exists via analytical arguments, but cannot be found in polynomial time.
1	This paper studies curves of the form (ρ(λX))λ≥0, called risk profiles, where ρ is a convex risk measure and X a random variable. Financially, this captures the sensitivity of risk to the size of the investment in X, which the original axiomatic foundations of convex risk measures suggest to interpret as liquidity risk. The shape of a risk profile is intimately linked with the tail behavior of X for some notable classes of risk measures, namely shortfall risk measures and optimized certainty equivalents, and shares many useful features with cumulant generating functions. Exploiting this link leads to tractable necessary and sufficient conditions for pointwise bounds on risk profiles, which we call concentration inequalities. These inequalities admit useful dual representations related to transport inequalities, and this leads to efficient uniform bounds for risk profiles for large classes of X. Several interesting mathematical results emerge from this analysis, including a new perspective on nonexponential concentration estimates and some peculiar characterizations of classical transport inequalities. Lastly, the analysis is deepened by means of a surprising connection between time consistency properties of law invariant risk measures and the tensorization of concentration inequalities.
1	We study the infinite-horizon optimal control problem for N-network queueing systems, which consists of two customer classes and two server pools, under average (ergodic) criteria in the Halfin–Whitt regime. We consider three control objectives: (1) minimizing the queueing (and idleness) cost, (2) minimizing the queueing cost while imposing a constraint on idleness at each server pool, and (3) minimizing the queueing cost while requiring fairness on idleness. The running costs can be any nonnegative convex functions having at most polynomial growth. For all three problems, we establish asymptotic optimality; namely, the convergence of the value functions of the diffusion-scaled state process to the corresponding values of the controlled diffusion limit. We also present a simple state-dependent priority scheduling policy under which the diffusion-scaled state process is geometrically ergodic in the Halfin–Whitt regime, and some results on convergence of mean empirical measures, which facilitate the proofs.
1	We consider queueing systems with n parallel queues under a Join the Shortest Queue (JSQ) policy in the Halfin-Whitt heavy-traffic regime. We use the martingale method to prove that a scaled process counting the number of idle servers and queues of length exactly two weakly converges to a two-dimensional reflected Ornstein-Uhlenbeck process, while processes counting longer queues converge to a deterministic system decaying to zero in constant time. This limiting system is comparable to that of the traditional Halfin-Whitt model, but there are key differences in the queueing behavior of the JSQ model. In particular, only a vanishing fraction of customers will have to wait, but those who do incur a constant order waiting time.
1	We derive sharp probability bounds on the tails of a product of symmetric nonnegative random variables using only information about their first two moments. If the covariance matrix of the random variables is known exactly, these bounds can be computed numerically using semidefinite programming. If only an upper bound on the covariance matrix is available, the probability bounds on the right tails can be evaluated analytically. The bounds under precise and imprecise covariance information coincide for all left tails as well as for all right tails corresponding to quantiles that are either sufficiently small or sufficiently large. We also prove that all left probability bounds reduce to the trivial bound 1 if the number of random variables in the product exceeds an explicit threshold. Thus, in the worst case, the weak-sense geometric random walk defined through the running product of the random variables is absorbed at 0 with certainty as soon as time exceeds the given threshold. The techniques devised for constructing Chebyshev bounds for products can also be used to derive Chebyshev bounds for sums, maxima, and minima of nonnegative random variables.
1	The proximal gradient algorithm for minimizing the sum of a smooth and nonsmooth convex function often converges linearly even without strong convexity. One common reason is that a multiple of the step length at each iteration may linearly bound the “error”—the distance to the solution set. We explain the observed linear convergence intuitively by proving the equivalence of such an error bound to a natural quadratic growth condition. Our approach generalizes to linear and quadratic convergence analysis for proximal methods (of Gauss-Newton type) for minimizing compositions of nonsmooth functions with smooth mappings. We observe incidentally that short step-lengths in the algorithm indicate near-stationarity, suggesting a reliable termination criterion.
1	For a GI/GI/1 queue, we show that the average sojourn time under the (blind) Randomized Multilevel Feedback algorithm is no worse than that under the Shortest Remaining Processing Time algorithm times a logarithmic function of the system load. Moreover, it is verified that this bound is tight in heavy traffic, up to a constant multiplicative factor. We obtain this result by combining techniques from two disparate areas: competitive analysis and applied probability.
1	We present a novel method for deriving tight Monte Carlo confidence intervals for solutions of stochastic dynamic programming equations. Taking some approximate solution to the equation as an input, we construct pathwise recursions with a known bias. Suitably coupling the recursions for lower and upper bounds ensures that the method is applicable even when the dynamic program does not satisfy a comparison principle. We apply our method to three nonlinear option pricing problems, pricing under bilateral counterparty risk, under uncertain volatility, and under negotiated collateralization.
1	We introduce new classes of utility functions and production sets, called Leontief-free, which are applicable when goods are substitutes and utilities/production are subadditive (to model inter-good satiation). When goods are complements, the well studied Leontief utility functions do an adequate job; however, to the best of our knowledge, a similar concept for the case of goods that are substitutes was not known.For markets with these utility functions and production sets, we obtain the following results: Rational-valued equilibria, despite the fact that these utility functions and production sets are nonseparable. We prove that the problem of computing an equilibrium is PPAD-complete, where PPAD stands for Polynomial Parity Arguments on Directed Graphs. We obtain complementary pivot algorithms based on a suitable adaptation of Lemke’s classic algorithm. Our algorithms run in strongly polynomial time if the number of goods is a constant, despite the fact that the set of solutions is disconnected. Experimental verification confirms that our algorithms are practical.
1	This work is concerned with Markov decision chains on a denumerable state space. The controller has a positive risk-sensitivity coefficient, and the performance of a control policy is measured by a risk-sensitive average cost criterion. Besides standard continuity-compactness conditions, it is assumed that the state process is communicating under any stationary policy, and that the simultaneous Doeblin condition holds. In this context, it is shown that if the cost function is bounded from below, and the superior limit average index is finite at some point, then (i) the optimal superior and inferior limit average value functions coincide and are constant, (ii) the optimal average cost is characterized via an extended version of the Collatz-Wielandt formula in the theory of positive matrices, and (iii) an optimality inequality is established, from which a stationary optimal policy is obtained. Moreover, an explicit example is given to show that, even if the cost function is bounded, the strict inequality may occur in the optimality relation.
1	Random projections are random linear maps, sampled from appropriate distributions, which approximately preserve certain geometrical invariants so that the approximation improves as the dimension of the space grows. The well known Johnson-Lindenstrauss lemma states that there are random matrices with surprisingly few rows which approximately preserve pairwise Euclidean distances among a set of points. This is commonly used to speed up algorithms based on Euclidean distances. We prove that these matrices also preserve other quantities, such as the distance to a cone. We exploit this result to devise a probabilistic algorithm to approximately solve linear programs. We show that this algorithm can approximately solve very large randomly generated LP instances. We also showcase its application to an error correction coding problem.
1	The low-rank matrix approximation problem with respect to the component-wise ℓ1-norm (ℓ1-LRA), which is closely related to robust principal component analysis (PCA), has become a very popular tool in data mining and machine learning. Robust PCA aims to recover a low-rank matrix that was perturbed with sparse noise, with applications for example in foreground-background video separation. Although ℓ1-LRA is strongly believed to be NP-hard, there is, to our knowledge, no formal proof of this fact. In this paper, we prove that ℓ1-LRA is NP-hard, already in the rank-one case, using a reduction from MAX CUT. Our derivations draw interesting connections between ℓ1-LRA and several other well-known problems, i.e., robust PCA, ℓ0-LRA, binary matrix factorization, a particular densest bipartite subgraph problem, the computation of the cut norm of {−1, + 1} matrices, and the discrete basis problem, all of which we prove to be NP-hard.
1	Discrete structures like graphs make it possible to naturally and flexibly model complex phenomena. Since graphs that represent various types of information are increasingly available today, their analysis has become a popular subject of research. Yet, even an algorithm for locating the average position in graphs is lacking although this knowledge would be of primary interest for statistical analysis or representation problems. In this work, we develop a stochastic algorithm for finding the Fréchet mean of weighted undirected metric graphs. This method relies on a noisy simulated annealing algorithm dealt with using homogenization. We then illustrate our algorithm with three examples (subgraphs of a social network, subgraph of a collaboration and citation network, and a transport network).
1	We consider the canonical revenue management (RM) problem wherein a seller must sell an inventory of some product over a finite horizon via an anonymous, posted price mechanism. Unlike typical models in RM, we assume that customers are forward looking. In particular, customers arrive randomly over time and strategize about their times of purchases. The private valuations of these customers decay over time and the customers incur monitoring costs; both the rates of decay and these monitoring costs are private information. This setting has resisted the design of optimal dynamic mechanisms heretofore. Optimal pricing schemes—an almost necessary mechanism format for practical RM considerations—have been similarly elusive.The present paper proposes a mechanism we dub robust pricing. Robust pricing is guaranteed to achieve expected revenues that are at least within 29% of those under an optimal (not necessarily posted price) dynamic mechanism. We thus provide the first approximation algorithm for this problem. The robust pricing mechanism is practical, since it is an anonymous posted price mechanism and since the seller can compute the robust pricing policy for a problem without any knowledge of the distribution of customer discount factors and monitoring costs. The robust pricing mechanism also enjoys the simple interpretation of solving a dynamic pricing problem for myopic customers with the additional requirement of a novel “restricted sub-martingale constraint” on prices that discourages rapid discounting. We believe this interpretation is attractive to practitioners. Finally, numerical experiments suggest that the robust pricing mechanism is, for all intents, near optimal.
1	We develop a framework for quantitative convergence analysis of Picard iterations of expansive set-valued fixed point mappings. There are two key components of the analysis. The first is a natural generalization of single-valued averaged mappings to expansive set-valued mappings that characterizes a type of strong calmness of the fixed point mapping. The second component to this analysis is an extension of the well-established notion of metric subregularity—or inverse calmness—of the mapping at fixed points. Convergence of expansive fixed point iterations is proved using these two properties, and quantitative estimates are a natural by-product of the framework. To demonstrate the application of the theory, we prove, for the first time, a number of results showing local linear convergence of nonconvex cyclic projections for inconsistent (and consistent) feasibility problems, local linear convergence of the forward-backward algorithm for structured optimization without convexity, strong or otherwise, and local linear convergence of the Douglas-Rachford algorithm for structured nonconvex minimization. This theory includes earlier approaches for known results, convex and nonconvex, as special cases.
1	This paper studies the optimal stopping problem in the presence of model uncertainty (ambiguity). We develop a numerically implementable method to solve this problem in a general setting, allowing for general time-consistent ambiguity-averse preferences and general payoff processes driven by jump diffusions. Our method consists of three steps. First, we construct a suitable Doob martingale associated with the solution to the optimal stopping problem using backward stochastic calculus. Second, we employ this martingale to construct an approximated upper bound to the solution using duality. Third, we introduce backward-forward simulation to obtain a genuine upper bound to the solution, which converges to the true solution asymptotically. We also provide asymptotically optimal exercise rules. We analyze the limiting behavior and convergence properties of our method. We illustrate the generality and applicability of our method and the potentially significant impact of ambiguity to optimal stopping in a few examples.
1	We introduce a novel approach addressing global analysis of a difficult class of nonconvex-nonsmooth optimization problems within the important framework of Lagrangian-based methods. This genuine nonlinear class captures many problems in modern disparate fields of applications. It features complex geometries, qualification conditions, and other regularity properties do not hold everywhere. To address these issues, we work along several research lines to develop an original general Lagrangian methodology, which can deal, all at once, with the above obstacles. A first innovative feature of our approach is to introduce the concept of Lagrangian sequences for a broad class of algorithms. Central to this methodology is the idea of turning an arbitrary descent method into a multiplier method. Secondly, we provide these methods with a transitional regime allowing us to identify in finitely many steps a zone where we can tune the step-sizes of the algorithm for the final converging regime. Then, despite the min-max nature of Lagrangian methods, using an original Lyapunov method we prove that each bounded sequence generated by the resulting monitoring schemes are globally convergent to a critical point for some fundamental Lagrangian-based methods in the broad semialgebraic setting, which to the best of our knowledge, are the first of this kind.
1	In this work, we are interested in nonlinear symmetric cone problems (NSCPs), which contain as special cases nonlinear semidefinite programming, nonlinear second-order cone programming, and the classical nonlinear programming problems. We explore the possibility of reformulating NSCPs as common nonlinear programs (NLPs), with the aid of squared slack variables. Through this connection, we show how to obtain second-order optimality conditions for NSCPs in an easy manner, thus bypassing a number of difficulties associated to the usual variational analytical approach. We then discuss several aspects of this connection. In particular, we show a “sharp” criterion for membership in a symmetric cone that also encodes rank information. Also, we discuss the possibility of importing convergence results from nonlinear programming to NSCPs, which we illustrate by discussing a simple augmented Lagrangian method for nonlinear symmetric cones. We show that, employing the slack variable approach, we can use the results for NLPs to prove convergence results, thus extending a special case (i.e., the case with strict complementarity) of an earlier result by Sun et al. [Sun D, Sun J, Zhang L (2008) The rate of convergence of the augmented Lagrangian method for nonlinear semidefinite programming. Math. Programming 114(2):349–391] for nonlinear semidefinite programs.
1	Data-driven segmentation is the powerhouse behind the success of online advertising. Various underlying challenges for successful segmentation have been studied by the academic community, with one notable exception—consumers’ incentives have been typically ignored. This lacuna is troubling, as consumers have much control over the data being collected. Missing or manipulated data could lead to inferior segmentation. The current work proposes a model of prior-free segmentation, inspired by models of facility location and, to the best of our knowledge, provides the first segmentation mechanism that addresses incentive compatibility, efficient market segmentation, and privacy in the absence of a common prior.
1	In this paper, we study the long-standing open question regarding the computational complexity of one of the core problems in supply chains management, the periodic joint replenishment problem. This problem has received a lot of attention over the years, and many heuristic and approximation algorithms have been suggested. However, in spite of the vast effort, the complexity of the problem remains unresolved. In this paper, we provide a proof that the problem is indeed strongly 𝒩𝒫-hard.
1	In the context of sparse recovery, it is known that most of the existing regularizers such as ℓ1 suffer from some bias incurred by some leading entries (in magnitude) of the associated vector. To neutralize this bias, we propose a class of models with partial regularizers for recovering a sparse solution of a linear system. We show that every local minimizer of these models is substantially sparse or the magnitude of all of its nonzero entries is above a uniform constant depending only on the data of the linear system. Moreover, for a class of partial regularizers, any global minimizer of these models is a sparsest solution to the linear system. We also establish some sufficient conditions for local or global recovery of the sparsest solution to the linear system, among which one of the conditions is weaker than the best-known restricted isometry property condition for sparse recovery by ℓ1. In addition, a first-order augmented Lagrangian (FAL) method is proposed for solving these models, in which each subproblem is solved by a nonmonotone proximal gradient (NPG) method. Despite the complication of the partial regularizers, we show that each proximal subproblem in NPG can be solved as a certain number of one-dimensional optimization problems, which usually have a closed-form solution. We also show that any accumulation point of the sequence generated by FAL is a first-order stationary point of the models. Numerical results on compressed sensing and sparse logistic regression demonstrate that the proposed models substantially outperform the widely used ones in the literature in terms of solution quality.
1	We consider the problem of minimizing a continuous function f over a compact set K. We compare the hierarchy of upper bounds proposed by Lasserre [Lasserre JB (2011) A new look at nonnegativity on closed sets and polynomial optimization. SIAM J. Optim. 21(3):864–885] to bounds that may be obtained from simulated annealing. We show that, when f is a polynomial and K a convex body, this comparison yields a faster rate of convergence of the Lasserre hierarchy than what was previously known in the literature.
1	We propose a new proximal path-following framework for a class of constrained convex problems. We consider settings where the nonlinear—and possibly nonsmooth—objective part is endowed with a proximity operator, and the constraint set is equipped with a self-concordant barrier. Our approach relies on the following two main ideas. First, we reparameterize the optimality condition as an auxiliary problem, such that a good initial point is available; by doing so, a family of alternative paths toward the optimum is generated. Second, we combine the proximal operator with path-following ideas to design a single-phase, proximal path-following algorithm. We prove that our algorithm has the same worst-case iteration complexity bounds as in standard path-following methods from the literature but does not require an initial phase. Our framework also allows inexactness in the evaluation of proximal Newton directions, without sacrificing the worst-case iteration complexity. We demonstrate the merits of our algorithm via three numerical examples, where proximal operators play a key role.
1	We study a model of stochastic evolutionary game dynamics in which the probabilities that agents choose suboptimal actions are dependent on payoff consequences. We prove a sample path large deviation principle, characterizing the rate of decay of the probability that the sample path of the evolutionary process lies in a prespecified set as the population size approaches infinity. We use these results to describe excursion rates and stationary distribution asymptotics in settings where the mean dynamic admits a globally attracting state, and we compute these rates explicitly for the case of logit choice in potential games.
1	In this paper we propose an approach to investigate a model of consumption and investment with a mandatory retirement date and early retirement option; we analyze properties of the optimal strategy and thereby contribute to understanding the interaction between retirement, consumption, and portfolio decisions in the presence of both the important features of retirement. In particular, we provide a characterization of the threshold of wealth as a function of time, and we show that it is strictly decreasing near the mandatory retirement date. The threshold is similar to the early exercise boundary of an American option in the sense that if the agent’s wealth is above or equal to the threshold level, then the agent immediately retires. We also provide comparative static analysis.
1	It is well known that the gradient descent algorithm converges linearly when applied to a strongly convex function with Lipschitz gradient. In this case, the algorithm’s rate of convergence is determined by the condition number of the function. In a similar vein, it has been shown that a variant of the Frank–Wolfe algorithm with away steps converges linearly when applied to a strongly convex function with Lipschitz gradient over a polytope. In a nice extension of the unconstrained case, the algorithm’s rate of convergence is determined by the product of the condition number of the function and a certain condition number of the polytope. We shed new light on the latter type of polytope conditioning. In particular, we show that previous and seemingly different approaches to define a suitable condition measure for the polytope are essentially equivalent to each other. Perhaps more interesting, they can all be unified via a parameter of the polytope that formalizes a key premise linked to the algorithm’s linear convergence. We also give new insight into the linear convergence property. For a convex quadratic objective, we show that the rate of convergence is determined by a condition number of a suitably scaled polytope.
1	Discrete approximation of probability distributions is an important topic in stochastic programming. In this paper, we extend the research on this topic to distributionally robust optimization (DRO), where discretization is driven by either limited availability of empirical data (samples) or a computational need for improving numerical tractability. We start with a one-stage DRO where the ambiguity set is defined by generalized prior moment conditions and quantify the discrepancy between the discretized ambiguity set and the original one by employing the Kantorovich/Wasserstein metric. The quantification is achieved by establishing a new form of Hoffman’s lemma for moment problems under a general class of metrics—namely, ζ-structures. We then investigate how the discrepancy propagates to the optimal value in one-stage DRO and discuss further the multistage DRO under nested distance. The technical results lay down a theoretical foundation for various discrete approximation schemes to be applied to solve one-stage and multistage distributionally robust optimization problems.
1	We consider a nonatomic selfish routing model with independent stochastic travel times for each edge, represented by mean and variance latency functions that depend on edge flows. This model can apply to traffic in the Internet or in a road network. Variability negatively impacts packets or drivers by introducing jitter in transmission delays, which lowers quality of streaming audio or video, or by making it more difficult to predict the arrival time at destination. At equilibrium, agents may select paths that do not minimize the expected latency so as to obtain lower variability. A social planner, who is likely to be more risk neutral than agents because it operates at a longer time scale, quantifies social cost with the total expected delay along routes. From that perspective, agents may make suboptimal decisions that degrade long-term quality. We define the price of risk aversion (PRA) as the worst-case ratio of the social cost at a risk-averse Wardrop equilibrium to that where agents are risk neutral. This inefficiency metric captures the degradation of system performance caused by variability and risk aversion.For networks with general delay functions and a single source–sink pair, we first show upper bounds for the PRA that depend linearly on the agents’ risk tolerance and on the degree of variability present in the network. We call these bounds structural, as they depend on the structure of the network. To get this result, we rely on a combinatorial proof that employs alternating paths that are reminiscent of those used in max-flow algorithms. For series-parallel graphs, the PRA becomes independent of the network topology and its size. Next, we provide tight and asymptotically tight lower bounds on the PRA by showing a family of structural lower bounds, which grow linearly with the number of nodes in the graph and players’ risk aversion. These are tight for graph sizes that are powers of 2. After that, by focusing on restricting the set of allowable mean latency and variance functions, we derive functional bounds on the PRA that are asymptotically tight and depend on the allowed latency functions but not on the topology. The functional bounds match the price-of-anarchy bounds for congestion games multiplied by an extra factor that accounts for risk aversion. Finally, we turn to the mean-standard deviation user objective—a much more complex model of risk aversion because the cost of a path is nonadditive over edge costs—and provide tight bounds for instances that admit alternating paths with one or two forward subpaths.
1	We describe a Markov chain Monte Carlo method to approximately simulate a centered d-dimensional Gaussian vector X with given covariance matrix. The standard Monte Carlo method is based on the Cholesky decomposition, which takes cubic time and has quadratic storage cost in d. By contrast, the additional storage cost of our algorithm is linear in d. We give a bound on the quadratic Wasserstein distance between the distribution of our sample and the target distribution. Our method can be used to estimate the expectation of h(X), where h is a real-valued function of d variables. Under certain conditions, we show that the mean square error of our method is inversely proportional to its running time. We also prove that, under suitable conditions, the total time needed by our method to obtain a given standardized mean square error is quadratic or nearly quadratic in d. A numerical example is given.
1	MacMahon’s Partition Analysis (MPA) is a combinatorial tool used in partition analysis to describe the solutions of a linear diophantine system. We show that MPA is useful in the context of weighted voting games. We introduce a new generalized generating function that gives, as special cases, extensions of the generating functions of the Banzhaf, Shapley-Shubik, Banzhaf-Owen, symmetric coalitional Banzhaf, and Owen power indices. Our extensions involve any coalition formation related to a linear diophantine system and multiple voting games. In addition, we show that a combination of ideas from MPA and Clifford algebras is useful in constructing generating functions for coalition configuration power indices. Finally, a brief account on how to design voting systems via MPA is advanced. More precisely, we obtain new generating functions that give, for fixed coalitions, all the distribution of weights of the players of the voting game such that a given player swings or not.
1	We consider a Nash equilibrium between two high-frequency traders (HFTs) in a simple market impact model with transient price impact and additional quadratic transaction costs. We prove existence and uniqueness of the Nash equilibrium and show that, for small transaction costs, the HFTs engage in a “hot potato game,” in which the same asset position is sold back and forth. We then identify a critical value for the size of the transaction costs above, for which all oscillations disappear and strategies become buy only or sell only. Numerical simulations show that, for both traders, the expected costs can be lower with transaction costs than without. Moreover, the costs can increase with the trading frequency if there are no transaction costs but decrease with the trading frequency if transaction costs are sufficiently high. We argue that these effects occur due to the need for protection against predatory trading in the regime of low transaction costs.
1	Following up a recent work by Ashlagi, Kanoria, and Leshno, we study a stable matching problem with unequal side sizes, n “men” and N > n “women,” whose preferences for a partner are uniformly random and independent. An asymptotic formula for the expected number of stable matchings is obtained. In particular, for N = n + 1 this number is close to n/(e log n), in notable contrast with (n log n)/e, the formula for the balanced case N = n that we obtained in 1988. We associate with each stable matching ℳ the parameters 𝒲(ℳ) and ℋ(ℳ), which are the total rank of “wives” and the total rank of “husbands,” as ranked by their “spouses” in ℳ. We found the deterministic parameters w(n, N) and h(n, N) such that the set of scaled pairs (𝒲(ℳ)/w(n, N), ℋ(ℳ)/h(n, N)) converges to a single point. In particular, w(n, n + 1) ∼ n log n, h(n, n + 1) ∼ n2/log n. To compare, for the balanced case n = N we previously found that w(n, n) = h(n, n) = n3/2, and that the pairs of scaled total ranks converged to a hyperbolic arc xy = 1, connecting the rank pairs of two extreme stable matchings, men-optimal and women-optimal. We also show that the expected fraction of persons with more than one stable spouse is vanishingly small if 𝑁−𝑛≫𝑛√N−n≫n.
1	The vertex cover problem is one of the most important and intensively studied combinatorial optimization problems. Khot and Regev [Khot S, Regev O (2008) Vertex cover might be hard to approximate to within 2 − ɛ. J. Comput. System Sci. 74(3):335–349] proved that the problem is NP-hard to approximate within a factor 2 − ɛ, assuming the unique games conjecture (UGC). This is tight because the problem has an easy 2-approximation algorithm. Without resorting to the UGC, the best inapproximability result for the problem is due to Dinur and Safra [Dinur I, Safra S (2005) On the hardness of approximating minimum vertex cover. Ann. Math. 162(1):439–485]: vertex cover is NP-hard to approximate within a factor 1.3606.We prove the following unconditional result about linear programming (LP) relaxations of the problem: every LP relaxation that approximates the vertex cover within a factor 2 − ɛ has super-polynomially many inequalities. As a direct consequence of our methods, we also establish that LP relaxations (as well as semidefinite programming relaxations) that approximate the independent set problem within any constant factor have a super-polynomial size.
1	As one of the most plausible convex optimization methods for sparse data reconstruction, ℓ1-minimization plays a fundamental role in the development of sparse optimization theory. The stability of this method has been addressed in the literature under various assumptions such as the restricted isometry property, null space property, and mutual coherence. In this paper, we propose a unified means to develop the so-called weak stability theory for ℓ1-minimization methods under the condition called the weak range space property of a transposed design matrix, which turns out to be a necessary and sufficient condition for the standard ℓ1-minimization method to be weakly stable in sparse data reconstruction. The reconstruction error bounds established in this paper are measured by the so-called Robinson’s constant. We also provide a unified weak stability result for standard ℓ1-minimization under several existing compressed sensing matrix properties. In particular, the weak stability of this method under the constant-free range space property of the transposed design matrix is established, to our knowledge, for the first time in this paper. Different from the existing analysis, we utilize the classic Hoffman’s lemma concerning the error bound of linear systems as well as Dudley’s theorem concerning the polytope approximation of the unit ball to show that ℓ1-minimization is robustly and weakly stable in recovering sparse data from inaccurate measurements.
1	Ideally, the properties of an economic mechanism should hold in a robust way across multiple equilibria and under varying assumptions regarding the information available to participants. Focusing on the design of robust position auctions, we seek mechanisms that possess an efficient equilibrium and guarantee high revenue in every efficient equilibrium, under complete and incomplete information. A generalized first-price auction that is expressive in the sense of allowing multidimensional bids turns out to be the only standard design able to achieve this goal, even when valuations are one dimensional. The equilibria under complete information are obtained via Bernheim and Whinston’s profit target strategies, those under incomplete information via an appropriate generalization thereof. Particularly interesting from a technical perspective is the incomplete information case, where the standard technique for establishing equilibrium existence due to Myerson is generalized to a setting in which the bid space has higher dimension than the valuation space.
1	We consider a game where a finite number of retailers choose a location, given that their potential consumers are distributed on a network. Retailers do not compete on price but only on location, therefore each consumer shops at the closest store. We show that when the number of retailers is large enough, the game admits a pure Nash equilibrium and we construct it. We then compare the equilibrium cost borne by the consumers with the cost that could be achieved if the retailers followed the dictate of a benevolent planner. We perform this comparison in terms of the Price of Anarchy (i.e., the ratio of the worst equilibrium cost and the optimal cost) and the Price of Stability (i.e., the ratio of the best equilibrium cost and the optimal cost). We show that, asymptotically in the number of retailers, these ratios are bounded by two and one, respectively.
1	We consider stochastic variational inequalities (VIs) with monotone operators where the feasible set is an intersection of a large number of convex sets. We propose a stochastic approximation method with incremental constraint projections, meaning that a projection method is taken after the random operator is sampled and a component of the feasible set is randomly chosen. Such a sequential scheme is well suited for large-scale online and distributed learning. First, we assume that the VI is weak sharp. We provide asymptotic convergence, infeasibility rate of O(1/k) in terms of the squared distance to the feasible set, and solvability rate of 𝑂(1/𝑘‾‾√)O(1/k) in terms of the distance to the solution set for a bounded or unbounded set. Then, we assume just a monotone operator and introduce an explicit iterative Tykhonov regularization to the method. We consider Cartesian VIs so as to encompass the distributed solution of multiagent problems under a limited coordination. We provide asymptotic convergence, infeasibility rate of O(1/k) in terms of the squared distance to the feasible set and, in the case of a compact feasible set (with possibly unbounded components), we obtain a near-optimal solvability convergence rate of 𝑂(𝑘𝛿/𝑘‾‾√)O(kδ/k) in terms of the dual gap function for any small 𝛿∈(0,12)δ∈(0,12).
1	We prove that the Cramér transform of the uniform measure on a convex body in ℝn is a (1 + o(1)) n-self-concordant barrier, improving a seminal result of Nesterov and Nemirovski. This gives the first explicit construction of a universal barrier for convex bodies with optimal self-concordance parameter. The proof is based on basic geometry of log-concave distributions and elementary duality in exponential families. As a side result, our calculations also show that the universal barrier of Nesterov and Nemirovski is exactly n-self-concordant on convex cones.
1	We study a model of two-player, zero-sum, stopping games with asymmetric information. We assume that the payoff depends on two independent continuous-time Markov chains, where the first Markov chain is only observed by player 1 and the second Markov chain is only observed by player 2, implying that the players have access to stopping times with respect to different filtrations. We show the existence of a value in mixed stopping times and provide a variational characterization for the value as a function of the initial distribution of the Markov chains. We also prove a verification theorem for optimal stopping rules, which allows to construct optimal stopping times. Finally we use our results to solve explicitly two generic examples.
1	This paper presents a new approximation formula for pricing multidimensional discretely monitored average options in a local-stochastic volatility (LSV) model with jump by applying an asymptotic expansion technique. Moreover, it provides a justification of the approximation method with some asymptotic error estimates for general payoff functions. Particularly, our model includes local volatility functions and jump components in the underlying asset price as well as its volatility processes. To the best of our knowledge, the proposed approximation is the first one that achieves analytic approximations for the average option prices in this environment.In numerical experiments, by employing several models, we provide approximate prices for the listed average and calendar spread options on the West Texas Intermediate (WTI) futures based on the parameters through calibration to the listed (plain-vanilla) futures options prices. Then, we compare those with the Chicago Mercantile Exchange (CME) settlement prices, which confirms the validity of the method.Moreover, we show that the LSV with jump model is able to replicate consistently and precisely listed futures option, calendar spread option, and average option prices with common parameters.
1	In this paper, we propose unbiased sensitivity estimators of the expected functionals of one-dimensional diffusion processes. Under general diffusion models, it is common to rely on discretization methods such as the Euler scheme for the generation of sample paths because of the lack of knowledge in the probability distributions associated with the diffusions. The Euler discretization method is easy to apply, but it is difficult to avoid discretization biases. As an alternative approach, we propose unbiased Monte Carlo estimators of sensitivities by taking advantage of the Beskos-Roberts method, which is an exact simulation algorithm for one-dimensional stochastic differential equations (SDEs), and applying the Poisson kernel method. The proposed estimators can be computed by discretely observed Brownian paths, and thus it is simple to implement our algorithms. We illustrate the ideas and algorithms with examples.
1	We study the stochastic versions of a broad class of combinatorial problems where the weights of the elements in the input data set are uncertain. The class of problems that we study includes shortest paths, minimum weight spanning trees, minimum weight matchings, and other combinatorial problems like knapsack. We observe that the expected value is inadequate in capturing different types of risk-averse or risk-prone behaviors, and, instead, we consider a more general objective, which is to maximize the expected utility of the solution for some given utility function, rather than the expected weight (expected weight becomes a special case). Under the assumption that there is a pseudopolynomial-time algorithm for the exact version of the problem (this is true for the problems mentioned above),1 we can obtain the following approximation results for several important classes of utility functions: If the utility function μ is continuous, upper bounded by a constant and μ(x) approaches 0 as x approaches infinity, we show that we can obtain a polynomial-time approximation algorithm with an additive errorε for any constant ε > 0.If the utility function μ is a concave increasing function, we can obtain a polynomial-time approximation scheme (PTAS).If the utility function μ is increasing and has a bounded derivative, we can obtain a PTAS.Our results recover or generalize several prior results on stochastic shortest-path, stochastic spanning tree, and stochastic knapsack. Our algorithm for utility maximization makes use of the separability of exponential utility and a technique to decompose a general utility function into exponential utility functions, which may be useful in other stochastic optimization problems.
1	We revisit lower bounds on the regret in the case of multiarmed bandit problems. We obtain nonasymptotic, distribution-dependent bounds and provide simple proofs based only on well-known properties of Kullback–Leibler divergences. These bounds show in particular that in the initial phase the regret grows almost linearly, and that the well-known logarithmic growth of the regret only holds in a final phase. The proof techniques come to the essence of the information-theoretic arguments used and they involve no unnecessary complications.
1	We study a static game played by a finite number of agents, in which agents are assigned independent and identically distributed random types and each agent minimizes its objective function by choosing from a set of admissible actions that depends on its type. The game is anonymous in the sense that the objective function of each agent depends on the actions of other agents only through the empirical distribution of their type-action pairs. We study the asymptotic behavior of Nash equilibria, as the number of agents tends to infinity, first by deriving laws of large numbers characterizing almost sure limit points of Nash equilibria in terms of so-called Cournot-Nash equilibria of an associated nonatomic game. Our main results are large deviation principles that characterize the probability of rare Nash equilibria and associated conditional limit theorems describing the behavior of equilibria conditioned on a rare event. The results cover situations when neither the finite-player game nor the associated nonatomic game has a unique equilibrium. In addition, we study the asymptotic behavior of the price of anarchy, complementing existing worst-case bounds with new probabilistic bounds in the context of congestion games, which are used to model traffic routing in networks.
1	Let X be a one-dimensional diffusion and let g be a real-valued function depending on time and the value of X. This article analyzes the inverse optimal stopping problem of finding a time-dependent real-valued function π depending only on time such that a given stopping time τ⋆ is a solution of the stopping problem supτ𝔼[g(τ,Xτ)+π(τ)]sup𝜏𝔼[𝑔(𝜏,𝑋𝜏)+𝜋(𝜏)].Under regularity and monotonicity conditions, there exists such a transfer π if and only if τ⋆ is the first time when X exceeds a time-dependent barrier b. We prove uniqueness of the solution π and derive a closed form representation. The representation is based on an auxiliary process that is a version of the original diffusion X reflected at b towards the continuation region. The results lead to a new integral equation characterizing the stopping boundary b of the stopping problem supτ𝔼[g(τ,Xτ)]sup𝜏𝔼[𝑔(𝜏,𝑋𝜏)].
1	In this paper, we investigate a moral hazard problem in finite time with lump-sum and continuous payments, involving infinitely many agents with mean-field type interactions, hired by one principal. By reinterpreting the mean-field game faced by each agent in terms of a mean-field forward-backward stochastic differential equation (FBSDE), we are able to rewrite the principal’s problem as a control problem of the McKean-Vlasov stochastic differential equations. We review one general approach to tackling it, introduced recently using dynamic programming and Hamilton-Jacobi-Bellman (HJB for short) equations, and mention a second one based on the stochastic Pontryagin maximum principle. We solve completely and explicitly the problem in special cases, going beyond the usual linear-quadratic framework. We finally show in our examples that the optimal contract in the N-players’ model converges to the mean-field optimal contract when the number of agents goes to +∞.
1	Since most of the traded options on individual stocks are of American type, it is of interest to generalize the results obtained in semistatic trading to the case when one is allowed to statically trade American options. However, this problem has proved to be elusive so far because of the asymmetric nature of the positions of holding versus shorting such options. Here, we provide a unified framework and generalize the fundamental theorem of asset pricing (FTAP) and hedging dualities in Bayraktar and Zhou [Bayraktar E, Zhou Z (2016) Arbitrage, hedging and utility maximization using semi-static trading strategies with American options. Ann. Appl. Probab. 26(6):3531–3558.] to the case where the investor can also short American options. Following Bayraktar and Zhou [Bayraktar E, Zhou Z (2016) Arbitrage, hedging, and utility maximization using semistatic trading strategies with American options. Ann. Appl. Probab. 26(6):3531–3558.], we assume that the longed American options are divisible. As for the shorted American options, we show that divisibility plays no role regarding arbitrage property and hedging prices. Then, using the method of enlarging probability spaces proposed in Deng and Tan [Deng S, Tan X (2016) Duality in nondominated discrete-time models for Americain options. ArXiv e-prints.], we convert the shorted American options to European options and establish the FTAP and subhedging and superhedging dualities in the enlarged space both with and without model uncertainty.
1	We consider the single-period joint assortment and inventory planning problem with stochastic demand and dynamic substitution across products, motivated by applications in highly differentiated markets, such as online retailing and airlines. This class of problems is known to be notoriously hard to deal with from a computational standpoint. In fact, prior to the present paper, only a handful of modeling approaches were shown to admit provably good algorithms, at the cost of strong restrictions on customers’ choice outcomes. Our main contribution is to provide the first efficient algorithms with provable performance guarantees for a broad class of dynamic assortment optimization models. Under general rank-based choice models, our approximation algorithm is best possible with respect to the price parameters, up to lower-order terms. In particular, we obtain a constant-factor approximation under horizontal differentiation, where product prices are uniform. In more structured settings, where the customers’ ranking behavior is motivated by price and quality cues, we derive improved guarantees through tailor-made algorithms. In extensive computational experiments, our approach dominates existing heuristics in terms of revenue performance, as well as in terms of speed, given the myopic nature of our methods. From a technical perspective, we introduce a number of novel algorithmic ideas of independent interest, and unravel hidden relations to submodular maximization.
1	In this paper we provide a complete theoretical analysis of a two-dimensional degenerate nonconvex singular stochastic control problem. The optimisation is motivated by a storage-consumption model in an electricity market, and features a stochastic real-valued spot price modelled by Brownian motion. We find analytical expressions for the value function, the optimal control, and the boundaries of the action and inaction regions. The optimal policy is characterised in terms of two monotone and discontinuous repelling free boundaries, although part of one boundary is constant and the smooth fit condition holds there.
1	Networks in which the processing of jobs occurs both sequentially and in parallel are prevalent in many application domains, such as computer systems, healthcare, manufacturing, and project management. The parallel processing of jobs gives rise to synchronization constraints that can be a main reason for job delay. In comparison with feed-forward queueing networks that have only sequential processing of jobs, the approximation and control of networks that have synchronization constraints is less understood. One well-known modeling framework in which synchronization constraints are prominent is the fork-join processing network. Our objective is to find scheduling rules for fork-join processing networks with multiple job types in which there is first a fork operation, then activities that can be performed in parallel, and then a join operation. The difficulty is that some of the activities that can be performed in parallel require a shared resource. We solve the scheduling problem for that shared server (that is, which type of job to prioritize at any given time) when that server is in heavy traffic and prove an asymptotic optimality result.The e-companion is available at https://doi.org/10.1287/moor.2018.0935.
1	This paper deals with the problem of quantifying the impact of model misspecification when computing general expected values of interest. The methodology that we propose is applicable in great generality; in particular, we provide examples involving path-dependent expectations of stochastic processes. Our approach consists of computing bounds for the expectation of interest regardless of the probability measure used, as long as the measure lies within a prescribed tolerance measured in terms of a flexible class of distances from a suitable baseline model. These distances, based on optimal transportation between probability measures, include Wasserstein’s distances as particular cases. The proposed methodology is well suited for risk analysis and distributionally robust optimization, as we demonstrate with applications. We also discuss how to estimate the tolerance region nonparametrically using Skorokhod-type embeddings in some of these applications.
1	We study a multiperiod network revenue management problem where a seller sells multiple products made from multiple resources with finite capacity in an environment where the underlying demand function is a priori unknown (in the nonparametric sense). The objective of the seller is to simultaneously learn the unknown demand function and dynamically price the products to minimize the expected revenue loss. For the problem where the number of selling periods and initial capacity are scaled by k>0𝑘>0, it is known that the expected revenue loss of any non-anticipating pricing policy is Ω(√k)Ω(𝑘‾‾√). However, there is a considerable gap between this theoretical lower bound and the performance bound of the best-known heuristic control in the literature. In this paper, we propose a nonparametric self-adjusting control and show that its expected revenue loss is O(k1/2+ϵlogk)𝒪(𝑘1/2+𝜖log𝑘) for any arbitrarily small ϵ>0𝜖>0, provided that the underlying demand function is sufficiently smooth. This is the tightest bound of its kind for the problem setting that we consider in this paper, and it significantly improves the performance bound of existing heuristic controls in the literature. In addition, our intermediate results on the large deviation bounds for spline estimation and nonparametric stability analysis of constrained optimization are of independent interest and are potentially useful for other applications.The online appendix is available at https://doi.org/10.1287/moor.2018.0937.
1	In this paper, we consider the linearly constrained composite convex optimization problem, whose objective is a sum of a smooth function and a possibly nonsmooth function. We propose an inexact augmented Lagrangian (IAL) framework for solving the problem. The stopping criterion used in solving the augmented Lagrangian subproblem in the proposed IAL framework is weaker and potentially much easier to check than the one used in most of the existing IAL frameworks/methods. We analyze the global convergence and the nonergodic convergence rate of the proposed IAL framework. Preliminary numerical results are presented to show the efficiency of the proposed IAL framework and the importance of the nonergodic convergence and convergence rate analysis.
1	The probabilistic bisection algorithm (PBA) solves a class of stochastic root-finding problems in one dimension by successively updating a prior belief on the location of the root based on noisy responses to queries at chosen points. The responses indicate the direction of the root from the queried point and are incorrect with a fixed probability. The fixed-probability assumption is problematic in applications, and so we extend the PBA to apply when this assumption is relaxed. The extension involves the use of a power-one test at each queried point. We explore the convergence behavior of the extended PBA, showing that it converges at a rate arbitrarily close to, but slower than, the canonical “square root” rate of stochastic approximation.
1	We study the classical multiperiod capacitated stochastic inventory control problems in a data-driven setting. Instead of assuming full knowledge of the demand distributions, we assume that the demand distributions can only be accessed through drawing random samples. Such data-driven models are ubiquitous in practice, where the cumulative distribution functions of the underlying random demand are either unavailable or too complex to work with. We consider the sample average approximation (SAA) method for the problem and establish an upper bound on the number of samples needed for the SAA method to achieve a near-optimal expected cost, under any level of required accuracy and prespecified confidence probability. The sample bound is polynomial in the number of time periods as well as the confidence and accuracy parameters. Moreover, the bound is independent of the underlying demand distributions. However, the SAA requires solving the SAA problem, which is #P-hard. Thus, motivated by the SAA analysis, we propose a polynomial time approximation scheme that also uses polynomially many samples. Finally, we establish a lower bound on the number of samples required to solve this data-driven newsvendor problem to near-optimality.
1	We provide the first perfect sampling algorithm for a generalized Jackson network of first-in, first-out queues under arbitrary topology and non-Markovian assumptions on the input of the network. We assume (in addition to stability) that the interarrival and service times of customers have a finite moment-generating function in a neighborhood of the origin, and the interarrival times have unbounded support.
1	Projection algorithms are well known for their simplicity and flexibility in solving feasibility problems. They are particularly important in practice owing to minimal requirements for software implementation and maintenance. In this work, we study linear convergence of several projection algorithms for systems of finitely many closed sets. The results complement contemporary research on the same topic.
1	We consider a multidimentional Brownian control problem (BCP) with model uncertainty that formally emerges from a multiclass M/M/1 queueing control problem under heavy traffic with model uncertainty. The BCP is formulated as a multidimensional stochastic differential game with two players: a minimizer who has an equivalent role to the decision maker in the queueing control problem and a maximizer whose role is to set up the uncertainty of the model. The dynamics are driven by a Brownian motion. We show that a state-space collapse properly holds. That is, the multidimensional BCP can be reduced to a one-dimensional BCP with model uncertainty that also takes the form of a two-player stochastic differential game. Then, the value function of both games is characterized as the unique solution to a free-boundary problem from which we extract equilibria for both games. Finally, we analyze the dependence of the value function and the equilibria on the ambiguity parameters.
1	We consider semivalues on pM∞—a vector space of games with a continuum of players (among which there may be atoms) that possess a robust differentiability feature. We introduce the notion of a derivative semivalue on pM∞ and extend the standard Banzhaf value from the domain of finite games onto pM∞ as a certain particularly simple derivative semivalue. Our main result shows that any semivalue on pM∞ is a derivative semivalue. It is also shown that the Banzhaf value is the only semivalue on pM∞ that satisfies a version of the composition property of Owen and that, in addition, is nonzero for all nonzero monotonic finite games.
1	While the computational complexity of many game-theoretic solution concepts, notably Nash equilibrium, has now been settled, the question of determining the exact complexity of computing an evolutionarily stable strategy has resisted solution since attention was drawn to it in 2004. In this paper, I settle this question by proving that deciding the existence of an evolutionarily stable strategy is Σ𝑃2Σ2P complete.
1	A framework is presented for constructing strong mixed-integer programming formulations for logical disjunctive constraints. This approach is a generalization of the logarithmically sized formulations of Vielma and Nemhauser for special ordered sets of type 2 (SOS2) constraints, and a complete characterization of its expressive power is offered. The framework is applied to a variety of disjunctive constraints, producing novel small and strong formulations for outer approximations of multilinear terms, generalizations of special ordered sets, piecewise linear functions over a variety of domains, and obstacle avoidance constraints.
1	We consider a single-server queue that serves a finite population of 𝑛n customers that will enter the queue (require service) only once, also known as the Δ(𝑖)/𝐺/1Δ(i)/G/1 queue. This paper presents a method for analyzing heavy-traffic behavior by using uniform acceleration, which simultaneously lets 𝑛n and the service rate grow large, while the initial resource utilization approaches one. A key feature of the model is that, as time progresses, more customers have joined the queue, and fewer customers can potentially join. This diminishing population gives rise to a class of reflected stochastic processes that vanish over time and hence do not have a stationary distribution. We establish that, when the arrival times are exponentially distributed, by suitably rescaling space and time, the queue-length process converges to a Brownian motion with a negative quadratic drift, a stochastic-process limit that captures the effect of the diminishing population. When the arrival times are generally distributed, our techniques provide information on the typical queue length and the first busy period.
1	Facility location games have been a topic of major interest in economics, operations research, and computer science, starting from the seminal work by Hotelling [Hotelling H (1929) Stability in competition. Econom. J. 39(153):41–57]. In the classical pure location Hotelling game, businesses compete for maximizing customer attraction by strategically locating their facilities, assuming no price competition, and customers are attracted to their closest facilities. Surprisingly, very little rigorous work has been presented on multiunit facility location games, where the classical pure location Hotelling games are extended to contexts where each player is to locate several facilities. In this paper, we present two major contributions to the study of multiunit pure location Hotelling games. In the first part of this paper, we deal with the two-player multiunit setting and fully characterize its equilibria. In the second part of this paper, we deal with multiunit facility location games, with 𝑁≥3N≥3 players; our main result in this part is the full characterization of the settings where pure strategy equilibria exist. Our results also extend classical results on necessary and sufficient conditions for a strategy profile to be in equilibrium in a pure location (single-unit) Hotelling game to multiunit pure location games.
1	We consider fundamental properties of stochastic loss networks, seeking to improve on the so-called Erlang fixed-point approximation. We propose a family of mathematical approximations for estimating the stationary loss probabilities and show that they always converge exponentially fast, provide asymptotically exact results, and yield greater accuracy than the Erlang fixed-point approximation. We further derive structural properties of the inverse of the classical Erlang loss function that characterize the region of capacities that ensures a workload is served within a set of loss probabilities. We then exploit these results to efficiently solve a general class of stochastic optimization problems involving loss networks. Computational experiments investigate various issues of both theoretical and practical interest, and demonstrate the benefits of our approach.
1	We propose a class of strongly efficient rare-event simulation estimators for random walks and compound Poisson processes with a regularly varying increment/jump-size distribution in a general large deviations regime. Our estimator is based on an importance sampling strategy that hinges on a recently established heavy-tailed sample-path large deviations result. The new estimators are straightforward to implement and can be used to systematically evaluate the probability of a wide range of rare events with bounded relative error. They are “universal” in the sense that a single importance sampling scheme applies to a very general class of rare events that arise in heavy-tailed systems. In particular, our estimators can deal with rare events that are caused by multiple big jumps (therefore, beyond the usual principle of a single big jump) as well as multidimensional processes such as the buffer content process of a queueing network. We illustrate the versatility of our approach with several applications that arise in the context of mathematical finance, actuarial science, and queueing theory.
1	Consider a storage system where the content is driven by a Brownian motion in the absence of control. At any time, one may increase or decrease the content at a cost proportional to the amount of adjustment. A decrease of the content takes effect immediately, while an increase is realized after a fixed lead time ℓ. Holding costs are incurred continuously over time and are a convex function of the content. The objective is to find a control policy that minimizes the expected present value of the total costs. Because of the positive lead time for upward adjustments, one needs to keep track of all of the outstanding upward adjustments as well as the actual content at time 𝑡 as there may also be downward adjustments during [𝑡,𝑡+ℓ)—that is, the state of the system is a function on [0,ℓ]. We first extend the concept of 𝐿♮-convexity to function spaces and establish the 𝐿♮-convexity of the optimal cost function. We then derive various properties of the cost function and identify the structure of the optimal policy as a state-dependent two-sided reflection mapping making the minimum amount of adjustment necessary to keep the system states within a certain region.
1	We consider an incomplete market with a nontradable stochastic factor and a continuous-time investment problem with an optimality criterion based on monotone mean-variance preferences. We formulate it as a stochastic differential game problem and use Hamilton–Jacobi–Bellman–Isaacs equations to find an optimal investment strategy and the value function. What is more, we show that our solution is also optimal for the classical Markowitz problem, and every optimal solution for the classical Markowitz problem is optimal also for the monotone mean-variance preferences. These results are interesting because the original Markowitz functional is not monotone, and it was observed that in the case of a static one-period optimization problem, the solutions for those two functionals (monotone mean variance and classical Markowitz) are different. In addition, we determine explicit Markowitz strategies in the square root factor models.
1	The study of combinatorial optimization problems with submodular objectives has attracted much attention in recent years. Such problems are important in both theory and practice because their objective functions are very general. Obtaining further improvements for many submodular maximization problems boils down to finding better algorithms for optimizing a relaxation of them known as the multilinear extension. In this work, we present an algorithm for optimizing the multilinear relaxation whose guarantee improves over the guarantee of the best previous algorithm (by Ene and Nguyen). Moreover, our algorithm is based on a new technique that is, arguably, simpler and more natural for the problem at hand. In a nutshell, previous algorithms for this problem rely on symmetry properties that are natural only in the absence of a constraint. Our technique avoids the need to resort to such properties, and thus seems to be a better fit for constrained problems.
1	Establishing the existence of Nash equilibria for partially observed stochastic dynamic games is known to be quite challenging, with the difficulties stemming from the noisy nature of the measurements available to individual players (agents) and the decentralized nature of this information. When the number of players is sufficiently large and the interactions among agents is of the mean-field type, one way to overcome this challenge is to investigate the infinite-population limit of the problem, which leads to a mean-field game. In this paper, we consider discrete-time partially observed mean-field games with infinite-horizon discounted-cost criteria. Using the technique of converting the original partially observed stochastic control problem to a fully observed one on the belief space and the dynamic programming principle, we establish the existence of Nash equilibria for these game models under very mild technical conditions. Then, we show that the mean-field equilibrium policy, when adopted by each agent, forms an approximate Nash equilibrium for games with sufficiently many agents.
1	We develop a robust framework for pricing and hedging of derivative securities in discrete-time financial markets. We consider markets with both dynamically and statically traded assets and make minimal measurability assumptions. We obtain abstract (pointwise) fundamental theorem of asset pricing and pricing–hedging duality. Our results are general and, in particular, cover both the so-called model independent case as well as the classical probabilistic case of Dalang–Morton–Willinger. Our analysis is scenario-based: a model specification is equivalent to a choice of scenarios to be considered. The choice can vary between all scenarios and the set of scenarios charged by a given probability measure. In this way, our framework interpolates between a model with universally acceptable broad assumptions and a model based on a specific probabilistic view of future asset dynamics.
1	We study an optimal auction problem for selecting a subset of agents to receive an item or service, whereby each agent’s service can be configured, the agent has multidimensional preferences over configurations, and there is a limit on the number of agents that can be simultaneously served. We give a polynomial time reduction from the multiagent problem to appropriately defined single-agent problems. We further generalize the setting to matroid feasibility constraints and obtain exact and approximately optimal reductions. As applications of this reduction we give polynomial time algorithms for the problem with quasi-linear preferences over configurations or with private budgets. Our approach is to characterize, and in polynomial time optimize and implement feasible interim allocation rules. With a single item, we give a new characterization showing that any mechanism has an ex post implementation as a simple token-passing process. These processes can be parameterized and optimized with a quadratic number of linear constraints. With multiple items, we generalize Border’s characterization and give algorithms for optimizing interim and implementing ex post allocation rules. These implementations have a simple form; they are randomizations over greedy mechanisms that serve types in a given order.
1	A symmetric tensor is completely positive (CP) if it is a sum of tensor powers of nonnegative vectors. This paper characterizes completely positive binary tensors. We show that a binary tensor is completely positive if and only if it satisfies two linear matrix inequalities. This result can be used to determine whether a binary tensor is completely positive or not. When it is, we give an algorithm for computing its cp-rank and the decomposition. When the order is odd, we show that the cp-rank decomposition is unique. When the order is even, we completely characterize when the cp-rank decomposition is unique. We also discuss how to compute the nearest cp-approximation when a binary tensor is not completely positive.
1	We study the computational complexity of finding stable outcomes in hedonic games, which are a class of coalition formation games. We restrict our attention to symmetric additively separable hedonic games, which are a nontrivial subclass of such games that are guaranteed to possess stable outcomes. These games are specified by undirected edge-weighted graphs: nodes are players, an outcome of the game is a partition of the nodes into coalitions, and the utility of a node is the sum of incident edge weights in the same coalition. We consider several stability requirements defined in the literature. These are based on restricting feasible player deviations, for example, by giving existing coalition members veto power. We extend these restrictions by considering more general forms of preference aggregation for coalition members. In particular, we consider voting schemes to decide whether coalition members will allow a player to enter or leave their coalition. For all of the stability requirements we consider, the existence of a stable outcome is guaranteed by a potential function argument, and local improvements will converge to a stable outcome. We provide an almost complete characterization of these games in terms of the tractability of computing such stable outcomes. Our findings comprise positive results in the form of polynomial-time algorithms and negative results in the form of proofs of polynomial local search (PLS)–hardness. The negative results extend to more general hedonic games.
1	We study Cournot competition among firms in a networked marketplace that is centrally managed by a market maker. In particular, we study a situation in which a market maker facilitates trade between geographically separate markets via a constrained transport network. Our focus is on understanding the consequences of the design of the market maker and providing tools for optimal design. To that end, we provide a characterization of the equilibrium outcomes of the game between the firms and the market maker. Our results highlight that the equilibrium structure is impacted dramatically by the market maker’s objective—depending on the objective, there may be a unique equilibrium, multiple equilibria, or no equilibria. Furthermore, the game may be a potential game (as in the case of classical Cournot competition) or not. Beyond characterizing the equilibria of the game, we provide an approach for designing the market maker to optimize a design objective (e.g., social welfare) at the equilibrium of the game. Additionally, we use our results to explore the value of transport (trade) and the efficiency of the market maker (compared with a single aggregate market).
1	A turnpike integer is the smallest finite horizon for which an optimal infinite horizon decision is the optimal initial decision. An important practical question considered in the literature is how to bound the turnpike integer using only the problem inputs. In this paper, we consider turnpike integers as a function of the discount factor. While a turnpike integer is finite for any fixed discount factor, we show that it approaches infinity in the neighborhood of a specific set of discount rates (for all but some exceptional finite Markov decision processes). We completely characterize this taboo set of discount factors and find necessary and sufficient conditions for a set of turnpike integers to be unbounded. This finding provides a cautionary tale for practitioners using point estimates of the discount factor to manage the length of rolling horizons by pointing to potential singularities in the procedure.
1	In this paper, we develop a general regularization-based continuous optimization framework for the maximum clique problem. In particular, we consider a broad class of regularization terms that can be included in the classic Motzkin–Straus formulation, and we develop conditions that guarantee the equivalence between the continuous regularized problem and the original one in both a global and a local sense. We further analyze, from a computational point of view, two different regularizers that satisfy the general conditions.
1	We establish sufficient conditions that ensure the uniqueness of Tarski-type fixed points of monotone operators. A first set of results relies on order concavity, whereas a second one uses subhomogeneity. A few applications that illustrate our results are presented.
1	In recent years, techniques based on convex optimization and real algebra that produce converging hierarchies of lower bounds for polynomial minimization problems have gained much popularity. At their heart, these hierarchies rely crucially on Positivstellensätze from the late 20th century (e.g., due to Stengle, Putinar, or Schmüdgen) that certify positivity of a polynomial on an arbitrary closed basic semialgebraic set. In this paper, we show that such hierarchies could in fact be designed from much more limited Positivstellensätze dating back to the early 20th century that only certify positivity of a polynomial globally. More precisely, we show that any inner approximation to the cone of positive homogeneous polynomials that is arbitrarily tight can be turned into a converging hierarchy of lower bounds for general polynomial minimization problems with compact feasible sets. This in particular leads to a semidefinite programming–based hierarchy that relies solely on Artin’s solution to Hilbert’s 17th problem. We also use a classical result from Pólya on global positivity of even forms to construct an “optimization-free” converging hierarchy for general polynomial minimization problems with compact feasible sets. This hierarchy requires only polynomial multiplication and checking nonnegativity of coefficients of certain fixed polynomials. As a corollary, we obtain new linear programming–based and second-order cone programming–based hierarchies for polynomial minimization problems that rely on the recently introduced concepts of diagonally dominant sum of squares and scaled diagonally dominant sum of squares polynomials. We remark that the scope of this paper is theoretical at this stage, as our hierarchies—though they involve at most two sum of squares constraints or only elementary arithmetic at each level—require the use of bisection and increase the number of variables (respectively, the degree) of the problem by the number of inequality constraints plus three (respectively, by a factor of two).
1	We study quantitative criteria for evaluating the strength of valid inequalities for Gomory and Johnson’s finite and infinite group models, and we describe valid inequalities that are optimal for these criteria. We justify and focus on the criterion of maximizing the volume of the nonnegative orthant cut off by a valid inequality. For the finite group model of prime order, we show that the unique maximizer is an automorphism of the Gomory mixed-integer (GMI) cut for a possibly different finite group problem of the same order. We extend the notion of volume of a simplex to the infinite-dimensional case. This is used to show that in the infinite group model, the GMI cut maximizes the volume of the nonnegative orthant cut off by an inequality.
1	The simplex algorithm for linear programming is based on the fact that any local optimum with respect to the polyhedral neighborhood is also a global optimum. We show that a similar result carries over to submodular maximization. In particular, every local optimum of a constrained monotone submodular maximization problem yields a 1/2-approximation, and we also present an appropriate extension to the nonmonotone setting. Moreover, we describe a very general local search procedure that applies to a wide range of constraint families and unifies as well as extends previous methods. In our framework, we match known approximation guarantees while disentangling and simplifying previous approaches. Moreover, despite its generality, we are able to show that our local search procedure is slightly faster than previous specialized methods. Furthermore, we negatively answer the open question whether a linear optimization oracle may be enough to obtain strong approximation algorithms for submodular maximization. We do this by providing an example of a constraint family on a ground set of size n for which, if only given a linear optimization oracle, any algorithm for submodular maximization with a polynomial number of calls to the linear optimization oracle has an approximation ratio of only O(logn/(√n·loglogn))𝑂(log𝑛/(√𝑛·loglog𝑛)).
1	We introduce a mean field game with rank-based reward: competing agents optimize their effort to achieve a goal, are ranked according to their completion time, and are paid a reward based on their relative rank. First, we propose a tractable Poissonian model in which we can describe the optimal effort for a given reward scheme. Second, we study the principal–agent problem of designing an optimal reward scheme. A surprising, explicit design is found to minimize the time until a given fraction of the population has reached the goal.
1	Jeroslow and Lowe gave an exact geometric characterization of subsets of ℝ𝑛Rn that are projections of mixed-integer linear sets, also known as MILP-representable or MILP-R sets. We give an alternate algebraic characterization by showing that a set is MILP-R if and only if the set can be described as the intersection of finitely many affine Chvátal inequalities in continuous variables (termed AC sets). These inequalities are a modification of a concept introduced by Blair and Jeroslow. Unlike the case for linear inequalities, allowing for integer variables in Chvátal inequalities and projection does not enhance modeling power. We show that the MILP-R sets are still precisely those sets that are modeled as affine Chvátal inequalites with integer variables. Furthermore, the projection of a set defined by affine Chvátal inequalites with integer variables is still a MILP-R set. We give a sequential variable elimination scheme that, when applied to a MILP-R set, yields the AC set characterization. This is related to the elimination scheme of Williams and Williams–Hooker, who describe projections of integer sets using disjunctions of affine Chvátal systems. We show that disjunctions are unnecessary by showing how to find the affine Chvátal inequalities that cannot be discovered by the Williams–Hooker scheme. This allows us to answer a long-standing open question due to Jennifer Ryan on designing an elimination scheme to represent finitely-generated integral monoids as a system of Chvátal inequalities without disjunctions. Finally, our work can be seen as a generalization of the approach of Blair and Jeroslow and of Schrijver for constructing consistency testers for integer programs to general AC sets.
1	This paper provides new bounds on the quality of equilibria in finite congestion games with affine cost functions, specifically for atomic network routing games. It is well known that the price of anarchy equals exactly 5/2 in general. For symmetric network routing games, it is at most (5n − 2)/(2n + 1), where n is the number of players. This paper answers to two open questions for congestion games. First, we show that the price of anarchy bound (5n − 2)/(2n + 1) is tight for symmetric network routing games, thereby answering a decade-old open question. Second, we ask whether sequential play and subgame perfection allows to evade worst-case Nash equilibria, and thereby reduces the price of anarchy. This is motivated by positive results for congestion games with a small number of players, as well as recent results for other resource allocation problems. Our main result is the perhaps surprising proof that subgame perfect equilibria of sequential symmetric network routing games with linear cost functions can have an unbounded price of anarchy. We complete the picture by analyzing the case with two players: we show that the sequential price of anarchy equals 7/5 and that computing the outcome of a subgame perfect equilibrium is NP-hard.
1	In the well-studied Colonel Blotto game, players must divide a pool of troops among a set of battlefields with the goal of winning a majority. Despite the importance of this game, only a few solutions for special variants of the problem are known. We provide a general technique for computing equilibria of the Colonel Blotto game. Our approach applies to variations of the Colonel Blotto game as well, including an infinite-strategy variant called the General Lotto game. We also apply our technique beyond Colonel Blotto games to create the first polynomial-time algorithms for computing equilibria for a variety of other zero-sum games. Our approach is to reformulate each zero-sum game into a bilinear form, then reduce equilibrium computation to linear optimization over a game-specific polytope.
1	A new condition, which we call uniform monotonicity, is shown to be necessary and almost sufficient for rationalizable implementation of correspondences. Uniform monotonicity is much weaker than Maskin monotonicity and reduces to it in the case of functions. Maskin monotonicity, the key condition for Nash implementation, had also been shown to be necessary for rationalizable implementation of social choice functions. Our conclusion is that the conditions for rationalizable implementation are not only starkly different from but also, much weaker than those for Nash implementation when we consider social choice correspondences. Thus, dropping rational expectations significantly expands the class of rules that can be decentralized by communication-based economic institutions.
1	We consider the problem of scheduling appointments for a finite customer population to a service facility with customer no-shows to minimize the sum of customer waiting time and server overtime costs. Because appointments need to be scheduled ahead of time, we refer to this problem as an optimization problem rather than a dynamic control one. We study this optimization problem in fluid and diffusion scales and identify asymptotically optimal schedules in both scales. In fluid scale, we show that it is optimal to schedule appointments so that the system is in critical load; thus, heavy-traffic conditions are obtained as a result of optimization rather than as an assumption. In diffusion scale, we solve this optimization problem in the large horizon limit. Our explicit stationary solution of the corresponding Brownian optimization problem translates the customer delay versus server overtime trade-off to a trade-off between the state of a reflected Brownian motion in the half-line and its local time at zero. Motivated by work on competitive ratios, we also consider a reference model in which an oracle provides the decision maker with the complete randomness information. The difference between the values of the scheduling problem for the two models, to which we refer as the stochasticity gap (SG), quantifies the degree to which it is harder to design a schedule under uncertainty than when the stochastic primitives (i.e., the no-shows and service times) are known in advance. In the fluid scale, the SG converges to zero, but in the diffusion scale, it converges to a positive constant that we compute.
1	In this paper we study necessary optimality conditions for the problem of minimizing a polynomial function over a set defined by polynomial inequalities. Assume that the problem is bounded below and has the Mangasarian–Fromovitz property at infinity. We first show that if the problem does not have an optimal solution, then a version at infinity of the Fritz John optimality conditions holds. From this we derive a version at infinity of the Karush–Kuhn–Tucker optimality conditions. As applications, we obtain a Frank–Wolfe type theorem which states that the optimal solution set of the problem is nonempty provided the objective function is “convenient.” Finally, in the unconstrained case, we show that the optimal value of the problem is the smallest critical value of some polynomial. All the results are presented in terms of the Newton polyhedra of the polynomials defining the problem.
1	In a recent and ongoing work, Baldwin and Klemperer explore a connection between tropical geometry and economics. They give a sufficient condition for the existence of competitive equilibrium in product-mix auctions of indivisible goods. This result, which we call the unimodularity theorem, can also be traced back to the work of Danilov, Koshevoy, and Murota in discrete convex analysis. We give a new proof of the unimodularity theorem via the classical unimodularity theorem in integer programming. We give a unified treatment of these results via tropical geometry and formulate a new sufficient condition for competitive equilibrium when there are only two types of products. Generalizations of our theorem in higher dimensions are equivalent to various forms of the Oda conjecture in algebraic geometry.
1	The infinite models in integer programming can be described as the convex hull of some points or as the intersection of halfspaces derived from valid functions. In this paper, we study the relationships between these two descriptions. Our results have implications for corner polyhedra. One consequence is that nonnegative, continuous valid functions suffice to describe corner polyhedra (with or without rational data).
1	Suppose that some objects are hidden in a finite set S of hiding places that must be examined one by one. The cost of searching subsets of S is given by a submodular function, and the probability that all objects are contained in a subset is given by a supermodular function. We seek an ordering of S that finds all the objects with minimal expected cost. This problem is NP-hard, and we give an efficient combinatorial 2-approximation algorithm, generalizing analogous results in scheduling theory. We also give a new scheduling application where a set of jobs must be ordered subject to precedence constraints to minimize the weighted sum of some concave function of the completion times of subsets of jobs. We go on to give better approximations for submodular functions with low total curvature, and we give a full solution when the problem is what we call series-parallel decomposable. Next, we consider a zero-sum game between a cost-maximizing hider and a cost-minimizing searcher. We prove that the equilibrium mixed strategies for the hider are in the base polyhedron of the cost function, suitably scaled, and we solve the game in the series-parallel decomposable case, giving approximately optimal strategies in other cases.
1	We consider common-value hybrid auctions among two asymmetrically informed bidders, in which the winning bidder pays his bid with some positive probability κ and the losing bid otherwise. Under the assumption of discrete and affiliated signals, we give an explicit characterization of the (unique) equilibrium based on a simple recurrence relation, which gives rise to a linear-time algorithm for explicitly computing the equilibrium. By analyzing the execution of the algorithm, we derive several insights about the equilibrium structure. First, we show that equilibrium revenue is decreasing in κ and that the limit second-price equilibrium selected as κ → 0 has the highest revenue, in stark contrast to the revenue collapse of the second-price auction predicted by the trembling-hand equilibrium selection proposed in prior work. We further show that the linkage principle can fail to hold even in a pure first-price auction with binary signals: public revelation of a signal to both bidders may decrease the auctioneer’s revenue. Finally, we analyze the effects of public acquisition of additional information on bidder utilities and exhibit cases in which both bidders strictly prefer for a specific bidder to receive additional information.
1	Given a finite Borel measure μ𝜇 on Rn and basic semialgebraic sets Ωi⊂Rn, i=1,…,p, we provide a systematic numerical scheme to approximate as closely as desired μ(∪iΩi), when all moments of μ are available (and finite). More precisely, we provide a hierarchy of semidefinite programs whose associated sequence of optimal values is monotone and converges to the desired value from above. The same methodology applied to the complement Rn∖(∪iΩi) provides a monotone sequence that converges to the desired value from below. When μ is the Lebesgue measure, we assume that Ω≔∪iΩi is compact and contained in a known box B≔[−a,a]n, and in this case the complement is taken to be B∖Ω. In fact, not only μ(Ω) but also every finite vector of moments of μΩ (the restriction of μ on Ω) can be approximated as closely as desired and so permits to approximate the integral on Ω of any given polynomial.
1	We present new techniques to analyze natural local search algorithms for several variants of the max-sum diversification problem which, in its most basic form, is as follows: given an n-point set 𝑋⊆ℝ𝑑 and an integer k, select k points in X so that the sum of all of their (𝑘2) Euclidean distances is maximized. This problem has recently received a lot of attention in the context of information retrieval and web search. We focus on distances of negative type, a class that includes Euclidean distances of unbounded dimension, as well as several other natural distances, including nonmetric ones. We prove that local search over these distances provides simple and fast polynomial-time approximation schemes (PTASs) for variants that are constrained by a matroid or even a matroid intersection, and asymptotically optimal O(1)-approximations when combining the sum-of-distances objective with a monotone submodular function.
1	The 𝑘-supplier problem is a fundamental location problem that involves opening 𝑘 facilities to minimize the maximum distance of any client to an open facility. We consider the 𝑘-supplier problem in Euclidean metrics (of arbitrary dimension) and present an algorithm with approximation ratio 1+3‾√<2.74. This improves upon the previously known 3-approximation algorithm, which also holds for general metrics. Our result is almost best possible as the Euclidean 𝑘-supplier problem is NP-hard to approximate better than a factor of 7‾√>2.64. We also present a nearly linear time algorithm for the Euclidean 𝑘-supplier in constant dimensions that achieves an approximation ratio better than three.
1	We study the problem of decomposing the Hessian matrix of a mixed integer convex quadratic program (MICQP) into the sum of positive semidefinite 2 × 2 matrices. Solving this problem enables the use of perspective reformulation techniques for obtaining strong lower bounds for MICQPs with semicontinuous variables but a nonseparable objective function. An explicit formula is derived for constructing 2 × 2 decompositions when the underlying matrix is weakly scaled diagonally dominant, and necessary and sufficient conditions are given for the decomposition to be unique. For matrices lying outside this class, two exact semidefinite programming approaches and an efficient heuristic are developed for finding approximate decompositions. We present preliminary results on the bound strength of a 2 × 2 perspective reformulation for the portfolio optimization problem, showing that, for some classes of instances, the use of 2 × 2 matrices can significantly improve the quality of the bound with respect to the best previously known approach, although at a possibly high computational cost.
1	We show that one can compute the least nonnegative solution (also known as the least fixed point) for a system of probabilistic min (max) polynomial equations, to any desired accuracy ɛ > 0 in time polynomial in both the encoding size of the system and in log(1/ɛ). These are Bellman optimality equations for important classes of infinite-state Markov decision processes (MDPs), including branching MDPs (BMDPs), which generalize classic multitype branching stochastic processes. We thus obtain the first polynomial time algorithm for computing, to any desired precision, optimal (maximum and minimum) extinction probabilities for BMDPs. Our algorithms are based on a novel generalization of Newton’s method, which employs linear programming in each iteration. We also provide polynomial-time (P-time) algorithms for computing an ɛ-optimal policy for both maximizing and minimizing extinction probabilities in a BMDP, whereas we note a hardness result for computing an exact optimal policy. Furthermore, improving on prior results, we provide more efficient P-time algorithms for qualitative analysis of BMDPs, that is, for determining whether the maximum or minimum extinction probability is 1, and, if so, computing a policy that achieves this. We also observe some complexity consequences of our results for branching simple stochastic games, which generalize BMDPs.
1	The stable matching (or stable marriage) model of Gale and Shapley [Gale D, Shapley LS (1962) College admissions and the stability of marriage. Amer. Math. Monthly 69(1):9–15.] has been generalized in various directions, such as matroid kernels by Fleiner [Fleiner T (2001) A matroid generalization of the stable matching polytope. Aardal K, Gerards AMH, eds. Proc. 8th Internat. Conf. Integer Programming Combin. Optim., Lecture Notes in Computer Science, vol. 2081 (Springer-Verlag, Berlin), 105–114.] and stable allocations in bipartite networks by Baïou and Balinski [Baïou M, Balinski M (2002) Erratum: The stable allocation (or ordinal transportation) problem. Math. Oper. Res. 27(4):662–680.]. Unifying these generalizations, we introduce the concept of stable allocations in polymatroid intersection. Our framework includes both integer and real variable versions. The integer variable version corresponds to a special case of the discrete concave function model of Eguchi et al. [Eguchi A, Fujishige S, Tamura A (2003) A generalized Gale-Shapley algorithm for a discrete-concave stable-marriage model. Ibaraki T, Katoh N, Ono H, eds. Proc. 14th Internat. Sympos. Algorithms Comput., Lecture Notes in Computer Science, vol. 2906 (Springer-Verlag, Berlin), 495–504.], who established the existence of a stable allocation by showing that a simple extension of the deferred acceptance algorithm of Gale and Shapley finds a stable allocation in pseudopolynomial time. It has been open to develop a polynomial time algorithm even for our special case. In this paper, we present the first strongly polynomial algorithm for finding a stable allocation in polymatroid intersection. To achieve this, we utilize the augmenting path technique for polymatroid intersection. In each iteration, the algorithm searches for an augmenting path by simulating a chain of proposes and rejects in the deferred acceptance algorithm. The running time of our algorithm is O(n3γ), where n and γ denote the cardinality of the ground set and the time for computing the saturation and exchange capacities, respectively. Moreover, we show that the output of our algorithm is optimal for one side, where this optimality is a generalization of the man optimality in the stable marriage model.
1	We study the convergence rate of a hierarchy of upper bounds for polynomial optimization problems, proposed by Lasserre, and a related hierarchy by de Klerk, Hess, and Laurent. For polynomial optimization over the hypercube, we show a refined convergence analysis for the first hierarchy. We also show lower bounds on the convergence rate for both hierarchies on a class of examples. These lower bounds match the upper bounds and thus establish the true rate of convergence on these examples. Interestingly, these convergence rates are determined by the distribution of extremal zeroes of certain families of orthogonal polynomials.
1	For a function defined on the integer lattice, we consider discrete versions of midpoint convexity, which offer a unifying framework for discrete convexity of functions, including integral convexity, L♮-convexity, and submodularity. By considering discrete midpoint convexity for all pairs at ℓ∞-distance equal to 2 or not smaller than 2, we identify new classes of discrete convex functions, called locally and globally discrete midpoint convex functions. These functions enjoy nice structural properties. They are stable under scaling and addition and satisfy a family of inequalities named parallelogram inequalities. Furthermore, they admit a proximity theorem with the same small proximity bound as that for L♮-convex functions. These structural properties allow us to develop an algorithm for the minimization of locally and globally discrete midpoint convex functions based on the proximity-scaling approach and on a novel 2-neighborhood steepest descent algorithm.
1	This paper finds optimal portfolios for the reference-dependent preferences by Kőszegi and Rabin with piecewise linear gain–loss utility in a one-period model with a safe and a risky asset. If the return of the risky asset is highly dispersed relative to its potential gains, two personal equilibria arise, one of them including risky investments and the other one only safe holdings. In the same circumstances, the risky personal equilibrium entails market participation that decreases with loss aversion and gain–loss sensitivity, whereas the preferred personal equilibrium is sensitive to market and preference parameters. Relevant market parameters are not the expected return and standard deviation, but rather the ratio of expected gains to losses and the Gini index of the return.
1	In this paper, we consider a stochastic Nash game in which each player minimizes a parameterized expectation-valued convex objective function. In deterministic regimes, proximal best-response (BR) schemes have been shown to be convergent under a suitable spectral property associated with the proximal BR map. However, a direct application of this scheme to stochastic settings requires obtaining exact solutions to stochastic optimization problems at each iteration. Instead, we propose an inexact generalization of this scheme in which an inexact solution to the BR problem is computed in an expected-value sense via a stochastic approximation (SA) scheme. On the basis of this framework, we present three inexact BR schemes: (i) First, we propose a synchronous inexact BR scheme where all players simultaneously update their strategies. (ii) Second, we extend this to a randomized setting where a subset of players is randomly chosen to update their strategies while the other players keep their strategies invariant. (iii) Third, we propose an asynchronous scheme, where each player chooses its update frequency while using outdated rival-specific data in updating its strategy. Under a suitable contractive property on the proximal BR map, we proceed to derive almost sure convergence of the iterates to the Nash equilibrium (NE) for (i) and (ii) and mean convergence for (i)–(iii). In addition, we show that for (i)–(iii), the generated iterates converge to the unique equilibrium in mean at a linear rate with a prescribed constant rather than a sublinear rate. Finally, we establish the overall iteration complexity of the scheme in terms of projected stochastic gradient (SG) steps for computing an ɛ-NE2 (or ɛ-NE∞) and note that in all settings, the iteration complexity is 𝒪(1/ɛ2(1+𝑐)+𝛿), where 𝑐=0 in the context of (i), and c > 0 represents the positive cost of randomization in (ii) and asynchronicity and delay in (iii). Notably, in the synchronous regime, we achieve a near-optimal rate from the standpoint of solving stochastic convex optimization problems by SA schemes. The schemes are further extended to settings where players solve two-stage stochastic Nash games with linear and quadratic recourse. Finally, preliminary numerics developed on a multiportfolio investment problem and a two-stage capacity expansion game support the rate and complexity statements.
1	We prove that integer programming with three alternating quantifiers is NP-complete, even for a fixed number of variables. This complements earlier results by Lenstra [16] [Lenstra H (1983) Integer programming with a fixed number of variables. Math. Oper. Res. 8(4):538–548.] and Kannan [13,14] [Kannan R (1990) Test sets for integer programs, ∀ ∃ sentences. Polyhedral Combinatorics (American Mathematical Society, Providence, RI), 39–47. Kannan R (1992) Lattice translates of a polytope and the Frobenius problem. Combinatorica 12(2):161–177.], which together say that integer programming with at most two alternating quantifiers can be done in polynomial time for a fixed number of variables. As a byproduct of the proof, we show that for two polytopes 𝑃,𝑄⊂ℝ3P,Q⊂R3, counting the projections of integer points in Q\P is #P-complete. This contrasts the 2003 result by Barvinok and Woods [5] [Barvinok A, Woods K (2003) Short rational generating functions for lattice point problems. J. Amer. Math. Soc. 16(4):957–979.], which allows counting in polynomial time the projections of integer points in P and Q separately.
1	We consider a general nonzero-sum impulse game with two players. The main mathematical contribution of this paper is a verification theorem that provides, under some regularity conditions, a suitable system of quasi-variational inequalities for the payoffs and the strategies of the two players at some Nash equilibrium. As an application, we study an impulse game with a one-dimensional state variable, following a real-valued scaled Brownian motion, and two players with linear and symmetric running payoffs. We fully characterize a family of Nash equilibria and provide explicit expressions for the corresponding equilibrium strategies and payoffs. We also prove some asymptotic results with respect to the intervention costs. Finally, we consider two further nonsymmetric examples where a Nash equilibrium is found numerically.
1	Random permutation is observed to be powerful for optimization algorithms: for multiblock ADMM (alternating direction method of multipliers), whereas the classical cyclic version diverges, the randomly permuted version converges in practice; for BCD (block coordinate descent), the randomly permuted version is typically faster than other versions. In this paper we provide strong theoretical evidence that random permutation has positive effects on ADMM and BCD, by analyzing randomly permuted ADMM (RP-ADMM) for solving linear systems of equations, and randomly permuted BCD (RP-BCD) for solving unconstrained quadratic problems. First, we prove that RP-ADMM converges in expectation for solving systems of linear equations. The key technical result is that the spectrum of the expected update matrix of RP-BCD lies in (−1/3, 1), instead of the typical range (−1, 1). Second, we establish expected convergence rates of RP-ADMM for solving linear systems and RP-BCD for solving unconstrained quadratic problems. This expected rate of RP-BCD is O(n) times better than the worst-case rate of cyclic BCD, thus establishing a gap of at least O(n) between RP-BCD and cyclic BCD. To analyze RP-BCD, we propose a conjecture of a new matrix algebraic mean-geometric mean inequality and prove a weaker version of it.
1	Gross substitutability is a central concept in economics and is connected to important notions in discrete convex analysis, number theory, and the analysis of greedy algorithms in computer science. Many different characterizations are known for this class, but providing a constructive description remains a major open problem. The construction problem asks how to construct all gross substitutes from a class of simpler functions using a set of operations. Because gross substitutes are a natural generalization of matroids to real-valued functions, matroid rank functions form a desirable such class of simpler functions. Shioura proved that a rich class of gross substitutes can be expressed as sums of matroid rank functions, but it is open whether all gross substitutes can be constructed this way. Our main result is a negative answer showing that some gross substitutes cannot be expressed as positive linear combinations of matroid rank functions. En route, we provide necessary and sufficient conditions for the sum to preserve substitutability, uncover a new operation preserving substitutability, and fully describe all substitutes with at most four items.
1	In this paper, we show how to transform any optimization problem that arises from fitting a machine learning model into one that (1) detects and removes contaminated data from the training set while (2) simultaneously fitting the trimmed model on the uncontaminated data that remains. To solve the resulting nonconvex optimization problem, we introduce a fast stochastic proximal-gradient algorithm that incorporates prior knowledge through nonsmooth regularization. For data sets of size n, our approach requires O(n2/3/ℇ) gradient evaluations to reach ℇ-accuracy, and when a certain error bound holds, the complexity improves to O(κn2/3 log(1/ℇ)), where κ is a “condition number.” These rates are n1/3 times better than those achieved by typical, nonstochastic methods.
1	We consider a stochastic online problem where n applicants arrive over time, one per time step. Upon the arrival of each applicant, their cost per time step is revealed, and we have to fix the duration of employment, starting immediately. This decision is irrevocable; that is, we can neither extend a contract nor dismiss a candidate once hired. In every time step, at least one candidate needs to be under contract, and our goal is to minimize the total hiring cost, which is the sum of the applicants’ costs multiplied with their respective employment durations. We provide a competitive online algorithm for the case that the applicants’ costs are drawn independently from a known distribution. Specifically, the algorithm achieves a competitive ratio of 2.965 for the case of uniform distributions. For this case, we give an analytical lower bound of 2 and a computational lower bound of 2.148. We then adapt our algorithm to stay competitive even in settings with one or more of the following restrictions: (i) at most two applicants can be hired concurrently; (ii) the distribution of the applicants’ costs is unknown; (iii) the total number n of time steps is unknown. On the other hand, we show that concurrent employment is a necessary feature of competitive algorithms by proving that no algorithm has a competitive ratio better than 𝛺(𝑛√/log𝑛) if concurrent employment is forbidden.
1	Fix a Pareto-optimal, strategy-proof, and nonbossy deterministic matching mechanism and define a random matching mechanism by assigning agents to the roles in the mechanism via a uniform lottery. Given a profile of preferences, the lottery over outcomes that arises under the random matching mechanism is identical to the lottery that arises under random serial dictatorship, where the order of dictators is uniformly distributed. This result extends the celebrated equivalence between the core from random endowments and random serial dictatorship to the grand set of all Pareto-optimal, strategy-proof, and nonbossy matching mechanisms.
1	The orthogonalpoint enclosure query (OPEQ) problem is a fundamental problem in the context of data management for modeling user preferences. Formally, preprocess a set S of n axis-aligned boxes (possibly overlapping) in ℝd into a data structure so that the boxes in S containing a given query point q can be reported efficiently. In the pointer machine model, optimal solutions for the OPEQ in ℝ1 and ℝ2 were discovered in the 1980s: linear-space data structures that can answer the query in O(log n + k) query time, where k is the number of boxes reported. However, for the past three decades, an optimal solution in ℝ3 has been elusive. In this work, we obtain the first data structure that almost optimally solves the OPEQ in ℝ3 in the pointer machine model: an O(n log* n)-space data structure with O(log2n · log log n + k) query time. Here, log* n is the iterated logarithm of n. This almost matches the lower-bound, which states that any data structure that occupies O(n) space requires Ω(log2n + k) time to answer an OPEQ in ℝ3. Finally, we also obtain the best known bounds for the OPEQ in higher dimensions (d ≥ 4).
1	In practice, one must recognize the inevitable incompleteness of information while making decisions. In this paper, we consider the optimal redeeming problem of stock loans under a state of incomplete information presented by the uncertainty in the (bull or bear) trends of the underlying stock. This is called drift uncertainty. Owing to the unavoidable need for the estimation of trends while making decisions, the related Hamilton–Jacobi–Bellman equation turns out to be of a degenerate parabolic type. Hence, it is very hard to obtain its regularity using the standard approach, making the problem different from the existing optimal redeeming problems without drift uncertainty. We present a thorough and delicate probabilistic and functional analysis to obtain the regularity of the value function and the optimal redeeming strategies. The optimal redeeming strategies of stock loans appear significantly different in the bull and bear trends.
1	In this paper we study a class of infinite horizon fully coupled forward–backward stochastic differential equations (FBSDEs) with random coefficients that are stimulated by various continuous time future expectations models. Under standard Lipschitz and monotonicity conditions and by means of the contraction mapping principle, we establish existence and uniqueness of an adapted solution, and we obtain results regarding the dependence of this solution on the data of the problem. Making further the connection with finite horizon quasilinear backward stochastic partial differential equations via a generalization of the well known four-step-scheme, we are led to the notion of stochastic viscosity solutions. As an application of this framework, we also provide a stochastic maximum principle for the optimal control problem of such FBSDEs, which in the linear-quadratic Markovian case boils down to the solvability of an infinite horizon fully coupled system of forward-backward Ricatti differential equations.
1	We prove that every multiplayer quitting game admits a sunspot ε-equilibrium for every ε>0, that is, an ε-equilibrium in an extended game in which the players observe a public signal at every stage. We also prove that, if a certain matrix that is derived from the payoffs in the game is not a Q-matrix in the sense of linear complementarity problems, then the game admits a uniform ε-equilibrium for every ε>0.
1	In this paper, we address the problem of counting integer points in a rational polytope described by P(y) = {x ∈ Rm: Ax= y, x ≥ 0}, where A is an n × m integer matrix and y is an n-dimensional integer vector. We study the Z transformation approach initiated in works by Brion and Vergne, Beck, and Lasserre and Zeron from the numerical analysis point of view and obtain a new algorithm on this problem. If A is nonnegative, then the number of integer points in P(y) can be computed in O(poly(n,m,‖y‖∞)(‖y‖∞+1)n)𝑂(poly(𝑛,𝑚,‖𝑦‖∞)(‖𝑦‖∞+1)𝑛) time and O(poly(n,m,‖y‖∞))𝑂(poly(𝑛,𝑚,‖𝑦‖∞)) space. This improves, in terms of space complexity, a naive DP algorithm with O((‖y‖∞+1)n)-size dynamic programming table. Our result is based on the standard error analysis of the numerical contour integration for the inverse Z transform and establishes a new type of an inclusion-exclusion formula for integer points in P(y).We apply our result to hypergraph b-matching and obtain a O(poly(n,m,‖b‖∞)(‖b‖∞+1)(1−1/k)n) time algorithm for counting b-matchings in a k-partite hypergraph with n vertices and m hyperedges. This result is viewed as a b-matching generalization of the classical result by Ryser for k = 2 and its multipartite extension by Björklund and Husfeldt.
1	The Whittle index, which characterizes optimal policies for controlling certain single restless bandit projects (a Markov decision process with two actions: active and passive) is the basis for a widely used heuristic index policy for the intractable restless multiarmed bandit problem. Yet two roadblocks need to be overcome to apply such a policy: the individual projects in the model at hand must be shown to be indexable, so that they possess a Whittle index; and the index must be evaluated. Such roadblocks can be especially vexing when project state spaces are real intervals, as in recent sensor scheduling applications. This paper presents sufficient conditions for indexability (relative to a generalized Whittle index) of general real-state discrete-time restless bandits under the discounted criterion, which are not based on elucidating properties of the optimal value function and do not require proving beforehand optimality of threshold policies as in prevailing approaches. The main contribution is a verification theorem establishing that, if project performance metrics under threshold policies and an explicitly defined marginal productivity (MP) index satisfy three conditions, then the project is indexable with its generalized Whittle index being given by the MP index, and threshold policies are optimal for dynamic project control.
1	This paper establishes performance guarantees for online algorithms that schedule stochastic, nonpreemptive jobs on unrelated machines to minimize the expected total weighted completion time. Prior work on unrelated machine scheduling with stochastic jobs was restricted to the offline case and required linear or convex programming relaxations for the assignment of jobs to machines. The algorithms introduced in this paper are purely combinatorial. The performance bounds are of the same order of magnitude as those of earlier work and depend linearly on an upper bound on the squared coefficient of variation of the jobs’ processing times. Specifically for deterministic processing times, without and with release times, the competitive ratios are 4 and 6, respectively. As to the technical contribution, this paper shows how dual fitting techniques can be used for stochastic and nonpreemptive scheduling problems.
1	We propose a novel randomized linear programming algorithm for approximating the optimal policy of the discounted-reward and average-reward Markov decision problems. By leveraging the value–policy duality, the algorithm adaptively samples state–action–state transitions and makes exponentiated primal–dual updates. We show that it finds an ɛ-optimal policy using nearly linear runtime in the worst case for a fixed value of the discount factor. When the Markov decision process is ergodic and specified in some special data formats, for fixed values of certain ergodicity parameters, the algorithm finds an ɛ-optimal policy using sample size and time linear in the total number of state–action pairs, which is sublinear in the input size. These results provide a new venue and complexity benchmarks for solving stochastic dynamic programs.
1	We address a general periodic review inventory control model with the simultaneous presence of the following complications: (a) bilateral inventory adjustment options, via procurement orders and salvage sales or returns to the supplier; (b) fixed costs associated with procurement orders and downward inventory adjustments (via salvage sales or returns); and (c) capacity limits associated with upward or downward inventory adjustments. We characterize the optimal adjustment strategy, both for finite and infinite horizon periodic review models, by showing that in each period the inventory position line is to be partitioned into (maximally) five regions. Our results are obtained by identifying a novel generalized convexity property for the value functions, which we refer to as strong (C1K1, C2K2)-convexity. To our knowledge, we recover most existing structural results for models with exogenous demands as special cases of a unified analysis.
1	We consider constrained versions of the prize-collecting traveling salesman and the prize-collecting minimum spanning tree problems. The goal is to maximize the number of vertices in the returned tour/tree subject to a bound on the tour/tree cost. Rooted variants of the problems have the additional constraint that a given vertex, the root, must be contained in the tour/tree. We present a 2-approximation algorithm for the rooted and unrooted versions of both the tree and tour variants. The algorithm is based on a parameterized primal–dual approach. It relies on first finding a threshold value for the dual variable corresponding to the budget constraint in the primal and then carefully constructing a tour/tree that is, in a precise sense, just within budget. We improve upon the best-known guarantee of 2 + ε for the rooted and unrooted tour versions and 3 + ε for the rooted and unrooted tree versions. Our analysis extends to the setting with weighted vertices, in which we want to maximize the total weight of vertices in the tour/tree. Interestingly enough, the algorithm and analysis for the rooted case and the unrooted case are almost identical.
1	We study infeasible-start, primal–dual interior-point methods for convex optimization problems given in a typically natural form we denote as domain-driven formulations. Our algorithms extend many advantages of primal–dual interior-point techniques available for conic formulations, such as the current best complexity bounds, and more robust certificates of approximate optimality, unboundedness, and infeasibility, to domain-driven formulations. The complexity results are new for the infeasible-start setup used even in the case of linear programming. In addition to complexity results, our algorithms aim for expanding the applications of and software for interior-point methods to wider classes of problems beyond optimization over symmetric cones.
1	Agents with different discount factors disagree about some intertemporal trade-offs, but they will also agree sometimes. We seek to understand precisely the nature of their agreements and disagreements. A group of agents is identified with a set of discount factors. We characterize the comparisons that a given interval of discount factors will agree on, including what all discount factors in the interval [0, 1] will agree on. Our result is analogous to how all risk-averse and monotone agents agree on mean-preserving spreads. Motivated by a maxmin representation, we also characterize the comparisons that are consistent with some set of discount factors, when the set is not known or exogenously given. In other words, we describe the Pareto comparisons that are consistent with a society, or group, of exponentially discounting agents.
1	This paper studies a multiconstrained problem for piecewise deterministic Markov decision processes (PDMDPs) with unbounded cost and transition rates. The goal is to minimize one type of expected finite-horizon cost over history-dependent policies while keeping some other types of expected finite-horizon costs lower than some tolerable bounds. Using the Dynkin formula for the PDMDPs, we obtain an equivalent characterization of occupancy measures and express the expected finite-horizon costs in terms of occupancy measures. Under suitable assumptions, the existence of constrained-optimal policies is shown, the linear programming formulation and its dual program for the constrained problem are derived, and the strong duality between the two programs is established. An example is provided to demonstrate our results.
1	We provide the first rate of convergence to stationarity analysis for reflected Brownian motion (RBM) as the dimension grows under some uniformity conditions. In particular, if the underlying routing matrix is uniformly contractive, uniform stability of the drift vector holds, and the variances of the underlying Brownian motion (BM) are bounded, then we show that the RBM converges exponentially fast to stationarity with a relaxation time of order 𝑂(𝑑4(log(𝑑))3) as the dimension d → ∞. Our bound for the relaxation time follows as a corollary of the nonasymptotic bound we obtain for the initial transient effect, which is explicit in terms of the RBM parameters.
1	We propose two numerical algorithms in the fully nonconvex setting for the minimization of the sum of a smooth function and the composition of a nonsmooth function with a linear operator. The iterative schemes are formulated in the spirit of the proximal alternating direction method of multipliers and its linearized variant, respectively. The proximal terms are introduced via variable metrics, a fact that allows us to derive new proximal splitting algorithms for nonconvex structured optimization problems, as particular instances of the general schemes. Under mild conditions on the sequence of variable metrics and by assuming that a regularization of the associated augmented Lagrangian has the Kurdyka–Łojasiewicz property, we prove that the iterates converge to a Karush–Kuhn–Tucker point of the objective function. By assuming that the augmented Lagrangian has the Łojasiewicz property, we also derive convergence rates for both the augmented Lagrangian and the iterates.
1	We study a certain polytope arising from embedding the Hamiltonian cycle problem in a discounted Markov decision process. The Hamiltonian cycle problem can be reduced to finding particular extreme points of a certain polytope associated with the input graph. This polytope is a subset of the space of discounted occupational measures. We characterize the feasible bases of the polytope for a general input graph G and determine the expected numbers of different types of feasible bases when the underlying graph is random. We utilize these results to demonstrate that augmenting certain additional constraints to reduce the polyhedral domain can eliminate a large number of feasible bases that do not correspond to Hamiltonian cycles. Finally, we develop a random walk algorithm on the feasible bases of the reduced polytope and present some numerical results. We conclude with a conjecture on the feasible bases of the reduced polytope.
1	We propose simple polynomial-time algorithms for two linear conic feasibility problems. For a matrix A∈ℝm×n𝐴∈ℝ𝑚×𝑛, the kernel problem requires a positive vector in the kernel of A, and the image problem requires a positive vector in the image of AT. Both algorithms iterate between simple first-order steps and rescaling steps. These rescalings improve natural geometric potentials. If Goffin’s condition measure ρA is negative, then the kernel problem is feasible, and the worst-case complexity of the kernel algorithm is O((m3n+mn2)log|ρA|−1)𝑂((𝑚3𝑛+𝑚𝑛2)log|𝜌𝐴|−1); if ρA>0𝜌𝐴>0, then the image problem is feasible, and the image algorithm runs in time O(m2n2logρ−1A). We also extend the image algorithm to the oracle setting. We address the degenerate case ρA = 0 by extending our algorithms to find maximum support nonnegative vectors in the kernel of A and in the image of AT. In this case, the running time bounds are expressed in the bit-size model of computation: for an input matrix A with integer entries and total encoding length L, the maximum support kernel algorithm runs in time O((m3n+mn2)L), whereas the maximum support image algorithm runs in time O(m2n2L). The standard linear programming feasibility problem can be easily reduced to either maximum support problems, yielding polynomial-time algorithms for linear programming.
1	We address the performance of selfish network routing in multicommodity flows where the latency or delay function on edges is dependent on the flow of individual commodities, rather than on the aggregate flow. An application of this study is the analysis of a network with differentiated traffic, that is, in transportation networks where there are multiple types of traffic and in networks where traffic is prioritized according to type classification. We consider the inefficiency of equilibrium in this model and provide price of anarchy bounds for networks with k (types of) commodities, where each link is associated with heterogeneous polynomial delays, that is, commodity i on edge e faces delay specified by a multivariate polynomial dependent on the individual flow of each commodity on the edge. We consider both atomic and nonatomic flows and show bounds on the price of anarchy that depend on the relative impact of each type of traffic on the edge delay when the delay functions are polynomials of degree θ, for example, ∑𝑖𝑎𝑖𝑓𝑖(𝑒)𝜃. The price of anarchy is unbounded for arbitrary polynomials. For networks with decomposable delay functions where the delay is the same for all commodities using the edge, we show improved bounds on the price of anarchy, for both nonatomic and atomic flows. The results illustrate that the inefficiency of selfish routing worsens in the case of heterogeneous delays compared with the standard delay functions that do not consider type differentiation.
1	Uncertainty pervades virtually every branch of science and engineering, and in many disciplines, the underlying phenomena can be modeled by partial differential equations (PDEs) with uncertain or random inputs. This work is motivated by risk-averse stochastic programming problems constrained by PDEs. These problems are posed in infinite dimensions, which leads to a significant increase in the scale of the (discretized) problem. In order to handle the inherent nonsmoothness of, for example, coherent risk measures and to exploit existing solution techniques for smooth, PDE-constrained optimization problems, we propose a variational smoothing technique called epigraphical (epi-)regularization. We investigate the effects of epi-regularization on the axioms of coherency and prove differentiability of the smoothed risk measures. In addition, we demonstrate variational convergence of the epi-regularized risk measures and prove the consistency of minimizers and first-order stationary points for the approximate risk-averse optimization problem. We conclude with numerical experiments confirming our theoretical results.
1	We consider resource sharing networks of the form introduced in work of Massoulié and Roberts as models for Internet flows. The goal is to study the open problem, formulated in Harrison et al. (2014) [Harrison JM, Mandayam C, Shah D, Yang Y (2014) Resource sharing networks: Overview and an open problem. Stochastic Systems 4(2):524–555.], of constructing simple form rate-allocation policies for broad families of resource sharing networks with associated costs converging to the hierarchical greedy ideal performance in the heavy traffic limit. We consider two types of cost criteria: an infinite horizon discounted cost and a long-time average cost per unit time. We introduce a sequence of rate-allocation control policies that are determined in terms of certain thresholds for the scaled queue-length processes and prove that, under conditions, both type of costs associated with these policies converge in the heavy traffic limit to the corresponding hierarchical greedy ideal (HGI) performance. The conditions needed for these results are satisfied by all the examples considered in the above cited paper of Harrison et al.
1	Consider the problem of minimizing the sum of a smooth convex function and a separable nonsmooth convex function subject to linear coupling constraints. Problems of this form arise in many contemporary applications, including signal processing, wireless networking, and smart grid provisioning. Motivated by the huge size of these applications, we propose a new class of first-order primal–dual algorithms called the block successive upper-bound minimization method of multipliers (BSUM-M) to solve this family of problems. The BSUM-M updates the primal variable blocks successively by minimizing locally tight upper bounds of the augmented Lagrangian of the original problem, followed by a gradient-type update for the dual variable in closed form. We show that under certain regularity conditions, and when the primal block variables are updated in either a deterministic or a random fashion, the BSUM-M converges to a point in the set of optimal solutions. Moreover, in the absence of linear constraints and under similar conditions as in the previous result, we show that the randomized BSUM-M (which reduces to the randomized block successive upper-bound minimization method) converges at an asymptotically linear rate without relying on strong convexity.
1	In multiserver distributed queueing systems, the access of stochastically arriving jobs to resources is often regulated by a dispatcher, also known as a load balancer. A fundamental problem consists in designing a load-balancing algorithm that minimizes the delays experienced by jobs. During the last two decades, the power-of-d-choice algorithm, based on the idea of dispatching each job to the least loaded server out of d servers randomly sampled at the arrival of the job itself, has emerged as a breakthrough in the foundations of this area because of its versatility and appealing asymptotic properties. In this paper, we consider the power-of-d-choice algorithm with the addition of a local memory that keeps track of the latest observations collected over time on the sampled servers. Then, each job is sent to a server with the lowest observation. We show that this algorithm is asymptotically optimal in the sense that the load balancer can always assign each job to an idle server in the large-system limit. This holds true if and only if the system load λ is less than 1−1𝑑. If this condition is not satisfied, we show that queue lengths are bounded by ⌈−log(1−𝜆)log(𝜆𝑑+1)⌉. This is in contrast with the classic version of the power-of-d-choice algorithm, in which, at the fluid scale, a strictly positive proportion of servers containing 𝑖 jobs exists for all 𝑖≥0 in equilibrium. Our results quantify and highlight the importance of using memory as a means to enhance performance in randomized load balancing.
1	We study the limit of equilibrium payoffs, as the discount factor goes to one, in non-zero-sum stochastic games. We first show that the set of stationary equilibrium payoffs always converges. We then provide two-player examples in which the whole set of equilibrium payoffs diverges. The construction is robust to perturbations of the payoffs and to the introduction of normal-form correlation.
1	Classic cake-cutting algorithms enable people with different preferences to divide among them a heterogeneous resource (“cake”) such that the resulting division is fair according to each agent’s individual preferences. However, these algorithms either ignore the geometry of the resource altogether or assume it is one-dimensional. In practice, it is often required to divide multidimensional resources, such as land estates or advertisement spaces in print or electronic media. In such cases, the geometric shape of the allotted piece is of crucial importance. For example, when building houses or designing advertisements, in order to be useful, the allotments should be squares or rectangles with bounded aspect ratio. We, thus, introduce the problem of fair land division—fair division of a multidimensional resource wherein the allocated piece must have a prespecified geometric shape. We present constructive division algorithms that satisfy the two most prominent fairness criteria, namely envy-freeness and proportionality. In settings in which proportionality cannot be achieved because of the geometric constraints, our algorithms provide a partially proportional division, guaranteeing that the fraction allocated to each agent be at least a certain positive constant. We prove that, in many natural settings, the envy-freeness requirement is compatible with the best attainable partial-proportionality.
1	We establish error estimates for the Longstaff–Schwartz algorithm, employing just a single set of independent Monte Carlo sample paths that is reused for all exercise time steps. We obtain, within the context of financial derivative payoff functions bounded according to the uniform norm, new bounds on the stochastic part of the error of this algorithm for an approximation architecture that may be any arbitrary set of L2 functions of finite Vapnik–Chervonenkis (VC) dimension whenever the algorithm’s least-squares regression optimization step is solved either exactly or approximately. Moreover, we show how to extend these estimates to the case of payoff functions bounded only in Lp, p a real number greater than 2<𝑝<∞2<p<∞. We also establish new overall error bounds for the Longstaff–Schwartz algorithm, including estimates on the approximation error also for unconstrained linear, finite-dimensional polynomial approximation. Our results here extend those in the literature by not imposing any uniform boundedness condition on the approximation architectures, allowing each of them to be any set of L2 functions of finite VC dimension and by establishing error estimates as well in the case of ɛ-additive approximate least-squares optimization, ɛ greater than or equal to 0.
1	We study the nonuniform capacitated multi-item lot-sizing problem. In this problem, there is a set of demands over a planning horizon of T discrete time periods, and all demands must be satisfied on time. We can place an order at the beginning of each period s, incurring an ordering cost Ks. In this order, we can order up to Cs units of products. On the other hand, carrying inventory from time to time incurs an inventory holding cost. The goal of the problem is to find a feasible solution that minimizes the sum of ordering and holding costs. Levi et al. [Levi R, Lodi A, Sviridenko M (2008) Approximation algorithms for the capacitated multi-item lot-sizing problem via flow-cover inequalities. Math. Oper. Res. 33(2):461–474.] gave a two-approximation for the problem when the capacities Cs are the same. Extending the result to the case of nonuniform capacities requires new techniques as pointed out in the discussion section of their paper. In this paper, we solve the problem by giving a 10-approximation algorithm for the capacitated multi-item lot-sizing problem with general capacities. The constant approximation is achieved by adding an exponential number of new covering inequalities to the natural facility location–type linear programming (LP) relaxation for the problem. Along the way of our algorithm, we reduce the lot-sizing problem to two generalizations of the classic knapsack-covering problem. We give LP-based constant approximation algorithms for both generalizations via the iterative rounding technique.
1	Sequential Bayesian optimization constitutes an important and broad class of problems where model parameters are not known a priori but need to be learned over time using Bayesian updating. It is known that the solution to these problems can in principle be obtained by solving the Bayesian dynamic programming (BDP) equation. Although the BDP equation can be solved in certain special cases (for example, when posteriors have low-dimensional representations), solving this equation in general is computationally intractable and remains an open problem. A second unresolved issue with the BDP equation lies in its (rather generic) interpretation. Beyond the standard narrative of balancing immediate versus future costs—an interpretation common to all dynamic programs with or without learning—the BDP equation does not provide much insight into the underlying mechanism by which sequential Bayesian optimization trades off between learning (exploration) and optimization (exploitation), the distinguishing feature of this problem class. The goal of this paper is to develop good approximations (with error bounds) to the BDP equation that help address the issues of computation and interpretation. To this end, we show how the BDP equation can be represented as a tractable single-stage optimization problem that trades off between a myopic term and a “variance regularization” term that measures the total solution variability over the remaining planning horizon. Intuitively, the myopic term can be regarded as a pure exploitation objective that ignores the impact of future learning, whereas the variance regularization term captures a pure exploration objective that only puts value on solutions that resolve statistical uncertainty. We develop quantitative error bounds for this representation and prove that the error tends to zero like o(n-1) almost surely in the number of stages n, which as a corollary, establishes strong consistency of the approximate solution.
1	This article contains various results on a class of nonmonotone, law-invariant risk functionals called the signed Choquet integrals. A functional characterization via comonotonic additivity is established along with some theoretical properties, including six equivalent conditions for a signed Choquet integral to be convex. We proceed to address two practical issues currently popular in risk management, namely robustness (continuity) issues and risk aggregation with dependence uncertainty, for signed Choquet integrals. Our results generalize in several directions those in the literature of risk functionals. From the results obtained in this paper, we see that many profound and elegant mathematical results in the theory of risk measures hold for the general class of signed Choquet integrals; thus, they do not rely on the assumption of monotonicity.
1	We consider the so-called GI/GI/N queue, in which a stream of jobs with independent and identically distributed service times arrive as a renewal process to a common queue that is served by NN identical parallel servers in a first-come, first-served manner. We introduce a new representation for the state of the system and, under suitable conditions on the service and interarrival distributions, establish convergence of the corresponding sequence of centered and scaled stationary distributions in the so-called Halfin–Whitt asymptotic regime. In particular, this resolves an open question posed by Halfin and Whitt in 1981. We also characterize the limit as the stationary distribution of an infinite-dimensional, two-component Markov process that is the unique solution to a certain stochastic partial differential equation. Previous results were essentially restricted to exponential service distributions or service distributions with finite support, for which the corresponding limit process admits a reduced finite-dimensional Markovian representation. We develop a different approach to deal with the general case when the Markovian representation of the limit is truly infinite dimensional. This approach is more broadly applicable to a larger class of networks.
1	A solution on a set of transferable utility (TU) games satisfies strong aggregate monotonicity (SAM) if every player can improve when the grand coalition becomes richer. It satisfies equal surplus division (ESD) if the solution allows the players to improve equally. We show that the set of weight systems generating weighted prenucleoli that satisfy SAM is open, which implies that for weight systems close enough to any regular system, the weighted prenucleolus satisfies SAM. We also provide a necessary condition for SAM for symmetrically weighted nucleoli. Moreover, we show that the per capita nucleolus on balanced games is characterized by single-valuedness (SIVA), translation covariance (TCOV) and scale covariance (SCOV), and equal adjusted surplus division (EASD), a property that is comparable to but stronger than ESD. These properties together with ESD characterize the per capita prenucleolus on larger sets of TU games. EASD and ESD can be transformed to independence of (adjusted) proportional shifting, and these properties may be generalized for arbitrary weight systems p to I(A)Sp. We show that the p-weighted prenucleolus on the set of balanced TU games is characterized by SIVA, TCOV, SCOV, and IASp and on larger sets by additionally requiring ISp.
1	This paper studies the steady-state properties of the join-the-shortest-queue model in the Halfin–Whitt regime. We focus on the process tracking the number of idle servers and the number of servers with nonempty buffers. Recently, Eschenfeldt and Gamarnik proved that a scaled version of this process converges, over finite time intervals, to a two-dimensional diffusion limit as the number of servers goes to infinity. In this paper, we prove that the diffusion limit is exponentially ergodic and that the diffusion scaled sequence of the steady-state number of idle servers and nonempty buffers is tight. Combined with the process-level convergence proved by Eschenfeldt and Gamarnik, our results imply convergence of steady-state distributions. The methodology used is the generator expansion framework based on Stein’s method, also referred to as the drift-based fluid limit Lyapunov function approach in Stolyar. One technical contribution to the framework is to show how it can be used as a general tool to establish exponential ergodicity.
1	Many service systems provide queue length information to customers, thereby allowing customers to choose among many options of service. However, queue length information is often delayed, and it is often not provided in real time. Recent work by Dong et al. [Dong J, Yom-Tov E, Yom-Tov GB (2018) The impact of delay announcements on hospital network coordination and waiting times. Management Sci. 65(5):1969–1994.] explores the impact of these delays in an empirical study in U.S. hospitals. Work by Pender et al. [Pender J, Rand RH, Wesson E (2017) Queues with choice via delay differential equations. Internat. J. Bifurcation Chaos Appl. Sci. Engrg. 27(4):1730016-1–1730016-20.] uses a two-dimensional fluid model to study the impact of delayed information and determine the exact threshold under which delayed information can cause oscillations in the dynamics of the queue length. In this work, we confirm that the fluid model analyzed by Pender et al. [Pender J, Rand RH, Wesson E (2017) Queues with choice via delay differential equations. Internat. J. Bifurcation Chaos Appl. Sci. Engrg. 27(4):1730016-1–1730016-20.] can be rigorously obtained as a functional law of large numbers limit of a stochastic queueing process, and we generalize their threshold analysis to arbitrary dimensions. Moreover, we prove a functional central limit theorem for the queue length process and show that the scaled queue length converges to a stochastic delay differential equation. Thus, our analysis sheds new insight on how delayed information can produce unexpected system dynamics.
1	We develop a dynamic model of interbank borrowing and lending activities in which banks are organized into clusters, and adjust their monetary reserve levels to meet prescribed capital requirements. Each bank has its own initial monetary reserve level and faces idiosyncratic risks characterized by an independent Brownian motion, whereas system wide, the banks form a hierarchical structure of clusters. We model the interbank transactional dynamics through a set of interacting measure-valued processes. Each individual process describes the intracluster borrowing/lending activities, and the interactions among the processes capture the intercluster financial transactions. We establish the weak limit of the interacting measure-valued processes as the number of banks in the system grows large. We then use the weak limit to develop asymptotic approximations of two proposed macromeasures (the liquidity stress index and the concentration index), both capturing the dynamics of systemic risk. We use numerical examples to illustrate the applications of the asymptotics and conduct-related sensitivity analysis with respect to various indicators of financial activity.
1	A sequence of random variables is exchangeable if the joint distribution of any finite subsequence is invariant to permutations. De Finetti’s representation theorem states that every exchangeable infinite sequence is a convex combination of independent and identically distributed processes. In this paper, we explore the relationship between exchangeability and frequency-dependent posteriors. We show that any stationary process is exchangeable if and only if its posteriors depend only on the empirical frequency of past events.
1	This work concerns the local convergence theory of Newton and quasi-Newton methods for convex-composite optimization: where one minimizes an objective that can be written as the composition of a convex function with one that is continuiously differentiable. We focus on the case in which the convex function is a potentially infinite-valued piecewise linear-quadratic function. Such problems include nonlinear programming, mini-max optimization, and estimation of nonlinear dynamics with non-Gaussian noise as well as many modern approaches to large-scale data analysis and machine learning. Our approach embeds the optimality conditions for convex-composite optimization problems into a generalized equation. We establish conditions for strong metric subregularity and strong metric regularity of the corresponding set-valued mappings. This allows us to extend classical convergence of Newton and quasi-Newton methods to the broader class of nonfinite valued piecewise linear-quadratic convex-composite optimization problems. In particular, we establish local quadratic convergence of the Newton method under conditions that parallel those in nonlinear programming.
1	An investor may invest in a riskless bank account and in a stock that is a standard Black–Scholes asset with occasional Gaussian jumps of the log price, as proposed by Merton [Merton RC (1976) Option pricing when underlying stock returns are discontinuous. J. Financial Econom. 3(1):125–144.]. It is well known how to solve the standard running consumption problem for this investor, which we take as a benchmark for comparing the performance of two different insiders, one who knows in advance of each jump exactly when the jump will happen, and the other who has information in advance of each jump about the size of the jump but no information about the time. These considerations give rise to two novel and concrete stochastic control problems. For each problem, rigorous verification proofs for optimality are presented.
1	We consider a discrete time financial market with proportional transaction costs under model uncertainty and study a numéraire-based semistatic utility maximization problem with an exponential utility preference. The randomization techniques recently developed in Bouchard, Deng, and Tan [Bouchard B, Deng S, Tan X (2019) Super-replication with proportional transaction cost under model uncertainty. Math. Finance 29(3):837–860.], allow us to transform the original problem into a frictionless counterpart on an enlarged space. By suggesting a different dynamic programming argument than in Bartl [Bartl D (2019) Exponential utility maximization under model uncertainty for unbounded endowments. Ann. Appl. Probab. 29(1):577–612.], we are able to prove the existence of the optimal strategy and the convex duality theorem in our context with transaction costs. In the frictionless framework, this alternative dynamic programming argument also allows us to generalize the main results in Bartl [Bartl D (2019) Exponential utility maximization under model uncertainty for unbounded endowments. Ann. Appl. Probab. 29(1):577–612.] to a weaker market condition. Moreover, as an application of the duality representation, some basic features of utility indifference prices are investigated in our robust setting with transaction costs.
1	We consider two-player, zero-sum stochastic games in which each player controls the player’s own state variable living in a compact metric space. The terminology comes from gambling problems in which the state of a player represents its wealth in a casino. Under standard assumptions (e.g., continuous running payoff and nonexpansive transitions), we consider for each discount factor the value vλ of the λ-discounted stochastic game and investigate its limit when λ goes to zero. We show that, under a new acyclicity condition, the limit exists and is characterized as the unique solution of a system of functional equations: the limit is the unique continuous excessive and depressive function such that each player, if the player’s opponent does not move, can reach the zone when the current payoff is at least as good as the limit value without degrading the limit value. The approach generalizes and provides a new viewpoint on the Mertens–Zamir system coming from the study of zero-sum repeated games with lack of information on both sides. A counterexample shows that under a slightly weaker notion of acyclicity, convergence of (vλ) may fail.
1	We study the effect of imperfect memory on decision making in the context of a stochastic sequential action-reward problem. An agent chooses a sequence of actions, which generate discrete rewards at different rates. She is allowed to make new choices at rate β, whereas past rewards disappear from her memory at rate μ. We focus on a family of decision rules where the agent makes a new choice by randomly selecting an action with a probability approximately proportional to the amount of past rewards associated with each action in her memory. We provide closed form formulas for the agent’s steady-state choice distribution in the regime where the memory span is large (𝜇→0) and show that the agent’s success critically depends on how quickly she updates her choices relative to the speed of memory decay. If 𝛽≫𝜇, the agent almost always chooses the best action (that is, the one with the highest reward rate). Conversely, if 𝛽≪𝜇, the agent chooses an action with a probability roughly proportional to its reward rate.
1	In this paper, we present a family of control-stopping games that arise naturally in equilibrium-based models of market microstructure as well as in other models with strategic buyers and sellers. A distinctive feature of this family of games is the fact that the agents do not have any exogenously given fundamental value for the asset, and they deduce the value of their position from the bid and ask prices posted by other agents (i.e., they are pure speculators). As a result, in such a game, the reward function of each agent at the time of stopping depends directly on the controls of other players. The equilibrium problem leads naturally to a system of coupled control-stopping problems (or, equivalently, reflected-backward stochastic differential equations), in which the individual reward functions (or reflecting barriers) depend on the value functions (or solution components) of other agents. The resulting system, in general, presents multiple mathematical challenges because of the nonstandard form of coupling (or reflection). In the present case, this system is also complicated by the fact that the continuous controls of the agents, describing their posted bid and ask prices, are constrained to take values in a discrete grid. The latter feature reflects the presence of a positive tick size in the market, and it creates additional discontinuities in the agents’ reward functions (or reflecting barriers). Herein we prove the existence of a solution to the associated system in a special Markovian framework, provide numerical examples, and discuss the potential applications.
1	An edge-weighted graph 𝐺G is called stable if the value of a maximum-weight matching equals the value of a maximum-weight fractional matching. Stable graphs play an important role in network bargaining games and cooperative matching games, because they characterize instances that admit stable outcomes. We give the first polynomial-time algorithm to find a minimum cardinality subset of vertices whose removal from G yields a stable graph, for any weighted graph G. The algorithm is combinatorial and exploits new structural properties of basic fractional matchings, which are of independent interest. In contrast, we show that the problem of finding a minimum cardinality subset of edges whose removal from a weighted graph G yields a stable graph, does not admit any constant-factor approximation algorithm, unless P = NP. In this setting, we develop an O(Δ)-approximation algorithm for the problem, where Δ is the maximum degree of a node in G.
1	This paper presents a systematic study of the notion of surplus invariance, which plays a natural and important role in the theory of risk measures and capital requirements. So far, this notion has been investigated in the setting of some special spaces of random variables. In this paper, we develop a theory of surplus invariance in its natural framework, namely, that of vector lattices. Besides providing a unifying perspective on the existing literature, we establish a variety of new results including dual representations and extensions of surplus-invariant risk measures and structural results for surplus-invariant acceptance sets. We illustrate the power of the lattice approach by specifying our results to model spaces with a dominating probability, including Orlicz spaces, as well as to robust model spaces without a dominating probability, where the standard topological techniques and exhaustion arguments cannot be applied.
1	Makespan scheduling on identical machines is one of the most basic and fundamental packing problems studied in the discrete optimization literature. It asks for an assignment of n jobs to a set of m identical machines that minimizes the makespan. The problem is strongly NP-hard, and thus we do not expect a (1+𝜀)-approximation algorithm with a running time that depends polynomially on 1/𝜀. It has been recently shown that a subexponential running time on 1/𝜀 would imply that the Exponential Time Hypothesis (ETH) fails. A long sequence of algorithms have been developed that try to obtain low dependencies on 1/𝜀, the better of which achieves a quadratic running time on the exponent. In this paper we obtain an algorithm with an almost-linear dependency on 1/𝜀 in the exponent, which is tight under ETH up to logarithmic factors. Our main technical contribution is a new structural result on the configuration-IP integer linear program. More precisely, we show the existence of a highly symmetric and sparse optimal solution, in which all but a constant number of machines are assigned a configuration with small support. This structure can then be exploited by integer programming techniques and enumeration. We believe that our structural result is of independent interest and should find applications to other settings. We exemplify this by applying our structural results to the minimum makespan problem on related machines and to a larger class of objective functions on parallel machines. For all these cases, we obtain an efficient PTAS with running time with an almost-linear dependency on 1/𝜀 and polynomial in n.
1	In network theory, Jackson and Wolinsky introduced a now widely used notion of stability for unweighted network formation called pairwise stability. We prove the existence of pairwise stable weighted networks under assumptions on payoffs that are similar to those in Nash's and Glicksberg’s existence theorem (continuity and quasi concavity). Then, we extend our result, allowing payoffs to depend not only on the network, but also on some game-theoretic strategies. The proof is not a standard application of tools from game theory, the difficulty coming from the fact that the pairwise stability notion has both cooperative and noncooperative features. Last, some examples are given and illustrate how our results may open new paths in the literature on network formation.
1	In this paper, we study the asymptotic behavior of a stochastic approximation scheme on two timescales with set-valued drift functions and in the presence of nonadditive iterate-dependent Markov noise. We show that the recursion on each timescale tracks the flow of a differential inclusion obtained by averaging the set-valued drift function in the recursion with respect to a set of measures accounting for both averaging with respect to the stationary distributions of the Markov noise terms and the interdependence between the two recursions on different timescales. The framework studied in this paper builds on a recent work by Ramaswamy and Bhatnagar, by allowing for the presence of nonadditive iterate-dependent Markov noise. As an application, we consider the problem of computing the optimum in a constrained convex optimization problem, where the objective function and the constraints are averaged with respect to the stationary distribution of an underlying Markov chain. Further, the proposed scheme neither requires the differentiability of the objective function nor the knowledge of the averaging measure.
1	We study several service providers that keep spare parts in stock to protect for downtime of their high-tech machines and that face different downtime costs per stockout. Service providers can cooperate by forming a joint spare parts pool, and we study the allocation of the joint costs to the individual service providers by studying an associated cooperative game. In extant literature, the joint spare parts pool is typically controlled by a suboptimal full-pooling policy. A full-pooling policy may lead to an empty core of the associated cooperative game, and we show this result in our setting as well. We then focus on situations where service providers apply an optimal policy: a stratification that determines, depending on the real-time on-hand inventory, which service providers may take parts from the pool. We formulate the associated stratified pooling game by defining each coalitional value in terms of the minimal long-run average costs of a Markov decision process. We present a proof demonstrating that stratified pooling games always have a nonempty core. This five-step proof is of interest in itself, because it may be more generally applicable for other cooperative games where coalitional values can be defined in terms of Markov decision processes.
1	We propose a model for optimizing the last-mile delivery of n packages from a distribution center to their final recipients, using a strategy that combines the use of ride-sharing platforms (e.g., Uber or Lyft) with traditional in-house van delivery systems. The main objective is to compute the optimal reward offered to private drivers for each of the n packages such that the total expected cost of delivering all packages is minimized. Our technical approach is based on the formulation of a discrete sequential packing problem, in which bundles of packages are picked up from the warehouse at random times during the interval [0,𝑇][0,T]. Our theoretical results include both exact and asymptotic (as 𝑛→∞n→∞) expressions for the expected number of packages that are picked up by time T. They are closely related to the classical Rényi’s parking/packing problem. Our proposed framework is scalable with the number of packages.
1	We consider the bin packing problem with d different item sizes and revisit the structure theorem given by Goemans and Rothvoß about solutions of the integer cone. We present new techniques on how solutions can be modified and give a new structure theorem that relies on the set of vertices of the underlying integer polytope. As a result of our new structure theorem, we obtain an algorithm for the bin packing problem with running time |V|2O(d)⋅enc(I)O(1)|𝑉|2𝑂(𝑑)⋅𝑒𝑛𝑐(𝐼)𝑂(1), where V is the set of vertices of the integer knapsack polytope, and enc(I)𝑒𝑛𝑐(𝐼) is the encoding length of the bin packing instance. The algorithm is fixed-parameter tractable, parameterized by the number of vertices of the integer knapsack polytope |V||𝑉|. This shows that the bin packing problem can be solved efficiently when the underlying integer knapsack polytope has an easy structure (i.e., has a small number of vertices). Furthermore, we show that the presented bounds of the structure theorem are asymptotically tight. We give a construction of bin packing instances using new structural insights and classical number theoretical theorems which yield the desired lower bound.
1	Experimental design is a classical statistics problem, and its aim is to estimate an unknown vector from linear measurements where a Gaussian noise is introduced in each measurement. For the combinatorial experimental design problem, the goal is to pick a subset of experiments so as to make the most accurate estimate of the unknown parameters. In this paper, we will study one of the most robust measures of error estimation—the D-optimality criterion, which corresponds to minimizing the volume of the confidence ellipsoid for the estimation error. The problem gives rise to two natural variants depending on whether repetitions of experiments are allowed or not. We first propose an approximation algorithm with a 1/e-approximation for the D-optimal design problem with and without repetitions, giving the first constant-factor approximation for the problem. We then analyze another sampling approximation algorithm and prove that it is asymptotically optimal. Finally, for D-optimal design with repetitions, we study a different algorithm proposed by the literature and show that it can improve this asymptotic approximation ratio. All the sampling algorithms studied in this paper are shown to admit polynomial-time deterministic implementations.
1	We consider a system of N identical server pools and a single dispatcher in which tasks with unit-exponential service requirements arrive at rate 𝜆(𝑁)λ(N). In order to optimize the experienced performance, the dispatcher aims to evenly distribute the tasks across the various server pools. Specifically, when a task arrives, the dispatcher assigns it to the server pool with the minimum number of tasks among d(N) randomly selected server pools. We construct a stochastic coupling to bound the difference in the system occupancy processes between the join-the-shortest-queue (JSQ) policy and a scheme with an arbitrary value of d(N). We use the coupling to derive the fluid limit in case 𝑑(𝑁)→∞d(N)→∞ and 𝜆(𝛮)/𝛮→𝜆λ(Ν)/Ν→λ as 𝑁→∞N→∞ along with the associated fixed point. The fluid limit turns out to be insensitive to the exact growth rate of d(N) and coincides with that for the JSQ policy. We further establish that the diffusion limit corresponds to that for the JSQ policy as well, as long as 𝑑(𝑁)/𝑁‾‾√log(𝑁)→∞d(N)/Nlog(N)→∞, and characterize the common limiting diffusion process. These results indicate that the JSQ optimality can be preserved at the fluid and diffusion levels while reducing the overhead by nearly a factor O(N) and O(𝑁‾‾√/log(𝑁)N/log(N)), respectively.
1	Stochastic programming problems generally lead to large-scale programs if the number of random outcomes is large or if the problem has many stages. A way to tackle them is provided by scenario-tree generation methods, which construct approximate problems from a reduced subset of outcomes. However, it is well known that the number of scenarios required to keep the approximation error within a given tolerance grows rapidly with the number of random parameters and stages. For this reason, to limit the fast growth of complexity, scenario-tree generation methods tailored to problems must be developed. These will use more information about the problem than just the underlying probability distributions; namely, they will also take into account the objective function and the constraints. In this paper, we develop a general framework to build problem-driven scenario trees. We do so by studying how the optimal-value error arises as a sum of lower-level errors made at each node of the tree. We show how these small but numerous node errors depend on the specific features of the problem and how they can be controlled by designing scenario trees with appropriate branching structures and discretization points and weights. We illustrate our approach on two examples.
1	In this paper, we study a class of discrete-time mean-field games under the infinite-horizon risk-sensitive optimality criterion. Risk sensitivity is introduced for each agent (player) via an exponential utility function. In this game model, each agent is coupled with the rest of the population through the empirical distribution of the states, which affects both the agent’s individual cost and its state dynamics. Under mild assumptions, we establish the existence of a mean-field equilibrium in the infinite-population limit as the number of agents (N) goes to infinity, and we then show that the policy obtained from the mean-field equilibrium constitutes an approximate Nash equilibrium when N is sufficiently large.
1	Motivated by database locking problems in today’s massive computing systems, we analyze a queueing network with many servers in parallel (files) to which jobs (writing access requests) arrive according to a Poisson process. Each job requests simultaneous access to a random number of files in the database and will lock them for a random period of time. Alternatively, one can think of a queueing system where jobs are split into several fragments that are then randomly routed to specific servers in the network to be served in a synchronized fashion. We assume that the system operates on a first-come, first-served basis. The synchronization and service discipline create blocking and idleness among the servers, which leads to a strict stability condition compared with other distributed queueing models. We analyze the stationary waiting time distribution of jobs under a many-server limit and provide exact tail asymptotics. These asymptotics generalize the celebrated Cramér–Lundberg approximation for the single-server queue.
1	This paper studies a two-player zero-sum Dynkin game arising from pricing an option on an asset whose rate of return is unknown to both players. Using filtering techniques, we first reduce the problem to a zero-sum Dynkin game on a bidimensional diffusion (X,Y). Then we characterize the existence of a Nash equilibrium in pure strategies in which each player stops at the hitting time of (X,Y) to a set with a moving boundary. A detailed description of the stopping sets for the two players is provided along with global C1 regularity of the value function.
1	We introduce a unified algorithmic framework, called the proximal-like incremental aggregated gradient (PLIAG) method, for minimizing the sum of a convex function that consists of additive relatively smooth convex components and a proper lower semicontinuous convex regularization function over an abstract feasible set whose geometry can be captured by using the domain of a Legendre function. The PLIAG method includes many existing algorithms in the literature as special cases, such as the proximal gradient method, the Bregman proximal gradient method (also called the NoLips algorithm), the incremental aggregated gradient method, the incremental aggregated proximal method, and the proximal incremental aggregated gradient method. It also includes some novel interesting iteration schemes. First, we show that the PLIAG method is globally sublinearly convergent without requiring a growth condition, which extends the sublinear convergence result for the proximal gradient algorithm to incremental aggregated-type first-order methods. Then, by embedding a so-called Bregman distance growth condition into a descent-type lemma to construct a special Lyapunov function, we show that the PLIAG method is globally linearly convergent in terms of both function values and Bregman distances to the optimal solution set, provided that the step size is not greater than some positive constant. The convergence results derived in this paper are all established beyond the standard assumptions in the literature (i.e., without requiring the strong convexity and the Lipschitz gradient continuity of the smooth part of the objective). When specialized to many existing algorithms, our results recover or supplement their convergence results under strictly weaker conditions.
1	Ideal matrices and clutters are prevalent in combinatorial optimization, ranging from balanced matrices, clutters of T-joins, to clutters of rooted arborescences. Most of the known examples of ideal clutters are combinatorial in nature. In this paper, rendered by the recently developed theory of cuboids, we provide a different class of ideal clutters, one that is geometric in nature. The advantage of this new class of ideal clutters is that it allows for infinitely many ideal minimally nonpacking clutters. We characterize the densest ideal minimally nonpacking clutters of the class. Using the tools developed, we then verify the replication conjecture for the class.
1	We consider the problem of makespan minimization on unrelated machines when job sizes are stochastic. The goal is to find a fixed assignment of jobs to machines, to minimize the expected value of the maximum load over all the machines. For the identical-machines special case when the size of a job is the same across all machines, a constant-factor approximation algorithm has long been known. Our main result is the first constant-factor approximation algorithm for the general case of unrelated machines. This is achieved by (i) formulating a lower bound using an exponential-size linear program that is efficiently computable and (ii) rounding this linear program while satisfying only a specific subset of the constraints that still suffice to bound the expected makespan. We also consider two generalizations. The first is the budgeted makespan minimization problem, where the goal is to minimize the expected makespan subject to scheduling a target number (or reward) of jobs. We extend our main result to obtain a constant-factor approximation algorithm for this problem. The second problem involves q-norm objectives, where we want to minimize the expected q-norm of the machine loads. Here we give an 𝑂(𝑞/log𝑞)O(q/log q)-approximation algorithm, which is a constant-factor approximation for any fixed q.
1	In cost-sharing games with delays, a set of agents jointly uses a subset of resources. Each resource has a fixed cost that has to be shared by the players, and each agent has a nonshareable player-specific delay for each resource. A separable cost-sharing protocol determines cost shares that are budget-balanced, separable, and guarantee existence of pure Nash equilibria (PNE). We provide black-box reductions reducing the design of such a protocol to the design of an approximation algorithm for the underlying cost-minimization problem. In this way, we obtain separable cost-sharing protocols in matroid games, single-source connection games, and connection games on n-series-parallel graphs. All these reductions are efficiently computable - given an initial allocation profile, we obtain a cheaper profile and separable cost shares turning the profile into a PNE. Hence, in these domains, any approximation algorithm yields a separable cost-sharing protocol with price of stability bounded by the approximation factor.
1	We consider an N-player multiarmed bandit game in which each player chooses one out of M arms for T turns. Each player has different expected rewards for the arms, and the instantaneous rewards are independent and identically distributed or Markovian. When two or more players choose the same arm, they all receive zero reward. Performance is measured using the expected sum of regrets compared with optimal assignment of arms to players that maximizes the sum of expected rewards. We assume that each player only knows that player’s own actions and the reward that player received each turn. Players cannot observe the actions of other players, and no communication between players is possible. We present a distributed algorithm and prove that it achieves an expected sum of regrets of near-𝑂(logT)O(logT). This is the first algorithm to achieve a near order optimal regret in this fully distributed scenario. All other works have assumed that either all players have the same vector of expected rewards or that communication between players is possible.
1	This paper studies the value of switching actions in the Prediction From Experts problem (PFE) and Adversarial Multiarmed Bandits problem (MAB). First, we revisit the well-studied and practically motivated setting of PFE with switching costs. Many algorithms achieve the minimax optimal order for both regret and switches in expectation; however, high probability guarantees are an open problem. We present the first algorithms that achieve this optimal order for both quantities with high probability. This also implies the first high probability guarantees for several other problems, and, in particular, is efficiently adaptable to online combinatorial optimization with limited switching. Next, to investigate the value of switching actions more granularly, we introduce the switching budget setting, which limits algorithms to a fixed number of (costless) switches. Using this result and several reductions, we unify previous work and completely characterize the complexity of this switching budget setting up to small polylogarithmic factors: for both PFE and MAB, for all switching budgets, and for both expectation and high probability guarantees. Interestingly, as the switching budget decreases, the minimax regret rate admits a phase transition for PFE but not for MAB. These results recover and generalize the known minimax rates for the (arbitrary) switching cost setting.
1	We revisit the problem of online linear optimization in the case where the set of feasible actions is accessible through an approximated linear optimization oracle with a factor α multiplicative approximation guarantee. This setting in particular is interesting because it captures natural online extensions of well-studied offline linear optimization problems that are NP-hard yet admit efficient approximation algorithms. The goal here is to minimize the α-regret, which is the natural extension to this setting of the standard regret in online learning. We present new algorithms with significantly improved oracle complexity for both the full-information and bandit variants of the problem. Mainly, for both variants, we present α-regret bounds of 𝑂(𝑇−1/3), were T is the number of prediction rounds, using only 𝑂(𝑙𝑜𝑔𝑇) calls to the approximation oracle per iteration, on average. These are the first results to obtain both the average oracle complexity of 𝑂(𝑙𝑜𝑔𝑇) (or even polylogarithmic in T) and α -regret bound 𝑂(𝑇−𝑐) for a constant c > 0 for both variants.
1	We solve a family of fractional Riccati equations with constant (possibly complex) coefficients. These equations arise, for example, in fractional Heston stochastic volatility models, which have received great attention in the recent financial literature because of their ability to reproduce a rough volatility behavior. We first consider the case of a zero initial value corresponding to the characteristic function of the log-price. Then we investigate the case of a general starting value associated to a transform also involving the volatility process. The solution to the fractional Riccati equation takes the form of power series, whose convergence domain is typically finite. This naturally suggests a hybrid numerical algorithm to explicitly obtain the solution also beyond the convergence domain of the power series. Numerical tests show that the hybrid algorithm is extremely fast and stable. When applied to option pricing, our method largely outperforms the only available alternative, based on the Adams method.
1	Zero-sum stochastic games, henceforth stochastic games, are a classical model in game theory in which two opponents interact and the environment changes in response to the players’ behavior. The central solution concepts for these games are the discounted values and the value, which represent what playing the game is worth to the players for different levels of impatience. In the present manuscript, we provide algorithms for computing exact expressions for the discounted values and for the value, which are polynomial in the number of pure stationary strategies of the players. This result considerably improves all the existing algorithms.
1	We consider a load-balancing problem for a network of parallel queues in which information on the state of the queues is subject to a delay. In this setting, adopting a routing policy that performs well when applied to the current state of the queues can perform quite poorly when applied to the delayed state of the queues. Viewing this as a problem of control under partial observations, we propose using an estimate of the current queue lengths as the input to the join-the-shortest-queue policy. For a general class of estimation schemes, under heavy traffic conditions, we prove convergence of the diffusion-scaled process to a solution of a so-called diffusion model, in which an important step toward this goal establishes that the estimated queue lengths undergo state-space collapse. In some cases, our diffusion model is given by a novel stochastic delay equation with reflection, in which the Skorokhod boundary term appears with delay. We illustrate our results with examples of natural estimation schemes, discuss their implementability, and compare their relative performance using simulations.
1	We consider the robust standard quadratic optimization problem (RStQP), in which an uncertain (possibly indefinite) quadratic form is optimized over the standard simplex. Following most approaches, we model the uncertainty sets by balls, polyhedra, or spectrahedra, more generally, by ellipsoids or order intervals intersected with subcones of the copositive matrix cone. We show that the copositive relaxation gap of the RStQP equals the minimax gap under some mild assumptions on the curvature of the aforementioned uncertainty sets and present conditions under which the RStQP reduces to the standard quadratic optimization problem. These conditions also ensure that the copositive relaxation of an RStQP is exact. The theoretical findings are accompanied by the results of computational experiments for a specific application from the domain of graph clustering, more precisely, community detection in (social) networks. The results indicate that the cardinality of communities tend to increase for ellipsoidal uncertainty sets and to decrease for spectrahedral uncertainty sets.
1	Many algorithms that are originally designed without explicitly considering incentive properties are later combined with simple pricing rules and used as mechanisms. A key question is therefore to understand which algorithms, or, more generally, which algorithm design principles, when combined with simple payment rules such as pay your bid, yield mechanisms with a small price of anarchy. Our main result concerns mechanisms that are based on the relax-and-round paradigm. It shows that oblivious rounding schemes approximately preserve price of anarchy guarantees provable via smoothness. By virtue of our smoothness proofs, our price of anarchy bounds extend to Bayes–Nash equilibria and learning outcomes. In fact, they even apply out of equilibrium, requiring only that agents have no regret for deviations to half their value. We demonstrate the broad applicability of our main result by instantiating it for a wide range of optimization problems ranging from sparse packing integer programs, over single-source unsplittable flow problems and combinatorial auctions with fractionally subadditive valuations, to a maximization variant of the traveling salesman problem.
1	Very few studies have explored the structure of optimal switching regimes. We extend the existing research on the infinite-horizon multiple-regime switching problem with an arbitrary number of switch options by replacing the linear running reward function with a quadratic function in the objective function. To make our analysis more rigorous, we establish the theoretical basis for the application of the simultaneous multiple-regime switches to the problem with the extended objective function, and provide the sufficient condition under which each certain separated region in the space includes, at most, one single connected optimal switching region, which determines the structure of the optimal switching regions, and we identify the structure of the optimal switching regions for the particular problem.
1	Via a family of monotone scalar functions, a preorder on a set is extended to its power set and then used to construct a hull operator and a corresponding complete lattice of sets. Functions mapping into the preordered set are extended to complete lattice-valued ones, and concepts for exact and approximate solutions for corresponding set optimization problems are introduced and existence results are given. Well-posedness for complete lattice-valued problems is introduced and characterized. The new approach is compared with existing ones in vector and set optimization. Its relevance is shown by means of many examples from multicriteria decision making, statistics, and mathematical economics and finance.
1	Competitive equilibrium from equal incomes (CEEI) is a classic solution to the problem of fair and efficient allocation of goods (Foley 1967, Varian 1974). Every agent receives an equal budget of artificial currency with which to purchase goods, and prices match demand and supply. However, a CEEI is not guaranteed to exist when the goods are indivisible even in the simple two-agent, single-item market. Yet it is easy to see that, once the two budgets are slightly perturbed (made generic), a competitive equilibrium does exist. In this paper, we aim to extend this approach beyond the single-item case and study the existence of equilibria in markets with two agents and additive preferences over multiple items. We show that, for agents with equal budgets, making the budgets generic—by adding vanishingly small random perturbations—ensures the existence of equilibrium. We further consider agents with arbitrary nonequal budgets, representing nonequal entitlements for goods. We show that competitive equilibrium guarantees a new notion of fairness among nonequal agents and that it exists in cases of interest (such as when the agents have identical preferences) if budgets are perturbed. Our results open opportunities for future research on generic equilibrium existence and fair treatment of nonequals.
1	Our input instance is a bipartite graph G where each vertex has a preference list ranking its neighbors in a strict order of preference. A matching M is popular if there is no matching N such that the number of vertices that prefer N to M outnumber those that prefer M to N. Each edge is associated with a utility and we consider the problem of matching vertices in a popular and utility-optimal manner. It is known that it is NP-hard to compute a max-utility popular matching. So we consider mixed matchings: a mixed matching is a probability distribution or a lottery over matchings. Our main result is that the popular fractional matching polytope 𝒫PG is half-integral and in the special case where a stable matching in G is a perfect matching, this polytope is integral. This implies that there is always a max-utility popular mixed matching which is the average of two integral matchings. So in order to implement a max-utility popular mixed matching in G, we need just a single random bit. We analyze the popular fractional matching polytope whose description may have exponentially many constraints via an extended formulation with a linear number of constraints. The linear program that gives rise to this formulation has an unusual property: self-duality. The self-duality of this LP plays a crucial role in our proof. Our result implies that a max-utility popular half-integral matching in G and also in the roommates problem (where the input graph need not be bipartite) can be computed in polynomial time.
1	A new definition of continuous-time equilibrium controls is introduced. As opposed to the standard definition, which involves a derivative-type operation, the new definition parallels how a discrete-time equilibrium is defined and allows for unambiguous economic interpretation. The terms “strong equilibria” and “weak equilibria” are coined for controls under the new and standard definitions, respectively. When the state process is a time-homogeneous continuous-time Markov chain, a careful asymptotic analysis gives complete characterizations of weak and strong equilibria. Thanks to the Kakutani–Fan fixed-point theorem, the general existence of weak and strong equilibria is also established under an additional compactness assumption. Our theoretic results are applied to a two-state model under nonexponential discounting. In particular, we demonstrate explicitly that there can be incentive to deviate from a weak equilibrium, which justifies the need for strong equilibria. Our analysis also provides new results for the existence and characterization of discrete-time equilibria under infinite horizon.
1	We provide a new tool for simulation of a random variable (target source) from a randomness source with side information. Considering the total variation distance as the measure of precision, this tool offers an upper bound for the precision of simulation, which is vanishing exponentially in the difference of Rényi entropies of the randomness and target sources. This tool finds application in games in which the players wish to generate their actions (target source) as a function of a randomness source such that they are almost independent of the observations of the opponent (side information). In particular, we study zero-sum repeated games in which the players are restricted to strategies that require only a limited amount of randomness. Let be the max-min value of the n stage game. Previous works have characterized lim𝑛→∞𝑣𝑛limn→∞vn, that is, the long-run max-min value, but they have not provided any result on the value of vn for a given finite n-stage game. Here, we utilize our new tool to study how vn converges to the long-run max-min value.
1	We develop a probabilistic approach to continuous-time finite state mean field games. Based on an alternative description of continuous-time Markov chains by means of semimartingales and the weak formulation of stochastic optimal control, our approach not only allows us to tackle the mean field of states and the mean field of control at the same time, but also extends the strategy set of players from Markov strategies to closed-loop strategies. We show the existence and uniqueness of Nash equilibrium for the mean field game as well as how the equilibrium of a mean field game consists of an approximative Nash equilibrium for the game with a finite number of players under different assumptions of structure and regularity on the cost functions and transition rate between states.
1	We study the efficiency of mechanisms for allocating a divisible resource. Given scalar signals submitted by all users, such a mechanism decides the fraction of the resource that each user will receive and a payment that will be collected from her. Users are self-interested and aim to maximize their utility (defined as their value for the resource fraction they receive minus their payment). Starting with the seminal work of Johari and Tsitsiklis, a long list of papers studied the price of anarchy (in terms of the social welfare—the total users’ value) of resource allocation mechanisms for a variety of allocation and payment rules. Here, we further assume that each user has a budget constraint that invalidates strategies that yield a payment that is higher than the user’s budget. This subtle assumption, which is arguably more realistic, constitutes the traditional price of anarchy analysis meaningless as the set of equilibria may change drastically and their social welfare can be arbitrarily far from optimal. Instead, we study the price of anarchy using the liquid welfare benchmark that measures efficiency taking budget constraints into account. We show a tight bound of 2 on the liquid price of anarchy of the well-known Kelly mechanism and prove that this result is essentially best possible among all multiuser resource allocation mechanisms. This comes in sharp contrast to the no-budget setting where there are mechanisms that considerably outperform Kelly in terms of social welfare and even achieve full efficiency. In our proofs, we exploit the particular structure of worst-case games and equilibria, which also allows us to design (nearly) optimal two-player mechanisms by solving simple differential equations.
1	Reflected Brownian motion (RBM) in a convex polyhedral cone arises in a variety of applications ranging from the theory of stochastic networks to mathematical finance, and under general stability conditions, it has a unique stationary distribution. In such applications, to implement a stochastic optimization algorithm or quantify robustness of a model, it is useful to characterize the dependence of stationary performance measures on model parameters. In this paper, we characterize parametric sensitivities of the stationary distribution of an RBM in a simple convex polyhedral cone, that is, sensitivities to perturbations of the parameters that define the RBM—namely the covariance matrix, drift vector, and directions of reflection along the boundary of the polyhedral cone. In order to characterize these sensitivities, we study the long-time behavior of the joint process consisting of an RBM along with its so-called derivative process, which characterizes pathwise derivatives of RBMs on finite time intervals. We show that the joint process is positive recurrent and has a unique stationary distribution and that parametric sensitivities of the stationary distribution of an RBM can be expressed in terms of the stationary distribution of the joint process. This can be thought of as establishing an interchange of the differential operator and the limit in time. The analysis of ergodicity of the joint process is significantly more complicated than that of the RBM because of its degeneracy and the fact that the derivative process exhibits jumps that are modulated by the RBM. The proofs of our results rely on path properties of coupled RBMs and contraction properties related to the geometry of the polyhedral cone and directions of reflection along the boundary. Our results are potentially useful for developing efficient numerical algorithms for computing sensitivities of functionals of stationary RBMs.
1	Consider a fractional Brownian motion (fBM) BH={BH(t):t∈[0,1]}𝐵𝐻={𝐵𝐻(𝑡):𝑡∈[0,1]} with Hurst index H∈(0,1)𝐻∈(0,1). We construct a probability space supporting both BH and a fully simulatable process ˆBH∈𝐵ˆ𝐻∈ such thatsupt∈[0,1]|BH(t)−ˆBH∈(t)|≤∈sup𝑡∈[0,1]∣∣∣𝐵𝐻(𝑡)−𝐵ˆ𝐻∈(𝑡)∣∣∣≤∈with probability one for any user-specified error bound ∈>0. When H>1/2, we further enhance our error guarantee to the α-Hölder norm for any α∈(1/2,H). This enables us to extend our algorithm to the simulation of fBM-driven stochastic differential equations Y={Y(t):t∈[0,1]}. Under mild regularity conditions on the drift and diffusion coefficients of Y, we construct a probability space supporting both Y and a fully simulatable process ˆY∈ such thatsupt∈[0,1]|Y(t)−ˆY∈(t)|≤∈with probability one. Our algorithms enjoy the tolerance-enforcement feature, under which the error bounds can be updated sequentially in an efficient way. Thus, the algorithms can be readily combined with other advanced simulation techniques to estimate the expectations of functionals of fBMs efficiently.
1	We consider nonconvex constrained optimization problems and propose a new approach to the convergence analysis based on penalty functions. We make use of classical penalty functions in an unconventional way, in that penalty functions only enter in the theoretical analysis of convergence while the algorithm itself is penalty free. Based on this idea, we are able to establish several new results, including the first general analysis for diminishing stepsize methods in nonconvex, constrained optimization, showing convergence to generalized stationary points, and a complexity study for sequential quadratic programming–type algorithms.
1	We consider the forecast aggregation problem in repeated settings where the forecasts are of a binary state of nature. In each period multiple experts provide forecasts about the state. The goal of the aggregator is to aggregate those forecasts into a subjective accurate forecast. We assume that the experts are Bayesian and the aggregator is non-Bayesian and ignorant of the information structure (i.e., the distribution over the signals) under which the experts make their forecasts. The aggregator observes the experts’ forecasts only. At the end of each period, the realized state is observed by the aggregator. We focus on the question of whether the aggregator can learn to optimally aggregate the forecasts of the experts, where the optimal aggregation is the Bayesian aggregation that takes into account all the information in the system. We consider the class of partial evidence information structures, where each expert is exposed to a different subset of conditionally independent signals. Our main results are positive: we show that optimal aggregation can be learned in polynomial time in quite a wide range of instances in partial evidence environments. We provide an exact characterization of the instances where optimal learning is possible as well as those where it is impossible.
