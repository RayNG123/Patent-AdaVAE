{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d-NB-6b6gtgi","outputId":"109b970a-c077-4ecf-abfe-9933bc6a86ac","executionInfo":{"status":"ok","timestamp":1691771602120,"user_tz":240,"elapsed":17503,"user":{"displayName":"Yusen Wu","userId":"08866010899412433739"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n","  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m114.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.16.4 safetensors-0.3.2 tokenizers-0.13.3 transformers-4.31.0\n","Collecting sentence_transformers\n","  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.31.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.65.0)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.0.1+cu118)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.15.2+cu118)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.23.5)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.10.1)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (3.8.1)\n","Collecting sentencepiece (from sentence_transformers)\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.16.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.12.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.31.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.7.1)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (16.0.6)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.10.31)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.3.2)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (8.1.6)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (1.3.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.2.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence_transformers) (9.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2023.7.22)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n","Building wheels for collected packages: sentence_transformers\n","  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125925 sha256=e010235cc80819263dafbc83c1a3e1233c83d0a1bbdcf7191fce9582ac80daef\n","  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n","Successfully built sentence_transformers\n","Installing collected packages: sentencepiece, sentence_transformers\n","Successfully installed sentence_transformers-2.2.2 sentencepiece-0.1.99\n"]}],"source":["!pip install transformers\n","!pip install sentence_transformers"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"Wm3TnrLxUdun","colab":{"base_uri":"https://localhost:8080/","height":413},"executionInfo":{"status":"error","timestamp":1692148254837,"user_tz":240,"elapsed":23774,"user":{"displayName":"Yusen Wu","userId":"08866010899412433739"}},"outputId":"2fa1e465-27e0-47a9-c7e3-694beb60b693"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]},{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-c6f5b7cb3901>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLoggingHandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreaders\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInputExample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sentence_transformers'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","from torch.utils.data import DataLoader\n","from sentence_transformers import losses, util,models\n","from sentence_transformers import LoggingHandler, SentenceTransformer, evaluation\n","from sentence_transformers.readers import InputExample\n","import logging\n","from datetime import datetime\n","import csv\n","import os\n","import random\n","import pandas as pd\n","import numpy as np\n","from tqdm import tqdm\n","from sentence_transformers.util import batch_to_device"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vp6_R4hVM8FO"},"outputs":[],"source":["import pandas as pd\n","grant = pd.read_csv('/content/drive/MyDrive/innovae-revision/innovae-adavae/data/patent_grant_cleanded_new.csv').sample(frac = 1,random_state = 10).reset_index(drop = True)\n","\n","#train set split\n","train_data = grant[grant['set'] == 'train'].reset_index(drop = True)\n","test_data = grant[grant['set'] == 'val'].reset_index(drop = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cBhlPtLsisQp"},"outputs":[],"source":["def prepare_data_for_paraphrase_mining_evaluator(similar_data):\n","    sentences_map = {}\n","    duplicates_list = []\n","\n","    for idx, pair in enumerate(similar_data):\n","        text1_id = f\"s{2 * idx + 1}\"\n","        text2_id = f\"s{2 * idx + 2}\"\n","\n","        sentences_map[text1_id] = pair[0]\n","        sentences_map[text2_id] = pair[1]\n","\n","        duplicates_list.append((text1_id, text2_id))\n","\n","    return sentences_map, duplicates_list\n","\n","def prepare_data_for_binary_classification_evaluator(similar_data, irrelevant_data):\n","    sentence1 = []\n","    sentence2 = []\n","    labels = []\n","\n","    # Add similar pairs and their labels\n","    for t in similar_data:\n","      sentence1.append(t[0])\n","      sentence2.append(t[1])\n","      labels.append(1)\n","\n","    for i,t in enumerate(similar_data):\n","      sentence1.append(t[0])\n","      sentence2.append(irrelevant_data[i])\n","      labels.append(0)\n","    return sentence1, sentence2, labels\n","\n","def callback(score,batch,step):\n","  print(score,batch,step)\n","#data_for_similar = [(i,j) for i,j in zip(test_data['primary_claim'],test_data['prior_art_primary_claim'])]\n","#irelavent_data_similar = [i for i in test_data['3']]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lTrVbS-mVZRL"},"outputs":[],"source":["queries = {}\n","corpus = {}\n","relevant_docs = {}\n","\n","# Prepare data structures\n","for i, (query, similar_text, dissimilar_text) in enumerate(zip(test_data['primary_claim'],test_data['prior_art_primary_claim'],test_data['0'])):\n","    query_id = f'query{i+1}'\n","    similar_doc_id = f'similar_doc{i+1}'\n","    dissimilar_doc_id = f'dissimilar_doc{i+1}'\n","\n","    queries[query_id] = query\n","    corpus[similar_doc_id] = similar_text\n","    corpus[dissimilar_doc_id] = dissimilar_text\n","\n","    relevant_docs[query_id] = {similar_doc_id}\n","\n","######### Read train data  ##########\n","train_samples = []\n","for idx,row in train_data.iterrows():\n","  train_samples.append(InputExample(texts=[row['new_text'], row['prior_text']], label=1))\n","  train_samples.append(InputExample(texts=[row['prior_text'], row['new_text']], label=1))\n","\n","#### Just some code to print debug information to stdout\n","logging.basicConfig(format='%(asctime)s - %(message)s',\n","                    datefmt='%Y-%m-%d %H:%M:%S',\n","                    level=logging.INFO,\n","                    handlers=[LoggingHandler()])\n","logger = logging.getLogger(__name__)\n","#### /print debug information to stdout\n","\n","\n","#As base model, we use DistilBERT-base that was pre-trained on NLI and STSb data\n","word_embedding_model = models.Transformer('anferico/bert-for-patents')\n","pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n","model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n","\n","#### Just some code to print debug information to stdout\n","logging.basicConfig(format='%(asctime)s - %(message)s',\n","                    datefmt='%Y-%m-%d %H:%M:%S',\n","                    level=logging.INFO,\n","                    handlers=[LoggingHandler()])\n","logger = logging.getLogger(__name__)\n","#### /print debug information to stdout\n","\n","\n","#As base model, we use DistilBERT-base that was pre-trained on NLI and STSb data\n","word_embedding_model = models.Transformer('anferico/bert-for-patents')\n","pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n","model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n","\n","auto_model = model._first_module().auto_model\n","\n","for name, param in auto_model.named_parameters():\n","  param.requires_grad = False\n","\n","for name, param in auto_model.named_parameters():\n","  if any(element in name for element in ['18','19','20','21','22','23','pooler.dense.weight','pooler.dense.bias']):\n","    param.requires_grad = True\n","\n","#Training for multiple epochs can be beneficial, as in each epoch a mini-batch is sampled differently\n","#hence, we get different negatives for each positive\n","num_epochs = 1\n","\n","#Increasing the batch size improves the performance for MultipleNegativesRankingLoss. Choose it as large as possible\n","#I achieved the good results with a batch size of 300-350 (requires about 30 GB of GPU memory)\n","train_batch_size = 32\n","\n","model_save_path = 'output/training_MultipleNegativesRankingLoss-'+datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n","\n","os.makedirs(model_save_path, exist_ok=True)\n","\n","# After reading the train_samples, we create a DataLoader\n","train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=train_batch_size,drop_last = True)\n","train_loss = losses.MultipleNegativesRankingLoss(model)\n","\n","\n","################### Development  Evaluators ##################\n","# We add 3 evaluators, that evaluate the model on Duplicate Questions pair classification,\n","# Duplicate Questions Mining, and Duplicate Questions Information Retrieval\n","#evaluators = []\n","\n","###### Classification ######\n","# Given (quesiton1, question2), is this a duplicate or not?\n","# The evaluator will compute the embeddings for both questions and then compute\n","# a cosine similarity. If the similarity is above a threshold, we have a duplicate.\n","#dev_sentences1, dev_sentences2, dev_labels = prepare_data_for_binary_classification_evaluator(data_for_similar, irelavent_data_similar)\n","#binary_acc_evaluator = evaluation.BinaryClassificationEvaluator(dev_sentences1, dev_sentences2, dev_labels,show_progress_bar = True)\n","#evaluators.append(binary_acc_evaluator)\n","\n","###### Duplicate Questions Mining ######\n","# Given a large corpus of questions, identify all duplicates in that corpus.\n","\n","# For faster processing, we limit the development corpus to only 10,000 sentences.\n","#sentences, paraphrase_pairs = prepare_data_for_paraphrase_mining_evaluator(data_for_paraphrase_mining)\n","#paraphrase_mining_evaluator = evaluation.ParaphraseMiningEvaluator(sentences, paraphrase_pairs, name='dev',show_progress_bar = True)\n","#evaluators.append(paraphrase_mining_evaluator)\n","\n","# Create a SequentialEvaluator. This SequentialEvaluator runs all three evaluators in a sequential order.\n","# We optimize the model with respect to the score from the last evaluator (scores[-1])\n","#seq_evaluator = evaluation.SequentialEvaluator(evaluators, main_score_function=lambda scores: scores[0])\n","train_dataloader.collate_fn = model.smart_batching_collate\n","\n","#logger.info(\"Evaluate model without training\")\n","#seq_evaluator(model, epoch=0, steps=0, output_path=model_save_path)\n","\n","# Train the model\n","model.fit(train_objectives=[(train_dataloader, train_loss)],\n","          epochs=num_epochs,\n","          warmup_steps=200,\n","          output_path=model_save_path,\n","          callback = callback\n","          )\n","\n","model.save(\"/content/drive/MyDrive/innovae-revision/innovae-adavae/patent_transformer\")\n","model = SentenceTransformer(\"/content/drive/MyDrive/innovae-revision/innovae-adavae/patent_transformer\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TMNZkSmGfDju"},"outputs":[],"source":["!pip install chromadb\n","import chromadb\n","import chromadb\n","from chromadb.config import Settings\n","from chromadb.utils import embedding_functions\n","import re\n","client = chromadb.Client(Settings(\n","    chroma_db_impl=\"duckdb+parquet\",\n","    persist_directory=\"/content/drive/MyDrive/innovae-revision/innovae-adavae/results/patent_vectorized_database/to/persist/directory\" # Optional, defaults to .chromadb/ in the current directory\n","))\n","\n","def emb_fn(texts):\n","  return model.encode(list(texts), convert_to_numpy=True).tolist()\n","\n","collection = client.create_collection(name=\"patent_collection_bert\", embedding_function=emb_fn,metadata={\"hnsw:space\": \"cosine\"})\n","\n","grant['prior_art_priority_date'] = grant['prior_art_priority_date'].astype('str')\n","grant['prior_art_subclass_id'] = grant['prior_art_subclass_id'].astype('str')\n","\n","test_patent = grant[['prior_art_no','prior_art_priority_date','prior_art_subclass_id','prior_text']]\n","test_patent = test_patent.drop_duplicates(subset = ['prior_art_no'])\n","test_patent['prior_art_priority_date'] = [str(i) for i in test_patent['prior_art_priority_date']]\n","docs = test_patent['prior_text'].tolist()\n","category = pd.DataFrame([i.split('/') for i in test_patent['prior_art_subclass_id']])\n","category1 = category[0].tolist()\n","category2 = category[1].tolist()\n","date = test_patent['prior_art_priority_date'].tolist()\n","metas = [{'date':d,'cat1':cat1,'cat2':cat2} for d,cat1,cat2 in zip(date, category1, category2)]\n","ids = [str(i) for i in test_patent['prior_art_no'].tolist()]\n","\n","recon_dataloader = DataLoader(\n","      docs,\n","      batch_size=40,\n","      pin_memory=True,\n","      drop_last=False,\n","      shuffle = False)\n","\n","embeds = []\n","\n","for text in tqdm(recon_dataloader):\n","  embed = model.encode(text)\n","  embeds.append(embed)\n","\n","embed = np.concatenate(embeds).tolist()\n","\n","collection.add(\n","    documents=docs,\n","    embeddings = embed,\n","    ids=ids\n",")\n","\n","count = 0\n","total = 0\n","for idx,row in test_data.iterrows():\n","  cat = row['prior_art_subclass_id'].split('/')[0]\n","  results = collection.query(\n","    query_texts=[row['new_text']],\n","    n_results=100)\n","  total += 1\n","  if str(row['prior_art_no']) in results['ids'][0]:\n","    count += 1\n","  print(total,count)"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"gpuClass":"premium","authorship_tag":"ABX9TyOr5uenHOZPJPpQyfmfu082"},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}