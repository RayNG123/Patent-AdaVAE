{"cells":[{"cell_type":"markdown","metadata":{"id":"uX2bsFM9OioK"},"source":["#Initiate Pre-trained AdaVAE Model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14268,"status":"ok","timestamp":1692134194882,"user":{"displayName":"Yusen Wu","userId":"08866010899412433739"},"user_tz":240},"id":"NDcm5zipOsi5","outputId":"740adc98-edb6-4122-b486-bfe7827ad30e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.31.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Requirement already satisfied: tensorboardX in /usr/local/lib/python3.10/dist-packages (2.6.2)\n","Requirement already satisfied: ipdb in /usr/local/lib/python3.10/dist-packages (0.13.13)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (1.23.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (23.1)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (3.20.3)\n","Requirement already satisfied: ipython>=7.31.1 in /usr/local/lib/python3.10/dist-packages (from ipdb) (7.34.0)\n","Requirement already satisfied: tomli in /usr/local/lib/python3.10/dist-packages (from ipdb) (2.0.1)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipdb) (4.4.2)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.31.1->ipdb) (67.7.2)\n","Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.31.1->ipdb) (0.19.0)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=7.31.1->ipdb) (0.7.5)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.31.1->ipdb) (5.7.1)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.31.1->ipdb) (3.0.39)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=7.31.1->ipdb) (2.16.1)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=7.31.1->ipdb) (0.2.0)\n","Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=7.31.1->ipdb) (0.1.6)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.31.1->ipdb) (4.8.0)\n","Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=7.31.1->ipdb) (0.8.3)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=7.31.1->ipdb) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.31.1->ipdb) (0.2.6)\n","Tue Aug 15 21:16:29 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   30C    P0    49W / 400W |   2251MiB / 40960MiB |      0%      Default |\n","|                               |                      |             Disabled |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","+-----------------------------------------------------------------------------+\n","Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (2.2.2)\n","Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.31.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.1)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.0.1+cu118)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.15.2+cu118)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.23.5)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.10.1)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (3.8.1)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.1.99)\n","Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.16.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.12.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.31.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.7.1)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (3.27.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (16.0.6)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.6.3)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.3.2)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (8.1.6)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.2.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers) (9.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.7.22)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n"]}],"source":["!pip install transformers\n","!pip install tensorboardX ipdb\n","!nvidia-smi\n","!pip install sentence-transformers\n","\n","# change trained information here\n","experiment = 'patent_claim_iter26272_as128_scalar1.0_cycle-auto_prenc-start_wsTrue_lg-latent_attn_add_attn_beta1.0_reg-kld_attn_mode-none_ffn_option-parallel_ffn_enc_layer-8_dec_layer-12_zdim-512_optFalse_ftFalse_zrate-0.5_fb-1sd-42_5.24'\n","latent_size = 512\n","max_length = 400\n","batch_size = 40\n","top_k = 0\n","top_p = 0"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":469},"executionInfo":{"elapsed":28472,"status":"error","timestamp":1692147881796,"user":{"displayName":"Yusen Wu","userId":"08866010899412433739"},"user_tz":240},"id":"YwpsjgTlCNhB","outputId":"eab31ce5-e796-48e5-d4c8-a6f7b555e827"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-1-d7d0b2dc1586>:12: DeprecationWarning: Please use `pearsonr` from the `scipy.stats` namespace, the `scipy.stats.stats` namespace is deprecated.\n","  from scipy.stats.stats import pearsonr\n"]},{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-d7d0b2dc1586>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0margparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArgumentParser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGPT2Tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGPT2LMHeadModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0madapters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfiguration_gpt2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGPT2Config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["import os\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount= True)\n","os.chdir('/content/drive/MyDrive/innovae-revision/innovae-adavae/adavae/src')\n","\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","\n","import collections\n","from collections import defaultdict\n","from scipy.stats.stats import pearsonr\n","from matplotlib import pyplot as plt\n","\n","import torch, math, argparse, copy, re\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from tqdm import tqdm\n","from torch.utils.data import DataLoader\n","from argparse import ArgumentParser\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel\n","\n","from adapters.configuration_gpt2 import GPT2Config\n","from adapters.vae import GPT2Adapter, AdaVAEModel\n","from adapters.common import AdapterConfig\n","from adaVAE import compute_loss\n","from data import GenerationDataset, DataFrameDataset\n","from utils import init_para_frompretrained, tokenize, sample_sequence\n","\n","parser = ArgumentParser()\n","\n","# Default parameters are set based on single GPU training\n","parser.add_argument(\"--seed\", type=int, default=42)\n","\n","## mode options\n","parser.add_argument('--adapter_size', type=int, default=128,\n","                    help=\"Hidden size of GPT2 encoder/decoder adapter\")\n","parser.add_argument('--latent_size', type=int, default=32,\n","                    help=\"Hidden size of latent code\")\n","parser.add_argument('--encoder_n_layer', type=int, default=8,\n","                    help=\"attention layer number of GPT-2 encoder\")\n","parser.add_argument('--decoder_n_layer', type=int, default=12,\n","                    help=\"attention layer number of GPT-2 decoder\")\n","parser.add_argument('--class_num', type=int, default=2,\n","                    help=\"class number for controllable generation\")\n","parser.add_argument('--adapter_scalar', type=str, default=\"1.0\",\n","                    help=\"adapter scalar\")\n","parser.add_argument('--ffn_option', type=str, default=\"parallel_ffn\",\n","                    choices=['sequential', 'parallel_attn', 'parallel_ffn', 'pfeiffer'],\n","                    help=\"adapter type option\")\n","parser.add_argument('--latent_gen', type=str, default=\"latent_attn\",\n","                    help=\"method for encoder to latent space, averaged_attn for average attention from \"\n","                         \"TransformerCVAE, linear for taken the first encoder token to a linear like Optimus\",\n","                    choices=['latent_attn', 'averaged_attn', 'linear', 'mean_max_linear'])\n","parser.add_argument('--attn_mode', type=str, default=\"none\",\n","                    choices=['prefix', 'adapter', 'lora', 'none'],\n","                    help=\"attention transfer type\")\n","parser.add_argument('--reg_loss', type=str, default=\"kld\",\n","                    choices=['kld', 'adversarial', 'symlog'],\n","                    help=\"regularization loss for latent space\")\n","\n","## testing paramters\n","parser.add_argument('--batch_size', type=int, default=128,\n","                    help='batch size per GPU. Lists the schedule.')\n","parser.add_argument('--max_length', type=int, default=30,\n","                    help='max length of every input sentence')\n","parser.add_argument('--data-dir', type=str, default='data/optimus_dataset')\n","parser.add_argument('--out-dir', type=str, default='out')\n","parser.add_argument('--experiment', type=str, help=\"ckpt dirctory\", default='out')\n","parser.add_argument('--adapter_init', type=str, default='bert', choices=['lora', 'bert', 'lisa', 'other'],\n","                    help=\"parameter initialization method for adapter layers.\")\n","parser.add_argument('--workers', default=2, type=int, metavar='N',  help='number of data loading workers')\n","parser.add_argument(\"--total_sents\", default=10, type=int, help=\"Total sentences to test recontruction/generation.\")\n","parser.add_argument(\"--max_test_batch\", default=10, type=int, help=\"Total sentence pairs to test interpolation/analogy.\")\n","parser.add_argument(\"--num_interpolation_step\", default=10, type=int)\n","parser.add_argument(\"--degree_to_target\", type=float, default=1.0)\n","parser.add_argument(\"--max_val_batches\", type=int, help=\"Max batch size number to test recontruction.\", default=30)\n","parser.add_argument(\"--latest_date\", type=str, help=\"Latest date for model testing.\", default=\"\")\n","\n","## metrics\n","parser.add_argument('--au_delta', type=float, default=0.01,\n","                    help=\"threshold for activated unit calculation.\")\n","\n","# use GPU\n","parser.add_argument('--gpu', default=0, type=int)\n","parser.add_argument('--no_gpu', action=\"store_true\")\n","\n","\n","# KL cost annealing, increase beta from beta_0 to 1 in beta_warmup steps\n","parser.add_argument('--beta_0', default=1.00, type=float)\n","parser.add_argument('--beta_warmup', type=int, default=1000)\n","parser.add_argument('--kl_rate', type=float, default=0.0)\n","\n","# cyc_vae parameters\n","parser.add_argument('--cycle', type=int, default=2000)\n","\n","## trigger\n","parser.add_argument('--load', action=\"store_true\")\n","parser.add_argument('--save_all', action=\"store_true\", help=\"save full parameters of the model\")\n","parser.add_argument('--add_input', action=\"store_true\")\n","parser.add_argument('--add_attn', action=\"store_true\")\n","parser.add_argument('--add_softmax', action=\"store_true\")\n","parser.add_argument('--add_mem', action=\"store_true\")\n","parser.add_argument('--attn_proj_vary', action=\"store_true\")\n","parser.add_argument('--finetune_enc', action=\"store_true\")\n","parser.add_argument('--finetune_dec', action=\"store_true\")\n","parser.add_argument('--weighted_sample', action=\"store_true\")\n","parser.add_argument('--add_z2adapters', action=\"store_true\")\n","parser.add_argument('--learn_prior', action=\"store_true\")\n","parser.add_argument('--test_model', action=\"store_true\")\n","parser.add_argument('--do_sample', action=\"store_true\", help=\"sample for reconstruction\")\n","\n","args = parser.parse_args(f'--add_attn --weighted_sample --latent_size {latent_size} --max_length {max_length} --batch_size {batch_size} --experiment {experiment}'.split())\n","\n","# Set random seed\n","gpu = torch.cuda.is_available()\n","np.random.seed(args.seed)\n","prng = np.random.RandomState()\n","torch.random.manual_seed(args.seed)\n","\n","if gpu:\n","    print(\"There are \", torch.cuda.device_count(), \" available GPUs!\")\n","    torch.cuda.set_device(args.gpu)\n","    torch.cuda.manual_seed(args.seed)\n","    torch.cuda.manual_seed_all(args.seed)\n","    print('Current single GPU: {}'.format(torch.cuda.current_device()))\n","device = torch.device(args.gpu if torch.cuda.is_available() else \"cpu\")\n","\n","# Load model and trained weights\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n","gpt2_model = GPT2LMHeadModel.from_pretrained('gpt2')\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","config = GPT2Config()\n","ada_config = AdapterConfig(hidden_size=768,\n","                            adapter_size=args.adapter_size,\n","                            adapter_act='relu',\n","                            adapter_initializer_range=1e-2,\n","                            latent_size=args.latent_size,\n","                            class_num=args.class_num,\n","                            encoder_n_layer=args.encoder_n_layer,\n","                            decoder_n_layer=args.decoder_n_layer,\n","                            dis_emb=128,\n","                            init='other',\n","                            adapter_scalar=args.adapter_scalar,\n","                            ffn_option=args.ffn_option,\n","                            attn_mode=args.attn_mode,\n","                            latent_gen=args.latent_gen,\n","                            attn_option='none',\n","                            mid_dim=30,\n","                            attn_bn=25,\n","                            prefix_dropout=0.1,\n","                            tune_enc=False,\n","                            tune_dec=False,\n","                            add_z2adapters=args.add_z2adapters)\n","\n","AdaVAE = AdaVAEModel(config, ada_config, add_input=args.add_input, add_attn=args.add_attn, add_softmax=args.add_softmax, add_mem=args.add_mem,\n","                attn_proj_vary=args.attn_proj_vary, learn_prior=args.learn_prior, reg_loss=args.reg_loss)\n","\n","## load pre-trained weights\n","init_para_frompretrained(AdaVAE.transformer, gpt2_model.transformer, share_para=False)\n","init_para_frompretrained(AdaVAE.encoder, gpt2_model.transformer, share_para=False)\n","AdaVAE.lm_head.weight = gpt2_model.lm_head.weight\n","AdaVAE.eval()\n","\n","## load trained parameters\n","print('Loading model weights...')\n","state = torch.load(os.path.join(\"./out\", args.experiment, 'model_best_val.pt'))\n","if 'module' in list(state.keys())[0]:  # model_path is data parallel model with attr 'module'\n","    keys = copy.copy(state).keys()\n","    for k in keys:\n","        state[k.replace('module.', '')] = state.pop(k)\n","\n","if not args.save_all:\n","    model_dict = AdaVAE.state_dict()\n","    additional_dict = {k: v for k, v in state.items() if k in model_dict}\n","    model_dict.update(additional_dict)\n","    AdaVAE.load_state_dict(model_dict)\n","else:\n","    AdaVAE.load_state_dict(state)\n","\n","AdaVAE = AdaVAE.to(device)"]},{"cell_type":"markdown","metadata":{"id":"pUbiI-qdQOLr"},"source":["#Vectorized Database Utils"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8NcC1Wv8oMLT"},"outputs":[],"source":["#training utils\n","from torch.utils.data import DataLoader\n","from sentence_transformers import losses, util,models\n","from sentence_transformers import LoggingHandler, SentenceTransformer, evaluation\n","from sentence_transformers.readers import InputExample\n","import logging\n","from datetime import datetime\n","import csv\n","import os\n","import random\n","import pandas as pd\n","from sentence_transformers import util\n","\n","import torch\n","from torch import nn, Tensor\n","from typing import Iterable, Dict\n","from sentence_transformers import util\n","from sentence_transformers.util import batch_to_device\n","\n","#model utils\n","import json\n","import logging\n","import os\n","import shutil\n","import stat\n","from collections import OrderedDict\n","from typing import List, Dict, Tuple, Iterable, Type, Union, Callable, Optional\n","import requests\n","import numpy as np\n","from numpy import ndarray\n","import transformers\n","from huggingface_hub import HfApi, HfFolder, Repository, hf_hub_url, cached_download\n","import torch\n","from torch import nn, Tensor, device\n","from torch.optim import Optimizer\n","from torch.utils.data import DataLoader\n","import torch.multiprocessing as mp\n","from tqdm.autonotebook import trange\n","import math\n","import queue\n","import tempfile\n","from distutils.dir_util import copy_tree"]},{"cell_type":"markdown","source":["##finetuning utils from sentence transformer"],"metadata":{"id":"Tggtl8Rbk9bK"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"86GuW4j-QRKs"},"outputs":[],"source":["def smart_batching_collate(batch):\n","    \"\"\"\n","    Transforms a batch from a SmartBatchingDataset to a batch of tensors for the model\n","    Here, batch is a list of tuples: [(tokens, label), ...]\n","    \"\"\"\n","    num_texts = len(batch[0].texts)\n","    texts = [[] for _ in range(num_texts)]\n","    labels = []\n","    for example in batch:\n","      for idx, text in enumerate(example.texts):\n","        texts[idx].append(text)\n","        labels.append(example.label)\n","    labels = torch.tensor(labels)\n","    sentence_features = []\n","    for idx in range(num_texts):\n","      tokenized = tokenizer(texts[idx],return_tensors = 'pt',padding = True, truncation = True, max_length = args.max_length)\n","      sentence_features.append(tokenized)\n","    return sentence_features, labels\n","\n","class MultipleNegativesRankingLoss(nn.Module):\n","    def __init__(self, model, scale = 20.0, similarity_fct = util.cos_sim):\n","        \"\"\"\n","        :param model: SentenceTransformer model\n","        :param scale: Output of similarity function is multiplied by scale value\n","        :param similarity_fct: similarity function between sentence embeddings. By default, cos_sim. Can also be set to dot product (and then set scale to 1)\n","        \"\"\"\n","        super(MultipleNegativesRankingLoss, self).__init__()\n","        self.model = model\n","        self.scale = scale\n","        self.similarity_fct = similarity_fct\n","        self.cross_entropy_loss = nn.CrossEntropyLoss()\n","\n","\n","    def forward(self, sentence_features, labels):\n","        reps = [self.model(**feature,from_mean=True,doc_ids = None,get_z_only = True, concat_z_var = True) for feature in sentence_features]\n","        embeddings_a = reps[0]\n","        embeddings_b = torch.cat(reps[1:])\n","\n","\n","        scores = self.similarity_fct(embeddings_a, embeddings_b) * self.scale\n","        labels = torch.tensor(range(len(scores)), dtype=torch.long, device=scores.device)  # Example a[i] should match with b[i]\n","        return self.cross_entropy_loss(scores, labels)\n","\n","def fit(train_objectives,\n","        epochs,\n","        steps_per_epoch = None,\n","        warmup_steps = 10000,\n","        optimizer_class = torch.optim.AdamW,\n","        optimizer_params = {'lr': 5e-5},\n","        weight_decay = 0.01,\n","        evaluation_steps = 0,\n","        max_grad_norm = 1,\n","        show_progress_bar = True):\n","        dataloaders = [dataloader for dataloader, _ in train_objectives]\n","\n","        # Use smart batching\n","        for dataloader in dataloaders:\n","            dataloader.collate_fn = smart_batching_collate\n","\n","        loss_models = [loss for _, loss in train_objectives]\n","        for loss_model in loss_models:\n","            loss_model.to('cuda')\n","\n","        if steps_per_epoch is None or steps_per_epoch == 0:\n","            steps_per_epoch = min([len(dataloader) for dataloader in dataloaders])\n","\n","        num_train_steps = int(steps_per_epoch * epochs)\n","\n","        # Prepare optimizers\n","        optimizers = []\n","        schedulers = []\n","        for loss_model in loss_models:\n","            param_optimizer = list(loss_model.named_parameters())\n","\n","            no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n","            optimizer_grouped_parameters = [\n","                {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': weight_decay},\n","                {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.01}\n","            ]\n","\n","            optimizer = optimizer_class(optimizer_grouped_parameters, **optimizer_params)\n","            optimizers.append(optimizer)\n","\n","        global_step = 0\n","        data_iterators = [iter(dataloader) for dataloader in dataloaders]\n","\n","        num_train_objectives = len(train_objectives)\n","\n","        for epoch in trange(epochs, desc=\"Epoch\", disable=not show_progress_bar):\n","            training_steps = 0\n","\n","            for loss_model in loss_models:\n","                loss_model.zero_grad()\n","                loss_model.train()\n","\n","            for _ in trange(steps_per_epoch, desc=\"Iteration\", smoothing=0.05, disable=not show_progress_bar):\n","                for train_idx in range(num_train_objectives):\n","                    loss_model = loss_models[train_idx]\n","                    optimizer = optimizers[train_idx]\n","                    data_iterator = data_iterators[train_idx]\n","\n","                    try:\n","                        data = next(data_iterator)\n","                    except StopIteration:\n","                        data_iterator = iter(dataloaders[train_idx])\n","                        data_iterators[train_idx] = data_iterator\n","                        data = next(data_iterator)\n","\n","                    features, labels = data\n","                    labels = labels.to('cuda')\n","                    features = list(map(lambda batch: batch_to_device(batch, 'cuda'), features))\n","                    loss_value = loss_model(features, labels)\n","                    loss_value.backward()\n","                    print(f'current loss is {loss_value}')\n","                    torch.nn.utils.clip_grad_norm_(loss_model.parameters(), max_grad_norm)\n","                    optimizer.step()\n","                    optimizer.zero_grad()\n","                    training_steps += 1\n","                    global_step += 1"]},{"cell_type":"markdown","source":["##finetune the innovae encoder for vectorized documents"],"metadata":{"id":"qJYKqMGVk2F6"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"fwZ4Wiw8onfc"},"outputs":[],"source":["data_grant = pd.read_csv('/content/drive/MyDrive/innovae-revision/innovae-adavae/data/patent_grant_cleanded_new.csv').sample(frac = 1,random_state = 10).reset_index(drop = True)\n","data_pregrant = pd.read_csv('/content/drive/MyDrive/innovae-revision/innovae-adavae/data/patent_pregrant_cleanded_new.csv').sample(frac = 1,random_state = 10).reset_index(drop = True)\n","total_data = pd.concat([data_grant,data_pregrant]).sample(frac = 1,random_state = 10).reset_index(drop = True)\n","train_data = total_data.iloc[:145000]\n","test_data = total_data.iloc[145000:]\n","\n","######### Read train data  ##########\n","train_samples = []\n","for idx,row in train_data.iterrows():\n","  train_samples.append(InputExample(texts=[row['prior_text'], row['new_text']], label=1))\n","  train_samples.append(InputExample(texts=[row['new_text'], row['prior_text']], label=1))\n","\n","for param in AdaVAE.parameters():\n","    param.requires_grad = False\n","\n","for name, param in AdaVAE.encoder.named_parameters():\n","  trained = ['wte','wpe','0','1','2','3','4','5','6','7','ln_f','LatentAttention','mean','logvar']\n","\n","  if any(element in name for element in trained):\n","     param.requires_grad = True\n","\n","num_epochs = 1\n","train_batch_size = 40\n","train_dataloader = DataLoader(train_samples, shuffle=True, batch_size=train_batch_size,drop_last = True)\n","train_dataloader.collate_fn = smart_batching_collate\n","train_loss = MultipleNegativesRankingLoss(AdaVAE)\n","\n","fit(train_objectives=[(train_dataloader, train_loss)],epochs=num_epochs)\n","\n","save_orderdict = collections.OrderedDict()\n","for name, parameter in AdaVAE.named_parameters():\n","  #if parameter.requires_grad:\n","      save_orderdict[name] = parameter\n","\n","#torch.save(save_orderdict, os.path.join('/content/drive/MyDrive/AdaVAE_for_Articles/adavae/all_layereencoder_epoch_aligned.pt'))"]},{"cell_type":"markdown","metadata":{"id":"kEWioYHVcYS-"},"source":["#Vectorized Database for Patents"]},{"cell_type":"markdown","source":["##vectorized all patent and store in the database"],"metadata":{"id":"4PWnacxEkWL_"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"uV4f1A6AEDER"},"outputs":[],"source":["!pip install chromadb\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","import pandas as pd\n","import chromadb\n","from chromadb.config import Settings\n","from chromadb.utils import embedding_functions\n","import pickle\n","import re\n","\n","client = chromadb.Client(Settings(\n","    chroma_db_impl=\"duckdb+parquet\",\n","    persist_directory=\"/content/drive/MyDrive/innovae-revision/innovae-adavae/results/patent_vectorized_database/to/persist/directory\" # Optional, defaults to .chromadb/ in the current directory\n","))\n","\n"," def emb_fn(text):\n","  x_ids, input_ids, attention_mask = tokenize(text, tokenizer, 'cuda', args)\n","  outputs = AdaVAE(input_ids=input_ids, attention_mask=attention_mask, from_mean=True,doc_ids = None,get_z_only = True,concat_z_var = True)\n","  return outputs.cpu().detach().numpy().tolist()\n","\n","client.delete_collection(name=\"patent_collection\")\n","collection = client.create_collection(name=\"patent_collection\",embedding_function=emb_fn, metadata={\"hnsw:space\": \"cosine\"})\n","\n","test_patent = total_data.iloc[50000:][['prior_art_no','prior_art_priority_date','prior_art_subclass_id','prior_text']].dropna()\n","test_patent['prior_art_priority_date'] = test_patent['prior_art_priority_date'].astype(str)\n","test_patent = test_patent.drop_duplicates(subset = ['prior_art_no'])\n","test_patent['prior_art_priority_date'] = [str(i) for i in test_patent['prior_art_priority_date']]\n","\n","docs = test_patent['prior_text'].tolist()\n","category = pd.DataFrame([i.split('/') for i in test_patent['prior_art_subclass_id']])\n","category1 = category[0].tolist()\n","category2 = category[1].tolist()\n","date = test_patent['prior_art_priority_date'].tolist()\n","metas = [{'date':d,'cat1':cat1,'cat2':cat2} for d,cat1,cat2 in zip(date, category1, category2)]\n","ids = [str(i) for i in test_patent['prior_art_no'].tolist()]\n","\n","val_dataloader = DataLoader(docs,\n","                            batch_size=args.batch_size,\n","                            pin_memory=True,\n","                            num_workers=args.workers,\n","                            shuffle = False)\n","\n","results = []\n","for i, batch in enumerate(tqdm(val_dataloader, desc=\"Reconstructing Documents:\")):\n","    with torch.no_grad():\n","        x_ids, input_ids, attention_mask = tokenize(batch, tokenizer, 'cuda', args)\n","        outputs = AdaVAE(input_ids= input_ids,\n","                         attention_mask= attention_mask,\n","                         from_mean=True,\n","                         doc_ids = None,\n","                         get_z_only = True,\n","                         concat_z_var = True)\n","        results += outputs.cpu().numpy().tolist()"]},{"cell_type":"markdown","source":["##test for information retrival accuracy"],"metadata":{"id":"GkzF10Ydka6d"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"GuBC1lfZwnA3"},"outputs":[],"source":["collection.add(\n","    documents=docs,\n","    embeddings = results,\n","    metadatas=metas,\n","    ids=ids\n",")\n","\n","count = 0\n","total = 0\n","for idx,row in new.iterrows():\n","  cat = row['prior_art_subclass_id'].split('/')[0]\n","  result = collection.query(\n","    query_texts=[row['new_text']],\n","    n_results=30,\n","    where={\"cat1\": cat})\n","  total += 1\n","  if str(row['prior_art_no']) in result['ids'][0]:\n","    count += 1\n","  print(total,count)"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"gpuClass":"premium","authorship_tag":"ABX9TyPbsUqHKm62vkJe9y6BmR0H"},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}